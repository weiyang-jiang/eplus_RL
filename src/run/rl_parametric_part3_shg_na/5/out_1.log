Using TensorFlow backend.
[2019-04-10 11:02:53,895] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=True, job_mode='Train', learning_rate=1e-05, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=1500000, metric_func='part3_v1', model_dir='Part3-NA-Shg-Train-v1-res1/model_data/model.ckpt-2500000', model_param=[19, 1], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res2', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-04-10 11:02:53,895] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-10 11:02:53.944457: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-10 11:03:15,200] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-10 11:03:15,200] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-04-10 11:03:15,210] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-04-10 11:03:15,215] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-04-10 11:03:15,221] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-04-10 11:03:15,226] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-04-10 11:03:15,230] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-04-10 11:03:15,230] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:15,230] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-10 11:03:15,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:15,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-04-10 11:03:16,231] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:16,232] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-10 11:03:16,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:16,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-04-10 11:03:17,213] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 11:03:17,213] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:03:17,213] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:03:17,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:17,214] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:03:17,214] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:03:17,214] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:03:17,214] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:17,214] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:17,214] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:17,214] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:17,217] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run1
[2019-04-10 11:03:17,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-04-10 11:03:17,223] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run1
[2019-04-10 11:03:17,240] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:17,241] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run1
[2019-04-10 11:03:17,242] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-10 11:03:17,249] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run1
[2019-04-10 11:03:17,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:17,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-04-10 11:03:18,242] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:18,243] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-10 11:03:18,310] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:18,311] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-04-10 11:03:19,259] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:19,263] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-10 11:03:19,502] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:19,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-04-10 11:03:20,264] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:20,265] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-10 11:03:20,542] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:20,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-04-10 11:03:21,268] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:21,269] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-10 11:03:21,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:21,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-04-10 11:03:22,272] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:22,280] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-10 11:03:22,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:22,361] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-04-10 11:03:23,281] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:23,282] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-10 11:03:23,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:23,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-04-10 11:03:24,285] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:24,286] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-10 11:03:24,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:24,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-04-10 11:03:25,141] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:03:25,142] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.8, 95.33333333333334, 1.0, 2.0, 0.9424317766578285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1407771.218817024, 1407771.218817024, 295150.583107924]
[2019-04-10 11:03:25,142] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:03:25,221] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.0237371e-17 1.0000000e+00 1.4508593e-19 3.6126198e-19 6.5756304e-28], sampled 0.6403243028143591
[2019-04-10 11:03:25,286] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:25,287] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-10 11:03:25,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:25,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-04-10 11:03:26,288] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:26,290] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-10 11:03:26,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:26,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-04-10 11:03:27,291] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:27,292] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-10 11:03:27,516] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:27,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-04-10 11:03:28,293] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:28,293] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-10 11:03:28,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:28,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-04-10 11:03:29,294] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:29,297] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-10 11:03:29,813] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:29,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-04-10 11:03:30,298] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 11:03:30,299] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-10 11:03:30,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:03:30,610] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run1
[2019-04-10 11:03:45,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:03:45,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.3, 83.0, 1.0, 2.0, 0.2855421832261788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461521.1496509329, 461521.1496509336, 164371.6814809116]
[2019-04-10 11:03:45,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:03:45,909] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3370884e-18 1.0000000e+00 5.6636800e-22 7.1135201e-24 6.0028359e-31], sampled 0.3046685899190852
[2019-04-10 11:04:00,951] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:04:00,953] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.43333333333333, 94.66666666666667, 1.0, 2.0, 0.3361926734803173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525136.4967936483, 525136.4967936483, 168819.4305102034]
[2019-04-10 11:04:00,954] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:04:00,956] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.7199760e-19 1.0000000e+00 2.5720576e-22 2.9952124e-23 1.7715131e-31], sampled 0.8545180721081748
[2019-04-10 11:04:31,079] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:04:31,080] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.0, 63.00000000000001, 1.0, 2.0, 1.001182281662596, 1.0, 2.0, 1.001182281662596, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2800618.095257174, 2800618.095257174, 529701.373915145]
[2019-04-10 11:04:31,083] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:04:31,085] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9640917e-10 3.0761003e-01 1.6774851e-10 6.9238997e-01 7.5304829e-17], sampled 0.5854317244486013
[2019-04-10 11:04:37,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:04:37,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.33333333333333, 45.66666666666667, 1.0, 2.0, 0.7519762558067026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1050944.185414766, 1050944.185414766, 232468.6601371278]
[2019-04-10 11:04:37,588] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:04:37,590] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4859027e-17 1.0000000e+00 2.7422294e-20 1.0578091e-19 5.5927032e-29], sampled 0.7238938160568597
[2019-04-10 11:04:42,407] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:04:42,408] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.9, 55.0, 1.0, 2.0, 0.7647700557697275, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979328021221, 6.9112, 168.9123160145021, 1965768.900995512, 1898529.438608789, 399320.0076131401]
[2019-04-10 11:04:42,409] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:04:42,410] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5866305e-10 9.9996114e-01 1.1365120e-10 3.8883918e-05 6.4878538e-18], sampled 0.07697832122138326
[2019-04-10 11:04:42,411] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1965768.900995512 W.
[2019-04-10 11:04:49,405] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:04:49,405] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.65, 91.16666666666667, 1.0, 2.0, 0.5794764250358997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809770.6032573377, 809770.6032573377, 196984.0555521998]
[2019-04-10 11:04:49,406] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:04:49,408] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5511595e-18 1.0000000e+00 3.2103584e-21 1.7271281e-21 3.6800455e-30], sampled 0.9070760417030707
[2019-04-10 11:05:09,016] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11117979], dtype=float32), 0.097286895]
[2019-04-10 11:05:09,017] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.03333333333333, 94.66666666666666, 1.0, 2.0, 0.451311847112216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644604.2572524298, 644604.2572524291, 178118.7040082693]
[2019-04-10 11:05:09,017] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:05:09,019] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6784373e-17 1.0000000e+00 1.0547020e-20 5.9967553e-21 3.2926002e-29], sampled 0.9563064929504332
[2019-04-10 11:05:11,847] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.5431 2778603316.2502 913.0000
[2019-04-10 11:05:12,068] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8047.7328 3001666739.5567 1611.0000
[2019-04-10 11:05:12,105] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8280.8382 2924565552.3095 1257.0000
[2019-04-10 11:05:12,216] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7944.8414 3157231350.8874 1597.0000
[2019-04-10 11:05:12,314] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8514.4856 2840338536.5697 1078.0000
[2019-04-10 11:05:13,325] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7944.841380048853, 3157231350.8873577, 1597.0, 8280.838243668903, 2924565552.3095336, 1257.0, 8664.543086891936, 2778603316.2502384, 913.0, 8047.732800091959, 3001666739.5567217, 1611.0, 8514.485625903922, 2840338536.5697427, 1078.0]
[2019-04-10 11:05:16,286] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5119282e-18 1.0000000e+00 1.7853482e-21 2.4413327e-22 8.8208336e-30], sum to 1.0000
[2019-04-10 11:05:16,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0003
[2019-04-10 11:05:16,425] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 86.0, 1.0, 2.0, 0.2889412922408359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462967.1389990058, 462967.1389990051, 164463.3166264564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 18000.0000, 
sim time next is 18600.0000, 
raw observation next is [21.43333333333333, 86.0, 1.0, 2.0, 0.3538382837698558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566489.6619203629, 566489.6619203629, 172316.7511963779], 
processed observation next is [1.0, 0.21739130434782608, 0.21484992101105835, 0.86, 1.0, 1.0, 0.22149190815645278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15735823942232302, 0.15735823942232302, 0.2571891808901163], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.72253263], dtype=float32), 0.7223643]. 
=============================================
[2019-04-10 11:05:16,472] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8293015e-17 1.0000000e+00 8.0458792e-21 8.9019005e-21 1.7422756e-28], sum to 1.0000
[2019-04-10 11:05:16,478] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9505
[2019-04-10 11:05:16,583] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 86.0, 1.0, 2.0, 0.2889412922408359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462967.1389990058, 462967.1389990051, 164463.3166264564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 18000.0000, 
sim time next is 18600.0000, 
raw observation next is [21.43333333333333, 86.0, 1.0, 2.0, 0.3538382837698558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566489.6619203629, 566489.6619203629, 172316.7511963779], 
processed observation next is [1.0, 0.21739130434782608, 0.21484992101105835, 0.86, 1.0, 1.0, 0.22149190815645278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15735823942232302, 0.15735823942232302, 0.2571891808901163], 
reward next is 0.7428, 
noisyNet noise sample is [array([1.8221573], dtype=float32), -0.45429784]. 
=============================================
[2019-04-10 11:05:19,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0813521e-16 1.0000000e+00 1.5351335e-19 5.1990183e-19 3.9737729e-28], sum to 1.0000
[2019-04-10 11:05:19,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0895
[2019-04-10 11:05:19,658] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 89.0, 1.0, 2.0, 0.3438353877293951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534082.3449685606, 534082.3449685606, 169459.2737652238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 91200.0000, 
sim time next is 91800.0000, 
raw observation next is [22.35, 89.0, 1.0, 2.0, 0.3444837827850009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534850.1042379659, 534850.1042379659, 169515.0482132377], 
processed observation next is [1.0, 0.043478260869565216, 0.25829383886255936, 0.89, 1.0, 1.0, 0.21022142504216976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14856947339943496, 0.14856947339943496, 0.2530075346466234], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.62182724], dtype=float32), 1.479737]. 
=============================================
[2019-04-10 11:05:20,155] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.7932377e-18 1.0000000e+00 6.0008089e-20 3.7144067e-20 2.8196979e-28], sum to 1.0000
[2019-04-10 11:05:20,155] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8827
[2019-04-10 11:05:20,288] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 90.66666666666667, 1.0, 2.0, 0.3710589140826002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569293.6442332807, 569293.6442332807, 172229.5895008858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 103200.0000, 
sim time next is 103800.0000, 
raw observation next is [22.58333333333334, 90.83333333333334, 1.0, 2.0, 0.3727829568078108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571299.4506704927, 571299.4506704921, 172387.3284338278], 
processed observation next is [1.0, 0.17391304347826086, 0.2693522906793052, 0.9083333333333334, 1.0, 1.0, 0.24431681543109737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15869429185291464, 0.15869429185291448, 0.25729452005048925], 
reward next is 0.7427, 
noisyNet noise sample is [array([-0.94464236], dtype=float32), 0.30486998]. 
=============================================
[2019-04-10 11:05:20,865] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8273446e-18 1.0000000e+00 2.7105917e-20 1.2469792e-19 2.3196148e-28], sum to 1.0000
[2019-04-10 11:05:20,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4958
[2019-04-10 11:05:20,975] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.0, 1.0, 2.0, 0.734012973731504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1108080.704503797, 1108080.704503796, 239083.8775810624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [22.9, 92.33333333333334, 1.0, 2.0, 0.7425352020826143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1119812.87522061, 1119812.87522061, 241048.7627609518], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.9233333333333335, 1.0, 1.0, 0.6898014482923063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.311059132005725, 0.311059132005725, 0.35977427277754], 
reward next is 0.6402, 
noisyNet noise sample is [array([-0.38201794], dtype=float32), -1.4865941]. 
=============================================
[2019-04-10 11:05:23,373] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7066377e-19 1.0000000e+00 5.9538768e-22 9.6192935e-23 8.5156873e-31], sum to 1.0000
[2019-04-10 11:05:23,382] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4284
[2019-04-10 11:05:23,500] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 96.0, 1.0, 2.0, 0.2972523655107827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475836.7244059556, 475836.7244059556, 165356.4646725159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 176400.0000, 
sim time next is 177000.0000, 
raw observation next is [20.16666666666666, 96.0, 1.0, 2.0, 0.2967270955109366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475323.1578026265, 475323.1578026265, 165322.9709950197], 
processed observation next is [0.0, 0.043478260869565216, 0.15481832543443896, 0.96, 1.0, 1.0, 0.1526832476035381, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13203421050072958, 0.13203421050072958, 0.24675070297764137], 
reward next is 0.7532, 
noisyNet noise sample is [array([0.03844565], dtype=float32), 0.7689197]. 
=============================================
[2019-04-10 11:05:23,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.461555]
 [76.21957 ]
 [75.966034]
 [75.649864]
 [75.547806]], R is [[76.63409424]
 [76.62094879]
 [76.60775757]
 [76.59452057]
 [76.58122253]].
[2019-04-10 11:05:26,460] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2939504e-19 1.0000000e+00 7.7861644e-24 2.1520338e-25 2.9800848e-33], sum to 1.0000
[2019-04-10 11:05:26,468] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4674
[2019-04-10 11:05:26,472] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 89.0, 1.0, 2.0, 0.2996505036284212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477729.2055036015, 477729.2055036021, 165467.9946676845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 242400.0000, 
sim time next is 243000.0000, 
raw observation next is [21.2, 89.0, 1.0, 2.0, 0.2985970192955962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476385.0334150187, 476385.0334150187, 165377.1718629935], 
processed observation next is [0.0, 0.8260869565217391, 0.20379146919431282, 0.89, 1.0, 1.0, 0.1549361678260195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13232917594861632, 0.13232917594861632, 0.2468315997955127], 
reward next is 0.7532, 
noisyNet noise sample is [array([0.3973506], dtype=float32), 0.972616]. 
=============================================
[2019-04-10 11:05:26,481] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.84731 ]
 [79.818275]
 [79.7672  ]
 [79.704834]
 [79.68519 ]], R is [[79.8290329 ]
 [79.78378296]
 [79.73892975]
 [79.6944809 ]
 [79.65045929]].
[2019-04-10 11:05:28,559] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3211751e-18 1.0000000e+00 3.1726888e-22 5.4164284e-25 1.7972641e-30], sum to 1.0000
[2019-04-10 11:05:28,566] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8576
[2019-04-10 11:05:28,711] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 81.0, 1.0, 2.0, 0.300180525459783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476816.5432192273, 476816.5432192279, 165373.6316426595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 292800.0000, 
sim time next is 293400.0000, 
raw observation next is [22.55, 80.5, 1.0, 2.0, 0.3010275442479547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478008.425114664, 478008.4251146634, 165455.8211644339], 
processed observation next is [0.0, 0.391304347826087, 0.26777251184834133, 0.805, 1.0, 1.0, 0.15786451114211408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13278011808740667, 0.1327801180874065, 0.24694898681258792], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.12249252], dtype=float32), 0.31673557]. 
=============================================
[2019-04-10 11:05:28,978] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7941: loss 0.0068
[2019-04-10 11:05:29,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7942: learning rate 0.0000
[2019-04-10 11:05:29,061] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7954: loss 0.0054
[2019-04-10 11:05:29,062] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7954: loss 0.0058
[2019-04-10 11:05:29,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7956: learning rate 0.0000
[2019-04-10 11:05:29,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7957: learning rate 0.0000
[2019-04-10 11:05:29,071] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7958: loss 0.0053
[2019-04-10 11:05:29,074] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7958: learning rate 0.0000
[2019-04-10 11:05:29,074] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7958: loss 0.0058
[2019-04-10 11:05:29,076] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7959: learning rate 0.0000
[2019-04-10 11:05:29,103] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7979: loss 0.0049
[2019-04-10 11:05:29,105] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7981: learning rate 0.0000
[2019-04-10 11:05:29,110] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7983: loss 0.0053
[2019-04-10 11:05:29,114] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7985: learning rate 0.0000
[2019-04-10 11:05:29,127] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7996: loss 0.0044
[2019-04-10 11:05:29,128] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7996: learning rate 0.0000
[2019-04-10 11:05:29,130] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7996: loss 0.0044
[2019-04-10 11:05:29,131] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7997: loss 0.0050
[2019-04-10 11:05:29,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7997: learning rate 0.0000
[2019-04-10 11:05:29,132] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7997: learning rate 0.0000
[2019-04-10 11:05:29,133] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 7997: loss 0.0057
[2019-04-10 11:05:29,135] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 7997: learning rate 0.0000
[2019-04-10 11:05:29,136] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7997: loss 0.0054
[2019-04-10 11:05:29,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7997: learning rate 0.0000
[2019-04-10 11:05:29,158] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8010: loss 0.0049
[2019-04-10 11:05:29,159] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8011: loss 0.0039
[2019-04-10 11:05:29,159] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8011: loss 0.0048
[2019-04-10 11:05:29,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8011: learning rate 0.0000
[2019-04-10 11:05:29,160] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8011: learning rate 0.0000
[2019-04-10 11:05:29,165] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8011: learning rate 0.0000
[2019-04-10 11:05:29,233] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8057: loss 0.0044
[2019-04-10 11:05:29,235] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8059: learning rate 0.0000
[2019-04-10 11:05:33,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1696481e-17 1.0000000e+00 2.2862245e-20 2.6050024e-19 1.2103521e-28], sum to 1.0000
[2019-04-10 11:05:33,166] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8638
[2019-04-10 11:05:33,171] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 73.0, 1.0, 2.0, 0.301360185706277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487300.2156215706, 487300.2156215701, 166165.7320035826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 391200.0000, 
sim time next is 391800.0000, 
raw observation next is [22.68333333333333, 73.0, 1.0, 2.0, 0.3120328401531444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504413.3474573876, 504413.3474573876, 167408.9133559373], 
processed observation next is [1.0, 0.5217391304347826, 0.27409162717219576, 0.73, 1.0, 1.0, 0.17112390379896916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14011481873816323, 0.14011481873816323, 0.24986404978498103], 
reward next is 0.7501, 
noisyNet noise sample is [array([1.0894659], dtype=float32), -0.36919513]. 
=============================================
[2019-04-10 11:05:35,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.4783745e-17 1.0000000e+00 5.3605698e-21 2.4013857e-21 3.2213125e-29], sum to 1.0000
[2019-04-10 11:05:35,307] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7769
[2019-04-10 11:05:35,312] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 84.16666666666666, 1.0, 2.0, 0.2323332236833912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 383754.6070810878, 383754.6070810872, 159161.3058196733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 442200.0000, 
sim time next is 442800.0000, 
raw observation next is [19.6, 84.0, 1.0, 2.0, 0.2326657747229007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 384388.3960436397, 384388.3960436397, 159187.0100435298], 
processed observation next is [1.0, 0.13043478260869565, 0.127962085308057, 0.84, 1.0, 1.0, 0.07550093340108517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10677455445656658, 0.10677455445656658, 0.23759255230377582], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.22063436], dtype=float32), -0.3317934]. 
=============================================
[2019-04-10 11:05:36,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0349826e-17 1.0000000e+00 8.2115520e-21 3.1592364e-21 3.1388262e-29], sum to 1.0000
[2019-04-10 11:05:36,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5691
[2019-04-10 11:05:36,677] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 68.5, 1.0, 2.0, 0.4605003215267129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755739.554287621, 755739.5542876205, 188959.1340844114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [22.43333333333333, 67.66666666666667, 1.0, 2.0, 0.4671205029577186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766549.5944158662, 766549.5944158655, 190072.9993363132], 
processed observation next is [1.0, 0.43478260869565216, 0.2622432859399683, 0.6766666666666667, 1.0, 1.0, 0.35797650958761285, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21293044289329616, 0.21293044289329596, 0.2836910437855421], 
reward next is 0.7163, 
noisyNet noise sample is [array([0.8924102], dtype=float32), 1.4298785]. 
=============================================
[2019-04-10 11:05:37,007] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.8807038e-18 1.0000000e+00 8.5884806e-21 1.2067472e-20 7.0428369e-30], sum to 1.0000
[2019-04-10 11:05:37,016] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7960
[2019-04-10 11:05:37,022] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 64.33333333333334, 1.0, 2.0, 0.4487163907517874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736011.6630349646, 736011.6630349639, 187009.2207271415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [23.15, 63.5, 1.0, 2.0, 0.459211428534984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753088.038394804, 753088.038394804, 188738.8580953126], 
processed observation next is [1.0, 0.4782608695652174, 0.2962085308056872, 0.635, 1.0, 1.0, 0.3484475042590168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20919112177633445, 0.20919112177633445, 0.2816997882019591], 
reward next is 0.7183, 
noisyNet noise sample is [array([0.73309857], dtype=float32), -1.2404158]. 
=============================================
[2019-04-10 11:05:38,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2851701e-18 1.0000000e+00 2.9780673e-22 8.8746650e-23 2.9242641e-32], sum to 1.0000
[2019-04-10 11:05:38,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7776
[2019-04-10 11:05:38,482] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.53333333333333, 83.0, 1.0, 2.0, 0.2377349738012051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 393541.1061144191, 393541.1061144196, 159604.1388105924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 510000.0000, 
sim time next is 510600.0000, 
raw observation next is [19.46666666666667, 83.5, 1.0, 2.0, 0.2371244943561485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392546.6935223611, 392546.6935223617, 159545.2269239772], 
processed observation next is [1.0, 0.9130434782608695, 0.12164296998420236, 0.835, 1.0, 1.0, 0.08087288476644396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10904074820065586, 0.10904074820065603, 0.23812720436414508], 
reward next is 0.7619, 
noisyNet noise sample is [array([0.90919274], dtype=float32), -0.2243669]. 
=============================================
[2019-04-10 11:05:42,190] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15901: loss 0.0044
[2019-04-10 11:05:42,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15902: learning rate 0.0000
[2019-04-10 11:05:42,214] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15913: loss 0.0041
[2019-04-10 11:05:42,215] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15914: learning rate 0.0000
[2019-04-10 11:05:42,224] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15920: loss 0.0047
[2019-04-10 11:05:42,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15921: learning rate 0.0000
[2019-04-10 11:05:42,272] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15953: loss 0.0043
[2019-04-10 11:05:42,272] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15954: loss 0.0038
[2019-04-10 11:05:42,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15955: learning rate 0.0000
[2019-04-10 11:05:42,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15956: learning rate 0.0000
[2019-04-10 11:05:42,283] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15957: loss 0.0038
[2019-04-10 11:05:42,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15959: learning rate 0.0000
[2019-04-10 11:05:42,302] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15973: loss 0.0047
[2019-04-10 11:05:42,308] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15976: loss 0.0036
[2019-04-10 11:05:42,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15976: learning rate 0.0000
[2019-04-10 11:05:42,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15977: learning rate 0.0000
[2019-04-10 11:05:42,312] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15977: loss 0.0031
[2019-04-10 11:05:42,316] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15978: learning rate 0.0000
[2019-04-10 11:05:42,317] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15979: loss 0.0033
[2019-04-10 11:05:42,320] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15980: learning rate 0.0000
[2019-04-10 11:05:42,326] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15985: loss 0.0032
[2019-04-10 11:05:42,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15985: learning rate 0.0000
[2019-04-10 11:05:42,395] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16027: loss 0.0036
[2019-04-10 11:05:42,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16027: learning rate 0.0000
[2019-04-10 11:05:42,397] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16027: loss 0.0035
[2019-04-10 11:05:42,400] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16028: learning rate 0.0000
[2019-04-10 11:05:42,449] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16061: loss 0.0026
[2019-04-10 11:05:42,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16061: learning rate 0.0000
[2019-04-10 11:05:42,462] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16068: loss 0.0022
[2019-04-10 11:05:42,464] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16068: learning rate 0.0000
[2019-04-10 11:05:42,567] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16136: loss 0.0022
[2019-04-10 11:05:42,569] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16137: learning rate 0.0000
[2019-04-10 11:05:44,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6406701e-17 1.0000000e+00 2.0404345e-20 4.8046178e-20 1.4268554e-29], sum to 1.0000
[2019-04-10 11:05:44,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0633
[2019-04-10 11:05:44,383] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 58.66666666666667, 1.0, 2.0, 0.5545895935895162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908492.3237519702, 908492.3237519709, 206181.3662040037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [24.06666666666667, 58.33333333333334, 1.0, 2.0, 0.5886806297422197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964628.6644423761, 964628.6644423761, 213178.2764068074], 
processed observation next is [1.0, 0.4782608695652174, 0.3396524486571882, 0.5833333333333335, 1.0, 1.0, 0.5044344936653249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2679524067895489, 0.2679524067895489, 0.3181765319504588], 
reward next is 0.6818, 
noisyNet noise sample is [array([0.9216663], dtype=float32), 0.101415105]. 
=============================================
[2019-04-10 11:05:45,953] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7167610e-19 1.0000000e+00 1.3982914e-22 4.4721241e-23 1.0797857e-31], sum to 1.0000
[2019-04-10 11:05:45,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9402
[2019-04-10 11:05:45,964] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.66666666666667, 1.0, 2.0, 0.2393789772093673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395749.9689925067, 395749.9689925067, 159798.8369622887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 682800.0000, 
sim time next is 683400.0000, 
raw observation next is [19.6, 83.33333333333333, 1.0, 2.0, 0.2397787823615639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396477.27191647, 396477.2719164694, 159832.2482227832], 
processed observation next is [1.0, 0.9130434782608695, 0.127962085308057, 0.8333333333333333, 1.0, 1.0, 0.08407082212236615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11013257553235278, 0.1101325755323526, 0.238555594362363], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.8183744], dtype=float32), -1.4485611]. 
=============================================
[2019-04-10 11:06:03,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9863622e-19 1.0000000e+00 5.3193782e-22 4.4729581e-24 3.0647792e-31], sum to 1.0000
[2019-04-10 11:06:03,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1953
[2019-04-10 11:06:04,368] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 86.5, 1.0, 2.0, 0.277683325061373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 448518.5441800884, 448518.5441800891, 163500.584891153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 882600.0000, 
sim time next is 883200.0000, 
raw observation next is [21.0, 86.0, 1.0, 2.0, 0.278570810106858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 449665.4145380659, 449665.4145380653, 163577.5260373691], 
processed observation next is [0.0, 0.21739130434782608, 0.19431279620853087, 0.86, 1.0, 1.0, 0.1308082049480217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1249070595939072, 0.12490705959390704, 0.24414556124980463], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.03667777], dtype=float32), 0.62486786]. 
=============================================
[2019-04-10 11:06:05,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4628517e-18 1.0000000e+00 1.9661984e-22 3.5770917e-25 2.6323157e-32], sum to 1.0000
[2019-04-10 11:06:05,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9857
[2019-04-10 11:06:05,286] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.48333333333333, 83.5, 1.0, 2.0, 0.2840777943892459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457149.539446097, 457149.539446097, 164079.0088635158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 886200.0000, 
sim time next is 886800.0000, 
raw observation next is [21.56666666666667, 83.0, 1.0, 2.0, 0.2836067054072956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456236.4502784533, 456236.4502784533, 164016.7009061235], 
processed observation next is [0.0, 0.2608695652173913, 0.22116903633491333, 0.83, 1.0, 1.0, 0.13687554868348867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12673234729957036, 0.12673234729957036, 0.24480104612854253], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.04616993], dtype=float32), 2.2059755]. 
=============================================
[2019-04-10 11:06:05,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5483436e-18 1.0000000e+00 1.6485832e-22 1.6901635e-24 2.6656330e-31], sum to 1.0000
[2019-04-10 11:06:05,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5835
[2019-04-10 11:06:05,537] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 80.66666666666667, 1.0, 2.0, 0.2841501866107968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 456166.4625647859, 456166.4625647865, 164006.5777040067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 889800.0000, 
sim time next is 890400.0000, 
raw observation next is [22.1, 80.33333333333334, 1.0, 2.0, 0.2853998061894472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457698.5425917103, 457698.5425917097, 164106.6389629345], 
processed observation next is [0.0, 0.30434782608695654, 0.24644549763033188, 0.8033333333333335, 1.0, 1.0, 0.13903591107162314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12713848405325287, 0.1271384840532527, 0.2449352820342306], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.81261194], dtype=float32), -0.6919107]. 
=============================================
[2019-04-10 11:06:06,348] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23837: loss 0.0082
[2019-04-10 11:06:06,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23837: learning rate 0.0000
[2019-04-10 11:06:06,603] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23895: loss 0.0069
[2019-04-10 11:06:06,604] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23895: learning rate 0.0000
[2019-04-10 11:06:06,620] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23897: loss 0.0071
[2019-04-10 11:06:06,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23897: learning rate 0.0000
[2019-04-10 11:06:06,710] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23912: loss 0.0066
[2019-04-10 11:06:06,751] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23912: learning rate 0.0000
[2019-04-10 11:06:06,820] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23936: loss 0.0061
[2019-04-10 11:06:06,823] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23936: loss 0.0068
[2019-04-10 11:06:06,831] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23936: learning rate 0.0000
[2019-04-10 11:06:06,887] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23948: loss 0.0057
[2019-04-10 11:06:06,903] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23948: learning rate 0.0000
[2019-04-10 11:06:06,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23936: learning rate 0.0000
[2019-04-10 11:06:06,911] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23950: loss 0.0058
[2019-04-10 11:06:06,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23950: learning rate 0.0000
[2019-04-10 11:06:07,108] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23981: loss 0.0061
[2019-04-10 11:06:07,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23981: learning rate 0.0000
[2019-04-10 11:06:07,206] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23995: loss 0.0049
[2019-04-10 11:06:07,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23995: learning rate 0.0000
[2019-04-10 11:06:07,257] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24007: loss 0.0053
[2019-04-10 11:06:07,298] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24011: learning rate 0.0000
[2019-04-10 11:06:07,387] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24019: loss 0.0054
[2019-04-10 11:06:07,394] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24023: loss 0.0049
[2019-04-10 11:06:07,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24019: learning rate 0.0000
[2019-04-10 11:06:07,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24026: learning rate 0.0000
[2019-04-10 11:06:07,924] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24106: loss 0.0033
[2019-04-10 11:06:07,926] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24106: learning rate 0.0000
[2019-04-10 11:06:08,176] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24155: loss 0.0037
[2019-04-10 11:06:08,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24155: learning rate 0.0000
[2019-04-10 11:06:09,047] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24311: loss 0.0054
[2019-04-10 11:06:09,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24311: learning rate 0.0000
[2019-04-10 11:06:12,841] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 11:06:12,858] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:06:12,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:06:12,861] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-04-10 11:06:12,859] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:06:12,940] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:06:12,942] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run2
[2019-04-10 11:06:12,943] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:06:12,972] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:06:12,973] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run2
[2019-04-10 11:06:12,944] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:06:12,991] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:06:12,993] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run2
[2019-04-10 11:06:12,990] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:06:13,013] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:06:13,014] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run2
[2019-04-10 11:06:28,313] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:06:28,314] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.7, 86.0, 1.0, 2.0, 0.4349304584529792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633902.4856587674, 633902.4856587674, 177373.3204121177]
[2019-04-10 11:06:28,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:06:28,319] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4001897e-18 1.0000000e+00 6.4364239e-22 2.2467360e-23 5.9052633e-31], sampled 0.649788434618369
[2019-04-10 11:06:47,910] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:06:47,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438]
[2019-04-10 11:06:47,918] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:06:47,926] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.5205273e-19 1.0000000e+00 2.2965889e-22 2.5127047e-23 1.1993061e-31], sampled 0.2981345672113044
[2019-04-10 11:06:48,249] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:06:48,251] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.7, 64.83333333333333, 1.0, 2.0, 0.3394280257044892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529902.2204483144, 529902.2204483144, 169191.4773299366]
[2019-04-10 11:06:48,254] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:06:48,261] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1594876e-18 1.0000000e+00 2.1684360e-22 2.9893210e-24 1.3694206e-31], sampled 0.8874487189533407
[2019-04-10 11:06:56,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:06:56,950] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.41666666666667, 89.33333333333333, 1.0, 2.0, 0.4299142277639193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622704.6192785221, 622704.6192785228, 176169.7066020933]
[2019-04-10 11:06:56,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:06:56,971] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8683805e-18 1.0000000e+00 1.0183090e-21 7.5460655e-23 9.1267337e-31], sampled 0.22498114152679938
[2019-04-10 11:07:08,840] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:08,840] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.84115271, 86.29683491, 1.0, 2.0, 0.5989108717062613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836939.3425812568, 836939.3425812568, 200545.1783386826]
[2019-04-10 11:07:08,840] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:07:08,841] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4220431e-18 1.0000000e+00 1.2772068e-21 1.6183945e-21 8.3299799e-31], sampled 0.657930535633594
[2019-04-10 11:07:08,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:08,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.73333333333333, 91.66666666666666, 1.0, 2.0, 0.3696891328364355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561641.7412239156, 561641.7412239162, 171409.2562188615]
[2019-04-10 11:07:08,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:07:08,849] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1247107e-18 1.0000000e+00 8.8910021e-22 4.4349356e-23 9.5862129e-31], sampled 0.9657630096905746
[2019-04-10 11:07:17,902] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:17,906] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.68103077166667, 98.71961629666667, 1.0, 2.0, 0.2779332523210089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453994.4262180832, 453994.4262180838, 163763.000567167]
[2019-04-10 11:07:17,908] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:07:17,920] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8411810e-18 1.0000000e+00 3.7186845e-22 6.4076075e-24 3.2811367e-31], sampled 0.6826318184290209
[2019-04-10 11:07:23,905] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:23,908] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.69863345333333, 81.86234965333334, 1.0, 2.0, 0.464726889599701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669113.8711381999, 669113.8711381999, 180775.2670109636]
[2019-04-10 11:07:23,908] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:07:23,909] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0685001e-18 1.0000000e+00 3.8195403e-22 1.8173475e-22 1.8763800e-31], sampled 0.051855603544515794
[2019-04-10 11:07:30,778] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:30,779] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.83333333333334, 66.0, 1.0, 2.0, 0.979425836511304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1369027.262186772, 1369027.262186773, 292725.020717304]
[2019-04-10 11:07:30,779] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:07:30,781] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4286515e-16 1.0000000e+00 7.5569990e-19 1.5268303e-17 3.3155936e-27], sampled 0.9298018955033813
[2019-04-10 11:07:40,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:40,539] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.36666666666667, 66.33333333333334, 1.0, 2.0, 0.6313271638100707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 882257.8360025411, 882257.8360025417, 206727.3868561029]
[2019-04-10 11:07:40,540] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:07:40,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1488493e-18 1.0000000e+00 2.1826366e-21 1.8584169e-21 1.8881494e-30], sampled 0.18347974273750223
[2019-04-10 11:07:47,403] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:47,404] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.02568029, 89.28888988333333, 1.0, 2.0, 0.410045664576945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617130.0710649539, 617130.0710649539, 176258.2408302231]
[2019-04-10 11:07:47,405] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:07:47,406] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.5627910e-18 1.0000000e+00 6.7451373e-21 5.2632476e-21 8.6181430e-30], sampled 0.937844000710573
[2019-04-10 11:07:50,717] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:50,717] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 90.0, 1.0, 2.0, 0.5785907325545908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808532.4498498649, 808532.4498498656, 196822.1359977977]
[2019-04-10 11:07:50,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:07:50,720] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.0971090e-18 1.0000000e+00 5.9181477e-21 2.0557934e-21 7.8856780e-30], sampled 0.35285834186893894
[2019-04-10 11:07:56,516] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11237499], dtype=float32), 0.09922592]
[2019-04-10 11:07:56,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.03333333333333, 92.83333333333333, 1.0, 2.0, 0.4967750601745786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694164.4912485267, 694164.491248526, 183091.4817787403]
[2019-04-10 11:07:56,520] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:07:56,521] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2049459e-17 1.0000000e+00 6.6145736e-21 6.0196979e-21 1.7110610e-29], sampled 0.2023286450814027
[2019-04-10 11:08:08,631] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.6496 2778883062.3160 927.0000
[2019-04-10 11:08:08,696] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7924.4839 3158955234.1547 1643.0000
[2019-04-10 11:08:08,736] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8274.2220 2925524724.2597 1287.0000
[2019-04-10 11:08:08,738] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8509.0625 2841047949.9202 1098.0000
[2019-04-10 11:08:08,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8025.5036 3003719851.5812 1665.0000
[2019-04-10 11:08:09,885] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 25000, evaluation results [25000.0, 7924.483890367143, 3158955234.154716, 1643.0, 8274.222034537115, 2925524724.2597275, 1287.0, 8661.649594128754, 2778883062.316041, 927.0, 8025.503589402031, 3003719851.581241, 1665.0, 8509.062453335788, 2841047949.9201574, 1098.0]
[2019-04-10 11:08:12,794] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4005324e-16 1.0000000e+00 5.3763441e-19 1.1428534e-18 1.1980648e-27], sum to 1.0000
[2019-04-10 11:08:12,802] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3442
[2019-04-10 11:08:12,807] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.6386368761396531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 982137.0935775214, 982137.0935775208, 219174.3486969676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1008000.0000, 
sim time next is 1008600.0000, 
raw observation next is [21.7, 97.16666666666667, 1.0, 2.0, 0.670047564466006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029646.155231514, 1029646.155231514, 226111.9375705542], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.9716666666666667, 1.0, 1.0, 0.6024669451397663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2860128208976428, 0.2860128208976428, 0.33748050383664807], 
reward next is 0.6625, 
noisyNet noise sample is [array([-0.111178], dtype=float32), -0.71500844]. 
=============================================
[2019-04-10 11:08:13,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3951681e-17 1.0000000e+00 3.6658168e-19 9.5455154e-18 8.3758770e-28], sum to 1.0000
[2019-04-10 11:08:13,349] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2414
[2019-04-10 11:08:13,514] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.33333333333333, 1.0, 2.0, 0.3578481240162013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549494.6115644863, 549494.6115644869, 170554.0564070382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [21.7, 97.16666666666667, 1.0, 2.0, 0.3576113633991529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549453.8984225335, 549453.8984225335, 170560.0307967989], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.9716666666666667, 1.0, 1.0, 0.22603778722789505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1526260828951482, 0.1526260828951482, 0.254567210144476], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.8998656], dtype=float32), 0.72661364]. 
=============================================
[2019-04-10 11:08:14,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3504649e-16 1.0000000e+00 9.0509586e-20 8.7478951e-20 1.0909289e-28], sum to 1.0000
[2019-04-10 11:08:14,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2510
[2019-04-10 11:08:14,630] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.5, 1.0, 2.0, 0.3812903962043592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572786.0985696589, 572786.0985696589, 172185.1566277203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1042200.0000, 
sim time next is 1042800.0000, 
raw observation next is [22.53333333333333, 96.33333333333334, 1.0, 2.0, 0.3815997228179991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572996.761448401, 572996.761448401, 172195.7943425744], 
processed observation next is [1.0, 0.043478260869565216, 0.26698262243285936, 0.9633333333333334, 1.0, 1.0, 0.25493942508192663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1591657670690003, 0.1591657670690003, 0.2570086482724991], 
reward next is 0.7430, 
noisyNet noise sample is [array([0.7822948], dtype=float32), -0.2417332]. 
=============================================
[2019-04-10 11:08:16,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2744324e-17 1.0000000e+00 5.5630035e-20 1.1969236e-18 6.7259714e-28], sum to 1.0000
[2019-04-10 11:08:16,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1538
[2019-04-10 11:08:16,220] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 76.5, 1.0, 2.0, 0.6154148951735493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970709.9612730056, 970709.9612730056, 216602.6874939174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1079400.0000, 
sim time next is 1080000.0000, 
raw observation next is [23.6, 76.0, 1.0, 2.0, 0.6042018325341043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 950976.3712487163, 950976.371248717, 214021.3898155386], 
processed observation next is [1.0, 0.5217391304347826, 0.3175355450236968, 0.76, 1.0, 1.0, 0.5231347379928968, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2641601031246434, 0.2641601031246436, 0.3194349101724457], 
reward next is 0.6806, 
noisyNet noise sample is [array([0.05690749], dtype=float32), -2.8637772]. 
=============================================
[2019-04-10 11:08:16,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.27453 ]
 [70.44995 ]
 [70.62185 ]
 [70.757385]
 [70.84068 ]], R is [[70.19502258]
 [70.16978455]
 [70.1492691 ]
 [70.13624573]
 [70.12751007]].
[2019-04-10 11:08:18,984] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.42060681e-18 1.00000000e+00 2.24082380e-21 1.21904471e-21
 1.43666945e-30], sum to 1.0000
[2019-04-10 11:08:18,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1397
[2019-04-10 11:08:18,995] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 94.33333333333333, 1.0, 2.0, 0.2725155493758764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 440664.0383251179, 440664.0383251173, 162982.4112130433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [20.05, 93.5, 1.0, 2.0, 0.2755818247337372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445192.6284017785, 445192.6284017785, 163280.6944681586], 
processed observation next is [1.0, 0.21739130434782608, 0.14928909952606645, 0.935, 1.0, 1.0, 0.1272070177514906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12366461900049404, 0.12366461900049404, 0.2437025290569531], 
reward next is 0.7563, 
noisyNet noise sample is [array([1.1566334], dtype=float32), 0.23179395]. 
=============================================
[2019-04-10 11:08:19,011] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.63892 ]
 [73.62076 ]
 [73.56396 ]
 [73.5041  ]
 [73.493484]], R is [[73.66989136]
 [73.68993378]
 [73.70981598]
 [73.72924042]
 [73.7483902 ]].
[2019-04-10 11:08:19,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5989403e-17 1.0000000e+00 5.8409105e-19 3.5627372e-18 2.6514403e-27], sum to 1.0000
[2019-04-10 11:08:19,879] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5646
[2019-04-10 11:08:19,884] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 69.0, 1.0, 2.0, 0.7513651351288666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1152749.972506847, 1152749.972506846, 245571.9352812504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1162800.0000, 
sim time next is 1163400.0000, 
raw observation next is [25.8, 68.5, 1.0, 2.0, 0.7205659639855024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1104329.640723859, 1104329.640723859, 237746.4048036471], 
processed observation next is [1.0, 0.4782608695652174, 0.42180094786729866, 0.685, 1.0, 1.0, 0.6633324867295209, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3067582335344053, 0.3067582335344053, 0.3548453803039509], 
reward next is 0.6452, 
noisyNet noise sample is [array([-0.97507185], dtype=float32), -1.6563903]. 
=============================================
[2019-04-10 11:08:21,288] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31826: loss 0.4077
[2019-04-10 11:08:21,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31826: learning rate 0.0000
[2019-04-10 11:08:21,302] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31832: loss 0.4111
[2019-04-10 11:08:21,304] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31833: learning rate 0.0000
[2019-04-10 11:08:21,351] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31855: loss 0.4075
[2019-04-10 11:08:21,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31855: learning rate 0.0000
[2019-04-10 11:08:21,361] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31863: loss 0.4027
[2019-04-10 11:08:21,365] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31863: learning rate 0.0000
[2019-04-10 11:08:21,475] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31931: loss 0.3925
[2019-04-10 11:08:21,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31931: learning rate 0.0000
[2019-04-10 11:08:21,525] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31960: loss 0.3872
[2019-04-10 11:08:21,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31960: learning rate 0.0000
[2019-04-10 11:08:21,531] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31961: loss 0.3845
[2019-04-10 11:08:21,532] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31962: learning rate 0.0000
[2019-04-10 11:08:21,539] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31964: loss 0.3848
[2019-04-10 11:08:21,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31966: learning rate 0.0000
[2019-04-10 11:08:21,551] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31974: loss 0.3837
[2019-04-10 11:08:21,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31975: learning rate 0.0000
[2019-04-10 11:08:21,583] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31995: loss 0.3877
[2019-04-10 11:08:21,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31996: learning rate 0.0000
[2019-04-10 11:08:21,590] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31997: loss 0.3787
[2019-04-10 11:08:21,592] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31998: learning rate 0.0000
[2019-04-10 11:08:21,640] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32032: loss 0.3760
[2019-04-10 11:08:21,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32032: learning rate 0.0000
[2019-04-10 11:08:21,666] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32046: loss 0.3678
[2019-04-10 11:08:21,669] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32046: learning rate 0.0000
[2019-04-10 11:08:21,740] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32091: loss 0.3673
[2019-04-10 11:08:21,741] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32093: learning rate 0.0000
[2019-04-10 11:08:21,790] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32117: loss 0.3526
[2019-04-10 11:08:21,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32118: learning rate 0.0000
[2019-04-10 11:08:22,150] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32340: loss 0.3606
[2019-04-10 11:08:22,152] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32340: learning rate 0.0000
[2019-04-10 11:08:22,640] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5579032e-19 1.0000000e+00 2.8419858e-21 2.1687183e-21 3.0192476e-29], sum to 1.0000
[2019-04-10 11:08:22,645] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2114
[2019-04-10 11:08:22,647] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.05, 93.0, 1.0, 2.0, 0.3535431868440564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546104.3756138663, 546104.3756138663, 170362.8192823101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1229400.0000, 
sim time next is 1230000.0000, 
raw observation next is [22.2, 92.33333333333334, 1.0, 2.0, 0.3527088546368498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543769.7504015875, 543769.7504015875, 170140.2191914745], 
processed observation next is [1.0, 0.21739130434782608, 0.2511848341232228, 0.9233333333333335, 1.0, 1.0, 0.22013115016487927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15104715288932985, 0.15104715288932985, 0.2539406256589172], 
reward next is 0.7461, 
noisyNet noise sample is [array([-1.3250127], dtype=float32), 1.8966699]. 
=============================================
[2019-04-10 11:08:22,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.778   ]
 [72.761116]
 [72.718124]
 [72.71919 ]
 [72.66316 ]], R is [[72.84448242]
 [72.861763  ]
 [72.87923431]
 [72.89205933]
 [72.90879822]].
[2019-04-10 11:08:24,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0337975e-11 9.9998796e-01 5.7044152e-12 1.2089939e-05 4.8669889e-18], sum to 1.0000
[2019-04-10 11:08:24,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-10 11:08:24,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1898267.271730465 W.
[2019-04-10 11:08:24,122] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.48333333333333, 73.0, 1.0, 2.0, 0.4525755715009391, 1.0, 2.0, 0.4525755715009391, 1.0, 1.0, 0.7673598312017718, 6.9112, 6.9112, 170.5573041426782, 1898267.271730465, 1898267.271730465, 380847.5201427532], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [28.5, 73.0, 1.0, 2.0, 0.6819897631906112, 1.0, 2.0, 0.6819897631906112, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1907017.24427381, 1907017.24427381, 366501.5828009552], 
processed observation next is [1.0, 0.6086956521739131, 0.5497630331753555, 0.73, 1.0, 1.0, 0.6168551363742304, 1.0, 1.0, 0.6168551363742304, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5297270122982806, 0.5297270122982806, 0.5470172877626197], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.66051984], dtype=float32), -1.7409749]. 
=============================================
[2019-04-10 11:08:24,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[56.5565  ]
 [60.657085]
 [62.648838]
 [66.185   ]
 [67.52362 ]], R is [[54.99637222]
 [54.87797928]
 [54.78006744]
 [54.62797928]
 [54.08169937]].
[2019-04-10 11:08:34,184] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39747: loss 0.0039
[2019-04-10 11:08:34,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39747: learning rate 0.0000
[2019-04-10 11:08:34,359] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39857: loss 0.0038
[2019-04-10 11:08:34,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39857: learning rate 0.0000
[2019-04-10 11:08:34,367] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39861: loss 0.0033
[2019-04-10 11:08:34,368] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39861: learning rate 0.0000
[2019-04-10 11:08:34,393] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39874: loss 0.0033
[2019-04-10 11:08:34,396] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39876: learning rate 0.0000
[2019-04-10 11:08:34,449] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39910: loss 0.0037
[2019-04-10 11:08:34,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39910: learning rate 0.0000
[2019-04-10 11:08:34,475] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39927: loss 0.0035
[2019-04-10 11:08:34,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39928: learning rate 0.0000
[2019-04-10 11:08:34,507] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39945: loss 0.0031
[2019-04-10 11:08:34,509] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39946: learning rate 0.0000
[2019-04-10 11:08:34,521] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39952: loss 0.0027
[2019-04-10 11:08:34,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39953: learning rate 0.0000
[2019-04-10 11:08:34,560] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39977: loss 0.0028
[2019-04-10 11:08:34,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39977: learning rate 0.0000
[2019-04-10 11:08:34,605] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40006: loss 0.0018
[2019-04-10 11:08:34,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40007: learning rate 0.0000
[2019-04-10 11:08:34,645] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40029: loss 0.0022
[2019-04-10 11:08:34,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40031: learning rate 0.0000
[2019-04-10 11:08:34,689] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40054: loss 0.0022
[2019-04-10 11:08:34,690] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40054: loss 0.0023
[2019-04-10 11:08:34,691] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40054: learning rate 0.0000
[2019-04-10 11:08:34,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40054: learning rate 0.0000
[2019-04-10 11:08:34,756] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40099: loss 0.0022
[2019-04-10 11:08:34,758] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40101: learning rate 0.0000
[2019-04-10 11:08:34,762] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40104: loss 0.0022
[2019-04-10 11:08:34,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40104: learning rate 0.0000
[2019-04-10 11:08:35,190] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40367: loss 0.0041
[2019-04-10 11:08:35,192] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40368: learning rate 0.0000
[2019-04-10 11:08:46,435] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0771782e-09 8.0229515e-01 1.7826999e-09 1.9770481e-01 2.6405564e-16], sum to 1.0000
[2019-04-10 11:08:46,444] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5783
[2019-04-10 11:08:46,444] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1907107.272237525 W.
[2019-04-10 11:08:46,464] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.5, 77.0, 1.0, 2.0, 0.6820219304686119, 1.0, 2.0, 0.6820219304686119, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1907107.272237525, 1907107.272237525, 366519.4930180989], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1699200.0000, 
sim time next is 1699800.0000, 
raw observation next is [28.56666666666667, 76.5, 1.0, 2.0, 0.7643663978900163, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.982151891941676, 6.9112, 168.9125342122333, 1965203.990291972, 1914868.402016883, 400084.6210219709], 
processed observation next is [1.0, 0.6956521739130435, 0.552922590837283, 0.765, 1.0, 1.0, 0.716104093843393, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007095189194167606, 0.0, 0.8294378714748231, 0.5458899973033255, 0.5319078894491341, 0.5971412254059266], 
reward next is 0.0481, 
noisyNet noise sample is [array([-0.7723593], dtype=float32), 0.98437357]. 
=============================================
[2019-04-10 11:08:49,566] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4304011e-17 1.0000000e+00 4.6760203e-20 1.2662733e-19 5.8097705e-29], sum to 1.0000
[2019-04-10 11:08:49,566] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4617
[2019-04-10 11:08:49,828] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 92.66666666666667, 1.0, 2.0, 0.5490223477861003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775670.7534867938, 775670.7534867938, 192719.2377094287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [24.63333333333333, 92.33333333333334, 1.0, 2.0, 0.5182720004318683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731460.8390015797, 731460.839001579, 187437.0584605661], 
processed observation next is [1.0, 0.2608695652173913, 0.3665086887835701, 0.9233333333333335, 1.0, 1.0, 0.41960481979743164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2031835663893277, 0.2031835663893275, 0.27975680367248673], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.2542006], dtype=float32), -1.3019301]. 
=============================================
[2019-04-10 11:08:52,752] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47719: loss 0.5971
[2019-04-10 11:08:52,760] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47719: learning rate 0.0000
[2019-04-10 11:08:52,868] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47765: loss 0.5944
[2019-04-10 11:08:52,876] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47765: learning rate 0.0000
[2019-04-10 11:08:52,900] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47774: loss 0.5872
[2019-04-10 11:08:52,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47774: learning rate 0.0000
[2019-04-10 11:08:53,163] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47860: loss 0.5812
[2019-04-10 11:08:53,191] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47860: learning rate 0.0000
[2019-04-10 11:08:53,264] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47887: loss 0.5683
[2019-04-10 11:08:53,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47896: learning rate 0.0000
[2019-04-10 11:08:53,319] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47918: loss 0.5690
[2019-04-10 11:08:53,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47919: learning rate 0.0000
[2019-04-10 11:08:53,361] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47931: loss 0.5679
[2019-04-10 11:08:53,367] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47932: learning rate 0.0000
[2019-04-10 11:08:53,446] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47955: loss 0.5580
[2019-04-10 11:08:53,449] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47957: loss 0.5659
[2019-04-10 11:08:53,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47955: learning rate 0.0000
[2019-04-10 11:08:53,470] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47957: learning rate 0.0000
[2019-04-10 11:08:53,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48006: loss 0.5505
[2019-04-10 11:08:53,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48010: learning rate 0.0000
[2019-04-10 11:08:53,894] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48098: loss 0.5376
[2019-04-10 11:08:53,896] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48098: loss 0.5414
[2019-04-10 11:08:53,916] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48098: learning rate 0.0000
[2019-04-10 11:08:53,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48098: learning rate 0.0000
[2019-04-10 11:08:53,952] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48111: loss 0.5376
[2019-04-10 11:08:53,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48112: learning rate 0.0000
[2019-04-10 11:08:54,102] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48159: loss 0.5314
[2019-04-10 11:08:54,106] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48160: learning rate 0.0000
[2019-04-10 11:08:54,360] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48242: loss 0.5208
[2019-04-10 11:08:54,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48242: learning rate 0.0000
[2019-04-10 11:08:54,677] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48347: loss 0.5163
[2019-04-10 11:08:54,708] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48355: learning rate 0.0000
[2019-04-10 11:08:59,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4546250e-11 9.9986827e-01 2.1211954e-11 1.3169844e-04 2.4297376e-18], sum to 1.0000
[2019-04-10 11:08:59,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2110
[2019-04-10 11:08:59,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1909863.87206336 W.
[2019-04-10 11:08:59,818] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 85.66666666666666, 1.0, 2.0, 0.7248218147565588, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.977599336354746, 6.9112, 168.9120594768671, 1909863.87206336, 1862758.147726735, 391119.6446863211], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [27.0, 85.83333333333334, 1.0, 2.0, 0.4416640075373798, 1.0, 1.0, 0.4416640075373798, 1.0, 2.0, 0.7557366891021274, 6.9112, 6.9112, 170.5573041426782, 1852460.622094487, 1852460.622094487, 375366.439037961], 
processed observation next is [1.0, 0.6521739130434783, 0.4786729857819906, 0.8583333333333334, 1.0, 1.0, 0.32730603317756596, 1.0, 0.5, 0.32730603317756596, 1.0, 1.0, 0.7021179135391797, 0.0, 0.0, 0.8375144448122397, 0.5145723950262464, 0.5145723950262464, 0.5602484164745687], 
reward next is 0.4398, 
noisyNet noise sample is [array([-0.29955697], dtype=float32), -1.2513816]. 
=============================================
[2019-04-10 11:09:00,036] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-10 11:09:00,037] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:09:00,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:09:00,044] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:09:00,046] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:09:00,049] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run3
[2019-04-10 11:09:00,047] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:09:00,054] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:09:00,053] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-04-10 11:09:00,052] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:09:00,123] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:09:00,124] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run3
[2019-04-10 11:09:00,053] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:09:00,150] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:09:00,151] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run3
[2019-04-10 11:09:00,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run3
[2019-04-10 11:09:08,087] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:09:08,087] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.63333333333333, 72.0, 1.0, 2.0, 0.7435760978390481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1049439.403040407, 1049439.403040406, 231934.2756856252]
[2019-04-10 11:09:08,088] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:09:08,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8183715e-17 1.0000000e+00 1.4392056e-20 8.0567521e-21 2.5539513e-29], sampled 0.1981723401610307
[2019-04-10 11:09:18,316] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:09:18,316] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.37789805, 97.367582595, 1.0, 2.0, 0.2399235550461554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398192.5742012316, 398192.5742012316, 159699.8182374582]
[2019-04-10 11:09:18,316] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:09:18,343] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3487182e-18 1.0000000e+00 3.1585117e-22 3.5264891e-24 1.8938689e-31], sampled 0.3450501624348572
[2019-04-10 11:09:24,194] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:09:24,196] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.9229223, 96.47099404166666, 1.0, 2.0, 0.3129631629790059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494174.1813523701, 494174.1813523701, 166576.7205122906]
[2019-04-10 11:09:24,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:09:24,199] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3509942e-18 1.0000000e+00 2.7328906e-22 1.8500252e-24 1.5467217e-31], sampled 0.5892127369978034
[2019-04-10 11:10:01,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:10:01,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.472042145, 59.161488665, 1.0, 2.0, 0.5265760032994317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735821.0135483716, 735821.0135483716, 187863.8361114865]
[2019-04-10 11:10:01,702] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:10:01,704] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9391404e-18 1.0000000e+00 2.2931410e-21 5.9575861e-20 1.7827880e-30], sampled 0.7626850930676519
[2019-04-10 11:10:04,068] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:10:04,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 67.0, 1.0, 2.0, 0.7781300895103687, 1.0, 2.0, 0.7781300895103687, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2176093.676982529, 2176093.676982529, 409845.4679652133]
[2019-04-10 11:10:04,072] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:10:04,073] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2725198e-09 6.6661590e-01 3.6375198e-09 3.3338407e-01 6.3969072e-16], sampled 0.6519963384950321
[2019-04-10 11:10:04,078] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2176093.676982529 W.
[2019-04-10 11:10:13,717] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:10:13,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.46666666666667, 83.33333333333334, 1.0, 2.0, 0.605497473499935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846147.3604180234, 846147.3604180234, 201773.9047813638]
[2019-04-10 11:10:13,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:10:13,719] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.9961334e-18 1.0000000e+00 6.5728700e-21 5.1382464e-21 6.2931533e-30], sampled 0.016998673678466192
[2019-04-10 11:10:31,915] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11346985], dtype=float32), 0.09938737]
[2019-04-10 11:10:31,915] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.92898785666667, 91.35978255, 1.0, 2.0, 0.327536006989163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513185.7239231779, 513185.7239231786, 167924.2223799643]
[2019-04-10 11:10:31,916] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:10:31,917] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1357086e-18 1.0000000e+00 2.5672250e-22 5.2245922e-24 1.2063907e-31], sampled 0.5551160109966118
[2019-04-10 11:10:36,453] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8046.1019 3000784700.9340 1593.0000
[2019-04-10 11:10:36,632] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8665.0046 2778548344.7237 915.0000
[2019-04-10 11:10:36,834] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7950.6746 3156463546.1489 1581.0000
[2019-04-10 11:10:36,846] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8282.1524 2924310660.5851 1260.0000
[2019-04-10 11:10:36,865] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8516.4240 2840014587.2944 1074.0000
[2019-04-10 11:10:37,879] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 50000, evaluation results [50000.0, 7950.674581781724, 3156463546.148859, 1581.0, 8282.152368844665, 2924310660.5851307, 1260.0, 8665.004636995069, 2778548344.7237043, 915.0, 8046.101879811155, 3000784700.9340434, 1593.0, 8516.423965667353, 2840014587.2943583, 1074.0]
[2019-04-10 11:10:40,053] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5126574e-15 1.0000000e+00 9.0584284e-18 7.1449331e-16 6.1194726e-26], sum to 1.0000
[2019-04-10 11:10:40,061] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4414
[2019-04-10 11:10:40,065] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 80.0, 1.0, 2.0, 0.9520116521349921, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564536985, 1348010.459809329, 1348010.459809328, 287288.505481712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1936800.0000, 
sim time next is 1937400.0000, 
raw observation next is [26.15, 79.83333333333334, 1.0, 2.0, 0.9054707897799024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104168, 1290499.347349971, 1290499.347349971, 275140.4198411833], 
processed observation next is [1.0, 0.43478260869565216, 0.43838862559241704, 0.7983333333333335, 1.0, 1.0, 0.8861093852769908, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.829439945152233, 0.3584720409305475, 0.3584720409305475, 0.4106573430465422], 
reward next is 0.5893, 
noisyNet noise sample is [array([0.0809143], dtype=float32), -1.8123]. 
=============================================
[2019-04-10 11:10:46,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0242843e-18 1.0000000e+00 2.0244619e-21 7.2607539e-21 2.3718141e-30], sum to 1.0000
[2019-04-10 11:10:46,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9580
[2019-04-10 11:10:46,219] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 90.0, 1.0, 2.0, 0.4764834798141854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665801.3348068182, 665801.3348068189, 179993.3708287652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2061000.0000, 
sim time next is 2061600.0000, 
raw observation next is [25.2, 90.33333333333334, 1.0, 2.0, 0.4763995910352775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665684.0783574794, 665684.0783574794, 179980.8065091273], 
processed observation next is [0.0, 0.8695652173913043, 0.3933649289099526, 0.9033333333333334, 1.0, 1.0, 0.3691561337774428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18491224398818873, 0.18491224398818873, 0.2686280694166079], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.05241919], dtype=float32), -0.52723026]. 
=============================================
[2019-04-10 11:10:46,948] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.4707414e-18 1.0000000e+00 1.8573327e-21 1.8091441e-22 1.2585137e-29], sum to 1.0000
[2019-04-10 11:10:46,954] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6688
[2019-04-10 11:10:46,957] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 97.5, 1.0, 2.0, 0.4565360894634583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645876.5576527448, 645876.5576527448, 178093.8675434302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [24.06666666666667, 97.0, 1.0, 2.0, 0.4631844348492491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 653250.2282410933, 653250.2282410928, 178808.4811557962], 
processed observation next is [0.0, 0.21739130434782608, 0.3396524486571882, 0.97, 1.0, 1.0, 0.3532342588545169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18145839673363703, 0.18145839673363687, 0.26687833008327794], 
reward next is 0.7331, 
noisyNet noise sample is [array([-0.19358419], dtype=float32), 2.4967601]. 
=============================================
[2019-04-10 11:10:47,276] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55702: loss 0.0004
[2019-04-10 11:10:47,279] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55703: learning rate 0.0000
[2019-04-10 11:10:47,356] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55746: loss 0.0002
[2019-04-10 11:10:47,360] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55747: learning rate 0.0000
[2019-04-10 11:10:47,460] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55813: loss 0.0002
[2019-04-10 11:10:47,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55813: learning rate 0.0000
[2019-04-10 11:10:47,490] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55828: loss 0.0002
[2019-04-10 11:10:47,491] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55828: learning rate 0.0000
[2019-04-10 11:10:47,520] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55850: loss 0.0002
[2019-04-10 11:10:47,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55850: learning rate 0.0000
[2019-04-10 11:10:47,552] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55867: loss 0.0002
[2019-04-10 11:10:47,553] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55867: learning rate 0.0000
[2019-04-10 11:10:47,709] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55963: loss 0.0002
[2019-04-10 11:10:47,712] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55963: learning rate 0.0000
[2019-04-10 11:10:47,754] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55987: loss 0.0002
[2019-04-10 11:10:47,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55987: learning rate 0.0000
[2019-04-10 11:10:47,825] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56030: loss 0.0002
[2019-04-10 11:10:47,828] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56031: learning rate 0.0000
[2019-04-10 11:10:47,839] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56043: loss 0.0002
[2019-04-10 11:10:47,840] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56043: learning rate 0.0000
[2019-04-10 11:10:47,847] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56047: loss 0.0002
[2019-04-10 11:10:47,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56049: learning rate 0.0000
[2019-04-10 11:10:47,905] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56076: loss 0.0002
[2019-04-10 11:10:47,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56076: learning rate 0.0000
[2019-04-10 11:10:48,007] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56137: loss 0.0002
[2019-04-10 11:10:48,009] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56138: learning rate 0.0000
[2019-04-10 11:10:48,023] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56147: loss 0.0003
[2019-04-10 11:10:48,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56147: learning rate 0.0000
[2019-04-10 11:10:48,203] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56253: loss 0.0002
[2019-04-10 11:10:48,206] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56255: learning rate 0.0000
[2019-04-10 11:10:48,267] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56293: loss 0.0002
[2019-04-10 11:10:48,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56294: learning rate 0.0000
[2019-04-10 11:10:50,457] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9237433e-17 1.0000000e+00 1.9482963e-20 3.0605088e-20 1.3180359e-29], sum to 1.0000
[2019-04-10 11:10:50,463] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8463
[2019-04-10 11:10:50,608] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 93.0, 1.0, 2.0, 0.5151240273938602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719812.954775663, 719812.954775663, 185999.4616185058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2160000.0000, 
sim time next is 2160600.0000, 
raw observation next is [25.76666666666667, 93.16666666666667, 1.0, 2.0, 0.5136989195474245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717820.895321216, 717820.8953212154, 185770.1529917105], 
processed observation next is [1.0, 0.0, 0.42022116903633505, 0.9316666666666668, 1.0, 1.0, 0.4140950837920777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19939469314478223, 0.19939469314478206, 0.27726888506225444], 
reward next is 0.7227, 
noisyNet noise sample is [array([-0.12488546], dtype=float32), -0.010238943]. 
=============================================
[2019-04-10 11:10:52,705] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9856967e-10 5.2378207e-02 1.3011842e-10 9.4762182e-01 4.1369799e-16], sum to 1.0000
[2019-04-10 11:10:52,712] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6202
[2019-04-10 11:10:52,715] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.9, 67.0, 1.0, 2.0, 0.8810380134981054, 1.0, 2.0, 0.8810380134981054, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2464205.793693491, 2464205.793693492, 461254.7796940929], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2217600.0000, 
sim time next is 2218200.0000, 
raw observation next is [31.88333333333333, 67.16666666666667, 1.0, 2.0, 0.935203760370006, 1.0, 2.0, 0.935203760370006, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2615862.436462428, 2615862.436462428, 491062.9377828368], 
processed observation next is [1.0, 0.6956521739130435, 0.7101105845181673, 0.6716666666666667, 1.0, 1.0, 0.9219322414096457, 1.0, 1.0, 0.9219322414096457, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7266284545728967, 0.7266284545728967, 0.7329297578848311], 
reward next is 0.2671, 
noisyNet noise sample is [array([0.6927063], dtype=float32), -0.49359697]. 
=============================================
[2019-04-10 11:10:56,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7969385e-11 9.9948883e-01 1.2364637e-11 5.1116658e-04 3.5261630e-18], sum to 1.0000
[2019-04-10 11:10:56,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9251
[2019-04-10 11:10:56,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1969066.052173235 W.
[2019-04-10 11:10:56,390] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.86666666666667, 64.33333333333334, 1.0, 2.0, 0.7041593587141183, 1.0, 2.0, 0.7041593587141183, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1969066.052173235, 1969066.052173235, 375913.2867143531], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2294400.0000, 
sim time next is 2295000.0000, 
raw observation next is [31.85, 64.5, 1.0, 2.0, 0.7445954240147413, 1.0, 2.0, 0.7445954240147413, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2082248.759797174, 2082248.759797174, 393802.2751277269], 
processed observation next is [1.0, 0.5652173913043478, 0.7085308056872038, 0.645, 1.0, 1.0, 0.6922836433912546, 1.0, 1.0, 0.6922836433912546, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5784024332769928, 0.5784024332769928, 0.5877645897428759], 
reward next is 0.4122, 
noisyNet noise sample is [array([-0.57305884], dtype=float32), 0.40843555]. 
=============================================
[2019-04-10 11:10:56,401] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[56.756676]
 [56.641884]
 [55.293484]
 [55.37193 ]
 [56.45416 ]], R is [[56.33346176]
 [56.20906067]
 [56.13860703]
 [55.57722092]
 [55.02145004]].
[2019-04-10 11:11:00,495] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63736: loss 0.2220
[2019-04-10 11:11:00,497] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63736: learning rate 0.0000
[2019-04-10 11:11:00,581] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63791: loss 1.1708
[2019-04-10 11:11:00,581] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63791: learning rate 0.0000
[2019-04-10 11:11:00,588] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63795: loss 2.8372
[2019-04-10 11:11:00,590] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63796: learning rate 0.0000
[2019-04-10 11:11:00,609] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63809: loss 0.2443
[2019-04-10 11:11:00,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63810: learning rate 0.0000
[2019-04-10 11:11:00,616] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63812: loss 0.2085
[2019-04-10 11:11:00,629] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63813: learning rate 0.0000
[2019-04-10 11:11:00,682] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63849: loss 0.8932
[2019-04-10 11:11:00,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63850: learning rate 0.0000
[2019-04-10 11:11:00,902] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63991: loss 0.7791
[2019-04-10 11:11:00,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63991: learning rate 0.0000
[2019-04-10 11:11:00,929] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64010: loss 0.6192
[2019-04-10 11:11:00,930] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64010: learning rate 0.0000
[2019-04-10 11:11:00,952] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64022: loss 0.9151
[2019-04-10 11:11:00,957] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64025: learning rate 0.0000
[2019-04-10 11:11:00,957] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64026: loss 0.6496
[2019-04-10 11:11:00,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64026: learning rate 0.0000
[2019-04-10 11:11:00,965] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64031: loss 0.6763
[2019-04-10 11:11:00,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64032: learning rate 0.0000
[2019-04-10 11:11:01,004] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64053: loss 0.5177
[2019-04-10 11:11:01,006] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64053: learning rate 0.0000
[2019-04-10 11:11:01,155] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64143: loss 0.7361
[2019-04-10 11:11:01,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64143: learning rate 0.0000
[2019-04-10 11:11:01,296] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64232: loss 1.8973
[2019-04-10 11:11:01,297] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64233: loss 1.7826
[2019-04-10 11:11:01,298] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64233: learning rate 0.0000
[2019-04-10 11:11:01,299] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64233: learning rate 0.0000
[2019-04-10 11:11:01,370] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64280: loss 0.3763
[2019-04-10 11:11:01,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64282: learning rate 0.0000
[2019-04-10 11:11:07,230] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8731283e-10 2.9286736e-01 8.7551960e-10 7.0713258e-01 1.8896356e-16], sum to 1.0000
[2019-04-10 11:11:07,236] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0703
[2019-04-10 11:11:07,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2030781.395316294 W.
[2019-04-10 11:11:07,249] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.2, 81.33333333333333, 1.0, 2.0, 0.7262085360431181, 1.0, 2.0, 0.7262085360431181, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2030781.395316294, 2030781.395316294, 385544.40039983], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2544000.0000, 
sim time next is 2544600.0000, 
raw observation next is [28.3, 80.66666666666667, 1.0, 2.0, 0.8512666434578939, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.989253608193088, 6.9112, 168.9124926558958, 2086825.486649277, 2031451.722662417, 421521.1239555578], 
processed observation next is [1.0, 0.43478260869565216, 0.5402843601895735, 0.8066666666666668, 1.0, 1.0, 0.8208031848890288, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00780536081930876, 0.0, 0.8294376674141812, 0.5796737462914658, 0.5642921451840047, 0.6291360059038177], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.03782631], dtype=float32), 0.8552201]. 
=============================================
[2019-04-10 11:11:13,638] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.6966211e-18 1.0000000e+00 5.7030545e-21 1.6494450e-22 5.1008631e-30], sum to 1.0000
[2019-04-10 11:11:13,661] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6915
[2019-04-10 11:11:13,665] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.4763915593832231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665672.8520295065, 665672.8520295065, 179979.603333494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2632800.0000, 
sim time next is 2633400.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.4773453548638306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667006.0307422256, 667006.0307422249, 180122.5480859272], 
processed observation next is [0.0, 0.4782608695652174, 0.4549763033175356, 0.815, 1.0, 1.0, 0.3702956082696755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18527945298395154, 0.18527945298395135, 0.2688396240088466], 
reward next is 0.7312, 
noisyNet noise sample is [array([0.48704866], dtype=float32), -1.6778406]. 
=============================================
[2019-04-10 11:11:18,633] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71732: loss 0.0025
[2019-04-10 11:11:18,653] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71732: learning rate 0.0000
[2019-04-10 11:11:19,017] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71822: loss 0.0023
[2019-04-10 11:11:19,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71822: learning rate 0.0000
[2019-04-10 11:11:19,153] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71869: loss 0.0026
[2019-04-10 11:11:19,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71870: learning rate 0.0000
[2019-04-10 11:11:19,215] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71889: loss 0.0034
[2019-04-10 11:11:19,220] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71889: learning rate 0.0000
[2019-04-10 11:11:19,418] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71967: loss 0.0030
[2019-04-10 11:11:19,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71968: learning rate 0.0000
[2019-04-10 11:11:19,429] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71973: loss 0.0023
[2019-04-10 11:11:19,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71973: learning rate 0.0000
[2019-04-10 11:11:19,488] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71998: loss 0.0027
[2019-04-10 11:11:19,498] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 72000: learning rate 0.0000
[2019-04-10 11:11:19,499] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72000: loss 0.0030
[2019-04-10 11:11:19,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72001: learning rate 0.0000
[2019-04-10 11:11:19,511] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72006: loss 0.0028
[2019-04-10 11:11:19,515] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 72006: loss 0.0027
[2019-04-10 11:11:19,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 72006: learning rate 0.0000
[2019-04-10 11:11:19,516] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72006: learning rate 0.0000
[2019-04-10 11:11:19,584] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72034: loss 0.0024
[2019-04-10 11:11:19,590] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72035: learning rate 0.0000
[2019-04-10 11:11:19,638] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72055: loss 0.0029
[2019-04-10 11:11:19,644] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72057: learning rate 0.0000
[2019-04-10 11:11:19,689] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72078: loss 0.0020
[2019-04-10 11:11:19,694] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72080: loss 0.0025
[2019-04-10 11:11:19,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72081: loss 0.0025
[2019-04-10 11:11:19,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72088: learning rate 0.0000
[2019-04-10 11:11:19,712] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72088: learning rate 0.0000
[2019-04-10 11:11:19,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72088: learning rate 0.0000
[2019-04-10 11:11:20,077] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72214: loss 0.0013
[2019-04-10 11:11:20,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72214: learning rate 0.0000
[2019-04-10 11:11:27,141] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3942683e-17 1.0000000e+00 4.2822559e-21 5.9829998e-21 4.1815069e-30], sum to 1.0000
[2019-04-10 11:11:27,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-10 11:11:27,340] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3875935231170022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597063.342476695, 597063.342476695, 174739.5927327472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2794200.0000, 
sim time next is 2794800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5208599807107414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802332.5437939562, 802332.5437939562, 195847.4851315653], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4227228683261944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287015105387672, 0.22287015105387672, 0.2923096793008437], 
reward next is 0.7077, 
noisyNet noise sample is [array([-0.22354598], dtype=float32), -0.5923011]. 
=============================================
[2019-04-10 11:11:28,556] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.9821862e-16 1.0000000e+00 8.4016156e-18 9.7965618e-16 1.1562989e-25], sum to 1.0000
[2019-04-10 11:11:28,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3293
[2019-04-10 11:11:28,583] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 88.0, 1.0, 2.0, 0.7977978938658373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173371.202606536, 1173371.202606536, 251394.3311992045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.7982621086786734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1175395.883493164, 1175395.883493165, 251692.2016356019], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.89, 1.0, 1.0, 0.7569422996128595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3264988565258789, 0.3264988565258792, 0.37566000244119685], 
reward next is 0.6243, 
noisyNet noise sample is [array([0.46178597], dtype=float32), -0.19393963]. 
=============================================
[2019-04-10 11:11:28,606] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 11:11:28,607] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:11:28,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:11:28,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-04-10 11:11:28,641] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:11:28,642] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:11:28,643] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run4
[2019-04-10 11:11:28,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:11:28,698] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:11:28,700] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run4
[2019-04-10 11:11:28,731] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:11:28,732] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:11:28,733] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run4
[2019-04-10 11:11:28,782] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:11:28,783] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:11:28,819] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run4
[2019-04-10 11:11:51,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:11:51,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.1, 68.0, 1.0, 2.0, 0.2476446285416343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 408028.743743778, 408028.743743778, 160658.9546116412]
[2019-04-10 11:11:51,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:11:51,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1685065e-18 1.0000000e+00 4.1858171e-22 1.9794567e-24 3.5994197e-31], sampled 0.11501590872007572
[2019-04-10 11:12:20,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:12:20,308] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.0, 63.0, 1.0, 2.0, 0.9100709841871512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1272025.976252002, 1272025.976252002, 272727.0228898615]
[2019-04-10 11:12:20,308] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:12:20,310] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2393356e-17 1.0000000e+00 4.4847219e-20 8.6547277e-19 9.9708324e-29], sampled 0.3373949196284828
[2019-04-10 11:12:28,420] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:12:28,421] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 70.0, 1.0, 2.0, 0.5366007063392637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749834.1676537087, 749834.1676537087, 189529.2308415456]
[2019-04-10 11:12:28,422] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:12:28,425] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1561418e-18 1.0000000e+00 2.9330280e-21 3.1431886e-21 2.6413108e-30], sampled 0.27358465059065495
[2019-04-10 11:12:33,075] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:12:33,076] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.7, 57.0, 1.0, 2.0, 0.54037471398818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755109.758436282, 755109.7584362815, 190163.5938632268]
[2019-04-10 11:12:33,076] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:12:33,079] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9056052e-17 1.0000000e+00 2.9610442e-20 1.1011649e-19 6.5898149e-29], sampled 0.3845956746187966
[2019-04-10 11:12:39,649] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:12:39,650] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.73333333333333, 57.0, 1.0, 2.0, 0.6020957532264538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 841391.7720906184, 841391.772090619, 201134.8424023972]
[2019-04-10 11:12:39,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:12:39,655] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9699872e-17 1.0000000e+00 1.4710160e-20 9.2677477e-21 2.3492945e-29], sampled 0.05623388768418258
[2019-04-10 11:12:45,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:12:45,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.146111025, 64.13535830500001, 1.0, 2.0, 0.5754859905345991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804192.188531331, 804192.188531331, 196267.4079033486]
[2019-04-10 11:12:45,766] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:12:45,768] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5115195e-17 1.0000000e+00 9.5576342e-21 3.0568783e-21 1.5934248e-29], sampled 0.21785614952047794
[2019-04-10 11:12:57,090] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1171795], dtype=float32), 0.09853396]
[2019-04-10 11:12:57,091] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.0, 91.00000000000001, 1.0, 2.0, 0.6990171926705849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977851.3881649631, 977851.3881649637, 220758.4389540063]
[2019-04-10 11:12:57,092] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:12:57,095] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.7698846e-17 1.0000000e+00 2.9765823e-20 3.2377988e-20 1.1139318e-28], sampled 0.9280379163866362
[2019-04-10 11:13:07,721] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8288.4512 2923500471.1823 1232.0000
[2019-04-10 11:13:08,363] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8668.5254 2778239794.8018 904.0000
[2019-04-10 11:13:08,526] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8072.4884 2998827994.5783 1544.0000
[2019-04-10 11:13:08,688] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7961.0959 3154676073.9174 1535.0000
[2019-04-10 11:13:08,721] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8527.0058 2838907877.9883 1043.0000
[2019-04-10 11:13:09,737] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 75000, evaluation results [75000.0, 7961.095859911373, 3154676073.9174395, 1535.0, 8288.451195276133, 2923500471.182316, 1232.0, 8668.525392209662, 2778239794.801836, 904.0, 8072.488368641164, 2998827994.578294, 1544.0, 8527.005754520516, 2838907877.988263, 1043.0]
[2019-04-10 11:13:15,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.6833707e-18 1.0000000e+00 9.4782652e-21 6.1805788e-21 3.4687074e-30], sum to 1.0000
[2019-04-10 11:13:15,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9120
[2019-04-10 11:13:15,350] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.3043100954086112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483045.0779612894, 483045.0779612888, 165814.5117032176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2946600.0000, 
sim time next is 2947200.0000, 
raw observation next is [20.66666666666667, 96.0, 1.0, 2.0, 0.3078368523663025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488082.7785863078, 488082.7785863078, 166169.8120022653], 
processed observation next is [1.0, 0.08695652173913043, 0.17851500789889443, 0.96, 1.0, 1.0, 0.16606849682687044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13557854960730772, 0.13557854960730772, 0.24801464477950044], 
reward next is 0.7520, 
noisyNet noise sample is [array([0.7627181], dtype=float32), 0.4713928]. 
=============================================
[2019-04-10 11:13:17,237] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79697: loss 1.5751
[2019-04-10 11:13:17,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79699: learning rate 0.0000
[2019-04-10 11:13:17,483] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1345052e-17 1.0000000e+00 1.6693994e-20 1.7901034e-19 8.7544313e-29], sum to 1.0000
[2019-04-10 11:13:17,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6577
[2019-04-10 11:13:17,498] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.5099191171375111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808332.3011028171, 808332.3011028176, 196071.1602527268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2986800.0000, 
sim time next is 2987400.0000, 
raw observation next is [20.83333333333333, 95.0, 1.0, 2.0, 0.5252021543128828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831588.4355212437, 831588.4355212437, 198793.9828026842], 
processed observation next is [1.0, 0.5652173913043478, 0.1864139020537123, 0.95, 1.0, 1.0, 0.4279544027866057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23099678764478992, 0.23099678764478992, 0.2967074370189316], 
reward next is 0.7033, 
noisyNet noise sample is [array([-1.7502198], dtype=float32), 0.0025321855]. 
=============================================
[2019-04-10 11:13:17,693] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79911: loss 1.5862
[2019-04-10 11:13:17,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79911: learning rate 0.0000
[2019-04-10 11:13:17,807] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79919: loss 1.5828
[2019-04-10 11:13:17,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79920: learning rate 0.0000
[2019-04-10 11:13:17,810] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79920: loss 1.5835
[2019-04-10 11:13:17,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79920: learning rate 0.0000
[2019-04-10 11:13:17,974] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79929: loss 1.5769
[2019-04-10 11:13:17,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79929: learning rate 0.0000
[2019-04-10 11:13:18,095] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79959: loss 1.5792
[2019-04-10 11:13:18,099] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79961: loss 1.5743
[2019-04-10 11:13:18,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79961: learning rate 0.0000
[2019-04-10 11:13:18,200] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79961: learning rate 0.0000
[2019-04-10 11:13:18,313] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79972: loss 1.5786
[2019-04-10 11:13:18,315] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79972: loss 1.5840
[2019-04-10 11:13:18,316] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79972: learning rate 0.0000
[2019-04-10 11:13:18,325] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79972: learning rate 0.0000
[2019-04-10 11:13:18,541] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80007: loss 1.5769
[2019-04-10 11:13:18,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80008: learning rate 0.0000
[2019-04-10 11:13:18,663] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80046: loss 1.5661
[2019-04-10 11:13:18,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80046: learning rate 0.0000
[2019-04-10 11:13:18,770] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80065: loss 1.5683
[2019-04-10 11:13:18,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80066: learning rate 0.0000
[2019-04-10 11:13:18,851] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80074: loss 1.5602
[2019-04-10 11:13:18,854] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80075: learning rate 0.0000
[2019-04-10 11:13:18,945] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80081: loss 1.5698
[2019-04-10 11:13:18,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80081: learning rate 0.0000
[2019-04-10 11:13:19,033] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80088: loss 1.5586
[2019-04-10 11:13:19,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80089: learning rate 0.0000
[2019-04-10 11:13:19,223] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80147: loss 1.5484
[2019-04-10 11:13:19,229] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80149: learning rate 0.0000
[2019-04-10 11:13:31,836] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87878: loss 0.0029
[2019-04-10 11:13:31,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87878: learning rate 0.0000
[2019-04-10 11:13:31,848] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87883: loss 0.0034
[2019-04-10 11:13:31,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87883: learning rate 0.0000
[2019-04-10 11:13:31,921] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87928: loss 0.0037
[2019-04-10 11:13:31,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87930: learning rate 0.0000
[2019-04-10 11:13:31,939] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87940: loss 0.0031
[2019-04-10 11:13:31,942] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87942: learning rate 0.0000
[2019-04-10 11:13:31,944] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87943: loss 0.0032
[2019-04-10 11:13:31,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87943: learning rate 0.0000
[2019-04-10 11:13:31,965] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87953: loss 0.0037
[2019-04-10 11:13:31,967] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87953: learning rate 0.0000
[2019-04-10 11:13:31,996] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 87968: loss 0.0039
[2019-04-10 11:13:31,998] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 87968: learning rate 0.0000
[2019-04-10 11:13:32,011] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87977: loss 0.0039
[2019-04-10 11:13:32,011] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87977: loss 0.0035
[2019-04-10 11:13:32,012] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87977: learning rate 0.0000
[2019-04-10 11:13:32,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87977: learning rate 0.0000
[2019-04-10 11:13:32,038] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87994: loss 0.0035
[2019-04-10 11:13:32,040] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87995: learning rate 0.0000
[2019-04-10 11:13:32,069] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88018: loss 0.0038
[2019-04-10 11:13:32,070] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88019: learning rate 0.0000
[2019-04-10 11:13:32,088] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88031: loss 0.0044
[2019-04-10 11:13:32,090] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88032: learning rate 0.0000
[2019-04-10 11:13:32,123] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88050: loss 0.0037
[2019-04-10 11:13:32,126] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88050: learning rate 0.0000
[2019-04-10 11:13:32,160] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88071: loss 0.0038
[2019-04-10 11:13:32,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88071: learning rate 0.0000
[2019-04-10 11:13:32,171] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88077: loss 0.0036
[2019-04-10 11:13:32,173] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88078: learning rate 0.0000
[2019-04-10 11:13:32,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4920758e-17 1.0000000e+00 1.9206158e-21 8.4300428e-22 5.1153606e-30], sum to 1.0000
[2019-04-10 11:13:32,217] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2844
[2019-04-10 11:13:32,223] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.421087259009552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616551.6015760098, 616551.6015760098, 175763.073442786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3304800.0000, 
sim time next is 3305400.0000, 
raw observation next is [25.16666666666667, 83.16666666666667, 1.0, 2.0, 0.4268892088796643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622013.9468405113, 622013.9468405107, 176205.7505025073], 
processed observation next is [0.0, 0.2608695652173913, 0.39178515007898923, 0.8316666666666667, 1.0, 1.0, 0.30950507093935453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17278165190014202, 0.17278165190014186, 0.2629936574664288], 
reward next is 0.7370, 
noisyNet noise sample is [array([-0.46816084], dtype=float32), 0.37668386]. 
=============================================
[2019-04-10 11:13:32,272] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88139: loss 0.0029
[2019-04-10 11:13:32,274] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88139: learning rate 0.0000
[2019-04-10 11:13:36,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3483539e-15 1.0000000e+00 3.2765574e-17 3.6704839e-15 7.4809962e-25], sum to 1.0000
[2019-04-10 11:13:36,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3557
[2019-04-10 11:13:36,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 89.83333333333333, 1.0, 2.0, 0.8089821532016161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1130656.691245038, 1130656.691245037, 246127.4654552584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3390600.0000, 
sim time next is 3391200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.8045236478928445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1124422.05687882, 1124422.05687882, 245024.2878413632], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.7644863227624632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3123394602441167, 0.3123394602441167, 0.36570789230054207], 
reward next is 0.6343, 
noisyNet noise sample is [array([0.15322916], dtype=float32), 2.4060638]. 
=============================================
[2019-04-10 11:13:38,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2961706e-16 1.0000000e+00 4.2977112e-18 3.4399913e-13 6.2558958e-27], sum to 1.0000
[2019-04-10 11:13:38,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3163
[2019-04-10 11:13:38,427] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5430648807823452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758870.2912250569, 758870.2912250574, 190619.3539496385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3435600.0000, 
sim time next is 3436200.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.538849562144199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752977.7837121148, 752977.7837121154, 189907.4777205657], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.72, 1.0, 1.0, 0.4443970628243361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20916049547558743, 0.2091604954755876, 0.28344399659785924], 
reward next is 0.7166, 
noisyNet noise sample is [array([-2.7741735], dtype=float32), 0.31981558]. 
=============================================
[2019-04-10 11:13:44,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3799982e-10 8.5041430e-03 3.8601550e-10 9.9149585e-01 5.4761241e-16], sum to 1.0000
[2019-04-10 11:13:44,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5310
[2019-04-10 11:13:44,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 66.16666666666667, 1.0, 2.0, 0.8015196261290929, 1.0, 2.0, 0.8015196261290929, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2241598.583343767, 2241598.583343768, 420610.7186537413], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3496200.0000, 
sim time next is 3496800.0000, 
raw observation next is [31.33333333333334, 66.33333333333334, 1.0, 2.0, 0.8118876206328441, 1.0, 2.0, 0.8118876206328441, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2270620.947336214, 2270620.947336215, 425700.0665081484], 
processed observation next is [1.0, 0.4782608695652174, 0.6840442338072673, 0.6633333333333334, 1.0, 1.0, 0.7733585790757158, 1.0, 1.0, 0.7733585790757158, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6307280409267262, 0.6307280409267264, 0.6353732335942514], 
reward next is 0.3646, 
noisyNet noise sample is [array([0.92446196], dtype=float32), -0.06397145]. 
=============================================
[2019-04-10 11:13:52,367] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3779204e-10 9.1980624e-01 6.1297978e-10 8.0193765e-02 4.6475554e-16], sum to 1.0000
[2019-04-10 11:13:52,377] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0209
[2019-04-10 11:13:52,379] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2276202.5061923 W.
[2019-04-10 11:13:52,414] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.9865686141656941, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005992598036802, 6.9112, 168.9123931331181, 2276202.5061923, 2208953.598932352, 459190.5847661751], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3590400.0000, 
sim time next is 3591000.0000, 
raw observation next is [32.5, 65.0, 1.0, 2.0, 0.5775066863483608, 1.0, 1.0, 0.5775066863483608, 1.0, 2.0, 1.002938129807515, 6.9112, 6.9112, 170.5573041426782, 2422832.666558122, 2422832.666558122, 472880.7752442278], 
processed observation next is [1.0, 0.5652173913043478, 0.7393364928909952, 0.65, 1.0, 1.0, 0.49097191126308526, 1.0, 0.5, 0.49097191126308526, 1.0, 1.0, 1.0035830851311158, 0.0, 0.0, 0.8375144448122397, 0.6730090740439227, 0.6730090740439227, 0.7057922018570564], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9718197], dtype=float32), 0.712651]. 
=============================================
[2019-04-10 11:13:52,479] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[51.762444]
 [52.91489 ]
 [51.64812 ]
 [51.95469 ]
 [55.983612]], R is [[49.1697464 ]
 [48.67805099]
 [48.6466217 ]
 [48.59316254]
 [48.10723114]].
[2019-04-10 11:13:52,764] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95813: loss 1.2564
[2019-04-10 11:13:52,767] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95813: learning rate 0.0000
[2019-04-10 11:13:52,787] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95822: loss 0.8028
[2019-04-10 11:13:52,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95823: learning rate 0.0000
[2019-04-10 11:13:52,925] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95871: loss 1.4932
[2019-04-10 11:13:52,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95872: learning rate 0.0000
[2019-04-10 11:13:52,965] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95887: loss 2.3956
[2019-04-10 11:13:52,968] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95887: learning rate 0.0000
[2019-04-10 11:13:53,028] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95897: loss 1.8345
[2019-04-10 11:13:53,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95898: learning rate 0.0000
[2019-04-10 11:13:53,167] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95952: loss 2.3043
[2019-04-10 11:13:53,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95953: learning rate 0.0000
[2019-04-10 11:13:53,269] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95975: loss 2.2298
[2019-04-10 11:13:53,272] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95976: learning rate 0.0000
[2019-04-10 11:13:53,315] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95995: loss 2.8712
[2019-04-10 11:13:53,318] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95995: learning rate 0.0000
[2019-04-10 11:13:53,365] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96014: loss 1.9900
[2019-04-10 11:13:53,395] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96014: learning rate 0.0000
[2019-04-10 11:13:53,445] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96045: loss 1.9217
[2019-04-10 11:13:53,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96048: learning rate 0.0000
[2019-04-10 11:13:53,475] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96055: loss 2.2648
[2019-04-10 11:13:53,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96063: learning rate 0.0000
[2019-04-10 11:13:53,551] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96077: loss -34.8141
[2019-04-10 11:13:53,553] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96077: learning rate 0.0000
[2019-04-10 11:13:53,565] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96079: loss 1.6807
[2019-04-10 11:13:53,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96081: learning rate 0.0000
[2019-04-10 11:13:53,653] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96109: loss 1.8985
[2019-04-10 11:13:53,663] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96114: loss 2.2159
[2019-04-10 11:13:53,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96114: learning rate 0.0000
[2019-04-10 11:13:53,676] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96116: learning rate 0.0000
[2019-04-10 11:13:53,758] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96142: loss 2.1455
[2019-04-10 11:13:53,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96153: learning rate 0.0000
[2019-04-10 11:13:54,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1658014e-18 1.0000000e+00 8.8910116e-20 5.0628430e-17 2.1036870e-28], sum to 1.0000
[2019-04-10 11:13:54,071] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-10 11:13:54,095] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5328923514161268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744650.3766942059, 744650.3766942065, 188910.2693065414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3612600.0000, 
sim time next is 3613200.0000, 
raw observation next is [29.66666666666666, 71.33333333333333, 1.0, 2.0, 0.5301660293181135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740839.3542057478, 740839.3542057478, 188457.5771578448], 
processed observation next is [1.0, 0.8260869565217391, 0.6050552922590835, 0.7133333333333333, 1.0, 1.0, 0.43393497508206447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2057887095015966, 0.2057887095015966, 0.28127996590723103], 
reward next is 0.7187, 
noisyNet noise sample is [array([1.4000916], dtype=float32), 1.1966025]. 
=============================================
[2019-04-10 11:14:03,715] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.1057955e-15 1.0000000e+00 3.7791988e-18 5.1163924e-16 2.9354494e-26], sum to 1.0000
[2019-04-10 11:14:03,725] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1379
[2019-04-10 11:14:03,763] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 76.5, 1.0, 2.0, 0.6064170940601058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869819.2734582076, 869819.2734582076, 204832.4250598399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3731400.0000, 
sim time next is 3732000.0000, 
raw observation next is [26.33333333333334, 77.33333333333333, 1.0, 2.0, 0.6237971950812394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 896020.8228380596, 896020.822838059, 208406.2818289112], 
processed observation next is [1.0, 0.17391304347826086, 0.44707740916271754, 0.7733333333333333, 1.0, 1.0, 0.5467436085316137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2488946730105721, 0.24889467301057194, 0.31105415198344954], 
reward next is 0.6889, 
noisyNet noise sample is [array([-0.69391614], dtype=float32), -1.9067719]. 
=============================================
[2019-04-10 11:14:03,798] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[63.570637]
 [63.573586]
 [63.45828 ]
 [63.366287]
 [63.420547]], R is [[63.6027298 ]
 [63.66098022]
 [63.71733093]
 [63.76452255]
 [63.79573441]].
[2019-04-10 11:14:05,837] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 11:14:05,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:14:05,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:14:05,850] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:14:05,852] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:14:05,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-04-10 11:14:05,884] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:14:05,885] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:14:05,887] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run5
[2019-04-10 11:14:05,912] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run5
[2019-04-10 11:14:05,949] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:14:05,950] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:14:05,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:14:05,950] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:14:05,952] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run5
[2019-04-10 11:14:05,970] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run5
[2019-04-10 11:14:14,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1147068], dtype=float32), 0.09538008]
[2019-04-10 11:14:14,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.41666666666666, 81.16666666666667, 1.0, 2.0, 0.424845530409471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619244.0810161383, 619244.0810161376, 175943.4604925032]
[2019-04-10 11:14:14,560] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:14:14,562] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0153339e-17 1.0000000e+00 2.6334568e-20 3.7506130e-21 9.4275264e-29], sampled 0.340304149967239
[2019-04-10 11:14:18,629] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1147068], dtype=float32), 0.09538008]
[2019-04-10 11:14:18,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.16666666666667, 60.83333333333334, 1.0, 2.0, 0.284666257473961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463223.989279853, 463223.989279853, 164436.8025794788]
[2019-04-10 11:14:18,648] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:14:18,652] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3061514e-17 1.0000000e+00 6.6676222e-21 1.6997419e-21 1.2620424e-29], sampled 0.6911387450375625
[2019-04-10 11:15:02,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1147068], dtype=float32), 0.09538008]
[2019-04-10 11:15:02,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.33333333333334, 50.33333333333333, 1.0, 2.0, 0.7745099503382065, 1.0, 2.0, 0.7745099503382065, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2166008.467097922, 2166008.467097922, 407369.0047412007]
[2019-04-10 11:15:02,966] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:15:02,968] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.2836664e-10 8.6316548e-02 5.2859928e-10 9.1368341e-01 2.8152006e-16], sampled 0.06497075108558714
[2019-04-10 11:15:02,968] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2166008.467097922 W.
[2019-04-10 11:15:06,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1147068], dtype=float32), 0.09538008]
[2019-04-10 11:15:06,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.0, 48.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.190946214026279, 6.9112, 168.9117132339697, 1652350.976554784, 1453890.844970823, 311351.0342132759]
[2019-04-10 11:15:06,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:15:06,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2862658e-11 9.9999988e-01 9.9399232e-13 8.9464521e-08 1.5220949e-19], sampled 0.46005202884692664
[2019-04-10 11:15:33,533] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1147068], dtype=float32), 0.09538008]
[2019-04-10 11:15:33,534] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.06666666666667, 90.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.327236105579157, 6.9112, 168.9101513064815, 1751077.305485374, 1455931.875007568, 311664.9424131413]
[2019-04-10 11:15:33,536] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:15:33,537] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.03678776e-11 1.00000000e+00 6.82524729e-13 4.25663860e-09
 3.58490093e-20], sampled 0.6571319611314033
[2019-04-10 11:15:33,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1751077.305485374 W.
[2019-04-10 11:15:41,034] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1147068], dtype=float32), 0.09538008]
[2019-04-10 11:15:41,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.53333333333333, 85.66666666666667, 1.0, 2.0, 0.5722948888604464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799731.2195721212, 799731.2195721206, 195698.1135647669]
[2019-04-10 11:15:41,036] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:15:41,037] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1085307e-14 1.0000000e+00 1.0915266e-16 3.1801796e-13 2.3925128e-24], sampled 0.6917787603953272
[2019-04-10 11:15:45,394] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8058.7653 3146112555.7930 1291.0000
[2019-04-10 11:15:45,514] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8336.1559 2918890254.1514 1110.0000
[2019-04-10 11:15:45,713] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8563.1235 2835314147.6039 939.0000
[2019-04-10 11:15:45,915] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8156.8121 2990526337.8692 1333.0000
[2019-04-10 11:15:45,920] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8699.2551 2775352707.4520 827.0000
[2019-04-10 11:15:46,934] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 100000, evaluation results [100000.0, 8058.765272674846, 3146112555.792956, 1291.0, 8336.155901771648, 2918890254.151415, 1110.0, 8699.255051529388, 2775352707.451976, 827.0, 8156.81212018961, 2990526337.8691707, 1333.0, 8563.123452796079, 2835314147.6038685, 939.0]
[2019-04-10 11:15:53,106] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103800: loss 0.0386
[2019-04-10 11:15:53,111] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103800: learning rate 0.0000
[2019-04-10 11:15:53,133] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103813: loss 0.0383
[2019-04-10 11:15:53,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103814: learning rate 0.0000
[2019-04-10 11:15:53,162] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103833: loss 0.0365
[2019-04-10 11:15:53,163] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103833: learning rate 0.0000
[2019-04-10 11:15:53,192] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103853: loss 0.0371
[2019-04-10 11:15:53,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103853: learning rate 0.0000
[2019-04-10 11:15:53,206] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103862: loss 0.0377
[2019-04-10 11:15:53,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103862: learning rate 0.0000
[2019-04-10 11:15:53,238] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103877: loss 0.0364
[2019-04-10 11:15:53,240] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103877: learning rate 0.0000
[2019-04-10 11:15:53,303] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103920: loss 0.0375
[2019-04-10 11:15:53,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103920: learning rate 0.0000
[2019-04-10 11:15:53,335] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103940: loss 0.0349
[2019-04-10 11:15:53,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103942: learning rate 0.0000
[2019-04-10 11:15:53,352] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103948: loss 0.0336
[2019-04-10 11:15:53,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103948: learning rate 0.0000
[2019-04-10 11:15:53,470] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104025: loss 0.0335
[2019-04-10 11:15:53,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104025: learning rate 0.0000
[2019-04-10 11:15:53,520] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104050: loss 0.0305
[2019-04-10 11:15:53,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104050: learning rate 0.0000
[2019-04-10 11:15:53,555] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104069: loss 0.0316
[2019-04-10 11:15:53,556] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104070: learning rate 0.0000
[2019-04-10 11:15:53,691] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104156: loss 0.0306
[2019-04-10 11:15:53,692] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104156: learning rate 0.0000
[2019-04-10 11:15:53,727] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104177: loss 0.0319
[2019-04-10 11:15:53,728] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104177: learning rate 0.0000
[2019-04-10 11:15:53,728] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104177: loss 0.0327
[2019-04-10 11:15:53,730] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104177: learning rate 0.0000
[2019-04-10 11:15:53,923] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104295: loss 0.0308
[2019-04-10 11:15:53,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104295: learning rate 0.0000
[2019-04-10 11:16:01,601] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.6615804e-16 1.0000000e+00 1.7411027e-17 1.3940996e-15 4.6258877e-25], sum to 1.0000
[2019-04-10 11:16:01,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3935
[2019-04-10 11:16:01,611] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6571349986278583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918338.9833539705, 918338.9833539705, 211865.1391908308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [27.0, 89.83333333333334, 1.0, 2.0, 0.7336170904601088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1025273.464897259, 1025273.464897259, 228269.2253772327], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8983333333333334, 1.0, 1.0, 0.6790567354941069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28479818469368307, 0.28479818469368307, 0.34070033638392944], 
reward next is 0.6593, 
noisyNet noise sample is [array([-1.4089965], dtype=float32), 0.3304564]. 
=============================================
[2019-04-10 11:16:01,618] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.89069 ]
 [62.77    ]
 [62.842648]
 [62.825924]
 [62.686703]], R is [[62.70314407]
 [62.75989914]
 [62.80186844]
 [62.8483963 ]
 [62.90681458]].
[2019-04-10 11:16:04,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1278249e-15 1.0000000e+00 5.0697978e-18 4.5873367e-16 9.1090222e-25], sum to 1.0000
[2019-04-10 11:16:04,528] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0413
[2019-04-10 11:16:04,531] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.5780699792746443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807804.4637092317, 807804.4637092317, 196731.6418384442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4153800.0000, 
sim time next is 4154400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5793735008465478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809626.7203588246, 809626.7203588246, 196966.2900170572], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4932210853572865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2248963112107846, 0.2248963112107846, 0.2939795373388914], 
reward next is 0.7060, 
noisyNet noise sample is [array([-0.4699702], dtype=float32), 0.7695488]. 
=============================================
[2019-04-10 11:16:05,547] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3315520e-14 1.0000000e+00 4.5547296e-16 4.7858120e-13 4.3267814e-24], sum to 1.0000
[2019-04-10 11:16:05,555] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1022
[2019-04-10 11:16:05,561] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1703713.217283246 W.
[2019-04-10 11:16:05,564] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 81.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.263297578968324, 6.9112, 168.9109766956921, 1703713.217283246, 1453926.004269188, 311358.1073930356], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [31.33333333333334, 80.66666666666667, 1.0, 2.0, 0.538723416837185, 1.0, 1.0, 0.538723416837185, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1506126.490958303, 1506126.490958303, 312742.9420290756], 
processed observation next is [1.0, 0.30434782608695654, 0.6840442338072673, 0.8066666666666668, 1.0, 1.0, 0.44424508052672895, 1.0, 0.5, 0.44424508052672895, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.41836846971063973, 0.41836846971063973, 0.4667805104911576], 
reward next is 0.5332, 
noisyNet noise sample is [array([0.9624533], dtype=float32), -0.8672235]. 
=============================================
[2019-04-10 11:16:06,134] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111781: loss -107.8014
[2019-04-10 11:16:06,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111781: learning rate 0.0000
[2019-04-10 11:16:06,170] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111801: loss -107.7930
[2019-04-10 11:16:06,175] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111801: learning rate 0.0000
[2019-04-10 11:16:06,240] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111848: loss -107.8754
[2019-04-10 11:16:06,241] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111848: loss -107.6719
[2019-04-10 11:16:06,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111848: learning rate 0.0000
[2019-04-10 11:16:06,244] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111848: learning rate 0.0000
[2019-04-10 11:16:06,371] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111931: loss -108.0096
[2019-04-10 11:16:06,373] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111931: learning rate 0.0000
[2019-04-10 11:16:06,394] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111943: loss -107.6073
[2019-04-10 11:16:06,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111943: learning rate 0.0000
[2019-04-10 11:16:06,398] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111944: loss -107.6302
[2019-04-10 11:16:06,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111944: learning rate 0.0000
[2019-04-10 11:16:06,415] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111951: loss -107.7709
[2019-04-10 11:16:06,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111951: learning rate 0.0000
[2019-04-10 11:16:06,436] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111962: loss -107.8818
[2019-04-10 11:16:06,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111962: learning rate 0.0000
[2019-04-10 11:16:06,536] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112024: loss -110.0955
[2019-04-10 11:16:06,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112025: learning rate 0.0000
[2019-04-10 11:16:06,584] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112050: loss -107.3974
[2019-04-10 11:16:06,585] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112050: learning rate 0.0000
[2019-04-10 11:16:06,625] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112079: loss -133.5930
[2019-04-10 11:16:06,627] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112081: learning rate 0.0000
[2019-04-10 11:16:06,654] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112098: loss -107.3035
[2019-04-10 11:16:06,657] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112098: learning rate 0.0000
[2019-04-10 11:16:06,739] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112149: loss -107.0876
[2019-04-10 11:16:06,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112150: learning rate 0.0000
[2019-04-10 11:16:06,845] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112213: loss -106.7924
[2019-04-10 11:16:06,846] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112214: learning rate 0.0000
[2019-04-10 11:16:06,874] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112232: loss -109.3052
[2019-04-10 11:16:06,879] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112235: learning rate 0.0000
[2019-04-10 11:16:07,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5602858e-13 9.9999988e-01 1.2189511e-14 8.3162341e-08 1.9515065e-23], sum to 1.0000
[2019-04-10 11:16:07,148] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0427
[2019-04-10 11:16:07,153] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.33333333333334, 54.0, 1.0, 2.0, 0.5594064954922578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 781714.2084383141, 781714.2084383148, 193430.1125978555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4210800.0000, 
sim time next is 4211400.0000, 
raw observation next is [35.16666666666666, 55.0, 1.0, 2.0, 0.5658835846663457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 790768.6570487025, 790768.657048703, 194564.8954387562], 
processed observation next is [1.0, 0.7391304347826086, 0.865718799368088, 0.55, 1.0, 1.0, 0.476968174296802, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21965796029130624, 0.2196579602913064, 0.2903953663265018], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.003175], dtype=float32), -0.004604862]. 
=============================================
[2019-04-10 11:16:13,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8338520e-16 1.0000000e+00 1.8872752e-18 1.2390536e-15 2.0129365e-26], sum to 1.0000
[2019-04-10 11:16:13,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5917
[2019-04-10 11:16:13,865] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6263210589470625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875259.0952998261, 875259.0952998255, 205752.1866650157], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4320600.0000, 
sim time next is 4321200.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6230108680228896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870631.3374349982, 870631.3374349982, 205111.4622171202], 
processed observation next is [1.0, 0.0, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5457962265336019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2418420381763884, 0.2418420381763884, 0.3061365107718212], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.5455704], dtype=float32), 0.2800606]. 
=============================================
[2019-04-10 11:16:15,994] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4678710e-11 9.9999905e-01 4.7451899e-12 1.0105763e-06 1.5451138e-19], sum to 1.0000
[2019-04-10 11:16:16,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8246
[2019-04-10 11:16:16,024] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 87.33333333333334, 1.0, 2.0, 0.9635826644922642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510413, 1346867.857631928, 1346867.857631929, 288031.9736425935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4342800.0000, 
sim time next is 4343400.0000, 
raw observation next is [29.5, 86.5, 1.0, 2.0, 0.9740883625094849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1361561.833918472, 1361561.833918471, 291136.0962137178], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.865, 1.0, 1.0, 0.9687811596499819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3782116205329089, 0.3782116205329086, 0.434531486886146], 
reward next is 0.5655, 
noisyNet noise sample is [array([-0.21263972], dtype=float32), 0.19726712]. 
=============================================
[2019-04-10 11:16:21,186] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1281863e-15 1.0000000e+00 1.4764650e-18 1.9160251e-16 5.3264948e-26], sum to 1.0000
[2019-04-10 11:16:21,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5182
[2019-04-10 11:16:21,259] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.617992481989888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863615.5040085978, 863615.5040085978, 204146.0588657027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4410000.0000, 
sim time next is 4410600.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.618586644002948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864446.1554464725, 864446.1554464732, 204259.9302113154], 
processed observation next is [0.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5404658361481302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24012393206846458, 0.24012393206846477, 0.3048655674795752], 
reward next is 0.6951, 
noisyNet noise sample is [array([0.70649725], dtype=float32), -2.1677718]. 
=============================================
[2019-04-10 11:16:22,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2917823e-16 1.0000000e+00 2.4569105e-19 5.2929716e-19 2.3845093e-27], sum to 1.0000
[2019-04-10 11:16:22,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4585
[2019-04-10 11:16:22,147] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5821232008402634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813470.6709085847, 813470.6709085847, 197462.7016949702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4429200.0000, 
sim time next is 4429800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5814528363471861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812533.5317684455, 812533.5317684455, 197341.4723990394], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49572630885203145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2257037588245682, 0.2257037588245682, 0.29453951104334236], 
reward next is 0.7055, 
noisyNet noise sample is [array([-0.6219819], dtype=float32), -1.9892672]. 
=============================================
[2019-04-10 11:16:27,744] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119736: loss 0.0082
[2019-04-10 11:16:27,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119736: learning rate 0.0000
[2019-04-10 11:16:27,746] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 119736: loss 0.0086
[2019-04-10 11:16:27,747] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 119736: learning rate 0.0000
[2019-04-10 11:16:28,000] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119804: loss 0.0083
[2019-04-10 11:16:28,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119804: learning rate 0.0000
[2019-04-10 11:16:28,128] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119834: loss 0.0081
[2019-04-10 11:16:28,133] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119835: learning rate 0.0000
[2019-04-10 11:16:28,295] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119889: loss 0.0082
[2019-04-10 11:16:28,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119896: learning rate 0.0000
[2019-04-10 11:16:28,342] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119896: loss 0.0076
[2019-04-10 11:16:28,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119897: learning rate 0.0000
[2019-04-10 11:16:28,529] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119961: loss 0.0074
[2019-04-10 11:16:28,565] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119961: learning rate 0.0000
[2019-04-10 11:16:28,784] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120043: loss 0.0076
[2019-04-10 11:16:28,788] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120044: learning rate 0.0000
[2019-04-10 11:16:28,800] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120044: loss 0.0080
[2019-04-10 11:16:28,801] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120044: learning rate 0.0000
[2019-04-10 11:16:28,891] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120092: loss 0.0075
[2019-04-10 11:16:28,908] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120099: loss 0.0081
[2019-04-10 11:16:28,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120097: learning rate 0.0000
[2019-04-10 11:16:28,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120099: learning rate 0.0000
[2019-04-10 11:16:28,934] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120108: loss 0.0083
[2019-04-10 11:16:28,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120108: learning rate 0.0000
[2019-04-10 11:16:28,966] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120117: loss 0.0086
[2019-04-10 11:16:28,967] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120117: learning rate 0.0000
[2019-04-10 11:16:29,181] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120177: loss 0.0089
[2019-04-10 11:16:29,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120177: learning rate 0.0000
[2019-04-10 11:16:29,192] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120179: loss 0.0074
[2019-04-10 11:16:29,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120182: learning rate 0.0000
[2019-04-10 11:16:29,423] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120259: loss 0.0072
[2019-04-10 11:16:29,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120259: learning rate 0.0000
[2019-04-10 11:16:30,118] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5456593e-17 1.0000000e+00 1.1992075e-20 2.4758742e-22 1.8149231e-29], sum to 1.0000
[2019-04-10 11:16:30,118] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2365
[2019-04-10 11:16:30,149] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.4890695399877618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683393.7833797967, 683393.7833797967, 181900.7411829323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4508400.0000, 
sim time next is 4509000.0000, 
raw observation next is [26.0, 86.5, 1.0, 2.0, 0.4916380989483277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686984.0802107382, 686984.0802107382, 182295.9628613867], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.865, 1.0, 1.0, 0.3875157818654551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1908289111696495, 0.1908289111696495, 0.27208352665878616], 
reward next is 0.7279, 
noisyNet noise sample is [array([0.13107616], dtype=float32), -1.2275715]. 
=============================================
[2019-04-10 11:16:30,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.20859]
 [71.20452]
 [71.20538]
 [71.20292]
 [71.27198]], R is [[71.18249512]
 [71.1991806 ]
 [71.21616364]
 [71.23305511]
 [71.24878693]].
[2019-04-10 11:16:44,046] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-10 11:16:44,051] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:16:44,052] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:16:44,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:16:44,058] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:16:44,057] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:16:44,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:16:44,058] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:16:44,059] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:16:44,059] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:16:44,059] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:16:44,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-04-10 11:16:44,062] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run6
[2019-04-10 11:16:44,091] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run6
[2019-04-10 11:16:44,106] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run6
[2019-04-10 11:16:44,123] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run6
[2019-04-10 11:16:55,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11667755], dtype=float32), 0.095939085]
[2019-04-10 11:16:55,622] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.44068623, 90.49967418, 1.0, 2.0, 0.2141201830840477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 357390.8620096584, 357390.8620096577, 156837.0693641643]
[2019-04-10 11:16:55,624] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:16:55,626] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0932353e-18 1.0000000e+00 7.9535637e-22 2.8303077e-24 1.3252902e-30], sampled 0.22960924846594533
[2019-04-10 11:16:57,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11667755], dtype=float32), 0.095939085]
[2019-04-10 11:16:57,247] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.38333333333333, 74.5, 1.0, 2.0, 0.3368884353347485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 523128.3034570876, 523128.303457087, 168577.4086057959]
[2019-04-10 11:16:57,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:16:57,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.0622934e-18 1.0000000e+00 2.2770235e-21 3.2868751e-23 4.4802874e-30], sampled 0.9963632784121934
[2019-04-10 11:17:28,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11667755], dtype=float32), 0.095939085]
[2019-04-10 11:17:28,552] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.78824344, 84.12307983333334, 1.0, 2.0, 0.5128820750944253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716679.0867772667, 716679.0867772674, 185638.4288122678]
[2019-04-10 11:17:28,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:17:28,555] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1409765e-17 1.0000000e+00 7.1129067e-21 1.1348710e-21 1.4208590e-29], sampled 0.8984158887817175
[2019-04-10 11:17:30,530] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11667755], dtype=float32), 0.095939085]
[2019-04-10 11:17:30,530] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3494694749731769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538347.7601012872, 538347.7601012879, 169680.8413601649]
[2019-04-10 11:17:30,532] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:17:30,533] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1695419e-17 1.0000000e+00 1.6667014e-20 3.4509979e-21 5.3101145e-29], sampled 0.15020683832555348
[2019-04-10 11:17:52,153] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11667755], dtype=float32), 0.095939085]
[2019-04-10 11:17:52,292] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.87010578833333, 72.447618525, 1.0, 2.0, 0.5721020065487846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799461.5822565866, 799461.5822565866, 195661.6474253914]
[2019-04-10 11:17:52,294] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:17:52,297] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8244072e-17 1.0000000e+00 2.2408762e-20 3.7147449e-21 7.5626996e-29], sampled 0.7557255551769381
[2019-04-10 11:17:53,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11667755], dtype=float32), 0.095939085]
[2019-04-10 11:17:53,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.7293091, 62.71494601, 1.0, 2.0, 0.469052779892406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 670911.7100910234, 670911.7100910228, 180873.0224205409]
[2019-04-10 11:17:53,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:17:53,151] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2801573e-17 1.0000000e+00 1.6831923e-20 2.5997382e-21 5.3297999e-29], sampled 0.49461612198377447
[2019-04-10 11:18:21,160] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8670.1943 2778250776.2863 903.0000
[2019-04-10 11:18:21,234] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7973.4915 3154365754.9282 1519.0000
[2019-04-10 11:18:21,239] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8076.8693 2998421277.3835 1537.0000
[2019-04-10 11:18:21,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8292.1682 2923337716.7394 1230.0000
[2019-04-10 11:18:21,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8526.8212 2838818460.7104 1042.0000
[2019-04-10 11:18:22,364] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 125000, evaluation results [125000.0, 7973.491546542115, 3154365754.9282184, 1519.0, 8292.168195469912, 2923337716.7394176, 1230.0, 8670.19433476632, 2778250776.286274, 903.0, 8076.869344024449, 2998421277.3835497, 1537.0, 8526.82120457481, 2838818460.7104473, 1042.0]
[2019-04-10 11:18:26,510] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127617: loss -176.1648
[2019-04-10 11:18:26,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127620: learning rate 0.0000
[2019-04-10 11:18:26,692] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127731: loss -205.5947
[2019-04-10 11:18:26,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127732: learning rate 0.0000
[2019-04-10 11:18:26,925] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127864: loss -285.5142
[2019-04-10 11:18:26,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127865: learning rate 0.0000
[2019-04-10 11:18:26,964] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127891: loss -284.8391
[2019-04-10 11:18:26,967] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127891: learning rate 0.0000
[2019-04-10 11:18:26,981] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127900: loss -337.7275
[2019-04-10 11:18:26,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127900: learning rate 0.0000
[2019-04-10 11:18:27,004] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127909: loss -320.1638
[2019-04-10 11:18:27,005] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127910: loss -244.4607
[2019-04-10 11:18:27,006] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127909: learning rate 0.0000
[2019-04-10 11:18:27,008] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127911: learning rate 0.0000
[2019-04-10 11:18:27,076] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0432365e-10 3.7120515e-01 1.1260386e-09 6.2879479e-01 1.7257253e-15], sum to 1.0000
[2019-04-10 11:18:27,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-10 11:18:27,086] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333333, 65.0, 1.0, 2.0, 0.8701591187801172, 1.0, 2.0, 0.8701591187801172, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2433748.604068491, 2433748.604068492, 455469.7128570908], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4812000.0000, 
sim time next is 4812600.0000, 
raw observation next is [31.16666666666667, 65.5, 1.0, 2.0, 0.862658847049602, 1.0, 2.0, 0.862658847049602, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2412750.837196485, 2412750.837196486, 451525.5173035601], 
processed observation next is [1.0, 0.6956521739130435, 0.6761453396524489, 0.655, 1.0, 1.0, 0.8345287313850627, 1.0, 1.0, 0.8345287313850627, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6702085658879126, 0.6702085658879128, 0.673918682542627], 
reward next is 0.3261, 
noisyNet noise sample is [array([-1.2096691], dtype=float32), 1.0717138]. 
=============================================
[2019-04-10 11:18:27,144] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127993: loss -165.2814
[2019-04-10 11:18:27,146] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127994: learning rate 0.0000
[2019-04-10 11:18:27,180] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128010: loss -378.6822
[2019-04-10 11:18:27,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128010: learning rate 0.0000
[2019-04-10 11:18:27,219] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128036: loss -177.1561
[2019-04-10 11:18:27,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128036: learning rate 0.0000
[2019-04-10 11:18:27,236] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128049: loss -293.0835
[2019-04-10 11:18:27,239] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128049: learning rate 0.0000
[2019-04-10 11:18:27,262] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128065: loss -234.2165
[2019-04-10 11:18:27,263] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128065: learning rate 0.0000
[2019-04-10 11:18:27,424] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128161: loss -209.1326
[2019-04-10 11:18:27,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128161: learning rate 0.0000
[2019-04-10 11:18:27,452] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128177: loss -178.2862
[2019-04-10 11:18:27,453] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128177: learning rate 0.0000
[2019-04-10 11:18:27,518] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128220: loss -270.4186
[2019-04-10 11:18:27,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128223: learning rate 0.0000
[2019-04-10 11:18:27,683] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128319: loss -315.6830
[2019-04-10 11:18:27,685] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128319: learning rate 0.0000
[2019-04-10 11:18:36,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6442480e-18 1.0000000e+00 1.6996445e-21 1.5554400e-20 1.0079244e-29], sum to 1.0000
[2019-04-10 11:18:36,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6042
[2019-04-10 11:18:36,212] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.0, 1.0, 2.0, 0.4954159881818189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692264.7845223047, 692264.7845223047, 182880.5298998326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4999800.0000, 
sim time next is 5000400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4897932312503533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684405.3479702373, 684405.3479702373, 182011.835797461], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.79, 1.0, 1.0, 0.38529304969922085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19011259665839925, 0.19011259665839925, 0.2716594564141209], 
reward next is 0.7283, 
noisyNet noise sample is [array([0.9575923], dtype=float32), -0.599814]. 
=============================================
[2019-04-10 11:18:36,580] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.9301734e-18 1.0000000e+00 1.2412455e-20 1.9597520e-21 4.2257509e-29], sum to 1.0000
[2019-04-10 11:18:36,590] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9407
[2019-04-10 11:18:36,594] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.4807277927597731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671733.8860906981, 671733.8860906975, 180631.5584659869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5019600.0000, 
sim time next is 5020200.0000, 
raw observation next is [26.0, 86.5, 1.0, 2.0, 0.4841421065163933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676506.317761109, 676506.3177611083, 181148.8198019235], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.865, 1.0, 1.0, 0.37848446568240157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18791842160030806, 0.18791842160030786, 0.2703713728386918], 
reward next is 0.7296, 
noisyNet noise sample is [array([0.2997948], dtype=float32), -0.7095942]. 
=============================================
[2019-04-10 11:18:39,398] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 135501: loss 4.6787
[2019-04-10 11:18:39,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 135501: learning rate 0.0000
[2019-04-10 11:18:39,563] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135598: loss 4.6526
[2019-04-10 11:18:39,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135599: learning rate 0.0000
[2019-04-10 11:18:40,009] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135871: loss 4.4791
[2019-04-10 11:18:40,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135871: learning rate 0.0000
[2019-04-10 11:18:40,019] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135876: loss 4.5060
[2019-04-10 11:18:40,021] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135876: loss 4.5306
[2019-04-10 11:18:40,021] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135876: learning rate 0.0000
[2019-04-10 11:18:40,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135877: learning rate 0.0000
[2019-04-10 11:18:40,089] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135924: loss 4.4703
[2019-04-10 11:18:40,098] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135925: learning rate 0.0000
[2019-04-10 11:18:40,116] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135938: loss 4.4867
[2019-04-10 11:18:40,119] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135938: learning rate 0.0000
[2019-04-10 11:18:40,249] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136016: loss 4.4141
[2019-04-10 11:18:40,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136017: learning rate 0.0000
[2019-04-10 11:18:40,287] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136035: loss 4.4502
[2019-04-10 11:18:40,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136036: learning rate 0.0000
[2019-04-10 11:18:40,308] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136048: loss 4.4157
[2019-04-10 11:18:40,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136050: learning rate 0.0000
[2019-04-10 11:18:40,339] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136067: loss 4.4341
[2019-04-10 11:18:40,342] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136067: learning rate 0.0000
[2019-04-10 11:18:40,374] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136093: loss 4.4352
[2019-04-10 11:18:40,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136093: learning rate 0.0000
[2019-04-10 11:18:40,522] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136179: loss 4.3964
[2019-04-10 11:18:40,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136179: learning rate 0.0000
[2019-04-10 11:18:40,536] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136186: loss 4.3760
[2019-04-10 11:18:40,537] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136186: learning rate 0.0000
[2019-04-10 11:18:40,644] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136252: loss 4.3649
[2019-04-10 11:18:40,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136252: learning rate 0.0000
[2019-04-10 11:18:40,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.7709122e-18 1.0000000e+00 6.1932761e-21 3.5514117e-23 2.0567096e-30], sum to 1.0000
[2019-04-10 11:18:40,730] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2474
[2019-04-10 11:18:40,736] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4814376300807592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 672851.568051295, 672851.5680512957, 180754.442465774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5115600.0000, 
sim time next is 5116200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4811471539177737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672445.3420356801, 672445.3420356801, 180710.5309765235], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3748760890575587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18679037278768892, 0.18679037278768892, 0.2697172104127216], 
reward next is 0.7303, 
noisyNet noise sample is [array([0.354505], dtype=float32), 0.32425374]. 
=============================================
[2019-04-10 11:18:40,778] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136334: loss 4.3692
[2019-04-10 11:18:40,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136334: learning rate 0.0000
[2019-04-10 11:18:47,396] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1527499e-16 1.0000000e+00 2.0393782e-18 4.7399626e-17 3.5058695e-26], sum to 1.0000
[2019-04-10 11:18:47,396] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8519
[2019-04-10 11:18:47,474] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 83.16666666666667, 1.0, 2.0, 0.7675019187544418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1072653.442791051, 1072653.442791051, 236088.4467856895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5206200.0000, 
sim time next is 5206800.0000, 
raw observation next is [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.7551541827924095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1055387.78787028, 1055387.78787028, 233199.0512750815], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.8233333333333335, 1.0, 1.0, 0.7050050395089271, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2931632744084111, 0.2931632744084111, 0.34805828548519624], 
reward next is 0.6519, 
noisyNet noise sample is [array([-0.43467173], dtype=float32), -0.5085219]. 
=============================================
[2019-04-10 11:18:47,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8762530e-16 1.0000000e+00 5.9488007e-19 1.5882640e-17 1.6561628e-26], sum to 1.0000
[2019-04-10 11:18:47,765] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2719
[2019-04-10 11:18:47,770] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.7764288341535345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1085135.990213606, 1085135.990213606, 238205.908052178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5208600.0000, 
sim time next is 5209200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.8338853739478573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1165481.24948663, 1165481.24948663, 252397.1261728504], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.79, 1.0, 1.0, 0.7998618963227196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32374479152406394, 0.32374479152406394, 0.37671212861619463], 
reward next is 0.6233, 
noisyNet noise sample is [array([-0.88135874], dtype=float32), 0.32567504]. 
=============================================
[2019-04-10 11:18:52,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3943138e-19 1.0000000e+00 6.6760433e-22 6.1287188e-20 2.5109519e-30], sum to 1.0000
[2019-04-10 11:18:52,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9907
[2019-04-10 11:18:52,212] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 78.66666666666667, 1.0, 2.0, 0.54229619340932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757795.756466966, 757795.7564669666, 190488.6051340579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5254800.0000, 
sim time next is 5255400.0000, 
raw observation next is [28.81666666666667, 78.83333333333334, 1.0, 2.0, 0.5452327300927857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761900.6960647892, 761900.6960647892, 190986.6117764753], 
processed observation next is [1.0, 0.8260869565217391, 0.5647709320695105, 0.7883333333333334, 1.0, 1.0, 0.4520876266178141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2116390822402192, 0.2116390822402192, 0.28505464444250045], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.5503112], dtype=float32), -1.751731]. 
=============================================
[2019-04-10 11:18:56,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9443778e-12 1.2873100e-04 1.5941005e-11 9.9987125e-01 3.0789598e-17], sum to 1.0000
[2019-04-10 11:18:56,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1134
[2019-04-10 11:18:56,084] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 1.00460144972611, 1.0, 2.0, 1.00460144972611, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2810193.32930466, 2810193.329304659, 531778.4429118078], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5321400.0000, 
sim time next is 5322000.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 6.918500603190168, 6.9112, 170.5573041426782, 2914565.579657204, 2909335.860622535, 553596.1879160126], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.0007300603190167898, 0.0, 0.8375144448122397, 0.8096015499047788, 0.8081488501729264, 0.8262629670388247], 
reward next is 0.1372, 
noisyNet noise sample is [array([-0.06062964], dtype=float32), -0.22045879]. 
=============================================
[2019-04-10 11:18:56,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[47.547302]
 [46.076096]
 [47.64647 ]
 [47.418053]
 [47.46474 ]], R is [[48.03995895]
 [47.55955887]
 [47.0839653 ]
 [46.61312485]
 [46.14699554]].
[2019-04-10 11:19:02,752] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 143514: loss -81.0398
[2019-04-10 11:19:02,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 143514: learning rate 0.0000
[2019-04-10 11:19:03,305] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 143709: loss -73.2096
[2019-04-10 11:19:03,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 143709: learning rate 0.0000
[2019-04-10 11:19:03,515] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143767: loss -92.8647
[2019-04-10 11:19:03,516] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143767: loss -95.3238
[2019-04-10 11:19:03,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143770: learning rate 0.0000
[2019-04-10 11:19:03,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143770: learning rate 0.0000
[2019-04-10 11:19:03,825] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143865: loss -95.3590
[2019-04-10 11:19:03,826] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143865: learning rate 0.0000
[2019-04-10 11:19:03,866] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143874: loss -77.0035
[2019-04-10 11:19:03,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143875: learning rate 0.0000
[2019-04-10 11:19:03,995] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143917: loss -60.8428
[2019-04-10 11:19:04,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143917: learning rate 0.0000
[2019-04-10 11:19:04,116] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143957: loss -111.4401
[2019-04-10 11:19:04,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143957: learning rate 0.0000
[2019-04-10 11:19:04,436] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144038: loss -105.9431
[2019-04-10 11:19:04,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144038: learning rate 0.0000
[2019-04-10 11:19:04,496] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144046: loss -83.7280
[2019-04-10 11:19:04,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144049: learning rate 0.0000
[2019-04-10 11:19:04,662] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144107: loss -90.1327
[2019-04-10 11:19:04,663] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144107: learning rate 0.0000
[2019-04-10 11:19:04,803] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144144: loss -73.4902
[2019-04-10 11:19:04,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144146: learning rate 0.0000
[2019-04-10 11:19:04,892] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144178: loss -108.5775
[2019-04-10 11:19:04,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144178: learning rate 0.0000
[2019-04-10 11:19:04,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.21059872e-11 8.66996590e-04 1.07358514e-11 9.99133050e-01
 5.29414055e-17], sum to 1.0000
[2019-04-10 11:19:04,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4116
[2019-04-10 11:19:04,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3528010.904760936 W.
[2019-04-10 11:19:05,007] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.95, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.773863981339918, 6.9112, 170.5573041426782, 3528010.904760936, 2910049.603999966, 548757.6816147923], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5416200.0000, 
sim time next is 5416800.0000, 
raw observation next is [32.26666666666667, 65.0, 1.0, 2.0, 0.8579084139506654, 1.0, 2.0, 0.7495442464895953, 1.0, 1.0, 1.03, 7.005110186392388, 6.9112, 170.5573041426782, 3145496.131552853, 3078224.44501784, 575843.7201932253], 
processed observation next is [1.0, 0.6956521739130435, 0.7282780410742499, 0.65, 1.0, 1.0, 0.8288053180128498, 1.0, 1.0, 0.6982460801079461, 1.0, 0.5, 1.0365853658536586, 0.009391018639238791, 0.0, 0.8375144448122397, 0.8737489254313481, 0.8550623458382888, 0.8594682390943661], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.305164], dtype=float32), 0.056016162]. 
=============================================
[2019-04-10 11:19:05,062] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144224: loss -82.4008
[2019-04-10 11:19:05,092] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144226: learning rate 0.0000
[2019-04-10 11:19:05,358] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144310: loss -102.9231
[2019-04-10 11:19:05,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144310: learning rate 0.0000
[2019-04-10 11:19:05,741] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144447: loss -95.1745
[2019-04-10 11:19:05,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144447: learning rate 0.0000
[2019-04-10 11:19:08,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.6244418e-15 1.0000000e+00 3.3738302e-16 8.3002995e-14 8.9719446e-24], sum to 1.0000
[2019-04-10 11:19:08,901] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6991
[2019-04-10 11:19:08,918] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.61666666666667, 92.0, 1.0, 2.0, 0.8805637317985187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564951003, 1230759.144096507, 1230759.144096508, 264650.0432139775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5460600.0000, 
sim time next is 5461200.0000, 
raw observation next is [27.6, 92.0, 1.0, 2.0, 1.038482639743257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104272, 1451632.448111555, 1451632.448111555, 310873.2560595723], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.92, 1.0, 1.0, 1.046364626196695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439945152284, 0.40323123558654306, 0.40323123558654306, 0.46398993441727204], 
reward next is 0.5360, 
noisyNet noise sample is [array([0.6726325], dtype=float32), 1.3003669]. 
=============================================
[2019-04-10 11:19:10,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4894893e-12 1.0000000e+00 1.2973795e-13 5.3273003e-10 2.0053244e-20], sum to 1.0000
[2019-04-10 11:19:10,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-10 11:19:10,154] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.934411804773425, 6.9112, 168.9126504088846, 1470233.42846847, 1453766.205115248, 311353.910216804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5454600.0000, 
sim time next is 5455200.0000, 
raw observation next is [27.83333333333334, 92.0, 1.0, 2.0, 0.9808559931284353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129376104057, 1371027.605375793, 1371027.605375794, 293150.6588595025], 
processed observation next is [1.0, 0.13043478260869565, 0.5181674565560824, 0.92, 1.0, 1.0, 0.9769349314800425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439852344526, 0.3808410014932759, 0.3808410014932761, 0.43753829680522766], 
reward next is 0.5625, 
noisyNet noise sample is [array([-0.06412891], dtype=float32), 0.2809301]. 
=============================================
[2019-04-10 11:19:18,322] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 11:19:18,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:19:18,323] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:19:18,325] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:19:18,326] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:19:18,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:19:18,327] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:19:18,328] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:19:18,329] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:19:18,328] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:19:18,330] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:19:18,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-04-10 11:19:18,346] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run7
[2019-04-10 11:19:18,347] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run7
[2019-04-10 11:19:18,413] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run7
[2019-04-10 11:19:18,473] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run7
[2019-04-10 11:19:20,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:19:20,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.91516509666667, 68.22562404333334, 1.0, 2.0, 0.486230959497601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717731.6843125867, 717731.6843125867, 186261.2129511277]
[2019-04-10 11:19:20,660] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:19:20,662] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4277753e-18 1.0000000e+00 2.5675888e-21 1.2505214e-22 9.5355583e-30], sampled 0.35165723232322976
[2019-04-10 11:19:34,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:19:34,200] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.8, 95.33333333333334, 1.0, 2.0, 0.4473531048899405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643523.4439464976, 643523.4439464976, 178123.9179750969]
[2019-04-10 11:19:34,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:19:34,211] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2730203e-18 1.0000000e+00 9.2246057e-22 1.1114685e-23 2.4976539e-30], sampled 0.47196379514521025
[2019-04-10 11:20:12,869] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:20:12,871] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.0, 56.0, 1.0, 2.0, 0.5984022742334975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836228.3306087685, 836228.3306087685, 200449.1914506043]
[2019-04-10 11:20:12,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:20:12,874] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0007462e-18 1.0000000e+00 2.8367000e-21 6.6598914e-22 8.9362578e-30], sampled 0.4083828413464178
[2019-04-10 11:20:17,315] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:20:17,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.16666666666666, 66.33333333333333, 1.0, 2.0, 0.5546009538050467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774996.4983625632, 774996.4983625638, 192595.2098592759]
[2019-04-10 11:20:17,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:20:17,319] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.0337237e-16 1.0000000e+00 3.4018450e-18 6.3460148e-13 3.1494936e-26], sampled 0.6577517766351745
[2019-04-10 11:20:18,528] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:20:18,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5025154840471426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702188.4758776496, 702188.4758776496, 183990.9712561955]
[2019-04-10 11:20:18,530] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:20:18,533] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.9870060e-18 1.0000000e+00 2.6412725e-21 5.8862605e-23 9.1948470e-30], sampled 0.7380969154277939
[2019-04-10 11:20:40,157] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:20:40,157] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 92.0, 1.0, 2.0, 0.5015317832001891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700813.4510792204, 700813.4510792197, 183835.4191295961]
[2019-04-10 11:20:40,158] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:20:40,158] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4453840e-17 1.0000000e+00 1.8829515e-20 7.2041955e-21 1.2656637e-28], sampled 0.4624358096354253
[2019-04-10 11:20:41,254] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11801893], dtype=float32), 0.09636653]
[2019-04-10 11:20:41,255] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.97507826833333, 58.51012867166666, 1.0, 2.0, 0.3836327998882491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582887.3669114357, 582887.3669114357, 173278.0220983095]
[2019-04-10 11:20:41,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:20:41,260] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.4043807e-18 1.0000000e+00 1.4129895e-21 4.6058546e-23 4.5288476e-30], sampled 0.11314446519989885
[2019-04-10 11:20:52,660] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7932.5252 3157833670.9727 1620.0000
[2019-04-10 11:20:52,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8665.1602 2778631186.5721 916.0000
[2019-04-10 11:20:52,759] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8510.2714 2840962697.4867 1096.0000
[2019-04-10 11:20:52,860] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8275.8696 2925657359.0962 1281.0000
[2019-04-10 11:20:52,877] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8030.2297 3003170060.5548 1646.0000
[2019-04-10 11:20:53,889] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 150000, evaluation results [150000.0, 7932.525226167435, 3157833670.9727244, 1620.0, 8275.869636010091, 2925657359.09623, 1281.0, 8665.160219261663, 2778631186.572092, 916.0, 8030.229737701739, 3003170060.554761, 1646.0, 8510.271393099629, 2840962697.486669, 1096.0]
[2019-04-10 11:20:54,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9152453e-18 1.0000000e+00 1.2119319e-21 1.2020734e-22 1.3292293e-30], sum to 1.0000
[2019-04-10 11:20:54,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3711
[2019-04-10 11:20:54,746] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 73.33333333333334, 1.0, 2.0, 0.5421571164561202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757601.3433202925, 757601.3433202932, 190464.949132379], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5649000.0000, 
sim time next is 5649600.0000, 
raw observation next is [29.83333333333334, 72.66666666666667, 1.0, 2.0, 0.541256075815571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756341.7958374814, 756341.795837482, 190312.8282231646], 
processed observation next is [0.0, 0.391304347826087, 0.6129541864139023, 0.7266666666666667, 1.0, 1.0, 0.4472964768862301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21009494328818928, 0.21009494328818942, 0.28404899734800687], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.36864775], dtype=float32), 0.7839631]. 
=============================================
[2019-04-10 11:20:55,325] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9845809e-19 1.0000000e+00 4.9940636e-22 9.0635214e-23 2.9223808e-31], sum to 1.0000
[2019-04-10 11:20:55,331] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4970
[2019-04-10 11:20:55,339] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333333, 60.0, 1.0, 2.0, 0.5424108588292609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757956.0452014727, 757956.0452014727, 190508.1461050543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5669400.0000, 
sim time next is 5670000.0000, 
raw observation next is [32.3, 60.0, 1.0, 2.0, 0.5416232201597805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756855.0197158619, 756855.0197158626, 190374.9048115258], 
processed observation next is [0.0, 0.6521739130434783, 0.7298578199052131, 0.6, 1.0, 1.0, 0.447738819469615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2102375054766283, 0.2102375054766285, 0.28414164897242655], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.29969457], dtype=float32), -0.95371956]. 
=============================================
[2019-04-10 11:20:55,353] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.21526 ]
 [73.17064 ]
 [73.11917 ]
 [73.0696  ]
 [73.018364]], R is [[73.28432465]
 [73.26714325]
 [73.24987793]
 [73.23246765]
 [73.21466064]].
[2019-04-10 11:20:56,002] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 151360: loss 0.0040
[2019-04-10 11:20:56,005] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 151361: learning rate 0.0000
[2019-04-10 11:20:56,484] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 151654: loss 0.0056
[2019-04-10 11:20:56,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 151655: learning rate 0.0000
[2019-04-10 11:20:56,648] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151759: loss 0.0063
[2019-04-10 11:20:56,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151759: learning rate 0.0000
[2019-04-10 11:20:56,729] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 151810: loss 0.0059
[2019-04-10 11:20:56,731] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 151810: learning rate 0.0000
[2019-04-10 11:20:56,853] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151880: loss 0.0057
[2019-04-10 11:20:56,856] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151883: learning rate 0.0000
[2019-04-10 11:20:56,887] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151905: loss 0.0054
[2019-04-10 11:20:56,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151906: learning rate 0.0000
[2019-04-10 11:20:56,919] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151921: loss 0.0051
[2019-04-10 11:20:56,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151921: learning rate 0.0000
[2019-04-10 11:20:56,946] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151935: loss 0.0042
[2019-04-10 11:20:56,949] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151935: learning rate 0.0000
[2019-04-10 11:20:57,090] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152023: loss 0.0042
[2019-04-10 11:20:57,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152024: learning rate 0.0000
[2019-04-10 11:20:57,155] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152066: loss 0.0037
[2019-04-10 11:20:57,157] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152066: learning rate 0.0000
[2019-04-10 11:20:57,163] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152069: loss 0.0041
[2019-04-10 11:20:57,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152070: learning rate 0.0000
[2019-04-10 11:20:57,273] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152134: loss 0.0037
[2019-04-10 11:20:57,274] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152134: learning rate 0.0000
[2019-04-10 11:20:57,338] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152175: loss 0.0032
[2019-04-10 11:20:57,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152175: learning rate 0.0000
[2019-04-10 11:20:57,508] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152278: loss 0.0026
[2019-04-10 11:20:57,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152279: learning rate 0.0000
[2019-04-10 11:20:57,624] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152351: loss 0.0022
[2019-04-10 11:20:57,627] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152351: learning rate 0.0000
[2019-04-10 11:20:57,792] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152451: loss 0.0017
[2019-04-10 11:20:57,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152452: learning rate 0.0000
[2019-04-10 11:20:59,337] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6510836e-18 1.0000000e+00 4.9070258e-22 1.6378949e-23 1.7624480e-30], sum to 1.0000
[2019-04-10 11:20:59,341] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2504
[2019-04-10 11:20:59,345] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.8, 53.0, 1.0, 2.0, 0.530629518880388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741487.2477603299, 741487.2477603304, 188535.2745747115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5755800.0000, 
sim time next is 5756400.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5359444086305802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748916.7481963548, 748916.7481963541, 189420.552626323], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 1.0, 1.0, 0.44089687786816895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.208032430054543, 0.2080324300545428, 0.28271724272585524], 
reward next is 0.7173, 
noisyNet noise sample is [array([-1.3723315], dtype=float32), 1.5933036]. 
=============================================
[2019-04-10 11:21:02,106] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4730294e-15 1.0000000e+00 1.2183922e-16 3.1766619e-14 2.1997697e-23], sum to 1.0000
[2019-04-10 11:21:02,111] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6053
[2019-04-10 11:21:02,115] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 93.16666666666667, 1.0, 2.0, 0.9362284334907864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104304, 1308609.343351201, 1308609.343351202, 280096.8980060474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5803800.0000, 
sim time next is 5804400.0000, 
raw observation next is [26.1, 93.33333333333334, 1.0, 2.0, 0.8847047730610226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1236550.425424683, 1236550.425424683, 265764.2286518241], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.9333333333333335, 1.0, 1.0, 0.8610900880253284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3434862292846341, 0.3434862292846341, 0.39666302783854346], 
reward next is 0.6033, 
noisyNet noise sample is [array([-0.1488394], dtype=float32), 0.23180538]. 
=============================================
[2019-04-10 11:21:06,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9025457e-09 9.9688196e-01 1.4849634e-09 3.1181064e-03 2.6434814e-15], sum to 1.0000
[2019-04-10 11:21:06,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0282
[2019-04-10 11:21:06,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2294462.710846333 W.
[2019-04-10 11:21:06,588] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.35, 77.33333333333333, 1.0, 2.0, 0.8204047092157479, 1.0, 2.0, 0.8204047092157479, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2294462.710846333, 2294462.710846333, 429933.7416670003], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5908200.0000, 
sim time next is 5908800.0000, 
raw observation next is [30.5, 76.66666666666667, 1.0, 2.0, 0.5949844383059492, 1.0, 2.0, 0.5949844383059492, 1.0, 1.0, 1.03, 6.914899781560131, 6.9112, 170.5573041426782, 2496230.890698977, 2493580.586669964, 486167.0970534913], 
processed observation next is [1.0, 0.391304347826087, 0.6445497630331753, 0.7666666666666667, 1.0, 1.0, 0.5120294437421073, 1.0, 1.0, 0.5120294437421073, 1.0, 0.5, 1.0365853658536586, 0.0003699781560131399, 0.0, 0.8375144448122397, 0.6933974696386047, 0.6926612740749899, 0.7256225329156586], 
reward next is 0.2559, 
noisyNet noise sample is [array([-0.5326027], dtype=float32), 0.49430373]. 
=============================================
[2019-04-10 11:21:09,192] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159425: loss 21.0171
[2019-04-10 11:21:09,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159425: learning rate 0.0000
[2019-04-10 11:21:09,526] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 159631: loss 4.5657
[2019-04-10 11:21:09,527] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 159631: learning rate 0.0000
[2019-04-10 11:21:09,703] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159742: loss 66.1315
[2019-04-10 11:21:09,706] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159742: learning rate 0.0000
[2019-04-10 11:21:09,893] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159852: loss 1.7213
[2019-04-10 11:21:09,894] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159853: learning rate 0.0000
[2019-04-10 11:21:09,927] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159871: loss -27.7412
[2019-04-10 11:21:09,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159872: learning rate 0.0000
[2019-04-10 11:21:10,009] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159920: loss -168.1099
[2019-04-10 11:21:10,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159920: learning rate 0.0000
[2019-04-10 11:21:10,020] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159925: loss 1.7581
[2019-04-10 11:21:10,021] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159926: learning rate 0.0000
[2019-04-10 11:21:10,076] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159956: loss 3.2820
[2019-04-10 11:21:10,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159956: learning rate 0.0000
[2019-04-10 11:21:10,118] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6716826e-09 7.1005040e-01 1.4881844e-09 2.8994960e-01 2.8332803e-14], sum to 1.0000
[2019-04-10 11:21:10,126] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3878
[2019-04-10 11:21:10,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2255978.755588196 W.
[2019-04-10 11:21:10,140] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.83333333333334, 70.33333333333334, 1.0, 2.0, 0.8066568513469341, 1.0, 2.0, 0.8066568513469341, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2255978.755588196, 2255978.755588196, 423133.4486807092], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6001800.0000, 
sim time next is 6002400.0000, 
raw observation next is [32.06666666666667, 69.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.051868271447173, 6.9112, 168.9068295221927, 3093624.452119021, 2284424.474798873, 473398.3100109489], 
processed observation next is [1.0, 0.4782608695652174, 0.7187993680884678, 0.6966666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.1140668271447173, 0.0, 0.8294098588351053, 0.859340125588617, 0.6345623541107981, 0.7065646418073864], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1878907], dtype=float32), 1.5746243]. 
=============================================
[2019-04-10 11:21:10,212] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 160027: loss 44.8417
[2019-04-10 11:21:10,214] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 160028: learning rate 0.0000
[2019-04-10 11:21:10,280] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160064: loss 5.6184
[2019-04-10 11:21:10,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160065: learning rate 0.0000
[2019-04-10 11:21:10,306] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160082: loss 72.9259
[2019-04-10 11:21:10,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160083: learning rate 0.0000
[2019-04-10 11:21:10,331] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160096: loss 33.2139
[2019-04-10 11:21:10,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160096: learning rate 0.0000
[2019-04-10 11:21:10,751] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160268: loss 59.2024
[2019-04-10 11:21:10,767] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160277: learning rate 0.0000
[2019-04-10 11:21:10,786] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160279: loss 12.0721
[2019-04-10 11:21:10,801] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160286: learning rate 0.0000
[2019-04-10 11:21:10,852] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160297: loss 5.1148
[2019-04-10 11:21:10,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160299: learning rate 0.0000
[2019-04-10 11:21:11,206] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160416: loss -13.9061
[2019-04-10 11:21:11,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160418: learning rate 0.0000
[2019-04-10 11:21:15,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8977080e-09 2.9045030e-01 6.5463337e-09 7.0954973e-01 3.6472438e-15], sum to 1.0000
[2019-04-10 11:21:15,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7315
[2019-04-10 11:21:15,730] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 1.003344726294949, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.989072946481705, 6.9112, 168.9124935588003, 2299683.589669887, 2244437.992671704, 464963.8251327997], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6090600.0000, 
sim time next is 6091200.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8472812079009117, 1.0, 1.0, 0.8472812079009117, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2369700.689324959, 2369700.689324958, 443541.5287830284], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.65, 1.0, 1.0, 0.8160014553023032, 1.0, 0.5, 0.8160014553023032, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6582501914791553, 0.658250191479155, 0.6620022817657141], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0329214], dtype=float32), -1.7324587]. 
=============================================
[2019-04-10 11:21:19,905] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2684548e-16 1.0000000e+00 1.2786071e-19 3.3434222e-19 2.8832760e-27], sum to 1.0000
[2019-04-10 11:21:19,919] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5152
[2019-04-10 11:21:19,938] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 92.0, 1.0, 2.0, 0.5419737015091537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757344.9509518993, 757344.9509518993, 190433.7015636399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6141600.0000, 
sim time next is 6142200.0000, 
raw observation next is [26.68333333333333, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.641743262424162, 6.9112, 169.6938334416075, 2687926.665419592, 1454544.080842924, 310321.3318516603], 
processed observation next is [1.0, 0.08695652173913043, 0.4636650868878356, 0.92, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.17305432624241623, 0.0, 0.8332744083713852, 0.7466462959498866, 0.4040400224563678, 0.4631661669427765], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6600649], dtype=float32), -1.0673585]. 
=============================================
[2019-04-10 11:21:20,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2310532e-16 1.0000000e+00 1.8698250e-19 9.2785821e-19 5.3632534e-27], sum to 1.0000
[2019-04-10 11:21:20,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8962
[2019-04-10 11:21:20,047] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.5, 1.0, 2.0, 0.5365688031394396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749789.5710812869, 749789.5710812869, 189524.1943014165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6136200.0000, 
sim time next is 6136800.0000, 
raw observation next is [26.96666666666667, 89.66666666666667, 1.0, 2.0, 0.5362423797057536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749333.27318802, 749333.2731880193, 189469.5242792782], 
processed observation next is [1.0, 0.0, 0.47709320695102697, 0.8966666666666667, 1.0, 1.0, 0.4412558791635585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20814813144111666, 0.20814813144111646, 0.28279033474519133], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.19687746], dtype=float32), 0.22398746]. 
=============================================
[2019-04-10 11:21:22,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9830396e-16 1.0000000e+00 1.7962537e-18 1.5839328e-16 1.9624337e-25], sum to 1.0000
[2019-04-10 11:21:22,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3086
[2019-04-10 11:21:22,992] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 87.0, 1.0, 2.0, 0.7811103214181025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1091682.194335292, 1091682.194335292, 239328.3936755353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6163200.0000, 
sim time next is 6163800.0000, 
raw observation next is [27.88333333333333, 86.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.15934877929128, 6.9112, 168.9118537293756, 1629919.627037903, 1453875.491732661, 311351.9455118412], 
processed observation next is [1.0, 0.34782608695652173, 0.5205371248025275, 0.865, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.024814877929128, 0.0, 0.8294345299924653, 0.4527554519549731, 0.40385430325907246, 0.46470439628633015], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57963586], dtype=float32), -0.64528644]. 
=============================================
[2019-04-10 11:21:24,537] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1036623e-09 5.5906719e-01 1.8418712e-09 4.4093281e-01 4.0911611e-15], sum to 1.0000
[2019-04-10 11:21:24,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9326
[2019-04-10 11:21:24,576] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.88333333333334, 73.5, 1.0, 2.0, 0.7907613696770721, 1.0, 1.0, 0.7907613696770721, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2211484.330883325, 2211484.330883325, 415398.4135846024], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6178200.0000, 
sim time next is 6178800.0000, 
raw observation next is [29.96666666666667, 73.0, 1.0, 2.0, 0.6515527565190027, 1.0, 2.0, 0.6515527565190027, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1821835.256147618, 1821835.256147617, 354075.9887791255], 
processed observation next is [1.0, 0.5217391304347826, 0.6192733017377569, 0.73, 1.0, 1.0, 0.5801840439987984, 1.0, 1.0, 0.5801840439987984, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5060653489298939, 0.5060653489298936, 0.5284716250434709], 
reward next is 0.4715, 
noisyNet noise sample is [array([0.75841975], dtype=float32), 1.132643]. 
=============================================
[2019-04-10 11:21:25,560] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4512497e-09 6.9954592e-01 3.6115710e-10 3.0045405e-01 2.3783165e-15], sum to 1.0000
[2019-04-10 11:21:25,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1900
[2019-04-10 11:21:25,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2255989.86002534 W.
[2019-04-10 11:21:25,577] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.3, 74.5, 1.0, 2.0, 0.8066608183137485, 1.0, 2.0, 0.8066608183137485, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2255989.86002534, 2255989.86002534, 423123.6157379125], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6193800.0000, 
sim time next is 6194400.0000, 
raw observation next is [29.2, 75.0, 1.0, 2.0, 0.8033374886758518, 1.0, 2.0, 0.8033374886758518, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2246687.143462393, 2246687.143462392, 421495.9127360173], 
processed observation next is [1.0, 0.6956521739130435, 0.5829383886255924, 0.75, 1.0, 1.0, 0.7630572152721106, 1.0, 1.0, 0.7630572152721106, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.624079762072887, 0.6240797620728866, 0.6290983772179363], 
reward next is 0.3709, 
noisyNet noise sample is [array([-0.07144225], dtype=float32), 1.3095652]. 
=============================================
[2019-04-10 11:21:27,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4168609e-18 1.0000000e+00 1.7252655e-20 6.7064250e-20 8.6770968e-29], sum to 1.0000
[2019-04-10 11:21:27,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9886
[2019-04-10 11:21:27,764] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 87.66666666666667, 1.0, 2.0, 0.5223443604804832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729905.8152167976, 729905.8152167976, 187171.2178433965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6216000.0000, 
sim time next is 6216600.0000, 
raw observation next is [26.83333333333334, 87.83333333333334, 1.0, 2.0, 0.5217383669992416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729058.7304384452, 729058.7304384445, 187072.326125949], 
processed observation next is [1.0, 0.9565217391304348, 0.4707740916271725, 0.8783333333333334, 1.0, 1.0, 0.42378116505932717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20251631401067924, 0.20251631401067904, 0.27921242705365523], 
reward next is 0.7208, 
noisyNet noise sample is [array([-1.5060704], dtype=float32), 0.2711876]. 
=============================================
[2019-04-10 11:21:33,605] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167520: loss 0.0348
[2019-04-10 11:21:33,607] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167520: learning rate 0.0000
[2019-04-10 11:21:33,854] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167597: loss 0.0326
[2019-04-10 11:21:33,883] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167597: learning rate 0.0000
[2019-04-10 11:21:34,029] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167641: loss 0.0299
[2019-04-10 11:21:34,033] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167641: learning rate 0.0000
[2019-04-10 11:21:34,106] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 167664: loss 0.0307
[2019-04-10 11:21:34,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 167664: learning rate 0.0000
[2019-04-10 11:21:34,495] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 167784: loss 0.0312
[2019-04-10 11:21:34,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 167785: learning rate 0.0000
[2019-04-10 11:21:34,592] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 167820: loss 0.0323
[2019-04-10 11:21:34,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 167820: learning rate 0.0000
[2019-04-10 11:21:34,949] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167934: loss 0.0274
[2019-04-10 11:21:35,008] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167949: loss 0.0279
[2019-04-10 11:21:35,009] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167934: learning rate 0.0000
[2019-04-10 11:21:35,025] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167949: learning rate 0.0000
[2019-04-10 11:21:35,345] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.8747842e-18 1.0000000e+00 6.2069202e-22 2.1260647e-22 1.4519862e-30], sum to 1.0000
[2019-04-10 11:21:35,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6323
[2019-04-10 11:21:35,393] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 168056: loss 0.0242
[2019-04-10 11:21:35,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 168056: learning rate 0.0000
[2019-04-10 11:21:35,397] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.33333333333334, 1.0, 2.0, 0.5297373591888676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740240.133698212, 740240.1336982113, 188386.3710716541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6308400.0000, 
sim time next is 6309000.0000, 
raw observation next is [27.3, 85.5, 1.0, 2.0, 0.5304310276095777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741209.7846891206, 741209.78468912, 188501.2586362029], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.855, 1.0, 1.0, 0.43425425013202135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20589160685808905, 0.20589160685808888, 0.2813451621435864], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.05377042], dtype=float32), 1.5769573]. 
=============================================
[2019-04-10 11:21:35,413] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 168064: loss 0.0236
[2019-04-10 11:21:35,415] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 168065: learning rate 0.0000
[2019-04-10 11:21:35,424] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.52834 ]
 [74.222626]
 [75.16045 ]
 [76.45319 ]
 [76.44622 ]], R is [[73.11851501]
 [73.1061554 ]
 [73.09412384]
 [73.08238983]
 [73.07087708]].
[2019-04-10 11:21:35,541] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168112: loss 0.0237
[2019-04-10 11:21:35,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168112: learning rate 0.0000
[2019-04-10 11:21:35,609] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168129: loss 0.0247
[2019-04-10 11:21:35,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168130: learning rate 0.0000
[2019-04-10 11:21:35,780] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168183: loss 0.0236
[2019-04-10 11:21:35,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168183: learning rate 0.0000
[2019-04-10 11:21:36,219] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168315: loss 0.0209
[2019-04-10 11:21:36,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168315: learning rate 0.0000
[2019-04-10 11:21:36,487] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168395: loss 0.0209
[2019-04-10 11:21:36,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168395: learning rate 0.0000
[2019-04-10 11:21:37,306] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168674: loss 0.0178
[2019-04-10 11:21:37,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168674: learning rate 0.0000
[2019-04-10 11:21:38,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8502504e-18 1.0000000e+00 5.7701008e-22 2.8339378e-24 1.1312826e-30], sum to 1.0000
[2019-04-10 11:21:38,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-10 11:21:38,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 86.33333333333334, 1.0, 2.0, 0.5240657996925021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732312.1235873987, 732312.1235873987, 187452.4993230246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6327600.0000, 
sim time next is 6328200.0000, 
raw observation next is [27.1, 85.66666666666667, 1.0, 2.0, 0.5237910485583507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731928.0632159087, 731928.0632159094, 187407.5278585231], 
processed observation next is [0.0, 0.21739130434782608, 0.4834123222748816, 0.8566666666666667, 1.0, 1.0, 0.42625427537150684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.203313350893308, 0.20331335089330818, 0.2797127281470494], 
reward next is 0.7203, 
noisyNet noise sample is [array([1.7909914], dtype=float32), 1.4892193]. 
=============================================
[2019-04-10 11:21:57,463] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 11:21:57,467] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:21:57,469] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:21:57,471] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:21:57,491] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:21:57,495] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:21:57,496] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:21:57,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-04-10 11:21:57,522] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run8
[2019-04-10 11:21:57,568] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:21:57,568] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:21:57,570] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run8
[2019-04-10 11:21:57,591] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run8
[2019-04-10 11:21:57,637] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:21:57,638] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:21:57,641] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run8
[2019-04-10 11:22:02,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:22:02,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.33333333333333, 51.0, 1.0, 2.0, 0.2831441553370182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 458979.9344996927, 458979.9344996933, 164185.6637123239]
[2019-04-10 11:22:02,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:22:02,615] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.3149174e-19 1.0000000e+00 7.7219511e-23 1.8315703e-25 1.5384832e-31], sampled 0.05652971938813689
[2019-04-10 11:22:09,231] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:22:09,232] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.6, 86.0, 1.0, 2.0, 0.3089433844769627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492721.9479815904, 492721.9479815904, 166552.5848857787]
[2019-04-10 11:22:09,232] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:22:09,254] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9883351e-19 1.0000000e+00 1.2011705e-22 4.0938272e-24 2.1550916e-31], sampled 0.584717045090047
[2019-04-10 11:22:25,306] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:22:25,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.08333333333334, 95.0, 1.0, 2.0, 0.4494937511500388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665620.2517701269, 665620.2517701269, 180780.0247312058]
[2019-04-10 11:22:25,346] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:22:25,361] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8549276e-18 1.0000000e+00 1.1859187e-21 4.8435129e-23 4.1396702e-30], sampled 0.5300142576424764
[2019-04-10 11:22:26,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:22:26,530] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.45, 43.16666666666667, 1.0, 2.0, 0.2970852400837067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490085.8417081877, 490085.8417081883, 165903.895691877]
[2019-04-10 11:22:26,543] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:22:26,557] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1683543e-18 1.0000000e+00 1.5804332e-22 7.4810534e-25 4.0018332e-31], sampled 0.9893869852163114
[2019-04-10 11:22:50,657] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:22:50,657] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.4954583617351405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692324.0140958324, 692324.0140958324, 182887.4061905441]
[2019-04-10 11:22:50,658] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:22:50,659] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0001514e-18 1.0000000e+00 2.5291710e-21 1.1841647e-21 1.0767020e-29], sampled 0.8696778025478588
[2019-04-10 11:23:07,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:23:07,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.64546667333333, 53.87112747666667, 1.0, 2.0, 0.9749303985947935, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992018173129, 6.9112, 168.9123159353138, 2259913.063349484, 2192664.598197902, 455689.8838643107]
[2019-04-10 11:23:07,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:23:07,669] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5664526e-10 9.9485022e-01 6.5739199e-11 5.1497538e-03 1.6733328e-17], sampled 0.7789699623424255
[2019-04-10 11:23:07,670] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2259913.063349484 W.
[2019-04-10 11:23:30,239] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11631389], dtype=float32), 0.09495175]
[2019-04-10 11:23:30,240] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.53333333333333, 94.33333333333334, 1.0, 2.0, 0.4169726614628092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 609649.1494993184, 609649.1494993191, 175079.6372785865]
[2019-04-10 11:23:30,241] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:23:30,243] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.0061672e-18 1.0000000e+00 1.1814754e-21 3.3048542e-23 4.1615201e-30], sampled 0.5178594301505168
[2019-04-10 11:23:33,147] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8017.3953 3004611744.3592 1687.0000
[2019-04-10 11:23:33,410] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8270.8856 2925855742.7961 1299.0000
[2019-04-10 11:23:33,448] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7924.9336 3159828387.7913 1666.0000
[2019-04-10 11:23:33,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6165 2779163423.8912 928.0000
[2019-04-10 11:23:33,539] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8508.8823 2841157061.9347 1101.0000
[2019-04-10 11:23:34,549] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 175000, evaluation results [175000.0, 7924.933597448033, 3159828387.791347, 1666.0, 8270.88560390792, 2925855742.796059, 1299.0, 8660.616522498769, 2779163423.891247, 928.0, 8017.395316667943, 3004611744.3591514, 1687.0, 8508.882339771297, 2841157061.9347057, 1101.0]
[2019-04-10 11:23:35,005] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175323: loss -349.0128
[2019-04-10 11:23:35,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175323: learning rate 0.0000
[2019-04-10 11:23:35,246] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175486: loss -365.9600
[2019-04-10 11:23:35,248] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175487: learning rate 0.0000
[2019-04-10 11:23:35,380] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 175571: loss -459.3507
[2019-04-10 11:23:35,383] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 175571: learning rate 0.0000
[2019-04-10 11:23:35,588] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175696: loss -436.1384
[2019-04-10 11:23:35,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175696: learning rate 0.0000
[2019-04-10 11:23:35,634] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175731: loss -291.1947
[2019-04-10 11:23:35,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175731: learning rate 0.0000
[2019-04-10 11:23:35,681] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175756: loss -118.1906
[2019-04-10 11:23:35,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175756: learning rate 0.0000
[2019-04-10 11:23:35,976] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175936: loss -189.3476
[2019-04-10 11:23:35,977] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175936: learning rate 0.0000
[2019-04-10 11:23:36,087] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176010: loss -478.4269
[2019-04-10 11:23:36,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176010: learning rate 0.0000
[2019-04-10 11:23:36,167] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 176061: loss -233.3564
[2019-04-10 11:23:36,170] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 176062: learning rate 0.0000
[2019-04-10 11:23:36,194] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176077: loss -283.2443
[2019-04-10 11:23:36,195] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 176077: loss -378.6249
[2019-04-10 11:23:36,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176077: learning rate 0.0000
[2019-04-10 11:23:36,196] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 176077: learning rate 0.0000
[2019-04-10 11:23:36,388] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176192: loss -411.2810
[2019-04-10 11:23:36,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176192: learning rate 0.0000
[2019-04-10 11:23:36,546] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176291: loss -103.6750
[2019-04-10 11:23:36,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176291: learning rate 0.0000
[2019-04-10 11:23:36,593] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176319: loss -292.5854
[2019-04-10 11:23:36,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176319: learning rate 0.0000
[2019-04-10 11:23:36,797] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176445: loss -170.6688
[2019-04-10 11:23:36,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176446: learning rate 0.0000
[2019-04-10 11:23:37,070] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176613: loss -518.4654
[2019-04-10 11:23:37,074] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176613: learning rate 0.0000
[2019-04-10 11:23:39,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6568824e-18 1.0000000e+00 2.8744678e-20 1.9372401e-19 5.1474071e-29], sum to 1.0000
[2019-04-10 11:23:39,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9395
[2019-04-10 11:23:39,412] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4989342556943306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697182.6152101486, 697182.6152101492, 183429.3100557103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6652800.0000, 
sim time next is 6653400.0000, 
raw observation next is [25.91666666666667, 89.5, 1.0, 2.0, 0.4981941303632086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696148.0668973145, 696148.0668973145, 183313.6742048307], 
processed observation next is [1.0, 0.0, 0.4273301737756717, 0.895, 1.0, 1.0, 0.395414614895432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19337446302703182, 0.19337446302703182, 0.27360249881318016], 
reward next is 0.7264, 
noisyNet noise sample is [array([0.56866455], dtype=float32), -0.31407338]. 
=============================================
[2019-04-10 11:23:40,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4925333e-11 9.9999988e-01 1.4395955e-12 8.5931255e-08 1.0823910e-18], sum to 1.0000
[2019-04-10 11:23:40,253] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1523
[2019-04-10 11:23:40,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1875404.730473237 W.
[2019-04-10 11:23:40,267] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.86666666666667, 80.0, 1.0, 2.0, 0.6706943442736591, 1.0, 2.0, 0.6706943442736591, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1875404.730473237, 1875404.730473237, 361826.5828686208], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6687600.0000, 
sim time next is 6688200.0000, 
raw observation next is [28.03333333333333, 79.0, 1.0, 2.0, 0.7330571264795503, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979888343811917, 6.9112, 168.9125475556665, 1921388.42288015, 1872658.665689419, 392906.7883970911], 
processed observation next is [1.0, 0.391304347826087, 0.5276461295418641, 0.79, 1.0, 1.0, 0.6783820800958438, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006868834381191657, 0.0, 0.8294379369971886, 0.5337190063555972, 0.5201829626915053, 0.586428042383718], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49158433], dtype=float32), -0.69980764]. 
=============================================
[2019-04-10 11:23:43,535] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6707106e-17 1.0000000e+00 7.9230736e-21 1.9503552e-21 4.3700142e-29], sum to 1.0000
[2019-04-10 11:23:43,545] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9788
[2019-04-10 11:23:43,548] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 81.66666666666667, 1.0, 2.0, 0.3487074423974256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551949.6496468761, 551949.6496468768, 171103.700153481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6762000.0000, 
sim time next is 6762600.0000, 
raw observation next is [22.65, 81.0, 1.0, 2.0, 0.3434887855873461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543394.8532013342, 543394.8532013348, 170404.7218079845], 
processed observation next is [1.0, 0.2608695652173913, 0.2725118483412322, 0.81, 1.0, 1.0, 0.20902263323776635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1509430147781484, 0.15094301477814856, 0.25433540568355895], 
reward next is 0.7457, 
noisyNet noise sample is [array([1.3633163], dtype=float32), -0.6648721]. 
=============================================
[2019-04-10 11:23:44,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4588682e-18 1.0000000e+00 2.4644738e-21 3.6102173e-22 2.2301805e-29], sum to 1.0000
[2019-04-10 11:23:44,220] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0337
[2019-04-10 11:23:44,225] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 76.0, 1.0, 2.0, 0.3695941449649225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 578925.4032060098, 578925.4032060092, 173304.9593922844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6768000.0000, 
sim time next is 6768600.0000, 
raw observation next is [24.03333333333333, 74.66666666666667, 1.0, 2.0, 0.4137599222939741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647443.1173423212, 647443.1173423212, 179452.465235368], 
processed observation next is [1.0, 0.34782608695652173, 0.3380726698262243, 0.7466666666666667, 1.0, 1.0, 0.2936866533662339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.179845310372867, 0.179845310372867, 0.2678395003512955], 
reward next is 0.7322, 
noisyNet noise sample is [array([-1.3358096], dtype=float32), 1.0933995]. 
=============================================
[2019-04-10 11:23:45,885] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0450163e-19 1.0000000e+00 1.5701874e-22 4.9235232e-23 2.1090593e-31], sum to 1.0000
[2019-04-10 11:23:45,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3595
[2019-04-10 11:23:45,925] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 58.66666666666667, 1.0, 2.0, 0.3170777072927994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501115.3531999768, 501115.3531999774, 167103.2722701032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6812400.0000, 
sim time next is 6813000.0000, 
raw observation next is [26.15, 59.5, 1.0, 2.0, 0.3197452436669642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504924.4537710003, 504924.453771001, 167382.0851305817], 
processed observation next is [1.0, 0.8695652173913043, 0.43838862559241704, 0.595, 1.0, 1.0, 0.18041595622525808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14025679271416674, 0.14025679271416694, 0.2498240076575846], 
reward next is 0.7502, 
noisyNet noise sample is [array([0.74469244], dtype=float32), 1.5795003]. 
=============================================
[2019-04-10 11:23:45,947] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.440155]
 [73.16279 ]
 [72.85987 ]
 [72.449326]
 [72.10695 ]], R is [[73.6911087 ]
 [73.70479584]
 [73.71844482]
 [73.73189545]
 [73.74485779]].
[2019-04-10 11:23:52,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3053030e-20 1.0000000e+00 1.6714358e-23 5.2975985e-26 9.8290859e-32], sum to 1.0000
[2019-04-10 11:23:52,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0474
[2019-04-10 11:23:52,090] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183201: loss 0.0451
[2019-04-10 11:23:52,156] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183209: learning rate 0.0000
[2019-04-10 11:23:52,191] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.01666666666667, 83.66666666666666, 1.0, 2.0, 0.3419692216390004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531489.2801868112, 531489.2801868106, 169258.3029952292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6843000.0000, 
sim time next is 6843600.0000, 
raw observation next is [23.0, 84.0, 1.0, 2.0, 0.3425703354289968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532022.6739180152, 532022.6739180159, 169290.5005890343], 
processed observation next is [0.0, 0.21739130434782608, 0.28909952606635075, 0.84, 1.0, 1.0, 0.20791606678192384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14778407608833755, 0.14778407608833774, 0.25267238893885713], 
reward next is 0.7473, 
noisyNet noise sample is [array([-1.3292896], dtype=float32), 0.58578897]. 
=============================================
[2019-04-10 11:23:54,267] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183535: loss 0.0479
[2019-04-10 11:23:54,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183535: learning rate 0.0000
[2019-04-10 11:23:54,972] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183642: loss 0.0461
[2019-04-10 11:23:54,988] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 183645: loss 0.0493
[2019-04-10 11:23:54,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183642: learning rate 0.0000
[2019-04-10 11:23:55,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 183645: learning rate 0.0000
[2019-04-10 11:23:55,120] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183672: loss 0.0441
[2019-04-10 11:23:55,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183672: learning rate 0.0000
[2019-04-10 11:23:55,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6811429e-20 1.0000000e+00 1.9031384e-24 2.4632550e-26 1.6454452e-33], sum to 1.0000
[2019-04-10 11:23:55,286] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4881
[2019-04-10 11:23:55,345] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 69.0, 1.0, 2.0, 0.3857923114257717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580118.9827022328, 580118.9827022334, 172857.2448070596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6901200.0000, 
sim time next is 6901800.0000, 
raw observation next is [26.31666666666667, 69.83333333333333, 1.0, 2.0, 0.3881709464628906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582580.531923671, 582580.5319236703, 173045.1849274878], 
processed observation next is [0.0, 0.9130434782608695, 0.4462875197472356, 0.6983333333333333, 1.0, 1.0, 0.2628565620034826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16182792553435307, 0.16182792553435288, 0.2582763954141609], 
reward next is 0.7417, 
noisyNet noise sample is [array([-0.26285192], dtype=float32), 0.5726138]. 
=============================================
[2019-04-10 11:23:55,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6269499e-20 1.0000000e+00 2.5379620e-24 8.9703569e-27 4.8620559e-33], sum to 1.0000
[2019-04-10 11:23:55,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6049
[2019-04-10 11:23:55,782] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 71.5, 1.0, 2.0, 0.3928187320567826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587529.9346267583, 587529.9346267583, 173433.5319510599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6903000.0000, 
sim time next is 6903600.0000, 
raw observation next is [26.06666666666667, 72.33333333333333, 1.0, 2.0, 0.3956558540815243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590778.5889644243, 590778.5889644243, 173701.1333464653], 
processed observation next is [0.0, 0.9130434782608695, 0.4344391785150081, 0.7233333333333333, 1.0, 1.0, 0.2718745229897883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16410516360122898, 0.16410516360122898, 0.25925542290517206], 
reward next is 0.7407, 
noisyNet noise sample is [array([0.47178027], dtype=float32), 0.33596867]. 
=============================================
[2019-04-10 11:23:55,982] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183825: loss 0.0431
[2019-04-10 11:23:55,994] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183825: learning rate 0.0000
[2019-04-10 11:23:56,865] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183974: loss 0.0435
[2019-04-10 11:23:56,890] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183979: learning rate 0.0000
[2019-04-10 11:23:56,969] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 183994: loss 0.0408
[2019-04-10 11:23:56,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 183996: learning rate 0.0000
[2019-04-10 11:23:57,201] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 184023: loss 0.0415
[2019-04-10 11:23:57,226] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 184023: learning rate 0.0000
[2019-04-10 11:23:57,455] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 184055: loss 0.0395
[2019-04-10 11:23:57,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 184065: learning rate 0.0000
[2019-04-10 11:23:57,546] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184071: loss 0.0376
[2019-04-10 11:23:57,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184077: learning rate 0.0000
[2019-04-10 11:23:58,456] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184243: loss 0.0369
[2019-04-10 11:23:58,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184243: learning rate 0.0000
[2019-04-10 11:23:58,808] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184321: loss 0.0300
[2019-04-10 11:23:58,820] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184321: learning rate 0.0000
[2019-04-10 11:23:59,268] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184410: loss 0.0309
[2019-04-10 11:23:59,293] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184410: learning rate 0.0000
[2019-04-10 11:23:59,780] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184509: loss 0.0313
[2019-04-10 11:23:59,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184511: learning rate 0.0000
[2019-04-10 11:24:00,657] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184701: loss 0.0262
[2019-04-10 11:24:00,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184701: learning rate 0.0000
[2019-04-10 11:24:05,954] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.72802855e-20 1.00000000e+00 1.08510855e-23 2.72966704e-25
 1.63586422e-32], sum to 1.0000
[2019-04-10 11:24:05,954] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-10 11:24:06,015] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 59.5, 1.0, 2.0, 0.4690658186551107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659796.1706858793, 659796.1706858799, 179454.4124499215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6972600.0000, 
sim time next is 6973200.0000, 
raw observation next is [30.0, 59.0, 1.0, 2.0, 0.464782329505518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656394.8245220148, 656394.8245220148, 179158.3524270908], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.59, 1.0, 1.0, 0.3551594331391783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18233189570055966, 0.18233189570055966, 0.2674005260105833], 
reward next is 0.7326, 
noisyNet noise sample is [array([-0.0655944], dtype=float32), 0.11743844]. 
=============================================
[2019-04-10 11:24:11,294] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.708808e-12 9.999980e-01 7.638987e-13 2.032486e-06 6.024148e-19], sum to 1.0000
[2019-04-10 11:24:11,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5575
[2019-04-10 11:24:11,310] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1772782.354889837 W.
[2019-04-10 11:24:11,329] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.7, 47.0, 1.0, 2.0, 0.6232664526820381, 1.0, 2.0, 0.6232664526820381, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1772782.354889837, 1772782.354889837, 346664.2540950858], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7045200.0000, 
sim time next is 7045800.0000, 
raw observation next is [31.78333333333333, 46.50000000000001, 1.0, 2.0, 0.362263974888654, 1.0, 2.0, 0.362263974888654, 1.0, 1.0, 0.6117038646064744, 6.9112, 6.9112, 170.5573041426782, 1551377.228617827, 1551377.228617827, 333584.813648944], 
processed observation next is [1.0, 0.5652173913043478, 0.7053712480252764, 0.4650000000000001, 1.0, 1.0, 0.23164334323934213, 1.0, 1.0, 0.23164334323934213, 1.0, 0.5, 0.5264681275688712, 0.0, 0.0, 0.8375144448122397, 0.4309381190605075, 0.4309381190605075, 0.4978877815655881], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1955992], dtype=float32), 0.96779895]. 
=============================================
[2019-04-10 11:24:18,338] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4807073e-09 9.2920887e-01 1.0831041e-09 7.0791066e-02 1.8683311e-15], sum to 1.0000
[2019-04-10 11:24:18,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1676
[2019-04-10 11:24:18,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1676907.271300919 W.
[2019-04-10 11:24:18,385] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.9, 71.0, 1.0, 2.0, 0.3998412698019906, 1.0, 2.0, 0.3998412698019906, 1.0, 2.0, 0.6715072901639467, 6.911200000000001, 6.9112, 170.5573041426782, 1676907.271300919, 1676907.271300918, 349511.3652228293], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7128000.0000, 
sim time next is 7128600.0000, 
raw observation next is [27.78333333333333, 71.83333333333334, 1.0, 2.0, 0.6347889948138247, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.945574676626894, 6.9112, 168.9127279662002, 1774936.856650665, 1750550.310962246, 372634.2171980688], 
processed observation next is [1.0, 0.5217391304347826, 0.5157977883096366, 0.7183333333333334, 1.0, 1.0, 0.5599867407395478, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.003437467662689375, 0.0, 0.8294388228955015, 0.4930380157362958, 0.48626397526729054, 0.5561704734299534], 
reward next is 0.2720, 
noisyNet noise sample is [array([-0.12726937], dtype=float32), 0.12575194]. 
=============================================
[2019-04-10 11:24:21,871] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191169: loss 0.0831
[2019-04-10 11:24:21,890] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191180: learning rate 0.0000
[2019-04-10 11:24:23,081] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191601: loss 0.0821
[2019-04-10 11:24:23,084] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191601: learning rate 0.0000
[2019-04-10 11:24:23,329] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191681: loss 0.0816
[2019-04-10 11:24:23,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191684: learning rate 0.0000
[2019-04-10 11:24:23,392] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191695: loss 0.0808
[2019-04-10 11:24:23,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191695: learning rate 0.0000
[2019-04-10 11:24:23,398] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 191696: loss 0.0835
[2019-04-10 11:24:23,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 191698: learning rate 0.0000
[2019-04-10 11:24:23,725] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191812: loss 0.0830
[2019-04-10 11:24:23,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191812: learning rate 0.0000
[2019-04-10 11:24:23,942] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191892: loss 0.0790
[2019-04-10 11:24:23,948] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191893: learning rate 0.0000
[2019-04-10 11:24:24,056] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 191930: loss 0.0762
[2019-04-10 11:24:24,057] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 191930: learning rate 0.0000
[2019-04-10 11:24:24,103] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191948: loss 0.0755
[2019-04-10 11:24:24,110] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191951: learning rate 0.0000
[2019-04-10 11:24:24,320] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 192023: loss 0.0742
[2019-04-10 11:24:24,321] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 192024: learning rate 0.0000
[2019-04-10 11:24:24,433] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192060: loss 0.0762
[2019-04-10 11:24:24,440] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192064: learning rate 0.0000
[2019-04-10 11:24:25,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8873122e-17 1.0000000e+00 2.2766599e-20 8.2569453e-20 9.4969020e-29], sum to 1.0000
[2019-04-10 11:24:25,029] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1174
[2019-04-10 11:24:25,034] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.78333333333333, 86.0, 1.0, 2.0, 0.4755511126304509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664498.1092045179, 664498.1092045173, 179853.913196464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7174200.0000, 
sim time next is 7174800.0000, 
raw observation next is [25.8, 86.0, 1.0, 2.0, 0.4761557408519673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665343.2341960453, 665343.2341960459, 179944.324408783], 
processed observation next is [1.0, 0.043478260869565216, 0.42180094786729866, 0.86, 1.0, 1.0, 0.36886233837586424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18481756505445704, 0.1848175650544572, 0.26857361852057166], 
reward next is 0.7314, 
noisyNet noise sample is [array([1.1263723], dtype=float32), 0.6577538]. 
=============================================
[2019-04-10 11:24:25,304] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192314: loss 0.0698
[2019-04-10 11:24:25,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192315: learning rate 0.0000
[2019-04-10 11:24:25,320] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192319: loss 0.0688
[2019-04-10 11:24:25,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192321: learning rate 0.0000
[2019-04-10 11:24:25,535] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192379: loss 0.0670
[2019-04-10 11:24:25,540] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192383: learning rate 0.0000
[2019-04-10 11:24:25,660] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192416: loss 0.0648
[2019-04-10 11:24:25,661] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192416: learning rate 0.0000
[2019-04-10 11:24:27,271] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 192891: loss 0.0672
[2019-04-10 11:24:27,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 192891: learning rate 0.0000
[2019-04-10 11:24:28,787] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7430378e-18 1.0000000e+00 1.9792483e-21 2.0613454e-21 8.0668328e-30], sum to 1.0000
[2019-04-10 11:24:28,794] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7811
[2019-04-10 11:24:28,798] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 88.16666666666667, 1.0, 2.0, 0.4175824591109574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657965.650762075, 657965.650762075, 180443.3782584387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7265400.0000, 
sim time next is 7266000.0000, 
raw observation next is [21.83333333333334, 88.33333333333334, 1.0, 2.0, 0.3454780147665782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544481.9037270518, 544481.9037270518, 170465.762353057], 
processed observation next is [1.0, 0.08695652173913043, 0.23380726698262277, 0.8833333333333334, 1.0, 1.0, 0.2114192948994918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15124497325751438, 0.15124497325751438, 0.254426510974712], 
reward next is 0.7456, 
noisyNet noise sample is [array([1.0646225], dtype=float32), -0.027224831]. 
=============================================
[2019-04-10 11:24:28,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.59441]
 [71.79544]
 [71.69595]
 [71.7425 ]
 [71.77824]], R is [[71.91595459]
 [71.92747498]
 [71.95783997]
 [71.98775482]
 [72.01724243]].
[2019-04-10 11:24:29,047] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5732004e-18 1.0000000e+00 4.1991071e-22 1.4066642e-22 5.2505898e-30], sum to 1.0000
[2019-04-10 11:24:29,053] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9426
[2019-04-10 11:24:29,055] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.61666666666667, 89.83333333333333, 1.0, 2.0, 0.3282927001426869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517834.9143904585, 517834.9143904591, 168359.0085167426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7271400.0000, 
sim time next is 7272000.0000, 
raw observation next is [21.6, 90.0, 1.0, 2.0, 0.3282137759430772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517660.6663926761, 517660.6663926768, 168344.5824127825], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.9, 1.0, 1.0, 0.19061900716033398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14379462955352115, 0.14379462955352135, 0.251260570765347], 
reward next is 0.7487, 
noisyNet noise sample is [array([-0.4277435], dtype=float32), 0.8229258]. 
=============================================
[2019-04-10 11:24:29,065] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.785126]
 [71.83779 ]
 [71.89222 ]
 [71.950935]
 [71.99324 ]], R is [[71.81539154]
 [71.8459549 ]
 [71.8757019 ]
 [71.90420532]
 [71.93269348]].
[2019-04-10 11:24:42,209] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8356816e-17 1.0000000e+00 1.7932327e-20 9.7154802e-21 3.1158020e-29], sum to 1.0000
[2019-04-10 11:24:42,392] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4864
[2019-04-10 11:24:42,437] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 92.33333333333333, 1.0, 2.0, 0.4111758867973876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651858.8488127897, 651858.8488127897, 179845.8991403281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7389600.0000, 
sim time next is 7390200.0000, 
raw observation next is [21.03333333333333, 92.16666666666667, 1.0, 2.0, 0.412606165552435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654922.4811184471, 654922.4811184466, 180124.86327734], 
processed observation next is [1.0, 0.5217391304347826, 0.19589257503949445, 0.9216666666666667, 1.0, 1.0, 0.29229658500293376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18192291142179087, 0.1819229114217907, 0.2688430795184179], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.5659519], dtype=float32), 0.78250116]. 
=============================================
[2019-04-10 11:24:48,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6556520e-19 1.0000000e+00 1.5884651e-23 1.2182147e-25 2.2127459e-32], sum to 1.0000
[2019-04-10 11:24:48,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3905
[2019-04-10 11:24:48,351] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 92.33333333333334, 1.0, 2.0, 0.315418636943349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497772.1140731876, 497772.1140731882, 166837.975021954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7441800.0000, 
sim time next is 7442400.0000, 
raw observation next is [21.3, 92.66666666666667, 1.0, 2.0, 0.3161169590869229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498397.8951746832, 498397.8951746839, 166874.1749414471], 
processed observation next is [0.0, 0.13043478260869565, 0.2085308056872039, 0.9266666666666667, 1.0, 1.0, 0.17604452902038903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13844385977074533, 0.13844385977074553, 0.2490659327484285], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.4638927], dtype=float32), -0.7646115]. 
=============================================
[2019-04-10 11:24:52,203] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199264: loss 0.0036
[2019-04-10 11:24:52,204] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199264: learning rate 0.0000
[2019-04-10 11:24:53,141] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 199511: loss 0.0062
[2019-04-10 11:24:53,142] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 199511: learning rate 0.0000
[2019-04-10 11:24:53,366] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199569: loss 0.0055
[2019-04-10 11:24:53,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199569: learning rate 0.0000
[2019-04-10 11:24:53,408] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199582: loss 0.0058
[2019-04-10 11:24:53,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199582: learning rate 0.0000
[2019-04-10 11:24:53,876] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 199753: loss 0.0076
[2019-04-10 11:24:53,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 199753: learning rate 0.0000
[2019-04-10 11:24:54,010] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199798: loss 0.0084
[2019-04-10 11:24:54,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199798: learning rate 0.0000
[2019-04-10 11:24:54,382] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199904: loss 0.0087
[2019-04-10 11:24:54,388] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199904: learning rate 0.0000
[2019-04-10 11:24:54,390] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199906: loss 0.0094
[2019-04-10 11:24:54,428] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199919: learning rate 0.0000
[2019-04-10 11:24:54,583] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199971: loss 0.0085
[2019-04-10 11:24:54,588] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199972: learning rate 0.0000
[2019-04-10 11:24:54,628] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-10 11:24:54,631] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:24:54,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:24:54,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:24:54,657] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:24:54,658] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run9
[2019-04-10 11:24:54,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-04-10 11:24:54,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:24:54,753] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:24:54,755] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:24:54,805] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:24:54,816] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:24:54,818] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:24:54,819] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run9
[2019-04-10 11:24:54,853] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run9
[2019-04-10 11:24:54,915] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run9
[2019-04-10 11:25:00,918] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11351083], dtype=float32), 0.09270198]
[2019-04-10 11:25:00,932] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.95, 64.66666666666667, 1.0, 2.0, 0.2395499942367866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397044.3263297544, 397044.3263297544, 159728.0183183488]
[2019-04-10 11:25:00,933] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:25:00,934] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1353037e-20 1.0000000e+00 1.9929813e-24 5.3064547e-27 1.3740764e-33], sampled 0.26177231684378965
[2019-04-10 11:25:30,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11351083], dtype=float32), 0.09270198]
[2019-04-10 11:25:30,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.19074571833333, 65.83494300000001, 1.0, 2.0, 0.7860625997896749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1098607.094242091, 1098607.094242091, 240519.4444974283]
[2019-04-10 11:25:30,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:25:30,248] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.2579578e-18 1.0000000e+00 2.2032656e-21 7.9990521e-22 8.4376014e-30], sampled 0.6154868448916905
[2019-04-10 11:25:37,554] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11351083], dtype=float32), 0.09270198]
[2019-04-10 11:25:37,555] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5175730678558684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723236.3073694353, 723236.3073694347, 186395.2683746903]
[2019-04-10 11:25:37,556] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:25:37,558] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8269482e-19 1.0000000e+00 4.5467279e-23 5.3947586e-24 5.4691661e-32], sampled 0.8615088384457044
[2019-04-10 11:25:41,422] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11351083], dtype=float32), 0.09270198]
[2019-04-10 11:25:41,423] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333333, 64.16666666666666, 1.0, 2.0, 0.9001389294659549, 1.0, 1.0, 0.9001389294659549, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2517683.630750454, 2517683.630750454, 471565.712302366]
[2019-04-10 11:25:41,423] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:25:41,425] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.0599525e-09 3.8133341e-01 2.0371047e-09 6.1866659e-01 4.2286091e-15], sampled 0.08204272216287034
[2019-04-10 11:25:41,426] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2517683.630750454 W.
[2019-04-10 11:25:41,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11351083], dtype=float32), 0.09270198]
[2019-04-10 11:25:41,758] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.48333333333333, 64.0, 1.0, 2.0, 0.8263140224573449, 1.0, 1.0, 0.8263140224573449, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2310976.340809948, 2310976.340809948, 433368.2949154394]
[2019-04-10 11:25:41,759] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:25:41,760] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.8809716e-11 9.9999607e-01 1.7785053e-11 3.9504994e-06 7.2105020e-18], sampled 0.7109535271030317
[2019-04-10 11:25:41,761] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2310976.340809948 W.
[2019-04-10 11:26:23,696] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.4180 2926946967.1747 1329.0000
[2019-04-10 11:26:23,908] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8503.6891 2841721503.2547 1114.0000
[2019-04-10 11:26:23,917] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.3309 2779010389.7887 929.0000
[2019-04-10 11:26:23,975] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7898.6139 3162561985.0252 1730.0000
[2019-04-10 11:26:24,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.6752 3006825886.2617 1741.0000
[2019-04-10 11:26:25,089] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 200000, evaluation results [200000.0, 7898.613922641847, 3162561985.025196, 1730.0, 8256.417972282276, 2926946967.1747274, 1329.0, 8663.330899866642, 2779010389.7887435, 929.0, 8005.675188070618, 3006825886.2617126, 1741.0, 8503.689130160465, 2841721503.2546983, 1114.0]
[2019-04-10 11:26:25,147] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 200042: loss 0.0089
[2019-04-10 11:26:25,150] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 200043: learning rate 0.0000
[2019-04-10 11:26:25,186] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200069: loss 0.0095
[2019-04-10 11:26:25,188] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200070: learning rate 0.0000
[2019-04-10 11:26:25,380] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200184: loss 0.0100
[2019-04-10 11:26:25,385] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200185: learning rate 0.0000
[2019-04-10 11:26:25,672] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200382: loss 0.0132
[2019-04-10 11:26:25,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200382: learning rate 0.0000
[2019-04-10 11:26:25,826] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200484: loss 0.0132
[2019-04-10 11:26:25,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200485: learning rate 0.0000
[2019-04-10 11:26:25,873] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200514: loss 0.0135
[2019-04-10 11:26:25,877] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200514: learning rate 0.0000
[2019-04-10 11:26:26,672] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 201014: loss 0.0172
[2019-04-10 11:26:26,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 201016: learning rate 0.0000
[2019-04-10 11:26:41,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.60854530e-08 8.81563365e-01 4.28614788e-09 1.18436605e-01
 5.58810548e-15], sum to 1.0000
[2019-04-10 11:26:41,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0232
[2019-04-10 11:26:41,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2075035.778238509 W.
[2019-04-10 11:26:41,067] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 71.0, 1.0, 2.0, 0.7420186154720821, 1.0, 2.0, 0.7420186154720821, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2075035.778238509, 2075035.778238509, 392629.1196797306], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7725600.0000, 
sim time next is 7726200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.714980494839124, 1.0, 2.0, 0.714980494839124, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1999353.818639994, 1999353.818639994, 380602.1827360025], 
processed observation next is [1.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.6566030058302698, 1.0, 1.0, 0.6566030058302698, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5553760607333317, 0.5553760607333317, 0.5680629593074664], 
reward next is 0.4319, 
noisyNet noise sample is [array([0.33549342], dtype=float32), 0.27008513]. 
=============================================
[2019-04-10 11:26:43,631] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207344: loss 0.0475
[2019-04-10 11:26:43,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207344: learning rate 0.0000
[2019-04-10 11:26:43,802] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207404: loss 0.0470
[2019-04-10 11:26:43,816] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207406: learning rate 0.0000
[2019-04-10 11:26:43,959] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 207466: loss 0.0468
[2019-04-10 11:26:43,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 207467: learning rate 0.0000
[2019-04-10 11:26:44,024] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207479: loss 0.0470
[2019-04-10 11:26:44,024] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207479: learning rate 0.0000
[2019-04-10 11:26:44,434] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 207616: loss 0.0466
[2019-04-10 11:26:44,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 207617: learning rate 0.0000
[2019-04-10 11:26:44,766] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207718: loss 0.0457
[2019-04-10 11:26:44,781] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207724: learning rate 0.0000
[2019-04-10 11:26:45,075] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207849: loss 0.0468
[2019-04-10 11:26:45,090] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207850: learning rate 0.0000
[2019-04-10 11:26:45,313] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207933: loss 0.0481
[2019-04-10 11:26:45,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207937: learning rate 0.0000
[2019-04-10 11:26:45,463] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207976: loss 0.0474
[2019-04-10 11:26:45,474] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207976: learning rate 0.0000
[2019-04-10 11:26:45,677] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 208061: loss 0.0462
[2019-04-10 11:26:45,678] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 208061: learning rate 0.0000
[2019-04-10 11:26:46,076] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208212: loss 0.0466
[2019-04-10 11:26:46,091] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208219: learning rate 0.0000
[2019-04-10 11:26:46,208] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208258: loss 0.0475
[2019-04-10 11:26:46,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208258: learning rate 0.0000
[2019-04-10 11:26:46,259] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208279: loss 0.0456
[2019-04-10 11:26:46,263] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208281: learning rate 0.0000
[2019-04-10 11:26:46,722] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208478: loss 0.0522
[2019-04-10 11:26:46,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208478: learning rate 0.0000
[2019-04-10 11:26:47,095] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208619: loss 0.0500
[2019-04-10 11:26:47,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208621: learning rate 0.0000
[2019-04-10 11:26:48,482] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 209099: loss 0.0488
[2019-04-10 11:26:48,485] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 209100: learning rate 0.0000
[2019-04-10 11:26:48,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.5898705e-18 1.0000000e+00 2.2451962e-21 8.3628808e-21 4.9857060e-30], sum to 1.0000
[2019-04-10 11:26:48,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0694
[2019-04-10 11:26:48,566] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.05, 83.16666666666667, 1.0, 2.0, 0.5330629759383613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744888.8867115227, 744888.8867115227, 188939.2872390155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7845000.0000, 
sim time next is 7845600.0000, 
raw observation next is [27.9, 84.33333333333334, 1.0, 2.0, 0.5362200177786424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749302.0141029314, 749302.0141029321, 189466.2023328312], 
processed observation next is [1.0, 0.8260869565217391, 0.5213270142180094, 0.8433333333333334, 1.0, 1.0, 0.4412289370827016, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20813944836192538, 0.20813944836192558, 0.282785376616166], 
reward next is 0.7172, 
noisyNet noise sample is [array([-0.61340964], dtype=float32), 0.18930273]. 
=============================================
[2019-04-10 11:26:55,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:55,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:55,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:55,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:55,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-04-10 11:26:55,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-04-10 11:26:55,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:55,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:55,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run2
[2019-04-10 11:26:56,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:56,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:56,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-04-10 11:26:56,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:56,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:56,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-04-10 11:26:56,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:56,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:56,506] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-04-10 11:26:56,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:56,634] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:56,657] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-04-10 11:26:56,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:56,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:56,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-04-10 11:26:56,833] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:56,833] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:56,834] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-04-10 11:26:57,273] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:57,273] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:57,274] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-04-10 11:26:57,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:57,502] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:57,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-04-10 11:26:57,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:57,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:57,822] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-04-10 11:26:58,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:58,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:58,152] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-04-10 11:26:58,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:58,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:58,214] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-04-10 11:26:58,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:58,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:58,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-04-10 11:26:59,813] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:26:59,813] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:26:59,814] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-04-10 11:27:11,866] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5422416e-19 1.0000000e+00 1.5842353e-23 2.1836981e-25 4.2594558e-32], sum to 1.0000
[2019-04-10 11:27:11,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1927
[2019-04-10 11:27:11,889] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2850184645548307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457923.3451514558, 457923.3451514558, 164128.196094876], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2850035450738455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457899.4545029787, 457899.4545029787, 164126.5735671236], 
processed observation next is [0.0, 0.08695652173913043, 0.1706161137440759, 0.92, 1.0, 1.0, 0.13855848804077772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1271942929174941, 0.1271942929174941, 0.24496503517481133], 
reward next is 0.7550, 
noisyNet noise sample is [array([-1.3697557], dtype=float32), -0.92387146]. 
=============================================
[2019-04-10 11:27:17,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.440069e-20 1.000000e+00 4.314420e-24 9.648122e-27 1.730514e-33], sum to 1.0000
[2019-04-10 11:27:17,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7095
[2019-04-10 11:27:17,462] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 86.0, 1.0, 2.0, 0.2733448314107534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442603.5072770095, 442603.5072770101, 163104.8118955987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 340200.0000, 
sim time next is 340800.0000, 
raw observation next is [20.76666666666667, 86.0, 1.0, 2.0, 0.2702746393103583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 437882.283578461, 437882.283578461, 162795.8371069399], 
processed observation next is [0.0, 0.9565217391304348, 0.18325434439178534, 0.86, 1.0, 1.0, 0.12081281844621479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1216339676606836, 0.1216339676606836, 0.24297886135364163], 
reward next is 0.7570, 
noisyNet noise sample is [array([-1.316653], dtype=float32), 0.40816197]. 
=============================================
[2019-04-10 11:27:20,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6264715e-19 1.0000000e+00 4.0796137e-22 1.4874170e-23 3.4947245e-31], sum to 1.0000
[2019-04-10 11:27:20,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8017
[2019-04-10 11:27:20,389] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.03333333333333, 89.66666666666667, 1.0, 2.0, 0.2571424099729964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418631.7409646709, 418631.7409646703, 161548.2531185804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 362400.0000, 
sim time next is 363000.0000, 
raw observation next is [20.01666666666667, 89.83333333333333, 1.0, 2.0, 0.2575832082847994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419326.6400830902, 419326.6400830902, 161591.6598176466], 
processed observation next is [1.0, 0.17391304347826086, 0.14770932069510287, 0.8983333333333333, 1.0, 1.0, 0.1055219376925294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11647962224530284, 0.11647962224530284, 0.241181581817383], 
reward next is 0.7588, 
noisyNet noise sample is [array([1.2686069], dtype=float32), -0.2307559]. 
=============================================
[2019-04-10 11:27:20,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.27622 ]
 [73.313354]
 [73.3625  ]
 [73.323814]
 [73.40905 ]], R is [[73.31554413]
 [73.34127045]
 [73.36682129]
 [73.39237213]
 [73.4176178 ]].
[2019-04-10 11:27:28,942] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-10 11:27:28,955] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:27:28,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:27:28,957] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:27:28,959] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:27:28,958] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:27:28,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:27:28,961] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:27:28,963] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run10
[2019-04-10 11:27:28,961] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:27:28,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-04-10 11:27:28,958] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:27:29,039] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:27:29,040] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run10
[2019-04-10 11:27:29,086] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run10
[2019-04-10 11:27:29,139] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run10
[2019-04-10 11:27:34,751] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11230472], dtype=float32), 0.091365606]
[2019-04-10 11:27:34,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.30122565166667, 86.05213463166666, 1.0, 2.0, 0.3881478529809047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578455.0553056346, 578455.0553056346, 172545.4420945304]
[2019-04-10 11:27:34,772] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:27:34,775] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2831611e-19 1.0000000e+00 1.1536433e-23 7.7243855e-26 1.0397551e-32], sampled 0.18025883805428233
[2019-04-10 11:27:56,561] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11230472], dtype=float32), 0.091365606]
[2019-04-10 11:27:56,561] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.49067847, 73.28165268, 1.0, 2.0, 0.6463183733777247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903216.4400215198, 903216.4400215198, 209682.6847312516]
[2019-04-10 11:27:56,562] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:27:56,564] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.01306886e-17 1.00000000e+00 6.10228425e-21 6.06344069e-21
 3.04552929e-29], sampled 0.15374973147050508
[2019-04-10 11:28:07,488] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11230472], dtype=float32), 0.091365606]
[2019-04-10 11:28:07,490] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.83333333333333, 65.0, 1.0, 2.0, 0.7501016743932674, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984211020597456, 6.9112, 168.912462286186, 1945241.087112499, 1893444.707807295, 396682.0307365981]
[2019-04-10 11:28:07,491] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:28:07,493] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2642911e-11 9.9999857e-01 7.8948289e-12 1.4774931e-06 1.2335792e-18], sampled 0.04641165761372068
[2019-04-10 11:28:07,495] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1945241.087112499 W.
[2019-04-10 11:28:19,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11230472], dtype=float32), 0.091365606]
[2019-04-10 11:28:19,640] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.56797588, 82.14105547, 1.0, 2.0, 0.5031224462251305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703036.8932826683, 703036.8932826683, 184085.5830071666]
[2019-04-10 11:28:19,642] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:28:19,643] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0156880e-18 1.0000000e+00 4.8467316e-22 3.4312256e-23 1.3184223e-30], sampled 0.8947801883955168
[2019-04-10 11:28:33,355] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11230472], dtype=float32), 0.091365606]
[2019-04-10 11:28:33,355] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.85, 94.0, 1.0, 2.0, 0.9677966715418845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1352761.827068296, 1352761.827068296, 289266.267975573]
[2019-04-10 11:28:33,356] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:28:33,358] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7993839e-15 1.0000000e+00 4.8516248e-18 2.6167188e-16 8.2807986e-26], sampled 0.8936820984703637
[2019-04-10 11:28:41,155] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11230472], dtype=float32), 0.091365606]
[2019-04-10 11:28:41,156] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.4, 90.66666666666667, 1.0, 2.0, 0.4991707532025946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697513.1919990899, 697513.1919990899, 183465.4357466913]
[2019-04-10 11:28:41,157] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:28:41,159] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6166878e-18 1.0000000e+00 5.0722055e-22 1.6572974e-22 1.3105698e-30], sampled 0.6311641348334663
[2019-04-10 11:28:53,218] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7899.8764 3161677848.5760 1725.0000
[2019-04-10 11:28:53,239] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.8229 2927239843.7882 1334.0000
[2019-04-10 11:28:53,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.3412 2779255631.2575 933.0000
[2019-04-10 11:28:53,604] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.0885 2841828153.9984 1114.0000
[2019-04-10 11:28:53,648] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.3015 3006168515.2686 1733.0000
[2019-04-10 11:28:54,662] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 225000, evaluation results [225000.0, 7899.876382293671, 3161677848.5760074, 1725.0, 8259.822879829691, 2927239843.7881765, 1334.0, 8660.341170285732, 2779255631.257508, 933.0, 8005.301485282486, 3006168515.2686234, 1733.0, 8500.088497310213, 2841828153.998377, 1114.0]
[2019-04-10 11:29:00,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9390916e-18 1.0000000e+00 5.2535365e-22 7.4231584e-22 3.6196052e-31], sum to 1.0000
[2019-04-10 11:29:00,309] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7744
[2019-04-10 11:29:00,320] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 68.5, 1.0, 2.0, 0.2563244028885566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420623.2616607983, 420623.2616607989, 161540.6153842802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 585000.0000, 
sim time next is 585600.0000, 
raw observation next is [22.16666666666666, 69.0, 1.0, 2.0, 0.2550291931835963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418790.1225689863, 418790.1225689856, 161410.4931594187], 
processed observation next is [1.0, 0.782608695652174, 0.24960505529225885, 0.69, 1.0, 1.0, 0.10244481106457384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1163305896024962, 0.11633058960249601, 0.2409111838200279], 
reward next is 0.7591, 
noisyNet noise sample is [array([-0.00825258], dtype=float32), -0.28308684]. 
=============================================
[2019-04-10 11:29:00,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0514879e-18 1.0000000e+00 1.1425422e-21 1.7784336e-23 4.5354518e-30], sum to 1.0000
[2019-04-10 11:29:00,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1360
[2019-04-10 11:29:00,651] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 67.5, 1.0, 2.0, 0.4726403767214698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777850.2612768414, 777850.2612768414, 191015.0072088683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [22.4, 66.0, 1.0, 2.0, 0.4795799976515278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 789525.9277171761, 789525.9277171755, 192210.1840587171], 
processed observation next is [1.0, 0.43478260869565216, 0.2606635071090047, 0.66, 1.0, 1.0, 0.37298794897774434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2193127576992156, 0.2193127576992154, 0.2868808717294285], 
reward next is 0.7131, 
noisyNet noise sample is [array([0.4546404], dtype=float32), -0.7755822]. 
=============================================
[2019-04-10 11:29:12,754] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0876653e-16 1.0000000e+00 8.0791877e-21 7.1751238e-21 4.5932776e-29], sum to 1.0000
[2019-04-10 11:29:12,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2331
[2019-04-10 11:29:12,780] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.75, 48.5, 1.0, 2.0, 0.5667411950691573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930732.9112582929, 930732.9112582929, 208653.4123552284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 743400.0000, 
sim time next is 744000.0000, 
raw observation next is [25.66666666666667, 48.66666666666666, 1.0, 2.0, 0.5705225740678224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937448.0702151209, 937448.0702151209, 209427.0785945094], 
processed observation next is [1.0, 0.6086956521739131, 0.4154818325434442, 0.4866666666666666, 1.0, 1.0, 0.4825573181540028, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26040224172642246, 0.26040224172642246, 0.3125777292455364], 
reward next is 0.6874, 
noisyNet noise sample is [array([-0.21596146], dtype=float32), 0.20189303]. 
=============================================
[2019-04-10 11:29:12,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.297485]
 [71.13519 ]
 [70.89598 ]
 [70.825905]
 [70.87211 ]], R is [[71.35009766]
 [71.32517242]
 [71.29915619]
 [71.26066589]
 [71.21656799]].
[2019-04-10 11:29:35,726] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4091793e-18 1.0000000e+00 2.8978841e-21 1.7671832e-20 4.9720527e-29], sum to 1.0000
[2019-04-10 11:29:35,728] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7273
[2019-04-10 11:29:35,731] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 76.0, 1.0, 2.0, 0.6042018325341043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 950976.3712487163, 950976.371248717, 214021.3898155386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1080000.0000, 
sim time next is 1080600.0000, 
raw observation next is [23.76666666666667, 75.33333333333334, 1.0, 2.0, 0.6293684352386377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 988845.7697850259, 988845.7697850252, 219286.1492592628], 
processed observation next is [1.0, 0.5217391304347826, 0.32543443917851517, 0.7533333333333334, 1.0, 1.0, 0.5534559460706479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2746793804958405, 0.27467938049584034, 0.32729276008845193], 
reward next is 0.6727, 
noisyNet noise sample is [array([-0.08389585], dtype=float32), 1.0016]. 
=============================================
[2019-04-10 11:29:37,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4917225e-18 1.0000000e+00 6.7761849e-22 8.7837988e-22 3.4210324e-30], sum to 1.0000
[2019-04-10 11:29:37,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8472
[2019-04-10 11:29:37,586] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 73.66666666666667, 1.0, 2.0, 0.3276070559093963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513846.8382420852, 513846.8382420846, 167988.2240346978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1105800.0000, 
sim time next is 1106400.0000, 
raw observation next is [23.93333333333334, 74.33333333333334, 1.0, 2.0, 0.3295784873726749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 517333.696293955, 517333.696293955, 168267.787656064], 
processed observation next is [1.0, 0.8260869565217391, 0.3333333333333337, 0.7433333333333334, 1.0, 1.0, 0.1922632377984035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14370380452609863, 0.14370380452609863, 0.25114595172546866], 
reward next is 0.7489, 
noisyNet noise sample is [array([3.0526195], dtype=float32), 1.0166974]. 
=============================================
[2019-04-10 11:29:45,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3143848e-16 1.0000000e+00 1.2345224e-18 2.5762388e-16 7.3467713e-26], sum to 1.0000
[2019-04-10 11:29:45,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0979
[2019-04-10 11:29:45,144] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 59.33333333333334, 1.0, 2.0, 0.9917971786540183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1512080.254822638, 1512080.254822638, 315012.3284870182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [27.6, 59.0, 1.0, 2.0, 0.99189320344214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1514832.984715675, 1514832.984715675, 315390.8894443341], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.59, 1.0, 1.0, 0.990232775231494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4207869401987986, 0.4207869401987986, 0.4707326708124389], 
reward next is 0.5293, 
noisyNet noise sample is [array([0.94406587], dtype=float32), -0.15434909]. 
=============================================
[2019-04-10 11:29:45,999] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5027431e-18 1.0000000e+00 6.9354752e-22 7.0312119e-23 1.3495334e-30], sum to 1.0000
[2019-04-10 11:29:46,037] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5781
[2019-04-10 11:29:46,048] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 94.0, 1.0, 2.0, 0.3523946565407417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 547468.391229226, 547468.3912292254, 170556.3222392198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1225800.0000, 
sim time next is 1226400.0000, 
raw observation next is [21.66666666666667, 94.33333333333333, 1.0, 2.0, 0.3556166477422023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552366.4180396085, 552366.4180396092, 170960.6782596908], 
processed observation next is [1.0, 0.17391304347826086, 0.22590837282780438, 0.9433333333333332, 1.0, 1.0, 0.22363451535205098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15343511612211347, 0.15343511612211366, 0.2551651914323743], 
reward next is 0.7448, 
noisyNet noise sample is [array([-0.44604474], dtype=float32), 0.581649]. 
=============================================
[2019-04-10 11:29:53,399] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8785651e-11 9.9998701e-01 1.4006674e-11 1.2959471e-05 5.6809288e-18], sum to 1.0000
[2019-04-10 11:29:53,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7927
[2019-04-10 11:29:53,422] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1907635.677776919 W.
[2019-04-10 11:29:53,438] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.5, 73.0, 1.0, 2.0, 0.4548071542635344, 1.0, 2.0, 0.4548071542635344, 1.0, 1.0, 0.7733652276618537, 6.9112, 6.9112, 170.5573041426782, 1907635.677776919, 1907635.677776919, 382590.0861499186], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1260000.0000, 
sim time next is 1260600.0000, 
raw observation next is [28.46666666666667, 73.16666666666667, 1.0, 2.0, 0.6824341325190121, 1.0, 2.0, 0.6824341325190121, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1908260.920761153, 1908260.920761153, 366687.1270927368], 
processed observation next is [1.0, 0.6086956521739131, 0.5481832543443919, 0.7316666666666667, 1.0, 1.0, 0.6173905211072435, 1.0, 1.0, 0.6173905211072435, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5300724779892092, 0.5300724779892092, 0.5472942195413981], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04858138], dtype=float32), 1.7018007]. 
=============================================
[2019-04-10 11:30:05,338] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 11:30:05,357] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:30:05,358] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:30:05,363] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:30:05,364] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:30:05,365] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:30:05,365] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:30:05,380] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:30:05,366] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:30:05,366] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:30:05,380] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:30:05,382] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run11
[2019-04-10 11:30:05,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-04-10 11:30:05,382] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run11
[2019-04-10 11:30:05,418] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run11
[2019-04-10 11:30:05,519] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run11
[2019-04-10 11:30:14,337] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:30:14,338] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.08333333333333, 91.83333333333334, 1.0, 2.0, 0.311811338062387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495304.7685855881, 495304.7685855881, 166715.9065330245]
[2019-04-10 11:30:14,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:30:14,342] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5649024e-19 1.0000000e+00 1.5726872e-22 1.7082089e-24 3.1443357e-31], sampled 0.8323121273973049
[2019-04-10 11:30:27,373] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:30:27,374] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.46666666666667, 92.66666666666667, 1.0, 2.0, 0.3767684419677563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575042.0037134396, 575042.0037134403, 172652.5203488798]
[2019-04-10 11:30:27,374] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:30:27,377] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5030024e-19 1.0000000e+00 1.7693466e-22 1.9469105e-24 3.0773847e-31], sampled 0.48334723510379807
[2019-04-10 11:30:30,219] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:30:30,220] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 89.33333333333334, 1.0, 2.0, 0.4184775930623051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 615912.0164337772, 615912.0164337772, 175790.8577585024]
[2019-04-10 11:30:30,222] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:30:30,223] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2803057e-19 1.0000000e+00 5.2536742e-23 1.9450249e-24 5.3390095e-32], sampled 0.02802948431460439
[2019-04-10 11:30:46,704] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:30:46,706] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.40664160333333, 73.65764824666667, 1.0, 2.0, 0.5833699164945113, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9845083462144515, 6.911200000000001, 6.9112, 168.9129565061013, 1631053.216447597, 1631053.216447597, 350809.5981262302]
[2019-04-10 11:30:46,706] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:30:46,708] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0410010e-13 1.0000000e+00 3.4135384e-14 1.7276683e-10 1.3772335e-21], sampled 0.9493149246324278
[2019-04-10 11:30:47,799] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:30:47,800] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.03333333333333, 71.0, 1.0, 2.0, 0.5492619214570611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767533.0667011031, 767533.0667011038, 191673.761798461]
[2019-04-10 11:30:47,800] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:30:47,803] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.0073290e-19 1.0000000e+00 1.1900602e-22 3.3420764e-23 1.3923768e-31], sampled 0.46374517194332887
[2019-04-10 11:30:49,578] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:30:49,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.2, 58.5, 1.0, 2.0, 1.009826394774507, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.0059938869092, 6.9112, 168.912315924127, 2308755.93284043, 2241506.141952642, 466357.7349029928]
[2019-04-10 11:30:49,580] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:30:49,582] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5357711e-12 1.0000000e+00 6.6990977e-13 1.1472972e-08 1.1407490e-19], sampled 0.2410536336929593
[2019-04-10 11:30:49,582] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2308755.93284043 W.
[2019-04-10 11:31:01,958] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:31:01,958] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.525435315, 78.85456069, 1.0, 2.0, 0.5011515372004876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700281.9407003131, 700281.9407003131, 183774.4526396506]
[2019-04-10 11:31:01,960] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:31:01,961] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.7863097e-19 1.0000000e+00 1.8591818e-22 5.7534156e-23 3.0068772e-31], sampled 0.14014196405947876
[2019-04-10 11:31:08,718] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:31:08,718] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.81935364666667, 82.24072832666667, 1.0, 2.0, 0.5644828974001848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788810.6026055004, 788810.6026054998, 194315.7456786159]
[2019-04-10 11:31:08,719] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:31:08,720] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0778422e-18 1.0000000e+00 3.9143268e-22 2.3456939e-22 5.5972221e-31], sampled 0.8586624348670622
[2019-04-10 11:31:10,538] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11273415], dtype=float32), 0.091949016]
[2019-04-10 11:31:10,538] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.31666666666666, 89.16666666666667, 1.0, 2.0, 0.6866506204013002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959605.3773132082, 959605.3773132089, 217991.2683076609]
[2019-04-10 11:31:10,541] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:31:10,543] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.5509647e-18 1.0000000e+00 5.4315434e-21 2.5028825e-21 2.5459554e-29], sampled 0.7916984179338863
[2019-04-10 11:31:28,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8266.2637 2926221834.8642 1309.0000
[2019-04-10 11:31:29,112] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8503.8493 2841763791.0890 1115.0000
[2019-04-10 11:31:29,119] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7909.9967 3160897177.7443 1699.0000
[2019-04-10 11:31:29,139] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8013.2184 3005704257.9187 1721.0000
[2019-04-10 11:31:29,299] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.3289 2778809039.6681 924.0000
[2019-04-10 11:31:30,314] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 250000, evaluation results [250000.0, 7909.996691911482, 3160897177.744325, 1699.0, 8266.263680138094, 2926221834.864195, 1309.0, 8664.328861143162, 2778809039.668081, 924.0, 8013.218397577398, 3005704257.9186916, 1721.0, 8503.849323549615, 2841763791.0890355, 1115.0]
[2019-04-10 11:31:45,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4908178e-18 1.0000000e+00 7.5875815e-21 4.3686320e-21 4.0521000e-29], sum to 1.0000
[2019-04-10 11:31:45,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7099
[2019-04-10 11:31:45,213] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3260160186877732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511347.4340524618, 511347.4340524618, 167795.3117008773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1561800.0000, 
sim time next is 1562400.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3257056396358542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 510860.6854647928, 510860.6854647922, 167757.8591853594], 
processed observation next is [1.0, 0.08695652173913043, 0.2274881516587678, 0.91, 1.0, 1.0, 0.18759715618777614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14190574596244246, 0.14190574596244226, 0.2503848644557603], 
reward next is 0.7496, 
noisyNet noise sample is [array([0.31045303], dtype=float32), -0.63085157]. 
=============================================
[2019-04-10 11:31:45,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2484863e-17 1.0000000e+00 5.7526127e-20 2.6868843e-19 1.0136320e-27], sum to 1.0000
[2019-04-10 11:31:45,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7541
[2019-04-10 11:31:45,433] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 85.0, 1.0, 2.0, 0.7538478250672412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1147119.853769633, 1147119.853769633, 245105.3527034428], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1590000.0000, 
sim time next is 1590600.0000, 
raw observation next is [23.58333333333333, 85.0, 1.0, 2.0, 0.7450475251985302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1133159.320191859, 1133159.320191859, 242818.495101554], 
processed observation next is [1.0, 0.391304347826087, 0.31674565560821466, 0.85, 1.0, 1.0, 0.692828343612687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.314766477831072, 0.314766477831072, 0.3624156643306776], 
reward next is 0.6376, 
noisyNet noise sample is [array([-1.2846045], dtype=float32), -0.63019586]. 
=============================================
[2019-04-10 11:31:48,517] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0555863e-17 1.0000000e+00 1.9138964e-19 1.2936253e-18 1.5707945e-27], sum to 1.0000
[2019-04-10 11:31:48,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6378
[2019-04-10 11:31:48,576] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.0, 1.0, 2.0, 0.8618038511760767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1204523.687623024, 1204523.687623024, 259643.2187026259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [25.31666666666667, 90.50000000000001, 1.0, 2.0, 0.9560773333030395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336370.52086463, 1336370.52086463, 285825.2544364419], 
processed observation next is [1.0, 0.43478260869565216, 0.39889415481832563, 0.9050000000000001, 1.0, 1.0, 0.9470811244614933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3712140335735083, 0.3712140335735083, 0.42660485736782366], 
reward next is 0.5734, 
noisyNet noise sample is [array([1.2704808], dtype=float32), 0.91817117]. 
=============================================
[2019-04-10 11:31:53,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8054817e-16 1.0000000e+00 8.2243823e-19 4.8183727e-17 5.4727071e-27], sum to 1.0000
[2019-04-10 11:31:53,185] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9388
[2019-04-10 11:31:53,213] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 89.5, 1.0, 2.0, 0.9582083846393582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1339351.106769004, 1339351.106769004, 286447.7388908998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1679400.0000, 
sim time next is 1680000.0000, 
raw observation next is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
processed observation next is [1.0, 0.43478260869565216, 0.4154818325434437, 0.89, 1.0, 1.0, 0.9098415594672309, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35920581814834446, 0.35920581814834446, 0.41336043502656483], 
reward next is 0.5866, 
noisyNet noise sample is [array([-2.0811799], dtype=float32), -0.8657951]. 
=============================================
[2019-04-10 11:31:53,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.550545]
 [66.92551 ]
 [67.12675 ]
 [67.60317 ]
 [67.6668  ]], R is [[66.28465271]
 [66.19426727]
 [66.12585449]
 [66.03799438]
 [65.99008942]].
[2019-04-10 11:32:05,244] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3937679e-14 1.0000000e+00 8.5932829e-17 7.6542112e-14 8.2466983e-24], sum to 1.0000
[2019-04-10 11:32:05,251] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5426
[2019-04-10 11:32:05,258] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1709070.507677749 W.
[2019-04-10 11:32:05,263] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.6, 87.0, 1.0, 2.0, 0.6112514849886735, 0.0, 2.0, 0.0, 1.0, 1.0, 1.019172840185394, 6.9112, 6.9112, 168.912522321832, 1709070.507677749, 1709070.507677749, 365264.8798667505], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1855800.0000, 
sim time next is 1856400.0000, 
raw observation next is [25.7, 86.66666666666666, 1.0, 2.0, 0.5794125237812582, 1.0, 1.0, 0.5794125237812582, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1619968.315858412, 1619968.315858412, 326761.0008651068], 
processed observation next is [1.0, 0.4782608695652174, 0.4170616113744076, 0.8666666666666666, 1.0, 1.0, 0.4932681009412749, 1.0, 0.5, 0.4932681009412749, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4499911988495589, 0.4499911988495589, 0.4877029863658311], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28745663], dtype=float32), 0.2010716]. 
=============================================
[2019-04-10 11:32:06,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8167588e-14 1.0000000e+00 1.6849367e-16 2.0257691e-12 1.3912039e-23], sum to 1.0000
[2019-04-10 11:32:06,206] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0327
[2019-04-10 11:32:06,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1764700.224961794 W.
[2019-04-10 11:32:06,215] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.08333333333334, 83.33333333333334, 1.0, 2.0, 0.6311360597785313, 1.0, 1.0, 0.6311360597785313, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1764700.224961794, 1764700.224961794, 346031.1012360798], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1865400.0000, 
sim time next is 1866000.0000, 
raw observation next is [27.06666666666667, 83.66666666666667, 1.0, 2.0, 0.5824676816433633, 1.0, 2.0, 0.5824676816433633, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1628516.662889316, 1628516.662889316, 327861.0144518954], 
processed observation next is [1.0, 0.6086956521739131, 0.48183254344391807, 0.8366666666666667, 1.0, 1.0, 0.4969490140281485, 1.0, 1.0, 0.4969490140281485, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.45236573969147664, 0.45236573969147664, 0.4893447976893961], 
reward next is 0.5107, 
noisyNet noise sample is [array([1.4704853], dtype=float32), 0.80518264]. 
=============================================
[2019-04-10 11:32:06,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.386143]
 [65.272545]
 [65.90477 ]
 [66.81588 ]
 [67.57299 ]], R is [[60.67710114]
 [60.55386734]
 [59.94832993]
 [59.34884644]
 [59.29380798]].
[2019-04-10 11:32:16,546] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5488406e-17 1.0000000e+00 1.7080551e-21 8.4630330e-23 3.2199961e-30], sum to 1.0000
[2019-04-10 11:32:16,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6079
[2019-04-10 11:32:16,567] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 94.0, 1.0, 2.0, 0.505891989172379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706908.1948985324, 706908.194898533, 184524.5759677409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2020800.0000, 
sim time next is 2021400.0000, 
raw observation next is [25.55, 94.0, 1.0, 2.0, 0.5061863276668502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707319.625740364, 707319.6257403634, 184571.1629275885], 
processed observation next is [0.0, 0.391304347826087, 0.40995260663507116, 0.94, 1.0, 1.0, 0.4050437682733135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19647767381676776, 0.19647767381676762, 0.27547934765311716], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.4724525], dtype=float32), -1.0106229]. 
=============================================
[2019-04-10 11:32:17,022] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3750518e-18 1.0000000e+00 1.8833384e-21 2.9432243e-23 4.8114324e-31], sum to 1.0000
[2019-04-10 11:32:17,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-10 11:32:17,061] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 95.5, 1.0, 2.0, 0.4817375651242758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673145.3152481278, 673145.3152481284, 180784.2955881653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2010600.0000, 
sim time next is 2011200.0000, 
raw observation next is [24.83333333333333, 95.33333333333333, 1.0, 2.0, 0.4835651054196176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675699.8003626856, 675699.8003626862, 181061.2809112976], 
processed observation next is [0.0, 0.2608695652173913, 0.3759873617693521, 0.9533333333333333, 1.0, 1.0, 0.3777892836380935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1876943889896349, 0.18769438898963506, 0.27024071777805614], 
reward next is 0.7298, 
noisyNet noise sample is [array([-0.3833845], dtype=float32), -1.0850817]. 
=============================================
[2019-04-10 11:32:30,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8034148e-18 1.0000000e+00 1.6422258e-21 1.3160918e-21 1.7795265e-30], sum to 1.0000
[2019-04-10 11:32:30,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2396
[2019-04-10 11:32:30,909] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 77.0, 1.0, 2.0, 0.5650146126120394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789553.8996732334, 789553.8996732334, 194409.2909536631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2138400.0000, 
sim time next is 2139000.0000, 
raw observation next is [29.53333333333333, 77.83333333333334, 1.0, 2.0, 0.5632618569419116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787103.6837038591, 787103.6837038591, 194101.4101277471], 
processed observation next is [0.0, 0.782608695652174, 0.598736176935229, 0.7783333333333334, 1.0, 1.0, 0.47380946619507414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21863991213996087, 0.21863991213996087, 0.2897035972055927], 
reward next is 0.7103, 
noisyNet noise sample is [array([0.62970966], dtype=float32), 1.40863]. 
=============================================
[2019-04-10 11:32:30,932] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.318054]
 [74.229645]
 [74.18881 ]
 [74.14543 ]
 [74.105736]], R is [[74.36378479]
 [74.32998657]
 [74.2964325 ]
 [74.26300812]
 [74.2293396 ]].
[2019-04-10 11:32:33,726] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7023975e-19 1.0000000e+00 8.5899140e-23 7.0386492e-22 2.0094714e-31], sum to 1.0000
[2019-04-10 11:32:33,727] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4145
[2019-04-10 11:32:33,743] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 79.33333333333334, 1.0, 2.0, 0.5582713141869784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780127.3244184323, 780127.3244184323, 193229.8405992058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2230800.0000, 
sim time next is 2231400.0000, 
raw observation next is [29.06666666666666, 80.16666666666666, 1.0, 2.0, 0.5576038116014002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779194.2153856759, 779194.2153856759, 193113.7596043959], 
processed observation next is [1.0, 0.8260869565217391, 0.5766192733017375, 0.8016666666666665, 1.0, 1.0, 0.46699254409807245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2164428376071322, 0.2164428376071322, 0.28822949194685954], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.4393753], dtype=float32), 1.943493]. 
=============================================
[2019-04-10 11:32:34,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5174629e-16 1.0000000e+00 1.1330689e-18 9.1366863e-18 2.2567395e-26], sum to 1.0000
[2019-04-10 11:32:34,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7809
[2019-04-10 11:32:34,039] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 96.0, 1.0, 2.0, 0.6021880596426645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 841520.8157285784, 841520.8157285777, 201146.3751646629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2174400.0000, 
sim time next is 2175000.0000, 
raw observation next is [24.75, 96.16666666666666, 1.0, 2.0, 0.6231774175942596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 870864.1788365295, 870864.1788365302, 205134.0757047103], 
processed observation next is [1.0, 0.17391304347826086, 0.3720379146919432, 0.9616666666666666, 1.0, 1.0, 0.5459968886677826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2419067163434804, 0.2419067163434806, 0.30617026224583627], 
reward next is 0.6938, 
noisyNet noise sample is [array([-1.0061096], dtype=float32), 1.2555124]. 
=============================================
[2019-04-10 11:32:34,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[66.66916]
 [66.68097]
 [66.63641]
 [66.48872]
 [66.33328]], R is [[66.50386047]
 [66.53860474]
 [66.57242584]
 [66.60325623]
 [66.63049316]].
[2019-04-10 11:32:45,474] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3365460e-17 1.0000000e+00 1.3117425e-19 7.6535610e-16 2.6651910e-27], sum to 1.0000
[2019-04-10 11:32:45,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6130
[2019-04-10 11:32:45,485] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.56666666666667, 68.66666666666667, 1.0, 2.0, 0.5619937678028335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785330.9973359333, 785330.9973359333, 193880.3136418755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [31.43333333333334, 69.33333333333334, 1.0, 2.0, 0.5688671780349456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794939.5060654316, 794939.5060654316, 195089.993033727], 
processed observation next is [1.0, 0.782608695652174, 0.6887835703001584, 0.6933333333333335, 1.0, 1.0, 0.48056286510234403, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2208165294626199, 0.2208165294626199, 0.29117909408018955], 
reward next is 0.7088, 
noisyNet noise sample is [array([-0.05614809], dtype=float32), 0.124214344]. 
=============================================
[2019-04-10 11:32:46,261] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-10 11:32:46,262] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:32:46,263] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:32:46,264] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:32:46,265] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:32:46,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:32:46,267] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:32:46,266] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:32:46,267] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:32:46,266] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:32:46,269] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:32:46,275] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-04-10 11:32:46,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run12
[2019-04-10 11:32:46,294] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run12
[2019-04-10 11:32:46,294] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run12
[2019-04-10 11:32:46,294] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run12
[2019-04-10 11:32:51,088] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:32:51,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.0324615, 84.49247822, 1.0, 2.0, 0.2865284715075066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462610.4146639381, 462610.4146639375, 164448.6018881731]
[2019-04-10 11:32:51,090] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:32:51,091] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1821632e-17 1.0000000e+00 4.4487498e-21 2.7660124e-22 2.6612464e-29], sampled 0.10005026598652156
[2019-04-10 11:32:51,588] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:32:51,590] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.10485359333333, 79.8583852, 1.0, 2.0, 0.3590392581970994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579563.8793614194, 579563.8793614194, 173346.5613246456]
[2019-04-10 11:32:51,591] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:32:51,595] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8376787e-18 1.0000000e+00 7.0724003e-22 1.9887824e-23 2.3683422e-30], sampled 0.3169723887802217
[2019-04-10 11:32:51,768] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:32:51,769] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.65, 83.5, 1.0, 2.0, 0.231066934285603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381818.5767483634, 381818.5767483634, 159035.0682385594]
[2019-04-10 11:32:51,770] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:32:51,771] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6686304e-18 1.0000000e+00 7.3111396e-22 5.7383987e-24 2.7438897e-30], sampled 0.3503722383670369
[2019-04-10 11:33:00,179] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:00,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.22233864166667, 97.31820716666667, 1.0, 2.0, 0.4394934197455064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642529.8415040801, 642529.8415040801, 178277.6209708631]
[2019-04-10 11:33:00,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:33:00,184] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4903793e-17 1.0000000e+00 2.5451805e-20 1.6146524e-20 2.7943096e-28], sampled 0.6533927783843565
[2019-04-10 11:33:13,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:13,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.56666666666667, 81.33333333333334, 1.0, 2.0, 0.4980609527384366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695961.9111179381, 695961.9111179381, 183291.6938047375]
[2019-04-10 11:33:13,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:33:13,986] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3022549e-18 1.0000000e+00 1.6660458e-21 4.8625093e-22 5.2947997e-30], sampled 0.14815826514912345
[2019-04-10 11:33:16,241] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:16,242] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.18333333333333, 63.5, 1.0, 2.0, 0.4992002175495853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697554.3773417699, 697554.3773417699, 183470.9101903871]
[2019-04-10 11:33:16,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:33:16,246] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0392225e-18 1.0000000e+00 8.4165471e-22 2.0885010e-22 2.6310334e-30], sampled 0.4437031042341787
[2019-04-10 11:33:31,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:31,363] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.6, 61.33333333333334, 1.0, 2.0, 0.5688427271764683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794905.3254566747, 794905.3254566747, 195084.6620088113]
[2019-04-10 11:33:31,363] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:33:31,366] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6829874e-18 1.0000000e+00 3.8926926e-21 5.5597127e-21 1.5063815e-29], sampled 0.03677030386197422
[2019-04-10 11:33:34,142] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:34,144] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.17364629666666, 64.75423657333334, 1.0, 2.0, 0.5351501930211845, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9051026968710272, 6.911200000000001, 6.9112, 168.9129565058953, 1496139.906585254, 1496139.906585253, 323016.5738250969]
[2019-04-10 11:33:34,145] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:33:34,148] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7042118e-10 9.9997330e-01 8.0676188e-11 2.6719548e-05 9.9368661e-18], sampled 0.906362876085116
[2019-04-10 11:33:36,180] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:36,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.41899848, 79.15703522, 1.0, 2.0, 0.490978518840254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689798.1293334153, 689798.1293334146, 182674.4966173026]
[2019-04-10 11:33:36,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:33:36,185] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4586217e-18 1.0000000e+00 3.3865485e-21 2.7304224e-21 1.5805905e-29], sampled 0.6127345692857443
[2019-04-10 11:33:38,231] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:38,232] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.449823755, 65.10736966, 1.0, 2.0, 0.5391186229068721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753353.8972600457, 753353.897260045, 189950.6690596145]
[2019-04-10 11:33:38,232] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:33:38,235] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8297343e-17 1.0000000e+00 9.7606149e-21 1.4438900e-21 6.7910144e-29], sampled 0.32710083654480593
[2019-04-10 11:33:41,566] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:41,567] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.35947272, 93.31038656, 1.0, 2.0, 0.7748443541462775, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005979913813366, 6.9112, 168.9123932085047, 1979867.707364058, 1912627.798668749, 401723.6227422815]
[2019-04-10 11:33:41,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:33:41,571] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.6880465e-09 7.9778719e-01 5.9362986e-09 2.0221277e-01 2.2068823e-15], sampled 0.4582724855876764
[2019-04-10 11:33:41,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1979867.707364058 W.
[2019-04-10 11:33:59,347] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11612038], dtype=float32), 0.09542315]
[2019-04-10 11:33:59,348] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.75030818, 95.18074439, 1.0, 2.0, 0.4039612463103124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605010.6155149802, 605010.6155149802, 175058.8211141186]
[2019-04-10 11:33:59,348] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:33:59,350] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.58676335e-18 1.00000000e+00 8.18271760e-22 1.03470366e-22
 2.37884547e-30], sampled 0.5384799355637125
[2019-04-10 11:34:11,860] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8289.7423 2924123336.0923 1249.0000
[2019-04-10 11:34:11,889] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.3964 2778464031.7327 913.0000
[2019-04-10 11:34:11,954] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8519.5908 2839857124.9211 1065.0000
[2019-04-10 11:34:11,990] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7950.2062 3156438580.9437 1576.0000
[2019-04-10 11:34:12,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8051.9918 3001330932.2505 1607.0000
[2019-04-10 11:34:13,132] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 275000, evaluation results [275000.0, 7950.206213831438, 3156438580.943653, 1576.0, 8289.742343657357, 2924123336.0923223, 1249.0, 8663.396416170117, 2778464031.7327194, 913.0, 8051.991808908659, 3001330932.2504773, 1607.0, 8519.590808010531, 2839857124.9210577, 1065.0]
[2019-04-10 11:34:19,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2937733e-10 1.3108934e-01 7.8003065e-10 8.6891067e-01 6.3222375e-16], sum to 1.0000
[2019-04-10 11:34:19,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2103
[2019-04-10 11:34:19,719] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.81666666666666, 83.66666666666667, 1.0, 2.0, 0.5257245830605222, 1.0, 1.0, 0.5257245830605222, 1.0, 2.0, 0.9040471126103469, 6.911200000000001, 6.9112, 170.5573041426782, 2205396.764275386, 2205396.764275385, 431673.5061868815], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2477400.0000, 
sim time next is 2478000.0000, 
raw observation next is [27.93333333333333, 83.33333333333334, 1.0, 2.0, 0.6984003070860192, 1.0, 2.0, 0.6984003070860192, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1952947.131664909, 1952947.131664909, 373439.5408205841], 
processed observation next is [1.0, 0.6956521739130435, 0.522906793048973, 0.8333333333333335, 1.0, 1.0, 0.636626876007252, 1.0, 1.0, 0.636626876007252, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5424853143513636, 0.5424853143513636, 0.5573724489859464], 
reward next is 0.4426, 
noisyNet noise sample is [array([-0.29705557], dtype=float32), 0.18664993]. 
=============================================
[2019-04-10 11:34:19,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[53.274277]
 [55.265633]
 [55.083   ]
 [55.725166]
 [54.063774]], R is [[54.74207687]
 [54.19465637]
 [53.65270996]
 [53.57894516]
 [53.5202713 ]].
[2019-04-10 11:34:36,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.21011364e-18 1.00000000e+00 3.69359014e-22 1.05642435e-22
 2.33030846e-30], sum to 1.0000
[2019-04-10 11:34:36,918] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7182
[2019-04-10 11:34:36,922] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.3976522725571837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591578.3125612068, 591578.3125612062, 173709.2446772233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2679000.0000, 
sim time next is 2679600.0000, 
raw observation next is [23.0, 96.0, 1.0, 2.0, 0.4020926291463947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595566.2606575809, 595566.2606575809, 173999.560208433], 
processed observation next is [0.0, 0.0, 0.28909952606635075, 0.96, 1.0, 1.0, 0.2796296736703551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1654350724048836, 0.1654350724048836, 0.2597008361319895], 
reward next is 0.7403, 
noisyNet noise sample is [array([0.2907573], dtype=float32), -0.5609057]. 
=============================================
[2019-04-10 11:34:38,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5234000e-19 1.0000000e+00 4.7948698e-23 1.9697397e-24 5.4068508e-32], sum to 1.0000
[2019-04-10 11:34:38,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9298
[2019-04-10 11:34:38,153] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.5016134653787779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700927.6270045955, 700927.6270045955, 183849.2239025121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2643600.0000, 
sim time next is 2644200.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4980252465494651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695912.0009869435, 695912.0009869428, 183287.144818409], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.815, 1.0, 1.0, 0.3952111404210423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19330888916303987, 0.19330888916303968, 0.27356290271404327], 
reward next is 0.7264, 
noisyNet noise sample is [array([-1.1886548], dtype=float32), 0.011880848]. 
=============================================
[2019-04-10 11:34:46,965] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.29578422e-17 1.00000000e+00 1.01124494e-20 2.05296551e-20
 1.11781764e-29], sum to 1.0000
[2019-04-10 11:34:46,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5419
[2019-04-10 11:34:47,016] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 91.5, 1.0, 2.0, 0.527734823461151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796695.6009873647, 796695.6009873652, 195321.1992612997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2896200.0000, 
sim time next is 2896800.0000, 
raw observation next is [23.0, 92.33333333333334, 1.0, 2.0, 0.516654771935371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777291.1511361116, 777291.1511361116, 193052.4794744494], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.9233333333333335, 1.0, 1.0, 0.41765635172936266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2159142086489199, 0.2159142086489199, 0.2881380290663424], 
reward next is 0.7119, 
noisyNet noise sample is [array([1.7292308], dtype=float32), 1.0061744]. 
=============================================
[2019-04-10 11:34:57,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0125829e-19 1.0000000e+00 1.7545741e-21 8.3769291e-21 6.4119895e-30], sum to 1.0000
[2019-04-10 11:34:57,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4699
[2019-04-10 11:34:57,104] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5216300405108971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 824621.5375973925, 824621.5375973919, 198019.8620681615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2996400.0000, 
sim time next is 2997000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5037235837189078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796358.1181283569, 796358.1181283569, 194773.7812918411], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 1.0, 1.0, 0.4020766068902503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22121058836898802, 0.22121058836898802, 0.29070713625647926], 
reward next is 0.7093, 
noisyNet noise sample is [array([-0.8785125], dtype=float32), 0.38521323]. 
=============================================
[2019-04-10 11:34:57,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.17887 ]
 [71.93652 ]
 [72.03393 ]
 [72.121185]
 [72.05279 ]], R is [[72.3616333 ]
 [72.34246063]
 [72.29944611]
 [72.25799561]
 [72.23304749]].
[2019-04-10 11:35:03,877] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7522587e-20 1.0000000e+00 4.1710232e-23 9.3063491e-24 2.9033673e-32], sum to 1.0000
[2019-04-10 11:35:03,878] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4233
[2019-04-10 11:35:03,946] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3064534176698839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488008.0487519372, 488008.0487519379, 166199.4117093935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3008400.0000, 
sim time next is 3009000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3060499230171704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487365.5165384243, 487365.5165384243, 166152.7471790351], 
processed observation next is [1.0, 0.8260869565217391, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16391556990020525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1353793101495623, 0.1353793101495623, 0.24798917489408223], 
reward next is 0.7520, 
noisyNet noise sample is [array([-1.1733001], dtype=float32), 0.9271437]. 
=============================================
[2019-04-10 11:35:03,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.99304 ]
 [76.8839  ]
 [76.75058 ]
 [76.593414]
 [76.43169 ]], R is [[77.10948944]
 [77.09033203]
 [77.0714035 ]
 [77.05259705]
 [77.03381348]].
[2019-04-10 11:35:16,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3802810e-18 1.0000000e+00 1.3554567e-22 1.5439035e-24 7.7923464e-31], sum to 1.0000
[2019-04-10 11:35:16,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4091
[2019-04-10 11:35:16,839] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4784003462120729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668480.6591032878, 668480.6591032878, 180281.2126089916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3226200.0000, 
sim time next is 3226800.0000, 
raw observation next is [27.33333333333334, 77.33333333333334, 1.0, 2.0, 0.4801970682299169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670992.0562059153, 670992.0562059153, 180551.6786721637], 
processed observation next is [0.0, 0.34782608695652173, 0.4944707740916275, 0.7733333333333334, 1.0, 1.0, 0.373731407505924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1863866822794209, 0.1863866822794209, 0.2694801174211398], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.6104124], dtype=float32), -0.58862144]. 
=============================================
[2019-04-10 11:35:18,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.2053260e-20 1.0000000e+00 4.0986732e-23 5.2801320e-24 6.3753976e-32], sum to 1.0000
[2019-04-10 11:35:18,757] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4815
[2019-04-10 11:35:18,760] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 77.66666666666667, 1.0, 2.0, 0.5705677971913229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 797316.8567040372, 797316.8567040366, 195390.7187670969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3264000.0000, 
sim time next is 3264600.0000, 
raw observation next is [29.33333333333333, 78.33333333333334, 1.0, 2.0, 0.5638418887388965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787914.5223730415, 787914.5223730408, 194202.8663751448], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494469, 0.7833333333333334, 1.0, 1.0, 0.47450829968541747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21886514510362265, 0.21886514510362245, 0.28985502444051464], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.08943889], dtype=float32), 0.55045956]. 
=============================================
[2019-04-10 11:35:20,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3450366e-18 1.0000000e+00 7.1539935e-22 2.7426018e-23 7.6425616e-31], sum to 1.0000
[2019-04-10 11:35:20,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0837
[2019-04-10 11:35:20,540] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.4921111322998788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687645.2811283108, 687645.2811283114, 182369.4389374428], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3313800.0000, 
sim time next is 3314400.0000, 
raw observation next is [28.66666666666666, 71.33333333333333, 1.0, 2.0, 0.4935120880577622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689603.5243930613, 689603.5243930606, 182585.9867746901], 
processed observation next is [0.0, 0.34782608695652173, 0.5576619273301735, 0.7133333333333333, 1.0, 1.0, 0.38977360006959305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19155653455362814, 0.19155653455362795, 0.27251639817117923], 
reward next is 0.7275, 
noisyNet noise sample is [array([0.66226995], dtype=float32), -0.33562618]. 
=============================================
[2019-04-10 11:35:20,679] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-10 11:35:20,681] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:35:20,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:35:20,682] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:35:20,682] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:35:20,682] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:35:20,683] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:35:20,684] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:35:20,684] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:35:20,685] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:35:20,684] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:35:20,698] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run13
[2019-04-10 11:35:20,710] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run13
[2019-04-10 11:35:20,711] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run13
[2019-04-10 11:35:20,743] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run13
[2019-04-10 11:35:20,760] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run13
[2019-04-10 11:35:27,233] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11393766], dtype=float32), 0.09302172]
[2019-04-10 11:35:27,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.65, 85.66666666666667, 1.0, 2.0, 0.3144465720124828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501421.4227728502, 501421.4227728502, 167193.7795349912]
[2019-04-10 11:35:27,237] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:35:27,239] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0109406e-19 1.0000000e+00 3.0122165e-23 9.2301183e-25 3.2245902e-32], sampled 0.544787366163951
[2019-04-10 11:35:48,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11393766], dtype=float32), 0.09302172]
[2019-04-10 11:35:48,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.8, 70.0, 1.0, 2.0, 0.5383372783804518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752261.6749679194, 752261.6749679194, 189819.9813311149]
[2019-04-10 11:35:48,369] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:35:48,371] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1879807e-18 1.0000000e+00 3.4271591e-22 4.8704152e-23 6.6543245e-31], sampled 0.58394845225699
[2019-04-10 11:35:54,360] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11393766], dtype=float32), 0.09302172]
[2019-04-10 11:35:54,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.0, 57.5, 1.0, 2.0, 0.5851184106941187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817657.8484618941, 817657.8484618941, 198006.148685387]
[2019-04-10 11:35:54,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:35:54,364] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1081437e-18 1.0000000e+00 3.1557778e-22 4.7956382e-23 6.1210445e-31], sampled 0.2797215584828493
[2019-04-10 11:35:54,864] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11393766], dtype=float32), 0.09302172]
[2019-04-10 11:35:54,865] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 68.0, 1.0, 2.0, 0.541624054099182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756856.1854636786, 756856.1854636786, 190373.7668322516]
[2019-04-10 11:35:54,865] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:35:54,867] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2231091e-18 1.0000000e+00 6.7148143e-22 2.6000628e-23 1.7409982e-30], sampled 0.9510329266531534
[2019-04-10 11:36:43,686] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11393766], dtype=float32), 0.09302172]
[2019-04-10 11:36:43,687] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.53333333333333, 59.5, 1.0, 2.0, 0.786100790026248, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005980660166442, 6.9112, 168.9123160057753, 1995621.118134625, 1928380.710685291, 404424.5534382718]
[2019-04-10 11:36:43,687] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:36:43,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.6837072e-11 9.9999321e-01 3.8040425e-11 6.8012882e-06 3.6177137e-18], sampled 0.5084459627591114
[2019-04-10 11:36:43,690] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1995621.118134625 W.
[2019-04-10 11:36:47,823] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.9831 2926370808.8378 1310.0000
[2019-04-10 11:36:48,243] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8013.3720 3005640944.9074 1717.0000
[2019-04-10 11:36:48,366] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8504.8121 2841723921.9419 1113.0000
[2019-04-10 11:36:48,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7914.7807 3160818144.6435 1693.0000
[2019-04-10 11:36:48,644] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3384 2779002991.0739 929.0000
[2019-04-10 11:36:49,658] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 300000, evaluation results [300000.0, 7914.780721174006, 3160818144.64347, 1693.0, 8263.983099510833, 2926370808.8377676, 1310.0, 8661.338420980926, 2779002991.073857, 929.0, 8013.372017337116, 3005640944.9074216, 1717.0, 8504.812058643378, 2841723921.9418616, 1113.0]
[2019-04-10 11:36:50,270] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5196073e-18 1.0000000e+00 7.4914311e-22 1.7177314e-22 7.5185663e-31], sum to 1.0000
[2019-04-10 11:36:50,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0894
[2019-04-10 11:36:50,278] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4842734569432579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676689.9161190981, 676689.9161190981, 181168.4367642173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3283800.0000, 
sim time next is 3284400.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4829951644860992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674903.1520813071, 674903.1520813066, 180974.3376002834], 
processed observation next is [0.0, 0.0, 0.4628751974723541, 0.8066666666666668, 1.0, 1.0, 0.3771026078145774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1874730978003631, 0.18747309780036292, 0.27011095164221405], 
reward next is 0.7299, 
noisyNet noise sample is [array([0.33095005], dtype=float32), 0.15055485]. 
=============================================
[2019-04-10 11:36:52,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1865993e-18 1.0000000e+00 2.8213203e-22 1.4416327e-22 3.0766336e-31], sum to 1.0000
[2019-04-10 11:36:52,389] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9900
[2019-04-10 11:36:52,403] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.5, 1.0, 2.0, 0.5371186532427136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750558.1911082439, 750558.1911082446, 189615.812435779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3357000.0000, 
sim time next is 3357600.0000, 
raw observation next is [28.0, 80.66666666666667, 1.0, 2.0, 0.532547906146615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744168.8889322068, 744168.8889322075, 188852.4513456892], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8066666666666668, 1.0, 1.0, 0.4368047062007409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20671358025894632, 0.2067135802589465, 0.28186933036670025], 
reward next is 0.7181, 
noisyNet noise sample is [array([0.40826365], dtype=float32), 1.4838345]. 
=============================================
[2019-04-10 11:36:58,972] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9949349e-18 1.0000000e+00 8.5802944e-21 9.3362461e-18 1.2838056e-29], sum to 1.0000
[2019-04-10 11:36:58,978] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9262
[2019-04-10 11:36:58,982] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5430648807823452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758870.2912250569, 758870.2912250574, 190619.3539496338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3435600.0000, 
sim time next is 3436200.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.538849562144199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752977.7837121148, 752977.7837121154, 189907.4777205655], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.72, 1.0, 1.0, 0.4443970628243361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20916049547558743, 0.2091604954755876, 0.28344399659785896], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.6186621], dtype=float32), -0.22106253]. 
=============================================
[2019-04-10 11:37:00,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2892433e-17 1.0000000e+00 1.9383036e-20 7.3150251e-20 2.0214373e-28], sum to 1.0000
[2019-04-10 11:37:00,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3214
[2019-04-10 11:37:00,141] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5115172011573851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714771.2296816761, 714771.2296816761, 185420.167520429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3449400.0000, 
sim time next is 3450000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5126816872501607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716398.979103515, 716398.979103515, 185606.722174719], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.412869502711037, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19899971641764308, 0.19899971641764308, 0.27702495846972985], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.10187747], dtype=float32), -1.1019349]. 
=============================================
[2019-04-10 11:37:00,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.14731 ]
 [71.033066]
 [71.0747  ]
 [71.29498 ]
 [71.218605]], R is [[71.14614868]
 [71.1579361 ]
 [71.16965485]
 [71.18073273]
 [71.19110107]].
[2019-04-10 11:37:02,420] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.2055561e-10 9.9997747e-01 3.1250089e-11 2.2586872e-05 6.5013670e-17], sum to 1.0000
[2019-04-10 11:37:02,427] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1026
[2019-04-10 11:37:02,428] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1836544.904654201 W.
[2019-04-10 11:37:02,493] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.6568089427552405, 1.0, 2.0, 0.6568089427552405, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1836544.904654201, 1836544.904654201, 356176.341883883], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3488400.0000, 
sim time next is 3489000.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.4547319206752379, 1.0, 2.0, 0.4547319206752379, 1.0, 1.0, 0.7757081223933603, 6.9112, 6.9112, 170.5573041426782, 1907319.838367826, 1907319.838367826, 382957.2411722079], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.34305050683763605, 1.0, 1.0, 0.34305050683763605, 1.0, 0.5, 0.7264733199919027, 0.0, 0.0, 0.8375144448122397, 0.529811066213285, 0.529811066213285, 0.5715779718988178], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5799468], dtype=float32), 1.8321804]. 
=============================================
[2019-04-10 11:37:02,505] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[49.4366  ]
 [48.517845]
 [51.031975]
 [54.67303 ]
 [60.87104 ]], R is [[48.06721878]
 [48.05494308]
 [47.9825592 ]
 [47.91736221]
 [47.83060837]].
[2019-04-10 11:37:08,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3457728e-16 1.0000000e+00 1.6854622e-18 3.6841413e-18 3.3907023e-26], sum to 1.0000
[2019-04-10 11:37:08,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2643
[2019-04-10 11:37:08,696] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6136135355518414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857493.6615310544, 857493.6615310544, 203300.2973600092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3649200.0000, 
sim time next is 3649800.0000, 
raw observation next is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6202235523851146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866734.5923844696, 866734.5923844696, 204564.513677921], 
processed observation next is [1.0, 0.21739130434782608, 0.470774091627172, 0.7983333333333335, 1.0, 1.0, 0.5424380149218249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.240759608995686, 0.240759608995686, 0.3053201696685388], 
reward next is 0.6947, 
noisyNet noise sample is [array([1.6796132], dtype=float32), -1.4391434]. 
=============================================
[2019-04-10 11:37:20,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3999179e-12 9.9999774e-01 2.3162748e-14 2.3086143e-06 4.1108524e-20], sum to 1.0000
[2019-04-10 11:37:20,078] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5467
[2019-04-10 11:37:20,115] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 64.33333333333334, 1.0, 2.0, 0.5500947618460185, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768697.2905700912, 768697.2905700918, 191819.8405200924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3777600.0000, 
sim time next is 3778200.0000, 
raw observation next is [32.5, 65.0, 1.0, 2.0, 0.5456485488708371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762481.964040226, 762481.964040226, 191060.3626144163], 
processed observation next is [1.0, 0.7391304347826086, 0.7393364928909952, 0.65, 1.0, 1.0, 0.4525886130973941, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21180054556672942, 0.21180054556672942, 0.2851647203200243], 
reward next is 0.7148, 
noisyNet noise sample is [array([-0.29788154], dtype=float32), -1.947967]. 
=============================================
[2019-04-10 11:37:24,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3182098e-17 1.0000000e+00 5.2141475e-21 4.6643111e-22 2.1103329e-29], sum to 1.0000
[2019-04-10 11:37:24,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5555
[2019-04-10 11:37:24,272] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 68.5, 1.0, 2.0, 0.5806769320256622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811448.8537698874, 811448.8537698874, 197202.1880360653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3835800.0000, 
sim time next is 3836400.0000, 
raw observation next is [32.33333333333334, 68.0, 1.0, 2.0, 0.5878277055969644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 821445.3431426948, 821445.3431426955, 198501.1549030291], 
processed observation next is [0.0, 0.391304347826087, 0.7314375987361774, 0.68, 1.0, 1.0, 0.5034068742132101, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2281792619840819, 0.2281792619840821, 0.2962703804522822], 
reward next is 0.7037, 
noisyNet noise sample is [array([-0.7937214], dtype=float32), -0.50125176]. 
=============================================
[2019-04-10 11:37:27,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1149992e-18 1.0000000e+00 2.7264346e-22 4.6048354e-23 2.0123556e-31], sum to 1.0000
[2019-04-10 11:37:27,517] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5216
[2019-04-10 11:37:27,521] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5357155380887247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748596.8169163256, 748596.8169163256, 189381.1172780632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3877200.0000, 
sim time next is 3877800.0000, 
raw observation next is [29.83333333333333, 71.5, 1.0, 2.0, 0.5427197607385906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758387.8537620524, 758387.8537620524, 190559.7242899053], 
processed observation next is [0.0, 0.9130434782608695, 0.6129541864139019, 0.715, 1.0, 1.0, 0.44905995269709714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21066329271168122, 0.21066329271168122, 0.28441749894015717], 
reward next is 0.7156, 
noisyNet noise sample is [array([1.2931548], dtype=float32), 0.9780777]. 
=============================================
[2019-04-10 11:37:38,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0014135e-18 1.0000000e+00 7.1623764e-22 8.1764461e-22 3.3101165e-30], sum to 1.0000
[2019-04-10 11:37:38,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7119
[2019-04-10 11:37:38,960] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 76.33333333333333, 1.0, 2.0, 0.6164178685432905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861414.1632499291, 861414.1632499298, 203844.9507847506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3969600.0000, 
sim time next is 3970200.0000, 
raw observation next is [31.16666666666667, 77.66666666666667, 1.0, 2.0, 0.6188350292305955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864793.4035982647, 864793.4035982647, 204307.8921634679], 
processed observation next is [0.0, 0.9565217391304348, 0.6761453396524489, 0.7766666666666667, 1.0, 1.0, 0.5407650954585488, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24022038988840685, 0.24022038988840685, 0.3049371524827879], 
reward next is 0.6951, 
noisyNet noise sample is [array([0.9704139], dtype=float32), 0.94711065]. 
=============================================
[2019-04-10 11:37:39,753] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7276546e-09 6.2515423e-02 3.3852490e-09 9.3748456e-01 3.4944388e-14], sum to 1.0000
[2019-04-10 11:37:39,762] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3887
[2019-04-10 11:37:39,768] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.7802928190846712, 1.0, 2.0, 0.7802928190846712, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2182177.615641641, 2182177.615641641, 410385.7300172135], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4010400.0000, 
sim time next is 4011000.0000, 
raw observation next is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 0.8791516315188328, 1.0, 2.0, 0.8791516315188328, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2458924.515224941, 2458924.515224941, 460237.7735108085], 
processed observation next is [1.0, 0.43478260869565216, 0.5892575039494474, 0.7266666666666667, 1.0, 1.0, 0.8543995560467865, 1.0, 1.0, 0.8543995560467865, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6830345875624836, 0.6830345875624836, 0.686922050016132], 
reward next is 0.3131, 
noisyNet noise sample is [array([0.40466708], dtype=float32), 0.29395723]. 
=============================================
[2019-04-10 11:37:39,777] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.712364]
 [48.095757]
 [47.42776 ]
 [49.13221 ]
 [48.55161 ]], R is [[48.49351501]
 [48.39606476]
 [47.91210556]
 [47.4329834 ]
 [46.9586525 ]].
[2019-04-10 11:37:44,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4653892e-10 7.4364460e-01 9.6299455e-11 2.5635535e-01 1.7064121e-16], sum to 1.0000
[2019-04-10 11:37:44,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7985
[2019-04-10 11:37:44,551] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 70.33333333333334, 1.0, 2.0, 0.8731615664855575, 1.0, 2.0, 0.8731615664855575, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2442154.358016365, 2442154.358016365, 457081.7150950134], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4108200.0000, 
sim time next is 4108800.0000, 
raw observation next is [34.0, 69.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.421414207206984, 6.9112, 170.5573041426782, 3275242.656642504, 2909755.465833133, 550904.0634704893], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6966666666666668, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.05102142072069844, 0.0, 0.8375144448122397, 0.90978962684514, 0.8082654071758703, 0.8222448708514767], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18555464], dtype=float32), 0.02803074]. 
=============================================
[2019-04-10 11:37:46,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2185276e-15 1.0000000e+00 3.0699363e-18 2.1736010e-17 1.9615581e-25], sum to 1.0000
[2019-04-10 11:37:46,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-10 11:37:46,423] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 88.66666666666666, 1.0, 2.0, 0.7809082167690075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1091399.587096579, 1091399.587096579, 239278.5963305474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [27.03333333333333, 88.83333333333334, 1.0, 2.0, 0.7728947693630688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1080194.273393798, 1080194.273393797, 237366.3914270941], 
processed observation next is [1.0, 0.17391304347826086, 0.48025276461295413, 0.8883333333333334, 1.0, 1.0, 0.7263792401964684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3000539648316105, 0.3000539648316103, 0.3542781961598419], 
reward next is 0.6457, 
noisyNet noise sample is [array([0.68698275], dtype=float32), -0.6101697]. 
=============================================
[2019-04-10 11:37:46,706] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9028992e-16 1.0000000e+00 4.5480404e-18 3.9658316e-16 1.9284967e-25], sum to 1.0000
[2019-04-10 11:37:46,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7867
[2019-04-10 11:37:46,717] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.9006624569647119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510424, 1258867.677126204, 1258867.677126204, 270122.7089204718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4167600.0000, 
sim time next is 4168200.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.8809010652799849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231230.906649581, 1231230.906649581, 264740.7812458202], 
processed observation next is [1.0, 0.21739130434782608, 0.5260663507109005, 0.89, 1.0, 1.0, 0.8565073075662468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34200858518043914, 0.34200858518043914, 0.3951354943967466], 
reward next is 0.6049, 
noisyNet noise sample is [array([-0.27704948], dtype=float32), 0.7471724]. 
=============================================
[2019-04-10 11:37:47,636] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1332989e-11 1.1088352e-01 9.9640556e-12 8.8911653e-01 7.3131596e-19], sum to 1.0000
[2019-04-10 11:37:47,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0533
[2019-04-10 11:37:47,645] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.66666666666667, 68.33333333333334, 1.0, 2.0, 0.3083886617554669, 1.0, 2.0, 0.3083886617554669, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 861913.3162742543, 861913.3162742543, 251538.5000548629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4123200.0000, 
sim time next is 4123800.0000, 
raw observation next is [33.5, 69.0, 1.0, 2.0, 0.6099288036292395, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104269, 852342.3702305679, 852342.3702305679, 202613.2304679008], 
processed observation next is [1.0, 0.7391304347826086, 0.7867298578199052, 0.69, 1.0, 1.0, 0.5300347031677585, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522826, 0.23676176950849107, 0.23676176950849107, 0.3024078066685087], 
reward next is 0.6976, 
noisyNet noise sample is [array([1.1661255], dtype=float32), 0.5658323]. 
=============================================
[2019-04-10 11:37:51,477] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 11:37:51,479] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:37:51,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:37:51,481] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:37:51,482] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:37:51,482] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:37:51,483] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:37:51,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:37:51,484] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:37:51,485] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:37:51,486] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:37:51,492] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run14
[2019-04-10 11:37:51,492] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run14
[2019-04-10 11:37:51,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run14
[2019-04-10 11:37:51,493] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run14
[2019-04-10 11:37:51,554] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run14
[2019-04-10 11:37:58,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:37:58,655] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.26666666666667, 69.0, 1.0, 2.0, 0.3463582587749585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 536979.0087007545, 536979.0087007552, 169666.9605585425]
[2019-04-10 11:37:58,656] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:37:58,660] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8237335e-18 1.0000000e+00 3.1798738e-22 2.1499672e-24 1.6716085e-30], sampled 0.004371901748233986
[2019-04-10 11:38:20,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:38:20,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.9, 74.0, 1.0, 2.0, 0.6062645165430521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 847219.6857878024, 847219.6857878019, 201912.6607975531]
[2019-04-10 11:38:20,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:38:20,191] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.3712747e-17 1.0000000e+00 3.4482626e-20 1.5254889e-20 5.0814858e-28], sampled 0.9763019589859351
[2019-04-10 11:38:20,689] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:38:20,689] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.15, 80.33333333333334, 1.0, 2.0, 0.5430169900859056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758803.3456186552, 758803.3456186552, 190609.5202305362]
[2019-04-10 11:38:20,690] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:38:20,692] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.748702e-18 1.000000e+00 1.410254e-21 5.950630e-22 8.552968e-30], sampled 0.8144168589986654
[2019-04-10 11:38:31,338] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:38:31,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.5, 69.5, 1.0, 2.0, 0.5585252017385659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780482.2367685057, 780482.2367685057, 193272.9130453407]
[2019-04-10 11:38:31,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:38:31,346] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0525682e-18 1.0000000e+00 5.7558992e-22 1.6416801e-22 2.6851989e-30], sampled 0.34093705940992736
[2019-04-10 11:38:45,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:38:45,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.46061287, 55.379654415, 1.0, 2.0, 0.6569894545971768, 1.0, 1.0, 0.6569894545971768, 0.0, 1.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 1837041.2272466, 1837041.2272466, 356477.5837240288]
[2019-04-10 11:38:45,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:38:45,346] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.7711143e-10 9.7575480e-01 1.8363483e-10 2.4245191e-02 6.0454859e-16], sampled 0.04236629128363023
[2019-04-10 11:38:45,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1837041.2272466 W.
[2019-04-10 11:38:55,409] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:38:55,411] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.3, 86.0, 1.0, 2.0, 0.5329319024052557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 744705.6636257971, 744705.6636257964, 188916.6048362001]
[2019-04-10 11:38:55,411] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:38:55,412] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6625573e-17 1.0000000e+00 8.1393861e-21 2.5119886e-21 1.1494672e-28], sampled 0.37256911044505536
[2019-04-10 11:39:06,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11663078], dtype=float32), 0.09613564]
[2019-04-10 11:39:06,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.81921866, 69.10677004, 1.0, 2.0, 0.454620939304646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675035.4996189678, 675035.4996189671, 181775.6919760647]
[2019-04-10 11:39:06,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:39:06,691] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.7641426e-18 1.0000000e+00 1.3846507e-21 8.3235339e-23 9.7661568e-30], sampled 0.9000695726876171
[2019-04-10 11:39:21,475] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8505.7939 2841323257.4707 1104.0000
[2019-04-10 11:39:21,695] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7919.0079 3159313453.0822 1670.0000
[2019-04-10 11:39:21,755] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8265.4684 2926353862.6350 1310.0000
[2019-04-10 11:39:22,139] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.5872 2778777754.2580 926.0000
[2019-04-10 11:39:22,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8019.2530 3004682453.3159 1699.0000
[2019-04-10 11:39:23,192] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 325000, evaluation results [325000.0, 7919.0079458850905, 3159313453.0822215, 1670.0, 8265.468436808515, 2926353862.6349607, 1310.0, 8663.58718169622, 2778777754.2580338, 926.0, 8019.252997232289, 3004682453.315931, 1699.0, 8505.793899458215, 2841323257.4706535, 1104.0]
[2019-04-10 11:39:27,201] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1775086e-15 1.0000000e+00 2.3617706e-18 3.9150037e-12 2.2773662e-25], sum to 1.0000
[2019-04-10 11:39:27,201] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5252
[2019-04-10 11:39:27,257] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.16666666666666, 49.66666666666667, 1.0, 2.0, 0.5571315156202983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778533.9882568476, 778533.9882568483, 193033.9268725471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4297800.0000, 
sim time next is 4298400.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.5650363426495978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789584.2766094601, 789584.2766094601, 194414.7933901661], 
processed observation next is [1.0, 0.782608695652174, 0.9052132701421801, 0.5, 1.0, 1.0, 0.4759474007826479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21932896572485003, 0.21932896572485003, 0.2901713334181584], 
reward next is 0.7098, 
noisyNet noise sample is [array([-0.6877722], dtype=float32), 0.8255106]. 
=============================================
[2019-04-10 11:39:35,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2892685e-14 1.0000000e+00 5.5424446e-17 2.2978968e-09 8.9724929e-24], sum to 1.0000
[2019-04-10 11:39:35,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3567
[2019-04-10 11:39:35,273] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5435442656958549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759540.4156386752, 759540.4156386758, 190702.9154409278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4383600.0000, 
sim time next is 4384200.0000, 
raw observation next is [32.16666666666666, 66.33333333333333, 1.0, 2.0, 0.5546009537642161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774996.4983054858, 774996.4983054864, 192595.2098552588], 
processed observation next is [1.0, 0.7391304347826086, 0.7235387045813582, 0.6633333333333333, 1.0, 1.0, 0.463374643089417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21527680508485716, 0.21527680508485733, 0.28745553709740124], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.40270042], dtype=float32), -0.33650815]. 
=============================================
[2019-04-10 11:39:36,010] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.3037266e-14 1.4659609e-08 9.0435878e-13 1.0000000e+00 1.6317942e-17], sum to 1.0000
[2019-04-10 11:39:36,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0101
[2019-04-10 11:39:36,027] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.66666666666666, 72.33333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.083625067139346, 6.9112, 170.5573041426782, 3032988.708005908, 2909473.618610698, 552763.8097105237], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4354800.0000, 
sim time next is 4355400.0000, 
raw observation next is [33.83333333333334, 71.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.548141480678005, 6.9112, 170.5573041426782, 3366128.311391583, 2909861.21965902, 550162.1589504193], 
processed observation next is [1.0, 0.391304347826087, 0.8025276461295423, 0.7166666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.06369414806780052, 0.0, 0.8375144448122397, 0.9350356420532175, 0.8082947832386167, 0.8211375506722676], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21542417], dtype=float32), 0.43979317]. 
=============================================
[2019-04-10 11:39:36,919] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3840206e-15 1.2708327e-10 5.6568475e-14 1.0000000e+00 3.8819772e-18], sum to 1.0000
[2019-04-10 11:39:36,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9817
[2019-04-10 11:39:36,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3403852.481874611 W.
[2019-04-10 11:39:37,001] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.66666666666667, 1.0, 2.0, 0.9808689515540783, 1.0, 2.0, 0.8110245152913015, 1.0, 2.0, 1.03, 7.005119888870016, 6.9112, 170.5573041426782, 3403852.481874611, 3336573.845060201, 624737.9920104753], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [38.0, 51.5, 1.0, 2.0, 0.9881256916141364, 1.0, 2.0, 0.8146528853213306, 1.0, 2.0, 1.03, 7.0051204616048, 6.9112, 170.5573041426782, 3419101.522972328, 3351822.475884695, 627800.0404923658], 
processed observation next is [1.0, 0.6086956521739131, 1.0, 0.515, 1.0, 1.0, 0.9856936043543812, 1.0, 1.0, 0.7766902232787116, 1.0, 1.0, 1.0365853658536586, 0.009392046160480038, 0.0, 0.8375144448122397, 0.9497504230478689, 0.9310617988568597, 0.9370149858095013], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29691923], dtype=float32), 1.3156409]. 
=============================================
[2019-04-10 11:39:39,073] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2418803e-11 1.0082415e-01 2.8182621e-11 8.9917588e-01 1.8996662e-16], sum to 1.0000
[2019-04-10 11:39:39,073] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6120
[2019-04-10 11:39:39,185] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.66666666666667, 59.16666666666667, 1.0, 2.0, 0.9946929300405004, 1.0, 2.0, 0.9946929300405004, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2782445.145188771, 2782445.145188772, 525786.9663828945], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4373400.0000, 
sim time next is 4374000.0000, 
raw observation next is [34.0, 56.0, 1.0, 2.0, 0.978897548549319, 1.0, 2.0, 0.978897548549319, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2738212.450427379, 2738212.450427379, 516362.4107918373], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.56, 1.0, 1.0, 0.9745753596979747, 1.0, 1.0, 0.9745753596979747, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7606145695631609, 0.7606145695631609, 0.7706901653609513], 
reward next is 0.2293, 
noisyNet noise sample is [array([-1.85914], dtype=float32), -0.95044905]. 
=============================================
[2019-04-10 11:39:39,237] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[49.48863 ]
 [49.467087]
 [49.122925]
 [47.65882 ]
 [45.916924]], R is [[49.46995163]
 [49.19049835]
 [48.91857529]
 [48.6923027 ]
 [48.44807434]].
[2019-04-10 11:39:43,402] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2074985e-19 1.0000000e+00 1.7160604e-23 2.4868341e-23 8.4273635e-32], sum to 1.0000
[2019-04-10 11:39:43,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2485
[2019-04-10 11:39:43,498] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 71.0, 1.0, 2.0, 0.5569137909536337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778229.628973624, 778229.628973624, 192994.92605801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [31.0, 73.66666666666667, 1.0, 2.0, 0.5728741468199305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800540.9861039179, 800540.9861039174, 195802.5889659582], 
processed observation next is [1.0, 0.8695652173913043, 0.6682464454976303, 0.7366666666666667, 1.0, 1.0, 0.4853905383372657, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2223724961399772, 0.22237249613997703, 0.29224267009844507], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.40422964], dtype=float32), 0.46756205]. 
=============================================
[2019-04-10 11:39:47,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1433412e-19 1.0000000e+00 1.1530197e-21 1.9757460e-23 1.7860689e-30], sum to 1.0000
[2019-04-10 11:39:47,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0943
[2019-04-10 11:39:47,168] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.581932232265512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 813203.7054230262, 813203.7054230255, 197428.1532226166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4423200.0000, 
sim time next is 4423800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5809484113354244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811828.3690380023, 811828.3690380023, 197250.3394906312], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49511856787400527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.225507880288334, 0.225507880288334, 0.2944034917770615], 
reward next is 0.7056, 
noisyNet noise sample is [array([-0.40478033], dtype=float32), 0.43516213]. 
=============================================
[2019-04-10 11:39:52,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.0497514e-20 1.0000000e+00 2.2351102e-23 8.6714894e-25 5.0991070e-32], sum to 1.0000
[2019-04-10 11:39:52,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5207
[2019-04-10 11:39:52,876] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 84.0, 1.0, 2.0, 0.5419766475752494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757349.0692028189, 757349.0692028189, 190433.6967397708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [27.5, 84.0, 1.0, 2.0, 0.5371457553079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750596.0763388888, 750596.0763388888, 189620.0435537461], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.84, 1.0, 1.0, 0.4423442835034939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2084989100941358, 0.2084989100941358, 0.2830149903787255], 
reward next is 0.7170, 
noisyNet noise sample is [array([1.6363357], dtype=float32), 0.6509923]. 
=============================================
[2019-04-10 11:39:54,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3044546e-20 1.0000000e+00 2.0253636e-23 2.1747367e-25 8.7987116e-32], sum to 1.0000
[2019-04-10 11:39:54,495] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2851
[2019-04-10 11:39:54,508] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5284096911494777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738384.2427864469, 738384.2427864476, 188166.5361804091], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.529300894799023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739630.018258668, 739630.0182586674, 188313.6533800974], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.43289264433617225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20545278284963, 0.20545278284962984, 0.28106515429865286], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.7746721], dtype=float32), -1.1188074]. 
=============================================
[2019-04-10 11:39:57,823] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5134184e-10 9.9943060e-01 1.1191839e-10 5.6939863e-04 9.0500335e-16], sum to 1.0000
[2019-04-10 11:39:57,842] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6745
[2019-04-10 11:39:57,842] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2759094.160428305 W.
[2019-04-10 11:39:57,869] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.986354423154224, 1.0, 2.0, 0.986354423154224, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2759094.160428305, 2759094.160428304, 520804.019162978], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4611600.0000, 
sim time next is 4612200.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.9450292874327132, 1.0, 2.0, 0.9450292874327132, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2643374.533653997, 2643374.533653997, 496665.0454877393], 
processed observation next is [1.0, 0.391304347826087, 0.7156398104265403, 0.75, 1.0, 1.0, 0.933770225822546, 1.0, 1.0, 0.933770225822546, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7342707037927769, 0.7342707037927769, 0.7412911126682676], 
reward next is 0.2587, 
noisyNet noise sample is [array([0.17041793], dtype=float32), -0.14484748]. 
=============================================
[2019-04-10 11:40:05,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5280958e-19 1.0000000e+00 2.6745256e-23 1.9353599e-23 1.2113527e-31], sum to 1.0000
[2019-04-10 11:40:05,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5305
[2019-04-10 11:40:05,085] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.08333333333333, 94.00000000000001, 1.0, 2.0, 0.4527319079443171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648668.7607734681, 648668.7607734681, 178584.2729055045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4655400.0000, 
sim time next is 4656000.0000, 
raw observation next is [24.16666666666666, 94.0, 1.0, 2.0, 0.4558572356147115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650958.3237914713, 650958.3237914713, 178766.5752197482], 
processed observation next is [1.0, 0.9130434782608695, 0.34439178515007873, 0.94, 1.0, 1.0, 0.34440630796953187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18082175660874203, 0.18082175660874203, 0.26681578391007194], 
reward next is 0.7332, 
noisyNet noise sample is [array([-1.3259761], dtype=float32), 1.1801386]. 
=============================================
[2019-04-10 11:40:05,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.44963]
 [70.20222]
 [70.28383]
 [70.35439]
 [70.51485]], R is [[70.34366608]
 [70.37368774]
 [70.40354156]
 [70.43260956]
 [70.46079254]].
[2019-04-10 11:40:09,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8442668e-17 1.0000000e+00 1.0192073e-20 4.1760763e-20 1.3194457e-27], sum to 1.0000
[2019-04-10 11:40:09,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4118
[2019-04-10 11:40:09,450] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4846000987575116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677146.4880560861, 677146.4880560861, 181218.174291246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4758600.0000, 
sim time next is 4759200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4846202610063223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677174.6703606546, 677174.670360654, 181221.2402177604], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3790605554293039, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18810407510018184, 0.18810407510018168, 0.2704794630115827], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.1677183], dtype=float32), 1.363817]. 
=============================================
[2019-04-10 11:40:43,290] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3870377e-20 1.0000000e+00 1.7689811e-23 1.2337695e-25 2.9286401e-32], sum to 1.0000
[2019-04-10 11:40:43,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2625
[2019-04-10 11:40:43,298] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 81.5, 1.0, 2.0, 0.5199351973651816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726538.1833107037, 726538.1833107043, 186778.0769745961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5088600.0000, 
sim time next is 5089200.0000, 
raw observation next is [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5188716237815977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725051.4777706177, 725051.4777706177, 186605.2996532201], 
processed observation next is [0.0, 0.9130434782608695, 0.4944707740916275, 0.8233333333333335, 1.0, 1.0, 0.42032725756818995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20140318826961603, 0.20140318826961603, 0.2785153726167464], 
reward next is 0.7215, 
noisyNet noise sample is [array([-1.4680778], dtype=float32), -1.5736545]. 
=============================================
[2019-04-10 11:40:43,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0742349e-19 1.0000000e+00 5.6483154e-23 2.0148274e-25 3.8646622e-32], sum to 1.0000
[2019-04-10 11:40:43,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9283
[2019-04-10 11:40:43,704] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.508664754023036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710784.0142643537, 710784.0142643544, 184964.7932893815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5035800.0000, 
sim time next is 5036400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5095876984073302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712074.1252579205, 712074.1252579205, 185111.9381594009], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40914180531003635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19779836812720014, 0.19779836812720014, 0.2762864748647774], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.3668924], dtype=float32), 0.8826275]. 
=============================================
[2019-04-10 11:40:46,033] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9367143e-19 1.0000000e+00 1.6453469e-23 1.7502729e-24 1.1064552e-31], sum to 1.0000
[2019-04-10 11:40:46,039] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2991
[2019-04-10 11:40:46,045] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5524797156795057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772031.213966883, 772031.213966883, 192227.284804925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5146200.0000, 
sim time next is 5146800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5515039770462621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770667.2291260133, 770667.2291260128, 192059.4410722862], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45964334583886995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2140742303127815, 0.21407423031278133, 0.2866558821974421], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.32409546], dtype=float32), -1.2930335]. 
=============================================
[2019-04-10 11:40:46,329] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 11:40:46,331] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:40:46,331] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:40:46,332] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:40:46,333] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:40:46,333] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:40:46,334] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:40:46,334] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:40:46,335] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:40:46,336] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:40:46,337] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:40:46,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run15
[2019-04-10 11:40:46,349] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run15
[2019-04-10 11:40:46,365] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run15
[2019-04-10 11:40:46,400] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run15
[2019-04-10 11:40:46,400] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run15
[2019-04-10 11:41:16,624] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11793616], dtype=float32), 0.09364393]
[2019-04-10 11:41:16,625] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 94.0, 1.0, 2.0, 0.3107188938489137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491345.1982242824, 491345.1982242829, 166382.8351046422]
[2019-04-10 11:41:16,626] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:41:16,629] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.24560993e-19 1.00000000e+00 1.03814284e-23 1.08570726e-26
 2.13847431e-32], sampled 0.18305393599430753
[2019-04-10 11:41:24,485] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11793616], dtype=float32), 0.09364393]
[2019-04-10 11:41:24,486] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.03333333333333, 62.83333333333333, 1.0, 2.0, 0.5574682457319465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779004.7065010567, 779004.706501056, 193089.8137943477]
[2019-04-10 11:41:24,486] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:41:24,490] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8689549e-19 1.0000000e+00 3.1946446e-23 3.8908308e-24 6.8526525e-32], sampled 0.10978544480160446
[2019-04-10 11:41:54,231] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11793616], dtype=float32), 0.09364393]
[2019-04-10 11:41:54,233] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.71444756, 59.67850423666667, 1.0, 2.0, 0.4923656450725536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688001.0365075596, 688001.0365075603, 182408.5154156498]
[2019-04-10 11:41:54,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:41:54,236] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1837187e-19 1.0000000e+00 1.1323401e-23 8.7486357e-26 1.9661360e-32], sampled 0.6886419110587575
[2019-04-10 11:42:21,841] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.8624 3164183160.0368 1764.0000
[2019-04-10 11:42:22,177] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9350 2927404666.0075 1338.0000
[2019-04-10 11:42:22,451] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1488 2779290230.6812 933.0000
[2019-04-10 11:42:22,454] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.1924 2842318514.2305 1127.0000
[2019-04-10 11:42:22,639] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4781 3007551249.4160 1760.0000
[2019-04-10 11:42:23,652] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 350000, evaluation results [350000.0, 7886.862378729417, 3164183160.0368185, 1764.0, 8254.934989388352, 2927404666.00751, 1338.0, 8659.148772991248, 2779290230.6812186, 933.0, 7997.478102919008, 3007551249.4159913, 1760.0, 8498.192374760767, 2842318514.2305374, 1127.0]
[2019-04-10 11:42:25,594] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0192430e-17 1.0000000e+00 3.1060732e-20 1.6018173e-21 7.0872290e-29], sum to 1.0000
[2019-04-10 11:42:25,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3298
[2019-04-10 11:42:25,605] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5159126785277046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720915.3571234473, 720915.3571234473, 186126.441042029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5189400.0000, 
sim time next is 5190000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5163661980249745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721549.3020272445, 721549.3020272438, 186199.6390236945], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4173086723192464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2004303616742346, 0.2004303616742344, 0.2779099089905888], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.42373264], dtype=float32), -0.26151407]. 
=============================================
[2019-04-10 11:42:25,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[67.62005 ]
 [67.798584]
 [68.02781 ]
 [68.32432 ]
 [68.469185]], R is [[67.57416534]
 [67.62062073]
 [67.66668701]
 [67.71226501]
 [67.75727844]].
[2019-04-10 11:42:30,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3321303e-16 1.0000000e+00 1.1729041e-19 3.2775057e-18 8.9803201e-27], sum to 1.0000
[2019-04-10 11:42:30,447] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4700
[2019-04-10 11:42:30,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.23333333333333, 81.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.13542675899951, 6.9112, 168.9114202113907, 1612936.604697984, 1453863.871387565, 311355.9506040112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5380800.0000, 
sim time next is 5381400.0000, 
raw observation next is [30.41666666666667, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.231930301401244, 6.9112, 168.9107479819956, 1681444.903759142, 1453910.764296407, 311355.9275346305], 
processed observation next is [1.0, 0.2608695652173913, 0.6406003159557664, 0.8016666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03207303014012437, 0.0, 0.8294291002666162, 0.4670680288219839, 0.4038641011934464, 0.46471033960392616], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23787178], dtype=float32), -0.90522677]. 
=============================================
[2019-04-10 11:42:44,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5376880e-17 1.0000000e+00 6.0302042e-20 2.5118665e-19 3.0214399e-27], sum to 1.0000
[2019-04-10 11:42:44,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2204
[2019-04-10 11:42:44,985] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 90.66666666666667, 1.0, 2.0, 0.5475336966681766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765117.1930401626, 765117.193040162, 191378.6003513641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5530800.0000, 
sim time next is 5531400.0000, 
raw observation next is [26.95, 91.0, 1.0, 2.0, 0.5467395098928274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764007.0065990146, 764007.006599014, 191243.0294276118], 
processed observation next is [1.0, 0.0, 0.476303317535545, 0.91, 1.0, 1.0, 0.4539030239672619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21222416849972628, 0.2122241684997261, 0.2854373573546445], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.8270523], dtype=float32), -0.93474066]. 
=============================================
[2019-04-10 11:42:55,794] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3184413e-18 1.0000000e+00 1.0490678e-21 9.8143738e-22 7.3531208e-30], sum to 1.0000
[2019-04-10 11:42:55,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3657
[2019-04-10 11:42:55,822] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 91.33333333333334, 1.0, 2.0, 0.5227985089389646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730540.6446770929, 730540.6446770923, 187244.9733231312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5617200.0000, 
sim time next is 5617800.0000, 
raw observation next is [26.15, 91.5, 1.0, 2.0, 0.5222979293180688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729840.9116384251, 729840.9116384251, 187163.156377542], 
processed observation next is [0.0, 0.0, 0.43838862559241704, 0.915, 1.0, 1.0, 0.42445533652779377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2027335865662292, 0.2027335865662292, 0.2793479945933463], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.22350043], dtype=float32), -1.5735749]. 
=============================================
[2019-04-10 11:43:01,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.8025128e-19 1.0000000e+00 6.9181082e-24 1.0006856e-25 2.9549894e-32], sum to 1.0000
[2019-04-10 11:43:01,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4914
[2019-04-10 11:43:01,667] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 72.66666666666667, 1.0, 2.0, 0.541256075815571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756341.7958374814, 756341.795837482, 190312.8282231646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5649600.0000, 
sim time next is 5650200.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.5419428748475508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757301.8589368007, 757301.8589368014, 190428.9813475051], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.72, 1.0, 1.0, 0.44812394559945873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21036162748244466, 0.21036162748244483, 0.28422236022015684], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.01079501], dtype=float32), -1.3916986]. 
=============================================
[2019-04-10 11:43:12,275] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5622042e-19 1.0000000e+00 2.9933977e-22 9.7949949e-23 2.4670867e-30], sum to 1.0000
[2019-04-10 11:43:12,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3661
[2019-04-10 11:43:12,287] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 89.66666666666666, 1.0, 2.0, 0.5295487083832199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739976.4265306772, 739976.4265306767, 188355.3711950968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5874600.0000, 
sim time next is 5875200.0000, 
raw observation next is [26.7, 90.0, 1.0, 2.0, 0.5293183546834279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739654.4246990532, 739654.4246990532, 188317.2765606468], 
processed observation next is [1.0, 0.0, 0.46445497630331756, 0.9, 1.0, 1.0, 0.43291368034147937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20545956241640365, 0.20545956241640365, 0.2810705620308161], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.07452913], dtype=float32), -0.7737773]. 
=============================================
[2019-04-10 11:43:14,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3529968e-18 1.0000000e+00 3.3523220e-20 3.5862687e-20 4.5460748e-28], sum to 1.0000
[2019-04-10 11:43:14,803] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2158
[2019-04-10 11:43:14,806] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 91.66666666666666, 1.0, 2.0, 0.7796612521139348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104278, 1089655.931503817, 1089655.931503817, 238978.7797241175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5971200.0000, 
sim time next is 5971800.0000, 
raw observation next is [26.23333333333333, 91.83333333333333, 1.0, 2.0, 0.7524190764490069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1051563.36746616, 1051563.367466161, 232565.8088089804], 
processed observation next is [1.0, 0.08695652173913043, 0.44233807266982617, 0.9183333333333333, 1.0, 1.0, 0.701709730661454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29210093540726667, 0.29210093540726695, 0.3471131474760901], 
reward next is 0.6529, 
noisyNet noise sample is [array([0.404221], dtype=float32), 1.7282683]. 
=============================================
[2019-04-10 11:43:15,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1768493e-17 1.0000000e+00 1.6258581e-20 2.1464642e-20 1.1468923e-27], sum to 1.0000
[2019-04-10 11:43:15,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1885
[2019-04-10 11:43:15,848] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 84.5, 1.0, 2.0, 0.750790183519081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049285.739953067, 1049285.739953067, 232192.690745666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5989800.0000, 
sim time next is 5990400.0000, 
raw observation next is [28.7, 84.0, 1.0, 2.0, 0.751059915969747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049662.897646974, 1049662.897646974, 232255.1746011922], 
processed observation next is [1.0, 0.34782608695652173, 0.5592417061611374, 0.84, 1.0, 1.0, 0.7000721879153577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29157302712415945, 0.29157302712415945, 0.34664951433013763], 
reward next is 0.6534, 
noisyNet noise sample is [array([0.9461167], dtype=float32), 0.81027424]. 
=============================================
[2019-04-10 11:43:20,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1447622e-09 9.8859578e-01 1.7430711e-10 1.1404229e-02 2.1596873e-15], sum to 1.0000
[2019-04-10 11:43:20,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1754
[2019-04-10 11:43:20,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2593030.594584293 W.
[2019-04-10 11:43:20,862] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.95, 65.5, 1.0, 2.0, 0.927049551024149, 1.0, 2.0, 0.927049551024149, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2593030.594584293, 2593030.594584292, 486457.5055297786], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6100200.0000, 
sim time next is 6100800.0000, 
raw observation next is [30.9, 65.66666666666667, 1.0, 2.0, 0.832043608051258, 1.0, 2.0, 0.832043608051258, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2327044.048366791, 2327044.048366791, 435768.8543459043], 
processed observation next is [1.0, 0.6086956521739131, 0.6635071090047393, 0.6566666666666667, 1.0, 1.0, 0.7976429012665759, 1.0, 1.0, 0.7976429012665759, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6464011245463308, 0.6464011245463308, 0.6504012751431407], 
reward next is 0.3496, 
noisyNet noise sample is [array([1.274752], dtype=float32), 1.0759815]. 
=============================================
[2019-04-10 11:43:22,290] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 11:43:22,292] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:43:22,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:43:22,297] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:43:22,299] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:43:22,299] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:43:22,300] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:43:22,300] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:43:22,301] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:43:22,302] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:43:22,303] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:43:22,310] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run16
[2019-04-10 11:43:22,334] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run16
[2019-04-10 11:43:22,334] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run16
[2019-04-10 11:43:22,335] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run16
[2019-04-10 11:43:22,388] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run16
[2019-04-10 11:43:40,361] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11873548], dtype=float32), 0.0955866]
[2019-04-10 11:43:40,362] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.76335321, 89.22995635666666, 1.0, 2.0, 0.6256292773901561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 962970.9658637434, 962970.965863744, 216450.0631156967]
[2019-04-10 11:43:40,362] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:43:40,363] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6332579e-17 1.0000000e+00 7.4246060e-21 2.2977644e-21 1.4073899e-28], sampled 0.6057938165101847
[2019-04-10 11:43:50,694] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11873548], dtype=float32), 0.0955866]
[2019-04-10 11:43:50,695] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.6, 89.0, 1.0, 2.0, 0.5688358856712286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794895.761505809, 794895.7615058097, 195082.8931782108]
[2019-04-10 11:43:50,696] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:43:50,698] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5561000e-18 1.0000000e+00 6.9637884e-22 6.2752926e-23 8.0404115e-30], sampled 0.9969331378445498
[2019-04-10 11:44:07,284] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11873548], dtype=float32), 0.0955866]
[2019-04-10 11:44:07,285] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.9687519, 49.34929871, 1.0, 2.0, 0.6938776296407949, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994878405108572, 6.9112, 168.9123904138209, 1866561.467195051, 1807197.32557685, 383750.8323207326]
[2019-04-10 11:44:07,285] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:44:07,286] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6986070e-14 1.0000000e+00 1.3993310e-16 6.7474715e-12 3.6276771e-24], sampled 0.9591196326334201
[2019-04-10 11:44:07,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1866561.467195051 W.
[2019-04-10 11:44:32,375] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11873548], dtype=float32), 0.0955866]
[2019-04-10 11:44:32,377] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.53333333333333, 63.83333333333334, 1.0, 2.0, 0.9753542663116854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1363332.42590237, 1363332.42590237, 291509.8636834967]
[2019-04-10 11:44:32,377] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:44:32,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1727553e-17 1.0000000e+00 1.7575895e-20 3.5663097e-20 4.4242453e-28], sampled 0.9614682158552511
[2019-04-10 11:44:39,577] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11873548], dtype=float32), 0.0955866]
[2019-04-10 11:44:39,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.41666666666666, 79.83333333333334, 1.0, 2.0, 0.4891461244815422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687999.721324603, 687999.721324603, 182491.2668226988]
[2019-04-10 11:44:39,582] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:44:39,584] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.03530934e-19 1.00000000e+00 1.05553352e-23 2.68566684e-25
 3.01039901e-32], sampled 0.01766286560843411
[2019-04-10 11:44:51,643] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11873548], dtype=float32), 0.0955866]
[2019-04-10 11:44:51,644] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.96666666666667, 64.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.128294515255268, 6.9112, 168.9118721910517, 1607873.735702751, 1453860.403443211, 311352.8106476541]
[2019-04-10 11:44:51,645] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:44:51,647] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.5343401e-17 1.0000000e+00 3.9474248e-20 2.6313278e-20 1.2937279e-27], sampled 0.027514716899501246
[2019-04-10 11:44:53,505] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-04-10 11:44:53,861] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7737 2927512725.0370 1338.0000
[2019-04-10 11:44:54,093] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.9411 3163855541.6997 1770.0000
[2019-04-10 11:44:54,194] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6129 3007557376.6723 1764.0000
[2019-04-10 11:44:54,246] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.2628 2842008524.9776 1123.0000
[2019-04-10 11:44:55,261] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 375000, evaluation results [375000.0, 7885.941137246544, 3163855541.6997232, 1770.0, 8254.773707254759, 2927512725.037018, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7998.612926690711, 3007557376.6723127, 1764.0, 8499.262848525006, 2842008524.977639, 1123.0]
[2019-04-10 11:44:56,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6281979e-11 9.9999928e-01 3.5947727e-12 7.4515032e-07 3.9651459e-17], sum to 1.0000
[2019-04-10 11:44:56,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9069
[2019-04-10 11:44:56,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1935016.497194414 W.
[2019-04-10 11:44:56,878] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.38333333333334, 83.33333333333333, 1.0, 2.0, 0.6919938550822328, 1.0, 2.0, 0.6919938550822328, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1935016.497194414, 1935016.497194413, 370718.3172584819], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6167400.0000, 
sim time next is 6168000.0000, 
raw observation next is [28.46666666666667, 82.66666666666667, 1.0, 2.0, 0.9388849305333569, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.997694062413821, 6.9112, 168.9124426295992, 2209463.335080583, 2148101.656988426, 445381.9910476263], 
processed observation next is [1.0, 0.391304347826087, 0.5481832543443919, 0.8266666666666667, 1.0, 1.0, 0.9263673861847673, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008649406241382085, 0.0, 0.829437421762163, 0.613739815300162, 0.5966949047190072, 0.6647492403695915], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2770977], dtype=float32), 0.52641904]. 
=============================================
[2019-04-10 11:44:56,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[48.013058]
 [46.43564 ]
 [46.04242 ]
 [48.562195]
 [49.629593]], R is [[47.24828339]
 [47.22249222]
 [46.75026703]
 [46.28276443]
 [45.81993866]].
[2019-04-10 11:44:57,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8838316e-12 1.0000000e+00 2.8444703e-13 1.0917971e-09 2.2516655e-19], sum to 1.0000
[2019-04-10 11:44:57,253] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0935
[2019-04-10 11:44:57,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2129937.661096368 W.
[2019-04-10 11:44:57,264] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.13333333333333, 85.0, 1.0, 2.0, 0.5077544366057096, 1.0, 2.0, 0.5077544366057096, 1.0, 2.0, 0.8818015394260105, 6.911199999999999, 6.9112, 170.5573041426782, 2129937.661096368, 2129937.661096369, 420549.4474663706], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6165600.0000, 
sim time next is 6166200.0000, 
raw observation next is [28.21666666666667, 84.5, 1.0, 2.0, 0.488204967836185, 1.0, 2.0, 0.488204967836185, 1.0, 2.0, 0.8478505772814611, 6.9112, 6.9112, 170.5573041426782, 2047852.803981987, 2047852.803981987, 407084.866617286], 
processed observation next is [1.0, 0.34782608695652173, 0.5363349131121644, 0.845, 1.0, 1.0, 0.3833794793207049, 1.0, 1.0, 0.3833794793207049, 1.0, 1.0, 0.8144519235139769, 0.0, 0.0, 0.8375144448122397, 0.5688480011061076, 0.5688480011061076, 0.6075893531601283], 
reward next is 0.3924, 
noisyNet noise sample is [array([-0.7019577], dtype=float32), 0.85825634]. 
=============================================
[2019-04-10 11:45:00,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.3048385e-19 1.0000000e+00 6.9043492e-23 2.9944095e-25 2.2964570e-30], sum to 1.0000
[2019-04-10 11:45:00,005] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5483
[2019-04-10 11:45:00,010] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 86.33333333333334, 1.0, 2.0, 0.5384226943869803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752381.0758475049, 752381.0758475049, 189835.6098219435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6247200.0000, 
sim time next is 6247800.0000, 
raw observation next is [27.7, 86.0, 1.0, 2.0, 0.5396545159886338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754103.0096651644, 754103.0096651651, 190042.825805492], 
processed observation next is [0.0, 0.30434782608695654, 0.5118483412322274, 0.86, 1.0, 1.0, 0.44536688673329367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20947305824032345, 0.20947305824032364, 0.2836460086649134], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.1169759], dtype=float32), -1.4869862]. 
=============================================
[2019-04-10 11:45:03,200] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.65824093e-21 1.00000000e+00 9.61568787e-25 1.21163766e-26
 8.66292390e-33], sum to 1.0000
[2019-04-10 11:45:03,202] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8318
[2019-04-10 11:45:03,230] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 63.0, 1.0, 2.0, 0.5053681343960709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706175.943087651, 706175.9430876516, 184441.5590048312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6280200.0000, 
sim time next is 6280800.0000, 
raw observation next is [30.46666666666667, 63.0, 1.0, 2.0, 0.5044110230729163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704838.0802898043, 704838.0802898037, 184290.20026298], 
processed observation next is [0.0, 0.6956521739130435, 0.6429699842022119, 0.63, 1.0, 1.0, 0.40290484707580276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19578835563605673, 0.19578835563605657, 0.2750600003925075], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.7749806], dtype=float32), -0.55246156]. 
=============================================
[2019-04-10 11:45:07,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9666256e-20 1.0000000e+00 1.3200972e-24 2.7468904e-26 7.1346306e-33], sum to 1.0000
[2019-04-10 11:45:07,400] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7241
[2019-04-10 11:45:07,407] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7929737e-19 1.0000000e+00 7.7374870e-24 1.7186782e-26 2.4215351e-33], sum to 1.0000
[2019-04-10 11:45:07,420] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.0, 1.0, 2.0, 0.5282064399111439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738100.1267763721, 738100.1267763727, 188133.333147824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6304800.0000, 
sim time next is 6305400.0000, 
raw observation next is [27.3, 85.0, 1.0, 2.0, 0.5277452520860603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737455.4527830102, 737455.4527830096, 188057.2809282167], 
processed observation next is [0.0, 1.0, 0.4928909952606636, 0.85, 1.0, 1.0, 0.4310183760073015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048487368841695, 0.20484873688416935, 0.2806825088480846], 
reward next is 0.7193, 
noisyNet noise sample is [array([0.21786001], dtype=float32), 0.032311883]. 
=============================================
[2019-04-10 11:45:07,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0222
[2019-04-10 11:45:07,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.43333333333333, 75.66666666666667, 1.0, 2.0, 0.5180286488138364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723873.1352450701, 723873.1352450701, 186468.7692289273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6372600.0000, 
sim time next is 6373200.0000, 
raw observation next is [28.36666666666667, 76.33333333333334, 1.0, 2.0, 0.5198048218945126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726355.9391702501, 726355.9391702507, 186756.9989314036], 
processed observation next is [0.0, 0.782608695652174, 0.543443917851501, 0.7633333333333334, 1.0, 1.0, 0.42145159264399107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017655386584028, 0.20176553865840297, 0.2787417894498561], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.10424075], dtype=float32), -1.1049274]. 
=============================================
[2019-04-10 11:45:21,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7739242e-19 1.0000000e+00 9.0351111e-23 1.6974139e-23 8.5381317e-31], sum to 1.0000
[2019-04-10 11:45:21,791] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1918
[2019-04-10 11:45:21,803] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 83.0, 1.0, 2.0, 0.5221058914024111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729572.4723983603, 729572.472398361, 187132.1495908636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6472800.0000, 
sim time next is 6473400.0000, 
raw observation next is [27.45, 83.66666666666667, 1.0, 2.0, 0.5240114768023031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732236.1884234525, 732236.1884234525, 187443.7484477302], 
processed observation next is [1.0, 0.9565217391304348, 0.5, 0.8366666666666667, 1.0, 1.0, 0.4265198515690398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2033989412287368, 0.2033989412287368, 0.2797667887279555], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.81727755], dtype=float32), 0.6401112]. 
=============================================
[2019-04-10 11:45:22,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4736905e-17 1.0000000e+00 1.4475417e-20 3.6922818e-21 7.7792605e-28], sum to 1.0000
[2019-04-10 11:45:22,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-10 11:45:22,947] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 90.33333333333334, 1.0, 2.0, 0.7539859906009506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1053754.336698398, 1053754.336698399, 232929.1351488232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6502800.0000, 
sim time next is 6503400.0000, 
raw observation next is [26.65, 90.0, 1.0, 2.0, 0.7446550703929742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1040707.256512482, 1040707.256512482, 230778.7999140825], 
processed observation next is [1.0, 0.2608695652173913, 0.462085308056872, 0.9, 1.0, 1.0, 0.6923555064975593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.289085349031245, 0.289085349031245, 0.3444459700210187], 
reward next is 0.6556, 
noisyNet noise sample is [array([-0.93011415], dtype=float32), -1.481436]. 
=============================================
[2019-04-10 11:45:29,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6369798e-19 1.0000000e+00 4.7831051e-23 2.2738281e-22 2.6883032e-31], sum to 1.0000
[2019-04-10 11:45:29,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9604
[2019-04-10 11:45:29,274] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 85.0, 1.0, 2.0, 0.5217050063649425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729012.0974720874, 729012.0974720881, 187066.8434337646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6633600.0000, 
sim time next is 6634200.0000, 
raw observation next is [27.21666666666667, 85.0, 1.0, 2.0, 0.5217097132521683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729018.6769665759, 729018.6769665766, 187067.5670465369], 
processed observation next is [1.0, 0.782608695652174, 0.48894154818325447, 0.85, 1.0, 1.0, 0.4237466424724919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20250518804627107, 0.20250518804627127, 0.27920532395005504], 
reward next is 0.7208, 
noisyNet noise sample is [array([1.699835], dtype=float32), -0.19133432]. 
=============================================
[2019-04-10 11:45:33,078] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.5971800e-18 1.0000000e+00 9.4316250e-22 5.3214891e-22 1.9042592e-29], sum to 1.0000
[2019-04-10 11:45:33,080] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6064
[2019-04-10 11:45:33,118] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 93.33333333333334, 1.0, 2.0, 0.5781019979394263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807849.2241314964, 807849.2241314964, 196731.3767457523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6675600.0000, 
sim time next is 6676200.0000, 
raw observation next is [25.4, 93.0, 1.0, 2.0, 0.6252880155336923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 873814.8638173329, 873814.8638173336, 205542.8369198811], 
processed observation next is [1.0, 0.2608695652173913, 0.4028436018957346, 0.93, 1.0, 1.0, 0.5485397777514365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24272635106037024, 0.24272635106037044, 0.30678035361176287], 
reward next is 0.6932, 
noisyNet noise sample is [array([-1.9988446], dtype=float32), 1.3743628]. 
=============================================
[2019-04-10 11:45:35,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1642877e-18 1.0000000e+00 8.0591721e-23 2.0643337e-23 3.1270288e-30], sum to 1.0000
[2019-04-10 11:45:35,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0572
[2019-04-10 11:45:35,882] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 83.5, 1.0, 2.0, 0.3659360682601785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579438.8657669369, 579438.8657669376, 173408.5297280036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6748200.0000, 
sim time next is 6748800.0000, 
raw observation next is [22.2, 83.66666666666667, 1.0, 2.0, 0.3422726798846468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542331.6899244603, 542331.6899244603, 170328.2951468949], 
processed observation next is [1.0, 0.08695652173913043, 0.2511848341232228, 0.8366666666666667, 1.0, 1.0, 0.2075574456441528, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1506476916456834, 0.1506476916456834, 0.25422133604014163], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.0438145], dtype=float32), 1.5014784]. 
=============================================
[2019-04-10 11:45:40,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8077753e-20 1.0000000e+00 4.4385215e-24 2.2145078e-27 3.4622379e-33], sum to 1.0000
[2019-04-10 11:45:40,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0108
[2019-04-10 11:45:40,325] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 78.33333333333333, 1.0, 2.0, 0.3801581559053236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573762.7834650266, 573762.7834650266, 172355.1561369583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6853800.0000, 
sim time next is 6854400.0000, 
raw observation next is [24.9, 78.0, 1.0, 2.0, 0.3824561498131069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576316.7930854935, 576316.7930854928, 172554.259451626], 
processed observation next is [0.0, 0.34782608695652173, 0.3791469194312796, 0.78, 1.0, 1.0, 0.25597126483506855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16008799807930374, 0.16008799807930355, 0.2575436708233224], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.6156941], dtype=float32), -0.37604782]. 
=============================================
[2019-04-10 11:45:43,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3990803e-19 1.0000000e+00 1.1863880e-23 8.1682300e-26 3.3876536e-32], sum to 1.0000
[2019-04-10 11:45:43,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3387
[2019-04-10 11:45:43,797] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 85.66666666666667, 1.0, 2.0, 0.4239116866811938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619978.5036087403, 619978.5036087403, 176072.6560641787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6918600.0000, 
sim time next is 6919200.0000, 
raw observation next is [24.6, 86.0, 1.0, 2.0, 0.4234272112586043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619357.0103098995, 619357.0103098989, 176015.1995710201], 
processed observation next is [0.0, 0.08695652173913043, 0.36492890995260674, 0.86, 1.0, 1.0, 0.3053339894681979, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17204361397497206, 0.17204361397497192, 0.26270925309107473], 
reward next is 0.7373, 
noisyNet noise sample is [array([1.565816], dtype=float32), 0.80262697]. 
=============================================
[2019-04-10 11:45:47,774] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.04622334e-19 1.00000000e+00 6.17564335e-24 1.36852067e-26
 5.11943292e-33], sum to 1.0000
[2019-04-10 11:45:47,780] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4570
[2019-04-10 11:45:47,784] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 58.5, 1.0, 2.0, 0.4528090363367156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646873.6021973578, 646873.6021973571, 178353.7599114793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6975000.0000, 
sim time next is 6975600.0000, 
raw observation next is [29.73333333333333, 58.33333333333333, 1.0, 2.0, 0.4491489299046257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644025.8896477638, 644025.8896477632, 178122.9336700251], 
processed observation next is [0.0, 0.7391304347826086, 0.6082148499210109, 0.5833333333333333, 1.0, 1.0, 0.336324011933284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17889608045771216, 0.17889608045771202, 0.26585512488063445], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.13409097], dtype=float32), 1.6047683]. 
=============================================
[2019-04-10 11:45:48,840] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9004108e-11 9.9987340e-01 9.8964309e-12 1.2656880e-04 1.5115875e-17], sum to 1.0000
[2019-04-10 11:45:48,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3172
[2019-04-10 11:45:48,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2172780.868114684 W.
[2019-04-10 11:45:48,856] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 59.5, 1.0, 2.0, 0.9126769492362456, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.956837414048065, 6.9112, 168.9126841976112, 2172780.868114684, 2140404.168066558, 439079.4394468215], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7055400.0000, 
sim time next is 7056000.0000, 
raw observation next is [29.7, 61.0, 1.0, 2.0, 0.7693664386641111, 1.0, 1.0, 0.7693664386641111, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2151590.060642596, 2151590.060642595, 405218.2697820147], 
processed observation next is [1.0, 0.6956521739130435, 0.6066350710900474, 0.61, 1.0, 1.0, 0.7221282393543508, 1.0, 0.5, 0.7221282393543508, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5976639057340544, 0.5976639057340541, 0.6048033877343503], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7119933], dtype=float32), -1.209794]. 
=============================================
[2019-04-10 11:45:48,861] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[50.687294]
 [50.044243]
 [51.474686]
 [50.968666]
 [52.39396 ]], R is [[50.6179657 ]
 [50.11178589]
 [49.61066818]
 [49.28422165]
 [48.79137802]].
[2019-04-10 11:45:49,609] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 11:45:49,611] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:45:49,613] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:45:49,614] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:45:49,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:45:49,615] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:45:49,616] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:45:49,616] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:45:49,615] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:45:49,618] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:45:49,622] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:45:49,631] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run17
[2019-04-10 11:45:49,631] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run17
[2019-04-10 11:45:49,644] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run17
[2019-04-10 11:45:49,644] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run17
[2019-04-10 11:45:49,675] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run17
[2019-04-10 11:46:13,580] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12043095], dtype=float32), 0.093979806]
[2019-04-10 11:46:13,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 78.33333333333333, 1.0, 2.0, 0.4142103960543995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 616434.6469711532, 616434.6469711526, 176023.0120321368]
[2019-04-10 11:46:13,582] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:46:13,585] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.5605995e-20 1.0000000e+00 7.9476719e-24 8.1694146e-26 1.7586987e-32], sampled 0.5101597627131866
[2019-04-10 11:46:32,898] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12043095], dtype=float32), 0.093979806]
[2019-04-10 11:46:32,899] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.42202127, 47.95827722833333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.202375320796546, 6.9112, 168.9108695449522, 1660463.644972256, 1453896.403144002, 311349.5128105946]
[2019-04-10 11:46:32,901] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:46:32,904] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2875065e-17 1.0000000e+00 1.1789054e-20 2.2592272e-20 2.1795253e-28], sampled 0.29079897864533255
[2019-04-10 11:46:32,906] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1660463.644972256 W.
[2019-04-10 11:46:33,156] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12043095], dtype=float32), 0.093979806]
[2019-04-10 11:46:33,157] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 79.0, 1.0, 2.0, 0.6246589710612662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872935.4382042255, 872935.4382042255, 205430.1732259692]
[2019-04-10 11:46:33,158] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:46:33,161] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3377980e-19 1.0000000e+00 2.3515163e-23 2.5790896e-24 6.6342726e-32], sampled 0.9768180879980407
[2019-04-10 11:46:42,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12043095], dtype=float32), 0.093979806]
[2019-04-10 11:46:42,597] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.98333333333333, 84.0, 1.0, 2.0, 0.6317272664036837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 882817.1978911513, 882817.1978911513, 206805.0143840828]
[2019-04-10 11:46:42,598] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 11:46:42,599] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7738163e-19 1.0000000e+00 2.9919576e-23 2.1917025e-24 8.5946275e-32], sampled 0.3204868429397919
[2019-04-10 11:47:26,045] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.7472 3163865840.2063 1768.0000
[2019-04-10 11:47:26,348] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 11:47:26,464] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4723 3007684075.4318 1765.0000
[2019-04-10 11:47:26,523] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.9263 2779170202.4700 929.0000
[2019-04-10 11:47:26,524] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.7883 2842308132.9862 1127.0000
[2019-04-10 11:47:27,540] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 400000, evaluation results [400000.0, 7885.747235355603, 3163865840.2062616, 1768.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8662.926332151339, 2779170202.4700394, 929.0, 7998.4722942245435, 3007684075.431787, 1765.0, 8499.788299844015, 2842308132.986159, 1127.0]
[2019-04-10 11:47:35,374] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.6262918e-21 1.0000000e+00 6.4435381e-25 2.5069277e-26 1.6749497e-34], sum to 1.0000
[2019-04-10 11:47:35,388] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2712
[2019-04-10 11:47:35,416] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 86.0, 1.0, 2.0, 0.4880792238462783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682009.5361680578, 682009.5361680578, 181749.329583553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7070400.0000, 
sim time next is 7071000.0000, 
raw observation next is [26.06666666666667, 86.0, 1.0, 2.0, 0.4877994035310249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681618.4083743417, 681618.4083743411, 181706.4279451883], 
processed observation next is [1.0, 0.8695652173913043, 0.4344391785150081, 0.86, 1.0, 1.0, 0.38289084762774084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18933844677065045, 0.18933844677065031, 0.27120362379878854], 
reward next is 0.7288, 
noisyNet noise sample is [array([-0.6038794], dtype=float32), 0.07776518]. 
=============================================
[2019-04-10 11:47:35,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[72.559944]
 [72.47412 ]
 [72.381516]
 [72.60779 ]
 [72.37154 ]], R is [[72.47241211]
 [72.47641754]
 [72.48052216]
 [72.48474884]
 [72.48921204]].
[2019-04-10 11:47:42,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0832075e-12 1.0000000e+00 3.1284995e-13 4.9503295e-09 5.0621427e-19], sum to 1.0000
[2019-04-10 11:47:42,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6407
[2019-04-10 11:47:42,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2031773.563577879 W.
[2019-04-10 11:47:42,410] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 83.16666666666667, 1.0, 2.0, 0.4843753332140571, 1.0, 2.0, 0.4843753332140571, 1.0, 2.0, 0.8411997684223473, 6.9112, 6.9112, 170.5573041426782, 2031773.563577879, 2031773.563577879, 404509.5298626889], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7204200.0000, 
sim time next is 7204800.0000, 
raw observation next is [29.0, 82.33333333333334, 1.0, 2.0, 0.7781221263356713, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005980118516794, 6.9112, 168.9123932073461, 1984454.933527972, 1917214.879609952, 402491.0446467924], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.8233333333333335, 1.0, 1.0, 0.7326772606453872, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00947801185167938, 0.0, 0.8294371790762746, 0.5512374815355477, 0.5325596887805422, 0.6007329024578991], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7403407], dtype=float32), 0.97305167]. 
=============================================
[2019-04-10 11:47:46,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5529617e-20 1.0000000e+00 2.5197790e-23 8.7684197e-25 2.4390303e-32], sum to 1.0000
[2019-04-10 11:47:46,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6675
[2019-04-10 11:47:46,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 92.0, 1.0, 2.0, 0.3660153972335382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558996.2606867763, 558996.2606867768, 171270.7980575553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7245600.0000, 
sim time next is 7246200.0000, 
raw observation next is [22.51666666666667, 92.0, 1.0, 2.0, 0.3657264946643853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558837.8636058999, 558837.8636058999, 171265.5716764953], 
processed observation next is [1.0, 0.8695652173913043, 0.2661927330173777, 0.92, 1.0, 1.0, 0.2358150538125124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15523273989052774, 0.15523273989052774, 0.2556202562335751], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.6471725], dtype=float32), 0.91986746]. 
=============================================
[2019-04-10 11:48:01,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0071083e-21 1.0000000e+00 6.6110035e-25 2.6868527e-27 2.1865734e-33], sum to 1.0000
[2019-04-10 11:48:01,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1408
[2019-04-10 11:48:01,809] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 77.66666666666667, 1.0, 2.0, 0.4154238338477537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606714.721921772, 606714.7219217714, 174781.5986146753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7485000.0000, 
sim time next is 7485600.0000, 
raw observation next is [25.93333333333334, 77.33333333333334, 1.0, 2.0, 0.4157669626163324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 606869.831827706, 606869.8318277065, 174785.830363647], 
processed observation next is [0.0, 0.6521739130434783, 0.42812006319115364, 0.7733333333333334, 1.0, 1.0, 0.2961047742365451, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16857495328547387, 0.16857495328547403, 0.26087437367708505], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.23728564], dtype=float32), -0.65751576]. 
=============================================
[2019-04-10 11:48:03,426] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8927518e-21 1.0000000e+00 5.5314177e-25 2.9243384e-27 4.7700321e-34], sum to 1.0000
[2019-04-10 11:48:03,426] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5547
[2019-04-10 11:48:03,455] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 83.66666666666667, 1.0, 2.0, 0.4037912282212824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594801.6969995507, 594801.6969995501, 173827.0260681139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7501200.0000, 
sim time next is 7501800.0000, 
raw observation next is [24.61666666666667, 84.33333333333334, 1.0, 2.0, 0.4028597106890718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593906.0344907525, 593906.0344907519, 173758.4386694146], 
processed observation next is [0.0, 0.8260869565217391, 0.3657187993680887, 0.8433333333333334, 1.0, 1.0, 0.2805538683000865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16497389846965346, 0.16497389846965332, 0.25934095323793227], 
reward next is 0.7407, 
noisyNet noise sample is [array([-0.43060702], dtype=float32), 0.02821383]. 
=============================================
[2019-04-10 11:48:06,986] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0981976e-20 1.0000000e+00 1.4027725e-24 1.1425387e-27 4.0822219e-33], sum to 1.0000
[2019-04-10 11:48:06,992] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6466
[2019-04-10 11:48:06,996] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 95.0, 1.0, 2.0, 0.3359611040407641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521705.286226209, 521705.2862262096, 168465.0910193421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7459200.0000, 
sim time next is 7459800.0000, 
raw observation next is [21.7, 94.66666666666667, 1.0, 2.0, 0.3381506709639861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524440.2747357157, 524440.2747357157, 168662.9221081334], 
processed observation next is [0.0, 0.34782608695652173, 0.2274881516587678, 0.9466666666666668, 1.0, 1.0, 0.20259116983612782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14567785409325437, 0.14567785409325437, 0.2517357046390051], 
reward next is 0.7483, 
noisyNet noise sample is [array([-1.4656881], dtype=float32), -2.267742]. 
=============================================
[2019-04-10 11:48:07,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0199701e-19 1.0000000e+00 4.2826904e-24 7.7183766e-26 3.3111593e-33], sum to 1.0000
[2019-04-10 11:48:07,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3434
[2019-04-10 11:48:07,215] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 73.33333333333334, 1.0, 2.0, 0.4558510037865461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 642940.3347757935, 642940.3347757929, 177741.299175611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7582800.0000, 
sim time next is 7583400.0000, 
raw observation next is [27.35, 74.5, 1.0, 2.0, 0.4594131902218633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645908.7461473537, 645908.7461473542, 177994.9319047655], 
processed observation next is [0.0, 0.782608695652174, 0.4952606635071091, 0.745, 1.0, 1.0, 0.34869059062875096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17941909615204268, 0.17941909615204285, 0.26566407746979925], 
reward next is 0.7343, 
noisyNet noise sample is [array([0.7969788], dtype=float32), -0.06334447]. 
=============================================
[2019-04-10 11:48:11,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3618759e-19 1.0000000e+00 4.9958165e-22 3.3799147e-21 6.1728925e-30], sum to 1.0000
[2019-04-10 11:48:11,464] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-10 11:48:11,468] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333333, 83.16666666666666, 1.0, 2.0, 0.5075454124171095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709219.3772228311, 709219.3772228304, 184786.9198064809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7671000.0000, 
sim time next is 7671600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5098492989891206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712439.7963159691, 712439.7963159684, 185153.6690967838], 
processed observation next is [1.0, 0.8260869565217391, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4094569867338802, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19789994342110254, 0.19789994342110234, 0.276348759845946], 
reward next is 0.7237, 
noisyNet noise sample is [array([-1.7098367], dtype=float32), -0.46789336]. 
=============================================
[2019-04-10 11:48:12,180] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.8932951e-17 1.0000000e+00 1.2727183e-20 2.5874712e-20 5.7508527e-29], sum to 1.0000
[2019-04-10 11:48:12,185] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6521
[2019-04-10 11:48:12,195] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 90.66666666666667, 1.0, 2.0, 0.4758353532023452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664895.4091513811, 664895.4091513811, 179896.3057775905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7690800.0000, 
sim time next is 7691400.0000, 
raw observation next is [25.05, 91.0, 1.0, 2.0, 0.4763874576163864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665667.1187408757, 665667.1187408757, 179978.8588216557], 
processed observation next is [1.0, 0.0, 0.3862559241706162, 0.91, 1.0, 1.0, 0.36914151520046556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1849075329835766, 0.1849075329835766, 0.26862516242038165], 
reward next is 0.7314, 
noisyNet noise sample is [array([1.528935], dtype=float32), 0.9700164]. 
=============================================
[2019-04-10 11:48:12,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.269183e-18 1.000000e+00 3.460054e-21 5.921419e-22 2.422427e-29], sum to 1.0000
[2019-04-10 11:48:12,265] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2140
[2019-04-10 11:48:12,269] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 94.5, 1.0, 2.0, 0.5409959958880484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755978.235357501, 755978.2353575015, 190264.0808968115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7702200.0000, 
sim time next is 7702800.0000, 
raw observation next is [24.53333333333333, 94.33333333333334, 1.0, 2.0, 0.5504400963028612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770515.9300809628, 770515.9300809623, 192043.8561128107], 
processed observation next is [1.0, 0.13043478260869565, 0.36176935229067925, 0.9433333333333335, 1.0, 1.0, 0.4583615618106761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21403220280026744, 0.2140322028002673, 0.2866326210638966], 
reward next is 0.7134, 
noisyNet noise sample is [array([0.5565055], dtype=float32), -0.22026011]. 
=============================================
[2019-04-10 11:48:22,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:22,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:22,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-04-10 11:48:22,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:22,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:22,947] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-04-10 11:48:23,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:23,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:23,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-04-10 11:48:23,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:23,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:23,546] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-04-10 11:48:23,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:23,778] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:23,779] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-04-10 11:48:23,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:23,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:23,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-04-10 11:48:23,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:23,996] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:23,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-04-10 11:48:24,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,160] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-04-10 11:48:24,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,214] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,216] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-04-10 11:48:24,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run3
[2019-04-10 11:48:24,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-04-10 11:48:24,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,734] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,745] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-04-10 11:48:24,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-04-10 11:48:24,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:24,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:24,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-04-10 11:48:25,149] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:25,149] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:25,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-04-10 11:48:26,728] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 11:48:26,728] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:26,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-04-10 11:48:27,644] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 11:48:27,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:48:27,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:48:27,646] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:48:27,646] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:48:27,647] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:27,648] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:27,648] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:27,647] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:48:27,651] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:27,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:48:27,659] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run18
[2019-04-10 11:48:27,672] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run18
[2019-04-10 11:48:27,677] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run18
[2019-04-10 11:48:27,704] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run18
[2019-04-10 11:48:27,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run18
[2019-04-10 11:48:29,651] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:48:29,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.96666666666667, 81.33333333333334, 1.0, 2.0, 0.4194133435157738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619673.6562750271, 619673.6562750271, 176213.7279296059]
[2019-04-10 11:48:29,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:48:29,655] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.8903080e-19 1.0000000e+00 3.3067474e-22 2.9284748e-23 1.3242088e-30], sampled 0.29456198846469783
[2019-04-10 11:48:40,233] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:48:40,234] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.221793545, 80.56197452166666, 1.0, 2.0, 0.4263005318120597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621671.4669247671, 621671.4669247678, 176186.1410967048]
[2019-04-10 11:48:40,235] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:48:40,237] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4859847e-19 1.0000000e+00 6.6556971e-23 2.0238710e-25 1.7163284e-31], sampled 0.3945528745561634
[2019-04-10 11:48:42,458] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:48:42,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.65779149, 93.56597267, 1.0, 2.0, 0.2662623012736623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 439561.2583752755, 439561.2583752755, 162509.2230148587]
[2019-04-10 11:48:42,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:48:42,461] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3518648e-19 1.0000000e+00 2.2435842e-23 1.9289677e-25 3.6016841e-32], sampled 0.7732265245328908
[2019-04-10 11:49:01,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:49:01,604] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.50294486333333, 77.92050333166667, 1.0, 2.0, 0.5159388617126737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720951.9568553416, 720951.9568553421, 186129.7056138627]
[2019-04-10 11:49:01,605] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:49:01,609] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9498507e-19 1.0000000e+00 7.2593618e-23 6.3981305e-24 1.5573550e-31], sampled 0.04013368165000464
[2019-04-10 11:49:14,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:49:14,168] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.502275365, 79.04701425, 1.0, 2.0, 0.4468218249981143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651374.639998494, 651374.6399984933, 179122.767582602]
[2019-04-10 11:49:14,168] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:49:14,174] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.1268751e-19 1.0000000e+00 1.5846532e-22 2.0215572e-24 5.3200034e-31], sampled 0.6652340335346858
[2019-04-10 11:49:46,434] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:49:46,439] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.76931638333333, 89.82622003833333, 1.0, 2.0, 0.5115172208789069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714771.2572489193, 714771.2572489193, 185419.2673775531]
[2019-04-10 11:49:46,442] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:49:46,446] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.6939550e-19 1.0000000e+00 1.6956739e-22 1.6635728e-24 5.5582423e-31], sampled 0.25486817753907
[2019-04-10 11:49:57,747] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:49:57,748] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.58282276666667, 88.89621710666667, 1.0, 2.0, 0.5793325738671675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870689.8207020862, 870689.8207020862, 204537.7011535113]
[2019-04-10 11:49:57,750] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:49:57,752] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2913502e-18 1.0000000e+00 2.9243694e-21 1.3753067e-21 1.4965872e-29], sampled 0.4690139790460375
[2019-04-10 11:50:02,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12211198], dtype=float32), 0.0926107]
[2019-04-10 11:50:02,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 89.0, 1.0, 2.0, 0.5042556198532036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704620.8557840367, 704620.8557840367, 184265.4910945388]
[2019-04-10 11:50:02,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:50:02,363] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4501891e-20 1.0000000e+00 3.1642731e-24 1.0593081e-25 2.3143853e-33], sampled 0.743698851443107
[2019-04-10 11:50:06,059] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.9236 2926989253.6994 1327.0000
[2019-04-10 11:50:06,194] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7126 2779213764.7472 932.0000
[2019-04-10 11:50:07,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7900.5739 3161645249.4089 1729.0000
[2019-04-10 11:50:07,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8007.0087 3006509570.8977 1735.0000
[2019-04-10 11:50:07,083] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.7396 2841860884.8197 1119.0000
[2019-04-10 11:50:08,111] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 425000, evaluation results [425000.0, 7900.573890200024, 3161645249.4088836, 1729.0, 8260.92364061519, 2926989253.699379, 1327.0, 8660.712638053283, 2779213764.7471786, 932.0, 8007.008650146003, 3006509570.8977156, 1735.0, 8500.739583871116, 2841860884.8197403, 1119.0]
[2019-04-10 11:50:15,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.3083306e-19 1.0000000e+00 2.6765390e-21 5.0648970e-22 4.3792259e-29], sum to 1.0000
[2019-04-10 11:50:15,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3748
[2019-04-10 11:50:15,235] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 89.0, 1.0, 2.0, 0.3446370056566652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535723.0073958393, 535723.0073958393, 169602.3903136275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 89400.0000, 
sim time next is 90000.0000, 
raw observation next is [22.3, 89.0, 1.0, 2.0, 0.3439598067111493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534671.6033206601, 534671.6033206594, 169517.3064282207], 
processed observation next is [1.0, 0.043478260869565216, 0.25592417061611383, 0.89, 1.0, 1.0, 0.20959012856764975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14851988981129446, 0.14851988981129427, 0.25301090511674734], 
reward next is 0.7470, 
noisyNet noise sample is [array([2.5103595], dtype=float32), 0.9638724]. 
=============================================
[2019-04-10 11:50:15,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[71.06332 ]
 [71.18659 ]
 [71.34178 ]
 [71.717415]
 [72.426506]], R is [[71.16064453]
 [71.19589996]
 [71.23074341]
 [71.26529694]
 [71.29949951]].
[2019-04-10 11:50:18,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0665327e-19 1.0000000e+00 6.8589050e-24 6.0146757e-27 7.9954077e-33], sum to 1.0000
[2019-04-10 11:50:18,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8956
[2019-04-10 11:50:18,278] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 94.5, 1.0, 2.0, 0.288313258333764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464202.8945721302, 464202.8945721308, 164560.2537644077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [20.13333333333333, 94.33333333333334, 1.0, 2.0, 0.2890022745360856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465186.9534925816, 465186.9534925816, 164627.7355783828], 
processed observation next is [0.0, 0.2608695652173913, 0.15323854660347538, 0.9433333333333335, 1.0, 1.0, 0.14337623438082606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1292185981923838, 0.1292185981923838, 0.24571303817669074], 
reward next is 0.7543, 
noisyNet noise sample is [array([0.50679123], dtype=float32), 0.28129622]. 
=============================================
[2019-04-10 11:50:18,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2969452e-17 1.0000000e+00 1.4068374e-20 4.2061033e-19 2.0646198e-28], sum to 1.0000
[2019-04-10 11:50:18,628] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0167
[2019-04-10 11:50:18,655] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.37936315630412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570915.4068064233, 570915.4068064233, 172052.029309508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 148800.0000, 
sim time next is 149400.0000, 
raw observation next is [22.5, 96.0, 1.0, 2.0, 0.3757333794794568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565510.5378395348, 565510.5378395341, 171578.450878132], 
processed observation next is [1.0, 0.7391304347826086, 0.2654028436018958, 0.96, 1.0, 1.0, 0.2478715415415142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1570862605109819, 0.1570862605109817, 0.25608724011661493], 
reward next is 0.7439, 
noisyNet noise sample is [array([0.35056725], dtype=float32), 0.08049376]. 
=============================================
[2019-04-10 11:50:23,143] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4731891e-21 1.0000000e+00 7.8492636e-25 4.3566320e-29 9.9980395e-34], sum to 1.0000
[2019-04-10 11:50:23,159] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6362
[2019-04-10 11:50:23,177] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 93.5, 1.0, 2.0, 0.2908045899499951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467461.3572708468, 467461.3572708468, 164783.221668899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 199800.0000, 
sim time next is 200400.0000, 
raw observation next is [20.33333333333333, 93.33333333333334, 1.0, 2.0, 0.2908920393438755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467476.7471486373, 467476.7471486373, 164783.842805928], 
processed observation next is [0.0, 0.30434782608695654, 0.16271721958925733, 0.9333333333333335, 1.0, 1.0, 0.14565305945045237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1298546519857326, 0.1298546519857326, 0.2459460340386985], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.6754007], dtype=float32), 0.7744746]. 
=============================================
[2019-04-10 11:50:29,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0162549e-20 1.0000000e+00 1.2859999e-24 1.7665096e-27 6.2922284e-34], sum to 1.0000
[2019-04-10 11:50:29,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9761
[2019-04-10 11:50:29,852] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 76.83333333333334, 1.0, 2.0, 0.3138066617509504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 494246.5703175248, 494246.5703175254, 166552.6161048481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 303000.0000, 
sim time next is 303600.0000, 
raw observation next is [23.46666666666667, 76.66666666666667, 1.0, 2.0, 0.3137326538755795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494021.2431159222, 494021.2431159222, 166533.2151803389], 
processed observation next is [0.0, 0.5217391304347826, 0.31121642969984215, 0.7666666666666667, 1.0, 1.0, 0.17317187213925245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13722812308775617, 0.13722812308775617, 0.2485570375825954], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.25323507], dtype=float32), 1.9261131]. 
=============================================
[2019-04-10 11:50:39,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9130367e-20 1.0000000e+00 5.9131968e-24 1.0831224e-25 2.5860937e-32], sum to 1.0000
[2019-04-10 11:50:39,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7955
[2019-04-10 11:50:39,660] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.75, 85.66666666666667, 1.0, 2.0, 0.2470723540427138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 406555.8672708419, 406555.8672708425, 160615.5944239727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 427800.0000, 
sim time next is 428400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.246315552570411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 405354.9084954147, 405354.9084954141, 160541.0956815287], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.09194644888001323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11259858569317076, 0.11259858569317059, 0.2396135756440727], 
reward next is 0.7604, 
noisyNet noise sample is [array([-0.27775398], dtype=float32), 0.71045136]. 
=============================================
[2019-04-10 11:50:42,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9800575e-20 1.0000000e+00 2.4242833e-23 4.8464676e-24 1.0567736e-32], sum to 1.0000
[2019-04-10 11:50:42,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5331
[2019-04-10 11:50:42,390] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 65.0, 1.0, 2.0, 0.2424079751973434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 399628.108440928, 399628.1084409273, 160144.5748213432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 498600.0000, 
sim time next is 499200.0000, 
raw observation next is [22.2, 66.33333333333334, 1.0, 2.0, 0.2401018453830852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 395993.0779871152, 395993.0779871159, 159918.0543452117], 
processed observation next is [1.0, 0.782608695652174, 0.2511848341232228, 0.6633333333333334, 1.0, 1.0, 0.0844600546784159, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1099980772186431, 0.1099980772186433, 0.23868366320180853], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.13842118], dtype=float32), 1.6707219]. 
=============================================
[2019-04-10 11:50:46,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0804047e-18 1.0000000e+00 5.9829315e-21 3.1004484e-21 6.6213128e-29], sum to 1.0000
[2019-04-10 11:50:46,132] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0875
[2019-04-10 11:50:46,135] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.81666666666667, 60.66666666666666, 1.0, 2.0, 0.7596594214233054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1242601.624228484, 1242601.624228484, 254190.4673776444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 571800.0000, 
sim time next is 572400.0000, 
raw observation next is [23.8, 61.0, 1.0, 2.0, 0.7461307550320919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1219838.501201951, 1219838.501201951, 250563.1228964446], 
processed observation next is [1.0, 0.6521739130434783, 0.3270142180094788, 0.61, 1.0, 1.0, 0.6941334397977011, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33884402811165304, 0.33884402811165304, 0.3739748102932009], 
reward next is 0.6260, 
noisyNet noise sample is [array([0.72150505], dtype=float32), -2.0600574]. 
=============================================
[2019-04-10 11:50:47,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2480261e-19 1.0000000e+00 1.3549596e-23 2.0842876e-25 3.9538487e-33], sum to 1.0000
[2019-04-10 11:50:47,707] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-10 11:50:47,710] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.35, 85.5, 1.0, 2.0, 0.2175637526914411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 362624.6104155854, 362624.6104155854, 157327.2344602548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 603000.0000, 
sim time next is 603600.0000, 
raw observation next is [18.26666666666667, 86.0, 1.0, 2.0, 0.2169482358025774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 361651.4476100616, 361651.4476100621, 157256.6847639962], 
processed observation next is [1.0, 1.0, 0.06477093206951046, 0.86, 1.0, 1.0, 0.056564139521177594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10045873544723934, 0.10045873544723946, 0.23471146979700924], 
reward next is 0.7653, 
noisyNet noise sample is [array([-0.94424915], dtype=float32), -1.2593753]. 
=============================================
[2019-04-10 11:50:51,721] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2473662e-19 1.0000000e+00 4.8284595e-23 4.0638576e-25 2.7351830e-31], sum to 1.0000
[2019-04-10 11:50:51,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7497
[2019-04-10 11:50:51,735] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.66666666666667, 93.0, 1.0, 2.0, 0.2334899960315701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388733.2498148087, 388733.2498148087, 158874.2273807432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 699600.0000, 
sim time next is 700200.0000, 
raw observation next is [17.65, 93.0, 1.0, 2.0, 0.2227443206335127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 370888.6993214728, 370888.6993214734, 157886.9921243822], 
processed observation next is [1.0, 0.08695652173913043, 0.035545023696682464, 0.93, 1.0, 1.0, 0.06354737425724422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10302463870040911, 0.10302463870040929, 0.23565222705131672], 
reward next is 0.7643, 
noisyNet noise sample is [array([-1.3350115], dtype=float32), 1.4171582]. 
=============================================
[2019-04-10 11:50:52,496] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3818549e-19 1.0000000e+00 1.6315404e-23 7.4395851e-26 3.4374329e-32], sum to 1.0000
[2019-04-10 11:50:52,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-10 11:50:52,507] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.73333333333333, 81.33333333333334, 1.0, 2.0, 0.2311561741317033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 382803.2406233324, 382803.2406233324, 158977.3018072551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [19.96666666666667, 80.16666666666667, 1.0, 2.0, 0.2334841922283851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 386343.1201557997, 386343.1201557997, 159220.1731400012], 
processed observation next is [1.0, 0.2608695652173913, 0.14533965244865735, 0.8016666666666667, 1.0, 1.0, 0.07648697858841577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10731753337661104, 0.10731753337661104, 0.23764204946268835], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.7104062], dtype=float32), -0.27677333]. 
=============================================
[2019-04-10 11:50:56,062] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4141725e-20 1.0000000e+00 1.2923932e-24 3.6446197e-28 3.8718001e-34], sum to 1.0000
[2019-04-10 11:50:56,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5201
[2019-04-10 11:50:56,073] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 63.0, 1.0, 2.0, 0.2898091305429463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464338.8374976719, 464338.8374976725, 164557.6143666503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 825000.0000, 
sim time next is 825600.0000, 
raw observation next is [24.73333333333333, 63.00000000000001, 1.0, 2.0, 0.288240026047877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462102.2383976206, 462102.2383976199, 164406.2240309341], 
processed observation next is [0.0, 0.5652173913043478, 0.3712480252764612, 0.6300000000000001, 1.0, 1.0, 0.14245786270828556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12836173288822794, 0.12836173288822775, 0.24538242392676732], 
reward next is 0.7546, 
noisyNet noise sample is [array([1.1945126], dtype=float32), 0.09025327]. 
=============================================
[2019-04-10 11:50:56,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0502455e-20 1.0000000e+00 3.8296305e-25 5.9883672e-28 3.4759857e-34], sum to 1.0000
[2019-04-10 11:50:56,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5701
[2019-04-10 11:50:56,827] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 73.83333333333334, 1.0, 2.0, 0.3114145046085787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491778.5111053013, 491778.5111053013, 166400.2785317352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 839400.0000, 
sim time next is 840000.0000, 
raw observation next is [23.6, 74.66666666666667, 1.0, 2.0, 0.3109348703224167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491277.5409015107, 491277.5409015114, 166369.0214944375], 
processed observation next is [0.0, 0.7391304347826086, 0.3175355450236968, 0.7466666666666667, 1.0, 1.0, 0.1698010485812249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13646598358375298, 0.13646598358375317, 0.24831197237975747], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.52730554], dtype=float32), 1.2689424]. 
=============================================
[2019-04-10 11:50:56,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.15799 ]
 [80.12201 ]
 [80.0758  ]
 [80.078804]
 [80.07771 ]], R is [[80.12736511]
 [80.0777359 ]
 [80.0286026 ]
 [79.98011017]
 [79.93226624]].
[2019-04-10 11:51:03,130] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-10 11:51:03,132] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:51:03,133] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:51:03,135] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:51:03,135] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:51:03,135] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:51:03,136] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:51:03,136] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:51:03,136] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:51:03,137] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:51:03,138] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:51:03,152] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run19
[2019-04-10 11:51:03,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run19
[2019-04-10 11:51:03,204] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run19
[2019-04-10 11:51:03,221] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run19
[2019-04-10 11:51:03,221] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run19
[2019-04-10 11:51:10,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:51:10,757] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.42609741, 83.46103071333333, 1.0, 2.0, 0.2056000337640555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343923.20848874, 343923.20848874, 145068.1765728116]
[2019-04-10 11:51:10,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:51:10,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2753628e-19 1.0000000e+00 1.2854365e-23 4.2074181e-27 2.3300039e-32], sampled 0.4409676259236356
[2019-04-10 11:51:35,010] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:51:35,013] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.68131174666667, 88.80035357, 1.0, 2.0, 0.3349517399492923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527608.2778786058, 527608.2778786058, 169109.7066233369]
[2019-04-10 11:51:35,014] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:51:35,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7924470e-19 1.0000000e+00 6.2200717e-23 9.0228057e-25 1.3070493e-31], sampled 0.3271838806509638
[2019-04-10 11:51:39,640] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:51:39,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.06666666666666, 51.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.096319609346777, 6.9112, 168.9116259718679, 1585174.057432684, 1453844.869733711, 311353.1932770706]
[2019-04-10 11:51:39,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:51:39,644] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7144746e-14 1.0000000e+00 1.2111684e-15 2.0027087e-10 2.0620827e-23], sampled 0.2695800733119178
[2019-04-10 11:51:52,891] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:51:52,891] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666667, 70.0, 1.0, 2.0, 0.5241060157277919, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129563246159, 732368.3395108683, 732368.3395108677, 187461.6774136624]
[2019-04-10 11:51:52,891] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:51:52,894] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2511926e-13 9.9999988e-01 2.1248245e-14 7.3955960e-08 1.7863153e-21], sampled 0.07789739874157564
[2019-04-10 11:51:53,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:51:53,069] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.00000000000001, 1.0, 2.0, 0.5159375113248671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720950.0692380589, 720950.0692380589, 186130.991501442]
[2019-04-10 11:51:53,074] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:51:53,090] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.977918e-21 1.000000e+00 3.486280e-25 6.465963e-27 4.894638e-34], sampled 0.32824523997788213
[2019-04-10 11:52:07,981] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:52:07,981] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.12415275666667, 67.1470601, 1.0, 2.0, 0.6825433803884874, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974193558217, 6.9112, 168.9123159602136, 1850701.088080352, 1783465.268266196, 380905.7589910615]
[2019-04-10 11:52:07,981] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:52:07,986] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.5255223e-15 1.0000000e+00 4.8920484e-17 8.3039643e-16 2.5563262e-24], sampled 0.8558536167141227
[2019-04-10 11:52:07,987] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1850701.088080352 W.
[2019-04-10 11:52:08,927] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:52:08,943] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.31666666666667, 94.0, 1.0, 2.0, 0.6209888196530418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867804.455958477, 867804.455958477, 204720.8448404808]
[2019-04-10 11:52:08,943] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:52:08,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3669193e-18 1.0000000e+00 4.7716629e-22 2.2921504e-23 1.7770031e-30], sampled 0.7808383011817752
[2019-04-10 11:52:30,108] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:52:30,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.88333333333333, 79.66666666666667, 1.0, 2.0, 0.5971273351780215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834445.9861867484, 834445.9861867484, 200211.4043873551]
[2019-04-10 11:52:30,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:52:30,112] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3486881e-19 1.0000000e+00 6.2627850e-23 1.8747922e-23 1.5931436e-31], sampled 0.4512674961660793
[2019-04-10 11:52:41,439] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.9040 2927252160.5107 1336.0000
[2019-04-10 11:52:41,786] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7897.1787 3162439610.8106 1732.0000
[2019-04-10 11:52:42,029] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8006.4823 3006675247.5435 1744.0000
[2019-04-10 11:52:42,319] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8502.3744 2841849789.2060 1122.0000
[2019-04-10 11:52:42,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12132698], dtype=float32), 0.09217013]
[2019-04-10 11:52:42,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.21039911666666, 85.84278293333334, 1.0, 2.0, 0.5994067711622387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837632.6033666856, 837632.6033666856, 200632.8848962577]
[2019-04-10 11:52:42,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:52:42,787] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.6800380e-19 1.0000000e+00 1.5621816e-22 2.9081802e-23 4.4778417e-31], sampled 0.852689527458585
[2019-04-10 11:52:42,813] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0510 2779264069.3335 933.0000
[2019-04-10 11:52:43,828] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 450000, evaluation results [450000.0, 7897.178700434137, 3162439610.8106313, 1732.0, 8255.904010374748, 2927252160.5107336, 1336.0, 8660.050958675192, 2779264069.3335457, 933.0, 8006.482272939088, 3006675247.5434694, 1744.0, 8502.374357887244, 2841849789.2059593, 1122.0]
[2019-04-10 11:52:50,454] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5089455e-19 1.0000000e+00 4.6902157e-23 2.3746007e-24 2.8970735e-31], sum to 1.0000
[2019-04-10 11:52:50,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5901
[2019-04-10 11:52:50,488] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 93.0, 1.0, 2.0, 0.4328314631610836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684621.5134002778, 684621.5134002785, 182991.014045255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1066800.0000, 
sim time next is 1067400.0000, 
raw observation next is [21.2, 92.0, 1.0, 2.0, 0.4860028210704364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769026.9202685299, 769026.9202685293, 191721.9695061481], 
processed observation next is [1.0, 0.34782608695652173, 0.20379146919431282, 0.92, 1.0, 1.0, 0.38072629044630896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21361858896348054, 0.21361858896348038, 0.28615219329275837], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.5211867], dtype=float32), 0.33880517]. 
=============================================
[2019-04-10 11:52:52,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.2615769e-20 1.0000000e+00 2.8849760e-23 2.1714207e-25 8.1445584e-32], sum to 1.0000
[2019-04-10 11:52:52,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2907
[2019-04-10 11:52:52,338] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 95.0, 1.0, 2.0, 0.2932622200729397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469811.226344271, 469811.226344271, 164936.77485295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1058400.0000, 
sim time next is 1059000.0000, 
raw observation next is [20.35, 95.0, 1.0, 2.0, 0.3116334845821996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498790.3928910713, 498790.3928910707, 167013.0434558021], 
processed observation next is [1.0, 0.2608695652173913, 0.16350710900473947, 0.95, 1.0, 1.0, 0.1706427525086742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13855288691418646, 0.1385528869141863, 0.24927319918776433], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.92547095], dtype=float32), -1.9525433]. 
=============================================
[2019-04-10 11:52:52,386] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.054085]
 [74.00711 ]
 [74.01956 ]
 [73.98618 ]
 [73.95351 ]], R is [[73.99663544]
 [74.01049042]
 [74.02450562]
 [74.03853607]
 [74.05279541]].
[2019-04-10 11:52:53,333] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1438522e-18 1.0000000e+00 7.6485172e-22 3.8357749e-23 3.8350298e-30], sum to 1.0000
[2019-04-10 11:52:53,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0104
[2019-04-10 11:52:53,351] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 82.33333333333334, 1.0, 2.0, 0.5205601970345691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827988.8692706674, 827988.8692706674, 198243.4293223979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [22.3, 81.5, 1.0, 2.0, 0.54169015138309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861854.4169165186, 861854.416916518, 202247.3206357979], 
processed observation next is [1.0, 0.43478260869565216, 0.25592417061611383, 0.815, 1.0, 1.0, 0.4478194594976987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23940400469903295, 0.23940400469903278, 0.30186167259074315], 
reward next is 0.6981, 
noisyNet noise sample is [array([0.28776008], dtype=float32), 2.3347125]. 
=============================================
[2019-04-10 11:53:02,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4971499e-17 1.0000000e+00 4.4930290e-19 2.7668412e-18 7.7498527e-27], sum to 1.0000
[2019-04-10 11:53:02,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7098
[2019-04-10 11:53:02,377] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 58.83333333333334, 1.0, 2.0, 0.9342209316084197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1428344.877944785, 1428344.877944785, 296995.1814569918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1177800.0000, 
sim time next is 1178400.0000, 
raw observation next is [27.6, 58.66666666666667, 1.0, 2.0, 0.9488908089846819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1452102.698456047, 1452102.698456047, 301799.5554521694], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.5866666666666667, 1.0, 1.0, 0.9384226614273277, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4033618606822353, 0.4033618606822353, 0.45044709768980506], 
reward next is 0.5496, 
noisyNet noise sample is [array([0.7432082], dtype=float32), -0.84048885]. 
=============================================
[2019-04-10 11:53:03,139] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.4857254e-20 1.0000000e+00 1.2538938e-22 1.5644470e-23 4.2784112e-32], sum to 1.0000
[2019-04-10 11:53:03,142] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-10 11:53:03,148] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 73.83333333333334, 1.0, 2.0, 0.3517874959090655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543006.3234376422, 543006.3234376422, 170095.7090169036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1195800.0000, 
sim time next is 1196400.0000, 
raw observation next is [24.6, 74.66666666666667, 1.0, 2.0, 0.3513624789805923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541945.5373656523, 541945.5373656517, 169996.6903790539], 
processed observation next is [1.0, 0.8695652173913043, 0.36492890995260674, 0.7466666666666667, 1.0, 1.0, 0.21850901081999072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15054042704601453, 0.15054042704601436, 0.2537264035508267], 
reward next is 0.7463, 
noisyNet noise sample is [array([1.2479998], dtype=float32), 0.58723795]. 
=============================================
[2019-04-10 11:53:05,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4564105e-11 9.9957103e-01 7.3637719e-12 4.2896796e-04 1.6668590e-18], sum to 1.0000
[2019-04-10 11:53:05,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4260
[2019-04-10 11:53:05,855] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 76.66666666666667, 1.0, 2.0, 0.2872459722857238, 1.0, 1.0, 0.2872459722857238, 1.0, 1.0, 0.4873478627567728, 6.911200000000001, 6.9112, 170.5573041426782, 1204424.89439742, 1204424.894397419, 298177.9736018421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1271400.0000, 
sim time next is 1272000.0000, 
raw observation next is [27.5, 77.33333333333334, 1.0, 2.0, 0.4742302707345872, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129561232457, 662651.8919060179, 662651.8919060179, 179657.7853824496], 
processed observation next is [1.0, 0.7391304347826086, 0.5023696682464456, 0.7733333333333334, 1.0, 1.0, 0.3665424948609484, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399432510455, 0.18406996997389385, 0.18406996997389385, 0.26814594833201433], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3528805], dtype=float32), -0.50957555]. 
=============================================
[2019-04-10 11:53:05,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.00083 ]
 [61.25171 ]
 [59.393677]
 [59.497864]
 [57.799732]], R is [[66.21250916]
 [65.55038452]
 [64.8948822 ]
 [64.24593353]
 [64.10242462]].
[2019-04-10 11:53:07,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6656087e-19 1.0000000e+00 1.7716024e-22 7.2782413e-24 5.9668098e-31], sum to 1.0000
[2019-04-10 11:53:07,082] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2285
[2019-04-10 11:53:07,096] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 94.33333333333334, 1.0, 2.0, 0.3727137809671536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577912.448543163, 577912.448543163, 173118.2213656409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1228200.0000, 
sim time next is 1228800.0000, 
raw observation next is [21.9, 93.66666666666667, 1.0, 2.0, 0.3507683142374582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542852.3509460131, 542852.3509460137, 170122.107738015], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9366666666666668, 1.0, 1.0, 0.21779314968368457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15079231970722587, 0.15079231970722604, 0.25391359363882837], 
reward next is 0.7461, 
noisyNet noise sample is [array([-0.40665507], dtype=float32), 1.1601409]. 
=============================================
[2019-04-10 11:53:08,632] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4149956e-19 1.0000000e+00 1.3559428e-22 8.6985660e-23 8.4682255e-32], sum to 1.0000
[2019-04-10 11:53:08,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0760
[2019-04-10 11:53:08,641] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 93.16666666666667, 1.0, 2.0, 0.4692794729524349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660787.5824006919, 660787.5824006925, 179575.0919112472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [24.53333333333333, 93.33333333333334, 1.0, 2.0, 0.4686659165150509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660260.844352134, 660260.8443521347, 179527.2354870941], 
processed observation next is [1.0, 0.9565217391304348, 0.36176935229067925, 0.9333333333333335, 1.0, 1.0, 0.3598384536325915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18340579009781502, 0.1834057900978152, 0.2679510977419315], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.2508252], dtype=float32), -2.0549178]. 
=============================================
[2019-04-10 11:53:08,650] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.93202 ]
 [73.83066 ]
 [73.82946 ]
 [73.823166]
 [73.73781 ]], R is [[73.91915894]
 [73.91194916]
 [73.9048233 ]
 [73.89785767]
 [73.89089966]].
[2019-04-10 11:53:10,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8035093e-20 1.0000000e+00 4.7190354e-24 1.1412989e-24 2.8575573e-33], sum to 1.0000
[2019-04-10 11:53:10,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3807
[2019-04-10 11:53:10,194] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 90.33333333333334, 1.0, 2.0, 0.4688239980778687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659663.1933926058, 659663.1933926065, 179445.2495757246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1284000.0000, 
sim time next is 1284600.0000, 
raw observation next is [24.93333333333333, 90.66666666666667, 1.0, 2.0, 0.4689051040121825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659438.2118562575, 659438.2118562581, 179413.6465499122], 
processed observation next is [1.0, 0.8695652173913043, 0.38072669826224315, 0.9066666666666667, 1.0, 1.0, 0.36012663133997896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18317728107118264, 0.1831772810711828, 0.2677815620147943], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.9980247], dtype=float32), -0.8846215]. 
=============================================
[2019-04-10 11:53:22,427] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5753258e-18 1.0000000e+00 2.6240605e-20 4.8337033e-19 1.4067028e-28], sum to 1.0000
[2019-04-10 11:53:22,429] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0307
[2019-04-10 11:53:22,434] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 87.0, 1.0, 2.0, 0.7590619797247687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1133001.807641249, 1133001.807641248, 243730.3944333877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1607400.0000, 
sim time next is 1608000.0000, 
raw observation next is [23.83333333333334, 87.66666666666666, 1.0, 2.0, 0.7740902642945382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1154497.710712508, 1154497.710712507, 247411.900702819], 
processed observation next is [1.0, 0.6086956521739131, 0.32859399684044266, 0.8766666666666666, 1.0, 1.0, 0.7278195955355882, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32069380853125223, 0.3206938085312519, 0.369271493586297], 
reward next is 0.6307, 
noisyNet noise sample is [array([-1.6336243], dtype=float32), 0.7735725]. 
=============================================
[2019-04-10 11:53:22,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.661354]
 [67.6588  ]
 [67.51585 ]
 [67.09554 ]
 [66.96305 ]], R is [[67.53239441]
 [67.49329376]
 [67.46736908]
 [67.45323944]
 [67.40904999]].
[2019-04-10 11:53:23,192] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.24937311e-19 1.00000000e+00 1.90721020e-22 2.24599902e-23
 1.31860345e-30], sum to 1.0000
[2019-04-10 11:53:23,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4682
[2019-04-10 11:53:23,206] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 99.0, 1.0, 2.0, 0.4749539791025801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679709.2318490242, 679709.2318490242, 181816.7391612908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1662000.0000, 
sim time next is 1662600.0000, 
raw observation next is [23.48333333333333, 99.0, 1.0, 2.0, 0.4689554287411384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 670649.5228793566, 670649.5228793573, 180843.2579851706], 
processed observation next is [1.0, 0.21739130434782608, 0.3120063191153238, 0.99, 1.0, 1.0, 0.3601872635435402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1862915341331546, 0.1862915341331548, 0.2699153104256278], 
reward next is 0.7301, 
noisyNet noise sample is [array([-1.8688197], dtype=float32), 1.3621972]. 
=============================================
[2019-04-10 11:53:23,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1348029e-18 1.0000000e+00 9.2432395e-22 2.0142531e-23 3.6426979e-30], sum to 1.0000
[2019-04-10 11:53:23,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-10 11:53:23,390] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.36666666666667, 99.0, 1.0, 2.0, 0.4450125520757766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639576.0243611212, 639576.0243611212, 177710.6118858715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1658400.0000, 
sim time next is 1659000.0000, 
raw observation next is [23.38333333333333, 99.0, 1.0, 2.0, 0.4489660344023746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644802.908381624, 644802.9083816233, 178227.8249370781], 
processed observation next is [1.0, 0.17391304347826086, 0.30726698262243274, 0.99, 1.0, 1.0, 0.3361036559064754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17911191899489556, 0.17911191899489537, 0.26601167901056433], 
reward next is 0.7340, 
noisyNet noise sample is [array([-0.23869707], dtype=float32), -0.47557124]. 
=============================================
[2019-04-10 11:53:23,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.94186]
 [72.00817]
 [72.03757]
 [72.00551]
 [72.06136]], R is [[71.9220047 ]
 [71.93754578]
 [71.95491791]
 [71.97312164]
 [71.98756409]].
[2019-04-10 11:53:27,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5368796e-19 1.0000000e+00 8.6264622e-23 1.9098386e-22 1.5788240e-30], sum to 1.0000
[2019-04-10 11:53:27,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9694
[2019-04-10 11:53:27,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 94.0, 1.0, 2.0, 0.503261366934298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703231.0780435816, 703231.0780435816, 184108.6900725193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [25.36666666666667, 94.0, 1.0, 2.0, 0.5013646783190098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700579.8707358699, 700579.8707358692, 183810.1755656344], 
processed observation next is [1.0, 1.0, 0.40126382306477115, 0.94, 1.0, 1.0, 0.399234552191578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19460551964885273, 0.19460551964885253, 0.27434354562034985], 
reward next is 0.7257, 
noisyNet noise sample is [array([-0.32266277], dtype=float32), 0.6793541]. 
=============================================
[2019-04-10 11:53:30,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3941179e-19 1.0000000e+00 3.9226806e-23 4.0119817e-24 9.1732564e-32], sum to 1.0000
[2019-04-10 11:53:30,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7410
[2019-04-10 11:53:30,433] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 90.0, 1.0, 2.0, 0.3193609982216318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504755.2680794219, 504755.2680794225, 167377.8087205701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1798200.0000, 
sim time next is 1798800.0000, 
raw observation next is [21.43333333333333, 90.33333333333334, 1.0, 2.0, 0.3184244464076827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 503569.4127113277, 503569.4127113271, 167293.9681354432], 
processed observation next is [1.0, 0.8260869565217391, 0.21484992101105835, 0.9033333333333334, 1.0, 1.0, 0.1788246342261237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13988039241981326, 0.13988039241981307, 0.24969248975439284], 
reward next is 0.7503, 
noisyNet noise sample is [array([1.3941381], dtype=float32), -0.75337875]. 
=============================================
[2019-04-10 11:53:31,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6731032e-19 1.0000000e+00 1.3123862e-22 1.3379835e-24 4.7978695e-31], sum to 1.0000
[2019-04-10 11:53:31,868] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4618
[2019-04-10 11:53:31,871] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.81666666666667, 97.66666666666667, 1.0, 2.0, 0.3535136926087159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540522.7813059741, 540522.7813059747, 169734.2354439703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1831800.0000, 
sim time next is 1832400.0000, 
raw observation next is [21.8, 98.0, 1.0, 2.0, 0.3550032613765913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542418.572054986, 542418.5720549854, 169879.2927229101], 
processed observation next is [1.0, 0.21739130434782608, 0.23222748815165886, 0.98, 1.0, 1.0, 0.2228954956344473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15067182557082945, 0.15067182557082928, 0.25355118316852254], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.59345126], dtype=float32), 0.44180974]. 
=============================================
[2019-04-10 11:53:33,497] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8712577e-13 1.0000000e+00 1.7215176e-13 2.4384555e-08 2.7424805e-20], sum to 1.0000
[2019-04-10 11:53:33,503] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8263
[2019-04-10 11:53:33,509] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1840330.622589392 W.
[2019-04-10 11:53:33,512] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 85.83333333333334, 1.0, 2.0, 0.4387744523079462, 1.0, 2.0, 0.4387744523079462, 1.0, 1.0, 0.7511227827204978, 6.9112, 6.9112, 170.5573041426782, 1840330.622589392, 1840330.622589392, 373683.959799514], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1871400.0000, 
sim time next is 1872000.0000, 
raw observation next is [27.0, 86.0, 1.0, 2.0, 0.6435550860425423, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98128012513933, 6.9112, 168.9125388169936, 1796145.320921364, 1746428.191118197, 373777.7216172992], 
processed observation next is [1.0, 0.6956521739130435, 0.4786729857819906, 0.86, 1.0, 1.0, 0.5705482964367979, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007008012513932993, 0.0, 0.8294378940863041, 0.49892925581149, 0.48511894197727695, 0.5578771964437301], 
reward next is 0.0917, 
noisyNet noise sample is [array([0.60343504], dtype=float32), -0.26445]. 
=============================================
[2019-04-10 11:53:33,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[56.845825]
 [58.78955 ]
 [58.979576]
 [56.244854]
 [54.691837]], R is [[56.71367645]
 [56.58880615]
 [56.0229187 ]
 [55.46268845]
 [55.42916107]].
[2019-04-10 11:53:35,675] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-10 11:53:35,677] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:53:35,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:53:35,678] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:53:35,678] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:53:35,679] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:53:35,679] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:53:35,680] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:53:35,680] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:53:35,680] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:53:35,682] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:53:35,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run20
[2019-04-10 11:53:35,705] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run20
[2019-04-10 11:53:35,726] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run20
[2019-04-10 11:53:35,727] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run20
[2019-04-10 11:53:35,762] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run20
[2019-04-10 11:54:02,891] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12239023], dtype=float32), 0.09147315]
[2019-04-10 11:54:02,892] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.58333333333334, 92.33333333333333, 1.0, 2.0, 0.5114828058663049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714723.1510731346, 714723.1510731346, 185414.1169959091]
[2019-04-10 11:54:02,892] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:54:02,895] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3268306e-19 1.0000000e+00 6.1371315e-23 2.3885722e-25 8.3998898e-32], sampled 0.9241387920999349
[2019-04-10 11:54:24,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12239023], dtype=float32), 0.09147315]
[2019-04-10 11:54:24,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.69178108, 48.62719151, 1.0, 2.0, 0.5207641693896305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727696.9548851614, 727696.9548851607, 186912.4218061659]
[2019-04-10 11:54:24,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:54:24,268] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6529308e-19 1.0000000e+00 4.5646452e-23 6.0199613e-25 5.2237341e-32], sampled 0.25545536182022766
[2019-04-10 11:54:27,547] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12239023], dtype=float32), 0.09147315]
[2019-04-10 11:54:27,548] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.67717991333333, 66.35298315, 1.0, 2.0, 0.506849759108969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708246.980924485, 708246.980924485, 184675.8964473616]
[2019-04-10 11:54:27,548] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:54:27,551] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6482063e-19 1.0000000e+00 1.5294729e-22 2.2655151e-23 1.8286257e-31], sampled 0.18387496614239196
[2019-04-10 11:54:28,683] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12239023], dtype=float32), 0.09147315]
[2019-04-10 11:54:28,686] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.765192825, 62.48113354333333, 1.0, 2.0, 0.6827670115606735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 954175.5328197844, 954175.5328197844, 217170.353274387]
[2019-04-10 11:54:28,688] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:54:28,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.6142785e-19 1.0000000e+00 3.3435702e-22 3.2023802e-23 5.1649073e-31], sampled 0.1487384359643965
[2019-04-10 11:54:36,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12239023], dtype=float32), 0.09147315]
[2019-04-10 11:54:36,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.5, 83.83333333333333, 1.0, 2.0, 0.5586093919426904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780599.9272565171, 780599.9272565171, 193288.5829771732]
[2019-04-10 11:54:36,970] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:54:36,973] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2284710e-19 1.0000000e+00 4.2995696e-23 3.5376992e-24 4.3350571e-32], sampled 0.41398335564794764
[2019-04-10 11:55:02,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12239023], dtype=float32), 0.09147315]
[2019-04-10 11:55:02,619] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.1, 68.0, 1.0, 2.0, 0.5894892426514151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823768.1175480718, 823768.1175480718, 198804.4405053689]
[2019-04-10 11:55:02,620] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 11:55:02,622] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9265552e-17 1.0000000e+00 1.2548815e-19 7.0056614e-18 1.9242977e-28], sampled 0.6477437800686533
[2019-04-10 11:55:15,453] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.5538 2927477542.9685 1336.0000
[2019-04-10 11:55:15,493] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7480 2779304950.4293 932.0000
[2019-04-10 11:55:15,652] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.4435 3163312908.7845 1759.0000
[2019-04-10 11:55:15,696] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.6299 2842040502.9381 1120.0000
[2019-04-10 11:55:15,973] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.7571 3007347568.4707 1760.0000
[2019-04-10 11:55:16,998] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 475000, evaluation results [475000.0, 7890.443513448225, 3163312908.7844887, 1759.0, 8255.553761323132, 2927477542.9684663, 1336.0, 8660.748017614782, 2779304950.429296, 932.0, 8001.757084891067, 3007347568.4707236, 1760.0, 8500.629937769052, 2842040502.9381204, 1120.0]
[2019-04-10 11:55:31,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4173583e-19 1.0000000e+00 2.5765651e-22 3.8521249e-23 7.6272421e-31], sum to 1.0000
[2019-04-10 11:55:31,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3616
[2019-04-10 11:55:31,959] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.03333333333333, 80.33333333333333, 1.0, 2.0, 0.5604960811354082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783237.3559466929, 783237.3559466923, 193617.3095072526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2140800.0000, 
sim time next is 2141400.0000, 
raw observation next is [28.86666666666667, 81.16666666666667, 1.0, 2.0, 0.5592848233189696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781544.1212854272, 781544.1212854272, 193406.0303552382], 
processed observation next is [0.0, 0.782608695652174, 0.567140600315956, 0.8116666666666668, 1.0, 1.0, 0.4690178594204452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21709558924595201, 0.21709558924595201, 0.2886657169481167], 
reward next is 0.7113, 
noisyNet noise sample is [array([0.4302707], dtype=float32), 0.691446]. 
=============================================
[2019-04-10 11:55:46,852] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.2109415e-19 1.0000000e+00 9.7038454e-22 1.4661923e-20 2.5102541e-29], sum to 1.0000
[2019-04-10 11:55:46,859] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5683
[2019-04-10 11:55:46,865] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 77.66666666666666, 1.0, 2.0, 0.5762160637791944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 805212.7898215837, 805212.7898215842, 196398.7064164983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2411400.0000, 
sim time next is 2412000.0000, 
raw observation next is [29.8, 78.0, 1.0, 2.0, 0.5741745914193467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802358.9288674291, 802358.9288674291, 196033.3951373182], 
processed observation next is [1.0, 0.9565217391304348, 0.6113744075829385, 0.78, 1.0, 1.0, 0.4869573390594538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287748024095255, 0.22287748024095255, 0.29258715692137044], 
reward next is 0.7074, 
noisyNet noise sample is [array([2.0321162], dtype=float32), 0.23641266]. 
=============================================
[2019-04-10 11:55:46,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[69.779854]
 [69.95402 ]
 [70.02786 ]
 [70.04258 ]
 [70.043365]], R is [[69.57814789]
 [69.58924103]
 [69.59973145]
 [69.60977173]
 [69.61936188]].
[2019-04-10 11:55:47,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6272679e-10 9.9983811e-01 5.6181695e-11 1.6181744e-04 3.6694238e-17], sum to 1.0000
[2019-04-10 11:55:47,648] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0754
[2019-04-10 11:55:47,654] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1952655.539249168 W.
[2019-04-10 11:55:47,657] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.86666666666667, 87.0, 1.0, 2.0, 0.4655307497570134, 1.0, 2.0, 0.4655307497570134, 1.0, 2.0, 0.7978837833258637, 6.9112, 6.9112, 170.5573041426782, 1952655.539249168, 1952655.539249168, 390353.0672678789], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2472000.0000, 
sim time next is 2472600.0000, 
raw observation next is [26.98333333333333, 86.5, 1.0, 2.0, 0.4776911978809369, 1.0, 2.0, 0.4776911978809369, 1.0, 2.0, 0.8195788649885682, 6.911200000000001, 6.9112, 170.5573041426782, 2003709.892955753, 2003709.892955753, 398334.8431232941], 
processed observation next is [1.0, 0.6086956521739131, 0.4778830963665086, 0.865, 1.0, 1.0, 0.37071228660353844, 1.0, 1.0, 0.37071228660353844, 1.0, 1.0, 0.779974225595815, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.556586081376598, 0.556586081376598, 0.5945296166019315], 
reward next is 0.4055, 
noisyNet noise sample is [array([0.52543116], dtype=float32), 0.15091634]. 
=============================================
[2019-04-10 11:55:47,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1044754e-11 9.9992359e-01 2.9011546e-12 7.6435950e-05 1.4323877e-18], sum to 1.0000
[2019-04-10 11:55:47,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3393
[2019-04-10 11:55:47,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1933902.331820906 W.
[2019-04-10 11:55:47,865] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.95, 89.0, 1.0, 2.0, 0.6915957704275622, 1.0, 2.0, 0.6915957704275622, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1933902.331820906, 1933902.331820906, 370538.533082462], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2463000.0000, 
sim time next is 2463600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.6026956003441257, 0.0, 1.0, 0.0, 1.0, 1.0, 1.01585036472582, 6.911200000000001, 6.9112, 168.9129565103873, 1685129.099266576, 1685129.099266575, 362505.563561798], 
processed observation next is [1.0, 0.5217391304347826, 0.4312796208530806, 0.89, 1.0, 1.0, 0.5213200004146092, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.019329713080268, 8.881784197001253e-17, 0.0, 0.8294399451520881, 0.46809141646293784, 0.4680914164629375, 0.5410530799429821], 
reward next is 0.4589, 
noisyNet noise sample is [array([-0.6779649], dtype=float32), -0.35721514]. 
=============================================
[2019-04-10 11:55:52,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9161270e-18 1.0000000e+00 3.5590916e-21 7.1116861e-21 4.9310644e-29], sum to 1.0000
[2019-04-10 11:55:52,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0518
[2019-04-10 11:55:52,346] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 91.66666666666666, 1.0, 2.0, 0.4971431653930173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694679.0281956465, 694679.0281956465, 183149.2888863903], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2590800.0000, 
sim time next is 2591400.0000, 
raw observation next is [25.3, 91.83333333333333, 1.0, 2.0, 0.4940800854852237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690397.4670792061, 690397.4670792067, 182673.3494316642], 
processed observation next is [1.0, 1.0, 0.39810426540284366, 0.9183333333333333, 1.0, 1.0, 0.3904579343195467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19177707418866838, 0.19177707418866854, 0.2726467901965137], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.51465595], dtype=float32), 0.045661807]. 
=============================================
[2019-04-10 11:55:55,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0395820e-17 1.0000000e+00 5.9429429e-21 1.9268985e-20 2.4011562e-29], sum to 1.0000
[2019-04-10 11:55:55,073] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4641
[2019-04-10 11:55:55,077] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 92.5, 1.0, 2.0, 0.4865391851448324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679856.8996106591, 679856.8996106591, 181513.3265422555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2593800.0000, 
sim time next is 2594400.0000, 
raw observation next is [24.86666666666667, 92.66666666666667, 1.0, 2.0, 0.4839522424962031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676240.9306145759, 676240.9306145759, 181119.3198428449], 
processed observation next is [0.0, 0.0, 0.3775671406003162, 0.9266666666666667, 1.0, 1.0, 0.37825571385084716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1878447029484933, 0.1878447029484933, 0.27032734304902223], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.80397016], dtype=float32), -0.9156845]. 
=============================================
[2019-04-10 11:56:05,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.7539949e-20 1.0000000e+00 9.1045851e-23 3.8720730e-23 8.6749984e-32], sum to 1.0000
[2019-04-10 11:56:05,269] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4731
[2019-04-10 11:56:05,273] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4137717687635458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609653.2496629966, 609653.2496629966, 175216.5964095425], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4094262076607182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603250.7644546658, 603250.7644546658, 174616.0758322382], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.2884653104346002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16756965679296273, 0.16756965679296273, 0.2606210087048331], 
reward next is 0.7394, 
noisyNet noise sample is [array([1.6530982], dtype=float32), -0.583481]. 
=============================================
[2019-04-10 11:56:05,428] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 11:56:05,431] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:56:05,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:56:05,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:56:05,431] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:56:05,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:56:05,432] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:56:05,435] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:56:05,434] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:56:05,437] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:56:05,437] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:56:05,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run21
[2019-04-10 11:56:05,441] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run21
[2019-04-10 11:56:05,473] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run21
[2019-04-10 11:56:05,498] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run21
[2019-04-10 11:56:05,499] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run21
[2019-04-10 11:56:10,397] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12230331], dtype=float32), 0.09178183]
[2019-04-10 11:56:10,400] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.1724961, 88.78063011, 1.0, 2.0, 0.2443393593855399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403152.6494055923, 403152.6494055917, 160317.1436066547]
[2019-04-10 11:56:10,402] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:56:10,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2205531e-20 1.0000000e+00 4.4411469e-24 1.7263127e-27 3.2892543e-33], sampled 0.021077610153024007
[2019-04-10 11:57:00,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12230331], dtype=float32), 0.09178183]
[2019-04-10 11:57:00,312] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.7526746236852463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051920.691221194, 1051920.691221194, 232622.368919132]
[2019-04-10 11:57:00,315] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:57:00,318] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5051405e-18 1.0000000e+00 1.7123218e-21 1.3014183e-22 8.1344450e-30], sampled 0.740764349008125
[2019-04-10 11:57:35,773] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12230331], dtype=float32), 0.09178183]
[2019-04-10 11:57:35,775] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.6, 90.0, 1.0, 2.0, 0.5236086750914937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731673.1328747467, 731673.1328747467, 187378.0084010936]
[2019-04-10 11:57:35,776] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:57:35,777] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6114411e-19 1.0000000e+00 4.2477440e-23 1.0923338e-25 5.5380695e-32], sampled 0.9556169704135701
[2019-04-10 11:57:48,752] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12230331], dtype=float32), 0.09178183]
[2019-04-10 11:57:48,756] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.076760155, 46.82960671166667, 1.0, 2.0, 0.3343427978663255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527057.6722147816, 527057.6722147816, 169074.2814848429]
[2019-04-10 11:57:48,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:57:48,764] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1718092e-20 1.0000000e+00 6.4522732e-24 9.8210533e-27 4.9292490e-33], sampled 0.12408401982270778
[2019-04-10 11:57:52,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12230331], dtype=float32), 0.09178183]
[2019-04-10 11:57:52,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.67160017666667, 66.24939113, 1.0, 2.0, 0.4543572346766204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 647230.3182007769, 647230.3182007762, 178345.0233404976]
[2019-04-10 11:57:52,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:57:52,570] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5430466e-19 1.0000000e+00 1.1517619e-22 9.2577340e-24 1.8913275e-31], sampled 0.46727627779612146
[2019-04-10 11:57:59,883] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12230331], dtype=float32), 0.09178183]
[2019-04-10 11:57:59,889] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.80778456, 85.51811543, 1.0, 2.0, 0.4234351894711123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670769.9649180229, 670769.9649180229, 181638.0865945697]
[2019-04-10 11:57:59,890] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 11:57:59,891] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2673268e-19 1.0000000e+00 3.4049392e-23 2.2685655e-25 4.5367969e-32], sampled 0.33055444586493654
[2019-04-10 11:58:10,322] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2906 2927366065.3023 1338.0000
[2019-04-10 11:58:10,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.0365 3163122863.5050 1756.0000
[2019-04-10 11:58:12,145] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.8908 3007353319.9822 1760.0000
[2019-04-10 11:58:12,282] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4920 2779501990.1428 934.0000
[2019-04-10 11:58:12,373] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6133 2842053051.1783 1123.0000
[2019-04-10 11:58:13,398] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 500000, evaluation results [500000.0, 7889.036478008502, 3163122863.5050287, 1756.0, 8254.290601694724, 2927366065.30227, 1338.0, 8659.492021980814, 2779501990.142769, 934.0, 7999.890834189696, 3007353319.9821677, 1760.0, 8498.613282290655, 2842053051.1782756, 1123.0]
[2019-04-10 11:58:13,928] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4885139e-18 1.0000000e+00 6.3497058e-21 6.3512047e-22 1.4107917e-29], sum to 1.0000
[2019-04-10 11:58:13,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3897
[2019-04-10 11:58:14,022] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 94.0, 1.0, 2.0, 0.7058009927329775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079800.944073781, 1079800.944073781, 233956.5914283535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2901000.0000, 
sim time next is 2901600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6864237728453633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1055366.531807516, 1055366.531807515, 229960.76234144], 
processed observation next is [1.0, 0.6086956521739131, 0.2417061611374408, 0.94, 1.0, 1.0, 0.6221973166811605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29315736994653224, 0.29315736994653196, 0.3432250184200597], 
reward next is 0.6568, 
noisyNet noise sample is [array([1.1912307], dtype=float32), -0.12962477]. 
=============================================
[2019-04-10 11:58:25,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2484148e-20 1.0000000e+00 2.0898868e-23 5.6280704e-25 2.0377772e-32], sum to 1.0000
[2019-04-10 11:58:25,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3444
[2019-04-10 11:58:25,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3134946486893489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 496952.0609642697, 496952.0609642704, 166820.8496323185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2928000.0000, 
sim time next is 2928600.0000, 
raw observation next is [20.75, 95.5, 1.0, 2.0, 0.3147153254597316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498600.5081672747, 498600.5081672741, 166938.3623574729], 
processed observation next is [1.0, 0.9130434782608695, 0.18246445497630337, 0.955, 1.0, 1.0, 0.1743558138069055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1385001411575763, 0.13850014115757614, 0.24916173486189988], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.987504], dtype=float32), 1.331257]. 
=============================================
[2019-04-10 11:58:25,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3722058e-19 1.0000000e+00 2.1597254e-23 3.1871692e-24 7.8438740e-33], sum to 1.0000
[2019-04-10 11:58:25,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5067
[2019-04-10 11:58:25,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3186198199692449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504778.9420151702, 504778.9420151702, 167401.0737023378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [20.5, 97.0, 1.0, 2.0, 0.3158403466178733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500954.1734709924, 500954.1734709924, 167123.6546730745], 
processed observation next is [1.0, 0.8260869565217391, 0.1706161137440759, 0.97, 1.0, 1.0, 0.17571126098538953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13915393707527568, 0.13915393707527568, 0.2494382905568276], 
reward next is 0.7506, 
noisyNet noise sample is [array([-1.4274709], dtype=float32), -0.5530305]. 
=============================================
[2019-04-10 11:58:27,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.9250559e-21 1.0000000e+00 3.4491387e-23 1.1758035e-25 1.6450256e-32], sum to 1.0000
[2019-04-10 11:58:27,846] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3891
[2019-04-10 11:58:27,850] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3387240040536023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526098.6374142431, 526098.6374142431, 168816.9948964921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2965800.0000, 
sim time next is 2966400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3381740776411795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 525244.7149331778, 525244.7149331785, 168748.9619089661], 
processed observation next is [1.0, 0.34782608695652173, 0.19431279620853087, 1.0, 1.0, 1.0, 0.2026193706520235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1459013097036605, 0.1459013097036607, 0.2518641222521882], 
reward next is 0.7481, 
noisyNet noise sample is [array([2.351467], dtype=float32), 0.010177626]. 
=============================================
[2019-04-10 11:58:28,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2322198e-21 1.0000000e+00 2.1016059e-24 2.0326294e-25 6.4261080e-33], sum to 1.0000
[2019-04-10 11:58:28,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7369
[2019-04-10 11:58:28,280] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3030254175235287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482550.5332816411, 482550.5332816406, 165804.8657590378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3022800.0000, 
sim time next is 3023400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3028285612138949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482237.5283235412, 482237.5283235412, 165782.3666037986], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1600344111010782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13395486897876144, 0.13395486897876144, 0.24743636806537106], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.15808786], dtype=float32), 1.0315968]. 
=============================================
[2019-04-10 11:58:29,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8329250e-19 1.0000000e+00 1.9971300e-23 6.9107155e-26 8.2383280e-33], sum to 1.0000
[2019-04-10 11:58:29,725] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4151
[2019-04-10 11:58:29,729] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3076281445635441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489877.7509462302, 489877.7509462296, 166335.5080246674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3006600.0000, 
sim time next is 3007200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3066347481513457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488296.1033477023, 488296.1033477029, 166220.340476851], 
processed observation next is [1.0, 0.8260869565217391, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16462017849559726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13563780648547286, 0.13563780648547302, 0.24809006041321047], 
reward next is 0.7519, 
noisyNet noise sample is [array([2.2338147], dtype=float32), 0.8184358]. 
=============================================
[2019-04-10 11:58:31,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2517502e-20 1.0000000e+00 5.6590960e-24 5.2673722e-26 1.4008096e-32], sum to 1.0000
[2019-04-10 11:58:31,754] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1674
[2019-04-10 11:58:31,760] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.3374536420638731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522158.545047688, 522158.5450476874, 168445.3945084656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3051000.0000, 
sim time next is 3051600.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.3408426909329262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526683.8322111608, 526683.8322111614, 168784.6009892069], 
processed observation next is [1.0, 0.30434782608695654, 0.22590837282780438, 0.96, 1.0, 1.0, 0.2058345673890677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1463010645031002, 0.14630106450310038, 0.251917314909264], 
reward next is 0.7481, 
noisyNet noise sample is [array([0.01404318], dtype=float32), 0.93900335]. 
=============================================
[2019-04-10 11:58:32,934] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.4196252e-20 1.0000000e+00 5.9952627e-23 2.2413077e-24 6.0746351e-32], sum to 1.0000
[2019-04-10 11:58:32,939] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2019
[2019-04-10 11:58:32,942] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.4113425000650328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607420.662355208, 607420.662355208, 175045.2776978148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3094800.0000, 
sim time next is 3095400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.4068317331916693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603477.5571900024, 603477.557190003, 174755.8674228257], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.95, 1.0, 1.0, 0.2853394375803245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16763265477500067, 0.16763265477500083, 0.2608296528698891], 
reward next is 0.7392, 
noisyNet noise sample is [array([-0.65127474], dtype=float32), 0.91265476]. 
=============================================
[2019-04-10 11:58:33,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1675096e-15 1.0000000e+00 3.1256780e-17 5.2557602e-15 4.5005289e-25], sum to 1.0000
[2019-04-10 11:58:33,680] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0392
[2019-04-10 11:58:33,685] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.5205376901517936, 1.0, 1.0, 0.5205376901517936, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1455249.551463034, 1455249.551463034, 306744.9676126415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3161400.0000, 
sim time next is 3162000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.9487891279823295, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956444115, 1326176.966682256, 1326176.966682256, 283706.7571881959], 
processed observation next is [1.0, 0.6086956521739131, 0.4312796208530806, 0.84, 1.0, 1.0, 0.9383001541955777, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399448266607, 0.3683824907450711, 0.3683824907450711, 0.42344292117641175], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0857306], dtype=float32), -1.2820033]. 
=============================================
[2019-04-10 11:58:33,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.022984]
 [61.822533]
 [61.47284 ]
 [66.05229 ]
 [66.99537 ]], R is [[61.98165894]
 [61.36184311]
 [60.74822617]
 [60.62657928]
 [60.02031326]].
[2019-04-10 11:58:35,279] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2085820e-18 1.0000000e+00 5.8407903e-22 1.7579631e-22 3.6425154e-31], sum to 1.0000
[2019-04-10 11:58:35,285] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4234
[2019-04-10 11:58:35,289] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4863212203183043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679552.2329093028, 679552.2329093022, 181480.6389402947], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3198000.0000, 
sim time next is 3198600.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4862664657376061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679475.6981089027, 679475.6981089032, 181472.2854978997], 
processed observation next is [0.0, 0.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3810439346236218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18874324947469517, 0.18874324947469534, 0.27085415745955177], 
reward next is 0.7291, 
noisyNet noise sample is [array([-0.717715], dtype=float32), 1.9871386]. 
=============================================
[2019-04-10 11:58:37,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0864873e-20 1.0000000e+00 1.2826543e-23 2.1348957e-25 1.3962429e-32], sum to 1.0000
[2019-04-10 11:58:37,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2422
[2019-04-10 11:58:37,092] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.00000000000001, 1.0, 2.0, 0.5573041287044418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778775.2857123173, 778775.285712318, 193062.1271549221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3244800.0000, 
sim time next is 3245400.0000, 
raw observation next is [32.5, 63.0, 1.0, 2.0, 0.5598152607579185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782285.6270513296, 782285.6270513296, 193499.3783222623], 
processed observation next is [0.0, 0.5652173913043478, 0.7393364928909952, 0.63, 1.0, 1.0, 0.469656940672191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21730156306981377, 0.21730156306981377, 0.28880504227203324], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.804715], dtype=float32), 0.3098262]. 
=============================================
[2019-04-10 11:58:38,067] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1619666e-19 1.0000000e+00 2.7598980e-23 2.2787916e-25 2.9854456e-32], sum to 1.0000
[2019-04-10 11:58:38,079] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4302
[2019-04-10 11:58:38,085] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4563138439760834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646089.1724248632, 646089.1724248632, 178129.0389977989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3217800.0000, 
sim time next is 3218400.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4559155698054902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645525.3728558918, 645525.3728558918, 178071.0172771911], 
processed observation next is [0.0, 0.2608695652173913, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3444765901270966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17931260357108106, 0.17931260357108106, 0.2657776377271509], 
reward next is 0.7342, 
noisyNet noise sample is [array([0.7159187], dtype=float32), 0.83437634]. 
=============================================
[2019-04-10 11:58:41,094] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1585763e-19 1.0000000e+00 6.5438921e-23 1.3205858e-24 5.4599943e-32], sum to 1.0000
[2019-04-10 11:58:41,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1052
[2019-04-10 11:58:41,107] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 80.66666666666667, 1.0, 2.0, 0.530733519724695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741632.6264427055, 741632.6264427049, 188551.2806606337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [28.0, 79.83333333333334, 1.0, 2.0, 0.5266959904319644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735988.7379402305, 735988.7379402305, 187884.2469297457], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.7983333333333335, 1.0, 1.0, 0.4297542053397161, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20444131609450847, 0.20444131609450847, 0.2804242491488742], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.3253152], dtype=float32), -0.61740977]. 
=============================================
[2019-04-10 11:58:44,690] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2954938e-19 1.0000000e+00 6.5132633e-22 3.5273382e-23 2.1828514e-30], sum to 1.0000
[2019-04-10 11:58:44,694] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2464
[2019-04-10 11:58:44,698] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5388179412332851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752933.5816124796, 752933.5816124802, 189901.3645148258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3369000.0000, 
sim time next is 3369600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5379667811787028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751743.7664364893, 751743.7664364887, 189758.3783296175], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4433334712996419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20881771289902482, 0.20881771289902465, 0.28322146019345895], 
reward next is 0.7168, 
noisyNet noise sample is [array([1.8819647], dtype=float32), -0.021984335]. 
=============================================
[2019-04-10 11:58:45,991] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0149203e-14 1.0000000e+00 1.3497651e-15 9.1573147e-13 1.8189983e-22], sum to 1.0000
[2019-04-10 11:58:46,000] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8509
[2019-04-10 11:58:46,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3109412.383087268 W.
[2019-04-10 11:58:46,010] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 82.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 9.243685614816242, 6.9112, 168.8994610517292, 3109412.383087268, 1454799.017423095, 308626.6585054461], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3399000.0000, 
sim time next is 3399600.0000, 
raw observation next is [29.33333333333334, 81.0, 1.0, 2.0, 0.9318121789493611, 1.0, 1.0, 0.9318121789493611, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2606365.934633783, 2606365.934633783, 489145.2145607366], 
processed observation next is [1.0, 0.34782608695652173, 0.5892575039494474, 0.81, 1.0, 1.0, 0.9178459987341699, 1.0, 0.5, 0.9178459987341699, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7239905373982731, 0.7239905373982731, 0.7300674844190098], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9867779], dtype=float32), -0.4744439]. 
=============================================
[2019-04-10 11:58:46,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6850598e-18 1.0000000e+00 3.6882012e-20 7.2477236e-20 2.2452653e-28], sum to 1.0000
[2019-04-10 11:58:46,501] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2005
[2019-04-10 11:58:46,504] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5079801494223136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709827.0606249967, 709827.060624996, 184855.9818521896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3459000.0000, 
sim time next is 3459600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5078598009336115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709658.8352535367, 709658.8352535367, 184836.8505617697], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4070600011248331, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19712745423709352, 0.19712745423709352, 0.2758758963608503], 
reward next is 0.7241, 
noisyNet noise sample is [array([-1.0454843], dtype=float32), 0.0010071453]. 
=============================================
[2019-04-10 11:58:47,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3934761e-19 1.0000000e+00 5.5073788e-23 7.7647905e-21 1.6379819e-31], sum to 1.0000
[2019-04-10 11:58:47,874] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6968
[2019-04-10 11:58:47,880] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 72.66666666666666, 1.0, 2.0, 0.5334975255055133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745496.3285065243, 745496.3285065236, 189011.3577720588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [29.33333333333333, 73.33333333333334, 1.0, 2.0, 0.5277161059006845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737414.7106371127, 737414.7106371133, 188052.9137473982], 
processed observation next is [1.0, 0.782608695652174, 0.5892575039494469, 0.7333333333333334, 1.0, 1.0, 0.4309832601213066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048374196214202, 0.20483741962142035, 0.2806759906677585], 
reward next is 0.7193, 
noisyNet noise sample is [array([0.8356857], dtype=float32), 0.22729698]. 
=============================================
[2019-04-10 11:58:52,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1044387e-17 1.0000000e+00 3.2926154e-20 1.2592596e-19 1.2794850e-27], sum to 1.0000
[2019-04-10 11:58:52,863] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2082
[2019-04-10 11:58:52,869] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 82.5, 1.0, 2.0, 0.6740105933406464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 941932.9008730919, 941932.9008730913, 215331.5916166144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [26.16666666666666, 83.0, 1.0, 2.0, 0.6581222760653266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 919719.2901947432, 919719.2901947432, 212061.7720320993], 
processed observation next is [1.0, 0.17391304347826086, 0.4391785150078987, 0.83, 1.0, 1.0, 0.5880991277895501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25547758060965087, 0.25547758060965087, 0.316510107510596], 
reward next is 0.6835, 
noisyNet noise sample is [array([0.926422], dtype=float32), 0.72760713]. 
=============================================
[2019-04-10 11:58:53,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0801257e-17 1.0000000e+00 4.2025268e-19 1.1817252e-18 5.9333880e-27], sum to 1.0000
[2019-04-10 11:58:53,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7912
[2019-04-10 11:58:53,701] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.7900727741863117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1104214.658699758, 1104214.658699759, 241487.7834728067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3570600.0000, 
sim time next is 3571200.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.8066907044493996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1127452.396411852, 1127452.396411851, 245558.9500192037], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.74, 1.0, 1.0, 0.7670972342763851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31318122122551445, 0.3131812212255142, 0.3665058955510503], 
reward next is 0.6335, 
noisyNet noise sample is [array([0.0828919], dtype=float32), 0.13932021]. 
=============================================
[2019-04-10 11:59:00,177] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.5042338e-12 1.0000000e+00 3.2714993e-13 3.8086370e-10 1.4134707e-19], sum to 1.0000
[2019-04-10 11:59:00,183] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7230
[2019-04-10 11:59:00,191] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2088521.55272127 W.
[2019-04-10 11:59:00,196] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.8524784363192753, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.969994472172504, 6.9112, 168.912557135727, 2088521.55272127, 2046810.829232838, 422419.6750745567], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3747000.0000, 
sim time next is 3747600.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.8089966406410334, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.969935009459197, 6.9112, 168.9125587959448, 2027664.636839348, 1985996.097736989, 411276.2215164255], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.7, 1.0, 1.0, 0.7698754706518475, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00587350094591974, 0.0, 0.8294379921921009, 0.5632401768998189, 0.5516655827047191, 0.6138451067409335], 
reward next is 0.0925, 
noisyNet noise sample is [array([0.22868682], dtype=float32), 1.1837]. 
=============================================
[2019-04-10 11:59:02,497] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 11:59:02,499] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 11:59:02,499] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 11:59:02,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:59:02,500] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 11:59:02,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 11:59:02,501] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:59:02,501] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 11:59:02,505] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:59:02,507] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:59:02,504] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 11:59:02,516] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run22
[2019-04-10 11:59:02,517] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run22
[2019-04-10 11:59:02,530] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run22
[2019-04-10 11:59:02,546] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run22
[2019-04-10 11:59:02,584] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run22
[2019-04-10 11:59:16,969] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11795065], dtype=float32), 0.09648255]
[2019-04-10 11:59:16,970] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.62534986666667, 89.62624635333334, 1.0, 2.0, 0.3956311696366838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590916.5662461667, 590916.5662461667, 173719.1756014882]
[2019-04-10 11:59:16,971] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:59:16,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.6548728e-19 1.0000000e+00 1.6381019e-22 9.2580469e-25 8.7626418e-31], sampled 0.8187594351262775
[2019-04-10 11:59:17,413] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11795065], dtype=float32), 0.09648255]
[2019-04-10 11:59:17,414] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.51713955833333, 88.47497448, 1.0, 2.0, 0.2844728348691868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461361.2145576761, 461361.2145576761, 164343.2189698937]
[2019-04-10 11:59:17,414] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 11:59:17,417] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2482666e-19 1.0000000e+00 3.6638241e-23 1.9427401e-25 1.3558435e-31], sampled 0.07326368606675038
[2019-04-10 11:59:40,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11795065], dtype=float32), 0.09648255]
[2019-04-10 11:59:40,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 79.0, 1.0, 2.0, 0.5512261777908358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770278.8939225607, 770278.8939225607, 192011.4572317818]
[2019-04-10 11:59:40,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 11:59:40,944] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.8359509e-20 1.0000000e+00 6.2188347e-24 1.1655189e-25 1.5161162e-32], sampled 0.2091360066333814
[2019-04-10 12:00:04,067] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11795065], dtype=float32), 0.09648255]
[2019-04-10 12:00:04,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666667, 85.00000000000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.865161628907547, 6.9112, 168.9020196626558, 2840759.071422057, 1454641.196465623, 309623.9403961797]
[2019-04-10 12:00:04,073] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:00:04,078] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.8295196e-17 1.0000000e+00 6.0194715e-20 6.3260641e-20 2.7478849e-27], sampled 0.4253010124667894
[2019-04-10 12:00:04,081] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2840759.071422057 W.
[2019-04-10 12:00:36,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11795065], dtype=float32), 0.09648255]
[2019-04-10 12:00:36,883] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.22624358, 67.66206256999999, 1.0, 2.0, 0.6781721097693901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031966.568355469, 1031966.568355468, 226837.8320159024]
[2019-04-10 12:00:36,883] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:00:36,893] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7640202e-17 1.0000000e+00 1.3987571e-20 1.8696019e-21 2.6156551e-28], sampled 0.8840314000485754
[2019-04-10 12:00:50,628] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8503.4459 2841534843.8877 1111.0000
[2019-04-10 12:00:51,136] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11795065], dtype=float32), 0.09648255]
[2019-04-10 12:00:51,137] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.95, 65.5, 1.0, 2.0, 0.3384120623817516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535733.3611210104, 535733.3611210104, 169794.5165134196]
[2019-04-10 12:00:51,152] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:00:51,155] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1451707e-19 1.0000000e+00 6.5357094e-23 6.9430616e-25 2.4519331e-31], sampled 0.7283725388034318
[2019-04-10 12:00:51,474] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.5395 2927064849.0996 1327.0000
[2019-04-10 12:00:51,813] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8010.7419 3005977106.3181 1725.0000
[2019-04-10 12:00:51,823] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7907.6671 3161405696.3721 1715.0000
[2019-04-10 12:00:51,874] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.8356 2779095766.2837 931.0000
[2019-04-10 12:00:52,889] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 525000, evaluation results [525000.0, 7907.667111179063, 3161405696.3721123, 1715.0, 8259.539493186681, 2927064849.099563, 1327.0, 8661.835563979652, 2779095766.283703, 931.0, 8010.741918526205, 3005977106.3180776, 1725.0, 8503.445870691135, 2841534843.887689, 1111.0]
[2019-04-10 12:00:58,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.6614944e-19 1.0000000e+00 9.9728408e-23 1.0496031e-23 6.8878533e-31], sum to 1.0000
[2019-04-10 12:00:58,686] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1882
[2019-04-10 12:00:58,689] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.66666666666667, 60.66666666666667, 1.0, 2.0, 0.6211640916239537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868049.490890921, 868049.4908909217, 204755.9348768251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3849600.0000, 
sim time next is 3850200.0000, 
raw observation next is [34.75, 60.5, 1.0, 2.0, 0.6203133274136174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866860.1001569517, 866860.1001569517, 204592.3819506234], 
processed observation next is [0.0, 0.5652173913043478, 0.8459715639810427, 0.605, 1.0, 1.0, 0.5425461776067679, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2407944722658199, 0.2407944722658199, 0.3053617641054081], 
reward next is 0.6946, 
noisyNet noise sample is [array([-0.5398589], dtype=float32), -0.8504074]. 
=============================================
[2019-04-10 12:00:58,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.2773424e-19 1.0000000e+00 9.5284653e-23 1.0551953e-23 6.6091427e-31], sum to 1.0000
[2019-04-10 12:00:58,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6646
[2019-04-10 12:00:58,717] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.75, 60.5, 1.0, 2.0, 0.6203133274136174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866860.1001569517, 866860.1001569517, 204592.3819506234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3850200.0000, 
sim time next is 3850800.0000, 
raw observation next is [34.83333333333334, 60.33333333333334, 1.0, 2.0, 0.62162509133096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868693.9811581243, 868693.9811581243, 204844.9359646072], 
processed observation next is [0.0, 0.5652173913043478, 0.8499210110584523, 0.6033333333333334, 1.0, 1.0, 0.5441266160613976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24130388365503452, 0.24130388365503452, 0.3057387103949361], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.5398589], dtype=float32), -0.8504074]. 
=============================================
[2019-04-10 12:01:00,056] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2229258e-19 1.0000000e+00 6.9448436e-23 7.3779774e-25 1.1551755e-31], sum to 1.0000
[2019-04-10 12:01:00,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1803
[2019-04-10 12:01:00,070] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5973702957298898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834785.6409243108, 834785.6409243108, 200257.6478620151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3936000.0000, 
sim time next is 3936600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5959703130598959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832828.4903472299, 832828.4903472299, 199998.3037417744], 
processed observation next is [0.0, 0.5652173913043478, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5132172446504769, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23134124731867498, 0.23134124731867498, 0.2985049309578722], 
reward next is 0.7015, 
noisyNet noise sample is [array([-0.3205415], dtype=float32), 0.30436376]. 
=============================================
[2019-04-10 12:01:00,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6184269e-19 1.0000000e+00 7.8477448e-23 2.2077118e-25 2.6313748e-30], sum to 1.0000
[2019-04-10 12:01:00,106] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9815
[2019-04-10 12:01:00,111] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 88.16666666666667, 1.0, 2.0, 0.5484333746407883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766374.8460644378, 766374.8460644378, 191532.1340111812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3910200.0000, 
sim time next is 3910800.0000, 
raw observation next is [27.66666666666667, 87.33333333333334, 1.0, 2.0, 0.5539799636984588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774128.413790344, 774128.413790344, 192485.5205019995], 
processed observation next is [0.0, 0.2608695652173913, 0.5102685624012641, 0.8733333333333334, 1.0, 1.0, 0.46262646228729976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21503567049731778, 0.21503567049731778, 0.2872918216447754], 
reward next is 0.7127, 
noisyNet noise sample is [array([0.52616936], dtype=float32), 0.3609068]. 
=============================================
[2019-04-10 12:01:05,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2761041e-20 1.0000000e+00 1.0424201e-24 1.1774288e-24 2.1238318e-33], sum to 1.0000
[2019-04-10 12:01:05,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6635
[2019-04-10 12:01:05,965] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 83.16666666666666, 1.0, 2.0, 0.5463904542340632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763519.0652480916, 763519.0652480922, 191183.7264357923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4045800.0000, 
sim time next is 4046400.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5449808257091711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761548.5621678563, 761548.5621678557, 190943.7741778573], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45178412736044704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21154126726884895, 0.2115412672688488, 0.2849907077281452], 
reward next is 0.7150, 
noisyNet noise sample is [array([-2.245092], dtype=float32), -1.4450703]. 
=============================================
[2019-04-10 12:01:08,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5145247e-17 1.0000000e+00 2.0103423e-19 6.3779451e-19 1.7924749e-26], sum to 1.0000
[2019-04-10 12:01:08,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9708
[2019-04-10 12:01:08,375] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 88.0, 1.0, 2.0, 0.7677713513770087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1073030.189535858, 1073030.189535858, 236153.9268283316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4075200.0000, 
sim time next is 4075800.0000, 
raw observation next is [27.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8433435168777597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1178707.775708078, 1178707.775708079, 254828.3455030523], 
processed observation next is [1.0, 0.17391304347826086, 0.4865718799368091, 0.8816666666666667, 1.0, 1.0, 0.8112572492503128, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3274188265855772, 0.3274188265855775, 0.3803408141836601], 
reward next is 0.6197, 
noisyNet noise sample is [array([0.12059217], dtype=float32), -0.5121518]. 
=============================================
[2019-04-10 12:01:10,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2655960e-09 3.2420203e-02 2.6127225e-09 9.6757978e-01 3.0399392e-14], sum to 1.0000
[2019-04-10 12:01:10,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6399
[2019-04-10 12:01:10,194] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.33333333333334, 63.66666666666667, 1.0, 2.0, 0.9819841684641912, 1.0, 2.0, 0.9819841684641912, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2746855.962576526, 2746855.962576526, 518204.6822406037], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4182000.0000, 
sim time next is 4182600.0000, 
raw observation next is [34.66666666666666, 61.83333333333333, 1.0, 2.0, 0.9879485867249341, 1.0, 2.0, 0.9879485867249341, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2763558.389859224, 2763558.389859224, 521755.778361882], 
processed observation next is [1.0, 0.391304347826087, 0.842022116903633, 0.6183333333333333, 1.0, 1.0, 0.9854802249698001, 1.0, 1.0, 0.9854802249698001, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7676551082942289, 0.7676551082942289, 0.7787399677043015], 
reward next is 0.2213, 
noisyNet noise sample is [array([1.3024478], dtype=float32), 0.7929239]. 
=============================================
[2019-04-10 12:01:10,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6571254e-12 9.9742270e-01 3.3127914e-13 2.5772878e-03 4.9426410e-20], sum to 1.0000
[2019-04-10 12:01:10,530] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7665
[2019-04-10 12:01:10,532] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.66666666666667, 68.33333333333334, 1.0, 2.0, 0.3083886617554669, 1.0, 2.0, 0.3083886617554669, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 861913.3162742543, 861913.3162742543, 251538.5000548629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4123200.0000, 
sim time next is 4123800.0000, 
raw observation next is [33.5, 69.0, 1.0, 2.0, 0.6099288036292395, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104269, 852342.3702305679, 852342.3702305679, 202613.2304679008], 
processed observation next is [1.0, 0.7391304347826086, 0.7867298578199052, 0.69, 1.0, 1.0, 0.5300347031677585, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522826, 0.23676176950849107, 0.23676176950849107, 0.3024078066685087], 
reward next is 0.6976, 
noisyNet noise sample is [array([-0.6693801], dtype=float32), -1.6943655]. 
=============================================
[2019-04-10 12:01:22,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5981107e-19 1.0000000e+00 1.9281246e-22 1.4585802e-24 5.0449161e-31], sum to 1.0000
[2019-04-10 12:01:22,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8272
[2019-04-10 12:01:22,634] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5826276356725464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814175.8484580095, 814175.8484580095, 197554.2035287762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4432800.0000, 
sim time next is 4433400.0000, 
raw observation next is [29.5, 81.5, 1.0, 2.0, 0.5835800211355796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815507.2424217336, 815507.2424217336, 197726.8947056104], 
processed observation next is [0.0, 0.30434782608695654, 0.5971563981042655, 0.815, 1.0, 1.0, 0.49828918209105977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22652978956159267, 0.22652978956159267, 0.29511476821732896], 
reward next is 0.7049, 
noisyNet noise sample is [array([1.4254528], dtype=float32), 0.86167145]. 
=============================================
[2019-04-10 12:01:23,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4232621e-19 1.0000000e+00 7.0851687e-23 1.4465340e-24 1.1279473e-30], sum to 1.0000
[2019-04-10 12:01:23,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-10 12:01:23,963] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.507506380853706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709164.8182007936, 709164.8182007929, 184780.0542154979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5074416623186471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709074.3534734903, 709074.3534734909, 184769.774726508], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4065562196610205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19696509818708066, 0.1969650981870808, 0.27577578317389256], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.79816824], dtype=float32), 1.0892886]. 
=============================================
[2019-04-10 12:01:23,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.74473 ]
 [69.78157 ]
 [69.70137 ]
 [69.427986]
 [69.25085 ]], R is [[69.70427704]
 [69.73144531]
 [69.75827026]
 [69.7848053 ]
 [69.81111145]].
[2019-04-10 12:01:24,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4967660e-19 1.0000000e+00 5.8664891e-23 2.8990503e-25 1.1359789e-30], sum to 1.0000
[2019-04-10 12:01:24,605] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2717
[2019-04-10 12:01:24,609] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6357747105557006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 888475.7281487472, 888475.7281487467, 207600.5647921237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4443600.0000, 
sim time next is 4444200.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.632968016525192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884551.8256887555, 884551.8256887548, 207049.2942383701], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5577927909942072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24570884046909874, 0.24570884046909855, 0.3090287973707016], 
reward next is 0.6910, 
noisyNet noise sample is [array([1.242616], dtype=float32), 1.0645459]. 
=============================================
[2019-04-10 12:01:26,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0816399e-20 1.0000000e+00 3.0795919e-24 4.8874189e-26 1.9726870e-32], sum to 1.0000
[2019-04-10 12:01:26,265] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3564
[2019-04-10 12:01:26,270] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5509894483882853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769947.97032294, 769947.97032294, 191970.2304227675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507253237462932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769578.751089806, 769578.7510898054, 191924.8852004196], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4587052093328834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21377187530272387, 0.2137718753027237, 0.28645505253793974], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.5021688], dtype=float32), -1.3351564]. 
=============================================
[2019-04-10 12:01:28,542] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8136027e-20 1.0000000e+00 2.5738216e-24 1.2185906e-26 3.9759612e-33], sum to 1.0000
[2019-04-10 12:01:28,549] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7493
[2019-04-10 12:01:28,553] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5498011070409204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768286.7916807865, 768286.791680786, 191766.3772246501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4575600.0000, 
sim time next is 4576200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507766209849794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769650.4593724336, 769650.4593724343, 191933.6903364013], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45876701323491487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21379179427012043, 0.21379179427012063, 0.2864681945319423], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.1036423], dtype=float32), -0.8459557]. 
=============================================
[2019-04-10 12:01:35,571] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:01:35,573] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:01:35,574] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:01:35,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:01:35,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:01:35,575] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:01:35,577] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:01:35,577] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:01:35,577] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:01:35,578] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:01:35,579] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:01:35,598] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run23
[2019-04-10 12:01:35,621] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run23
[2019-04-10 12:01:35,659] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run23
[2019-04-10 12:01:35,676] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run23
[2019-04-10 12:01:35,676] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run23
[2019-04-10 12:01:40,448] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11836884], dtype=float32), 0.09612239]
[2019-04-10 12:01:40,449] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.38896337, 86.66528309, 1.0, 2.0, 0.3169073460344258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498728.8431650928, 498728.8431650928, 166878.1770972839]
[2019-04-10 12:01:40,450] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:01:40,452] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1749697e-19 1.0000000e+00 5.5447115e-23 1.4256397e-25 3.9948475e-31], sampled 0.8451632791844612
[2019-04-10 12:01:51,933] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11836884], dtype=float32), 0.09612239]
[2019-04-10 12:01:51,936] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.2, 69.66666666666667, 1.0, 2.0, 0.3166951358926536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502566.768544328, 502566.7685443286, 167248.4755236394]
[2019-04-10 12:01:51,936] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:01:51,939] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2318419e-20 1.0000000e+00 1.6503498e-24 1.2020842e-27 3.2163943e-33], sampled 0.5079851967148398
[2019-04-10 12:01:57,015] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11836884], dtype=float32), 0.09612239]
[2019-04-10 12:01:57,016] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.56666666666667, 94.0, 1.0, 2.0, 0.505891989172379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706908.1948985324, 706908.194898533, 184524.5759677409]
[2019-04-10 12:01:57,017] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:01:57,019] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9185454e-19 1.0000000e+00 3.1572574e-23 6.5088038e-26 1.4013074e-31], sampled 0.2612223113213904
[2019-04-10 12:02:07,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11836884], dtype=float32), 0.09612239]
[2019-04-10 12:02:07,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.66704312, 96.41178999, 1.0, 2.0, 0.2847706101219004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 459991.6799569919, 459991.6799569924, 164269.5640353556]
[2019-04-10 12:02:07,407] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:02:07,409] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.6666337e-20 1.0000000e+00 8.4376910e-24 2.1117939e-26 2.2736691e-32], sampled 0.7253147259322348
[2019-04-10 12:03:12,776] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11836884], dtype=float32), 0.09612239]
[2019-04-10 12:03:12,777] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.18104716166667, 88.699493915, 1.0, 2.0, 0.4237523565766114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 620076.6977456344, 620076.6977456344, 176091.1550815746]
[2019-04-10 12:03:12,778] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:03:12,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7064678e-19 1.0000000e+00 2.6698467e-23 3.5274002e-26 1.3141988e-31], sampled 0.6667878493510812
[2019-04-10 12:03:16,750] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1744 2927443666.6318 1338.0000
[2019-04-10 12:03:17,119] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7887.1183 3164254251.6172 1772.0000
[2019-04-10 12:03:17,311] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4239 3007808101.3514 1765.0000
[2019-04-10 12:03:17,493] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-10 12:03:17,776] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6048 2842344950.3244 1129.0000
[2019-04-10 12:03:18,790] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 550000, evaluation results [550000.0, 7887.118346092067, 3164254251.617239, 1772.0, 8254.174443600286, 2927443666.631801, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.423869034422, 3007808101.3514414, 1765.0, 8498.604771333567, 2842344950.3243933, 1129.0]
[2019-04-10 12:03:22,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.38274795e-17 1.00000000e+00 7.23967859e-21 5.15496187e-18
 3.14491480e-29], sum to 1.0000
[2019-04-10 12:03:22,461] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0546
[2019-04-10 12:03:22,473] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5084562045137168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710492.4996086985, 710492.4996086991, 184933.4620823878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4816800.0000, 
sim time next is 4817400.0000, 
raw observation next is [29.83333333333333, 70.0, 1.0, 2.0, 0.5105331950420744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713395.7618843167, 713395.7618843167, 185264.1819900442], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7, 1.0, 1.0, 0.4102809578820173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1981654894123102, 0.1981654894123102, 0.27651370446275253], 
reward next is 0.7235, 
noisyNet noise sample is [array([-0.891748], dtype=float32), -0.12252168]. 
=============================================
[2019-04-10 12:03:24,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6134972e-17 1.0000000e+00 5.6328286e-21 3.6302715e-21 6.3545298e-28], sum to 1.0000
[2019-04-10 12:03:24,785] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0218
[2019-04-10 12:03:24,789] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7863176796647563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1098963.780301931, 1098963.780301931, 240576.5413430417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4848000.0000, 
sim time next is 4848600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7634146406370645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1066938.231714663, 1066938.231714663, 235125.1919017656], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7149573983579091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2963717310318509, 0.2963717310318509, 0.3509331222414412], 
reward next is 0.6491, 
noisyNet noise sample is [array([-1.9831518], dtype=float32), 0.92730683]. 
=============================================
[2019-04-10 12:03:26,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.91654157e-12 9.99999762e-01 6.03130870e-12 2.60701086e-07
 1.12487795e-17], sum to 1.0000
[2019-04-10 12:03:26,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0874
[2019-04-10 12:03:26,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2176460.395871567 W.
[2019-04-10 12:03:26,120] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.9153056525359312, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.983540964879776, 6.9112, 168.9124656178007, 2176460.395871567, 2125139.37475091, 439147.6912609649], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4872600.0000, 
sim time next is 4873200.0000, 
raw observation next is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.5106628525952385, 1.0, 1.0, 0.5106628525952385, 1.0, 2.0, 0.8791268755757927, 6.911199999999999, 6.9112, 170.5573041426782, 2142150.137144331, 2142150.137144331, 421165.7687589586], 
processed observation next is [1.0, 0.391304347826087, 0.6050552922590839, 0.7133333333333333, 1.0, 1.0, 0.4104371718014921, 1.0, 0.5, 0.4104371718014921, 1.0, 1.0, 0.8525937507021861, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5950417047623141, 0.5950417047623141, 0.628605625013371], 
reward next is 0.3714, 
noisyNet noise sample is [array([-0.8877513], dtype=float32), 0.57324606]. 
=============================================
[2019-04-10 12:03:26,552] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.20568159e-17 1.00000000e+00 3.04212925e-20 1.24931156e-20
 1.01813112e-27], sum to 1.0000
[2019-04-10 12:03:26,555] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-10 12:03:26,560] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 77.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.987081761574328, 6.9112, 168.9126192848853, 1507624.818489466, 1453791.792654888, 311349.3201276031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4868400.0000, 
sim time next is 4869000.0000, 
raw observation next is [28.5, 76.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.561204040691552, 6.9112, 168.9093175616714, 1915196.085861738, 1454070.782374474, 311349.5056219258], 
processed observation next is [1.0, 0.34782608695652173, 0.5497630331753555, 0.765, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.06500040406915523, 0.0, 0.8294220762479751, 0.5319989127393717, 0.4039085506595761, 0.46470075465959076], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8075738], dtype=float32), 0.26101032]. 
=============================================
[2019-04-10 12:03:26,579] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[61.368324]
 [62.70636 ]
 [63.116287]
 [63.28696 ]
 [63.27147 ]], R is [[59.42420578]
 [58.9858551 ]
 [59.03085709]
 [59.10052872]
 [59.19017792]].
[2019-04-10 12:03:28,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8838646e-10 9.9589336e-01 9.6573416e-11 4.1065793e-03 3.8256786e-17], sum to 1.0000
[2019-04-10 12:03:28,151] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6540
[2019-04-10 12:03:28,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1806540.173238432 W.
[2019-04-10 12:03:28,162] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.650983924620603, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.992267540346185, 6.9112, 168.9124739667451, 1806540.173238432, 1749028.234285686, 374902.9429372449], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4884000.0000, 
sim time next is 4884600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4258910127609989, 1.0, 1.0, 0.4258910127609989, 1.0, 2.0, 0.7356457226895737, 6.9112, 6.9112, 170.5573041426782, 1786249.189799513, 1786249.189799513, 367084.8266008461], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3083024250132517, 1.0, 0.5, 0.3083024250132517, 1.0, 1.0, 0.6776167349872849, 0.0, 0.0, 0.8375144448122397, 0.4961803304998647, 0.4961803304998647, 0.5478878008967852], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2790065], dtype=float32), -0.36532596]. 
=============================================
[2019-04-10 12:03:32,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7853992e-20 1.0000000e+00 1.0981723e-22 2.6972918e-23 4.2470785e-31], sum to 1.0000
[2019-04-10 12:03:32,738] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8242
[2019-04-10 12:03:32,742] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5125830964631871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716261.1662004346, 716261.1662004339, 185590.9120291731], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5007600.0000, 
sim time next is 5008200.0000, 
raw observation next is [26.91666666666666, 84.0, 1.0, 2.0, 0.5113504572866356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714538.1508985457, 714538.1508985457, 185393.2993440054], 
processed observation next is [1.0, 1.0, 0.4747235387045811, 0.84, 1.0, 1.0, 0.41126561118871763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1984828196940405, 0.1984828196940405, 0.27670641693135134], 
reward next is 0.7233, 
noisyNet noise sample is [array([-0.44237277], dtype=float32), 0.26024687]. 
=============================================
[2019-04-10 12:03:35,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4932352e-20 1.0000000e+00 3.4269987e-24 2.4902003e-26 4.3636753e-33], sum to 1.0000
[2019-04-10 12:03:35,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9771
[2019-04-10 12:03:35,286] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 88.16666666666667, 1.0, 2.0, 0.491929910150616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687391.9707137512, 687391.9707137519, 182341.4938957685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5021400.0000, 
sim time next is 5022000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4960570641905541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 693160.8782075542, 693160.8782075549, 182980.8162080559], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.39283983637416164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1925446883909873, 0.19254468839098748, 0.27310569583291927], 
reward next is 0.7269, 
noisyNet noise sample is [array([-0.7174183], dtype=float32), 0.20354982]. 
=============================================
[2019-04-10 12:03:35,295] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.624504]
 [71.82108 ]
 [71.93073 ]
 [72.056046]
 [71.873215]], R is [[71.52600098]
 [71.53859711]
 [71.55198669]
 [71.56609344]
 [71.58083344]].
[2019-04-10 12:03:36,923] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2786777e-20 1.0000000e+00 3.0439922e-24 1.6477563e-26 3.0907229e-33], sum to 1.0000
[2019-04-10 12:03:36,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1592
[2019-04-10 12:03:36,936] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 64.33333333333334, 1.0, 2.0, 0.5609148420134713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783822.7484076903, 783822.7484076909, 193690.6884477177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5143200.0000, 
sim time next is 5143800.0000, 
raw observation next is [32.0, 63.66666666666666, 1.0, 2.0, 0.557372386554527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778870.7040187203, 778870.7040187209, 193073.4920540554], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6366666666666666, 1.0, 1.0, 0.46671371874039397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21635297333853343, 0.2163529733385336, 0.2881693911254558], 
reward next is 0.7118, 
noisyNet noise sample is [array([-0.0546644], dtype=float32), -0.40979263]. 
=============================================
[2019-04-10 12:03:43,621] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6073631e-17 1.0000000e+00 1.0690576e-20 1.2915087e-20 2.4578179e-28], sum to 1.0000
[2019-04-10 12:03:43,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7262
[2019-04-10 12:03:43,635] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8229685654645147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1150215.090218088, 1150215.090218088, 249630.8066591812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5291400.0000, 
sim time next is 5292000.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.9495685690163033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1327267.117137036, 1327267.117137035, 283940.5516541216], 
processed observation next is [1.0, 0.2608695652173913, 0.5545023696682465, 0.88, 1.0, 1.0, 0.9392392397786786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36868531031584334, 0.368685310315843, 0.42379186814048003], 
reward next is 0.5762, 
noisyNet noise sample is [array([0.9381584], dtype=float32), -0.8656776]. 
=============================================
[2019-04-10 12:03:43,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.553093]
 [62.580044]
 [62.489143]
 [62.62055 ]
 [62.322067]], R is [[62.02162933]
 [62.02883148]
 [62.03491974]
 [62.03939438]
 [62.04602814]].
[2019-04-10 12:03:50,173] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1755520e-18 1.0000000e+00 1.7539181e-21 3.2507653e-22 1.5342534e-29], sum to 1.0000
[2019-04-10 12:03:50,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9290
[2019-04-10 12:03:50,181] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 90.33333333333334, 1.0, 2.0, 0.5889300058987476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822986.3223094663, 822986.3223094663, 198701.9466961817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5448000.0000, 
sim time next is 5448600.0000, 
raw observation next is [28.25, 91.0, 1.0, 2.0, 0.5898720787991006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 824303.3106689727, 824303.3106689721, 198874.4699950288], 
processed observation next is [1.0, 0.043478260869565216, 0.537914691943128, 0.91, 1.0, 1.0, 0.5058699744567478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2289731418524924, 0.22897314185249223, 0.2968275671567594], 
reward next is 0.7032, 
noisyNet noise sample is [array([-0.12756144], dtype=float32), 0.54430103]. 
=============================================
[2019-04-10 12:03:50,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5055318e-11 8.9579844e-01 3.7841126e-11 1.0420147e-01 5.2193062e-17], sum to 1.0000
[2019-04-10 12:03:50,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8768
[2019-04-10 12:03:50,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3308500.903148193 W.
[2019-04-10 12:03:50,609] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.63333333333333, 59.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.467788159453632, 6.9112, 170.5573041426782, 3308500.903148193, 2909794.163972777, 550618.6443349533], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5415600.0000, 
sim time next is 5416200.0000, 
raw observation next is [32.95, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.78721667371594, 6.9112, 170.5573041426782, 3537587.126168369, 2910060.748705657, 548673.1653365885], 
processed observation next is [1.0, 0.6956521739130435, 0.7606635071090049, 0.62, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.08760166737159399, 0.0, 0.8375144448122397, 0.9826630906023247, 0.8083502079737936, 0.818915172144162], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.93902755], dtype=float32), 0.39156243]. 
=============================================
[2019-04-10 12:03:50,826] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5070368e-19 1.0000000e+00 4.8040058e-23 2.1231297e-23 6.4596880e-31], sum to 1.0000
[2019-04-10 12:03:50,832] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3115
[2019-04-10 12:03:50,836] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.06666666666667, 82.0, 1.0, 2.0, 0.6108429708254568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853620.3825900599, 853620.3825900605, 202783.394811058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5435400.0000, 
sim time next is 5436000.0000, 
raw observation next is [30.0, 82.0, 1.0, 2.0, 0.6084672922804134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 850299.169977118, 850299.1699771174, 202333.9938318708], 
processed observation next is [1.0, 0.9565217391304348, 0.6208530805687204, 0.82, 1.0, 1.0, 0.5282738461209799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23619421388253278, 0.23619421388253262, 0.3019910355699564], 
reward next is 0.6980, 
noisyNet noise sample is [array([0.7280302], dtype=float32), -0.044583928]. 
=============================================
[2019-04-10 12:03:50,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.916794]
 [66.78035 ]
 [66.58713 ]
 [66.51692 ]
 [66.40236 ]], R is [[66.95001221]
 [66.97785187]
 [67.00489807]
 [67.03128052]
 [67.05690002]].
[2019-04-10 12:03:51,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0004556e-10 9.4678802e-03 3.9276063e-10 9.9053216e-01 2.1163802e-15], sum to 1.0000
[2019-04-10 12:03:51,443] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8870
[2019-04-10 12:03:51,447] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.1, 64.0, 1.0, 2.0, 0.9760545181876651, 1.0, 2.0, 0.9760545181876651, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2730251.118991099, 2730251.1189911, 514693.657586498], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5392800.0000, 
sim time next is 5393400.0000, 
raw observation next is [34.28333333333333, 63.33333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.4777573876881, 6.9112, 170.5573041426782, 3315650.586111833, 2909802.483229791, 550570.2925344399], 
processed observation next is [1.0, 0.43478260869565216, 0.8238546603475513, 0.6333333333333334, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.056655738768809985, 0.0, 0.8375144448122397, 0.9210140516977313, 0.8082784675638308, 0.8217467052752835], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37502974], dtype=float32), 0.84131444]. 
=============================================
[2019-04-10 12:03:54,958] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5145266e-10 8.0309987e-01 1.3461592e-10 1.9690007e-01 1.1421494e-15], sum to 1.0000
[2019-04-10 12:03:54,966] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9786
[2019-04-10 12:03:54,969] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.73333333333333, 46.66666666666667, 1.0, 2.0, 1.006281939208641, 1.0, 2.0, 1.006281939208641, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2814899.495187858, 2814899.495187858, 532793.1411659113], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5494800.0000, 
sim time next is 5495400.0000, 
raw observation next is [36.6, 47.0, 1.0, 2.0, 0.9992485572227686, 1.0, 2.0, 0.9992485572227686, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2795202.815090672, 2795202.815090672, 528531.4377207451], 
processed observation next is [1.0, 0.6086956521739131, 0.9336492890995262, 0.47, 1.0, 1.0, 0.9990946472563478, 1.0, 1.0, 0.9990946472563478, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7764452264140755, 0.7764452264140755, 0.7888528921205151], 
reward next is 0.2111, 
noisyNet noise sample is [array([-0.02165343], dtype=float32), -0.3870134]. 
=============================================
[2019-04-10 12:04:01,627] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 12:04:01,628] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:04:01,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:04:01,630] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:04:01,631] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:04:01,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:04:01,633] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:04:01,632] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:04:01,634] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:04:01,635] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:04:01,635] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:04:01,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run24
[2019-04-10 12:04:01,664] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run24
[2019-04-10 12:04:01,680] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run24
[2019-04-10 12:04:01,682] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run24
[2019-04-10 12:04:01,722] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run24
[2019-04-10 12:04:21,493] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11975646], dtype=float32), 0.095428035]
[2019-04-10 12:04:21,494] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.35, 92.5, 1.0, 2.0, 0.4532463085761164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646815.3388027243, 646815.3388027243, 178330.9582957907]
[2019-04-10 12:04:21,494] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:04:21,495] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2215497e-20 1.0000000e+00 1.1042370e-24 3.0273007e-27 2.7893822e-33], sampled 0.08201978597922799
[2019-04-10 12:04:33,193] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11975646], dtype=float32), 0.095428035]
[2019-04-10 12:04:33,196] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.08333333333334, 89.0, 1.0, 2.0, 0.5410289048880695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756024.2381773607, 756024.2381773613, 190271.4723658777]
[2019-04-10 12:04:33,198] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:04:33,200] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1784249e-19 1.0000000e+00 1.6523286e-23 2.7648570e-26 1.0126096e-31], sampled 0.9957016593534749
[2019-04-10 12:04:46,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11975646], dtype=float32), 0.095428035]
[2019-04-10 12:04:46,480] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.5, 51.0, 1.0, 2.0, 0.6230125810401356, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.970167769360242, 6.9112, 168.9125081377599, 1741981.739950212, 1700148.085898117, 368890.0780265255]
[2019-04-10 12:04:46,482] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:04:46,486] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6162086e-11 9.9998796e-01 3.9352150e-12 1.1983905e-05 3.5199371e-19], sampled 0.9576750118048171
[2019-04-10 12:04:46,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1741981.739950212 W.
[2019-04-10 12:06:53,524] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11975646], dtype=float32), 0.095428035]
[2019-04-10 12:06:53,546] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.13333333333333, 92.33333333333334, 1.0, 2.0, 0.4977291856088768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695498.1669282675, 695498.1669282675, 183240.219278948]
[2019-04-10 12:06:53,548] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:06:53,553] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2837975e-19 1.0000000e+00 2.3928060e-23 1.5032504e-25 1.8772965e-31], sampled 0.4009502732117336
[2019-04-10 12:07:10,196] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11975646], dtype=float32), 0.095428035]
[2019-04-10 12:07:10,196] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 83.0, 1.0, 2.0, 0.7979735061402288, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981401670416, 6.9112, 168.9123160051069, 2012237.267249692, 1944996.333754179, 407330.2221671297]
[2019-04-10 12:07:10,196] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:07:10,233] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3753314e-12 1.0000000e+00 8.2826656e-14 4.1970232e-08 2.0870378e-20], sampled 0.6853531589841785
[2019-04-10 12:07:10,233] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2012237.267249692 W.
[2019-04-10 12:07:33,021] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-04-10 12:07:33,073] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7737 2927512725.0370 1338.0000
[2019-04-10 12:07:33,506] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.0668 3164138271.1439 1778.0000
[2019-04-10 12:07:34,097] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.1051 2842530428.4868 1130.0000
[2019-04-10 12:07:34,188] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1218 3007784093.2705 1766.0000
[2019-04-10 12:07:35,203] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 575000, evaluation results [575000.0, 7881.066830009585, 3164138271.143879, 1778.0, 8254.773707254759, 2927512725.037018, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7998.121812721697, 3007784093.270549, 1766.0, 8497.105079041201, 2842530428.486751, 1130.0]
[2019-04-10 12:07:35,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.938860e-21 1.000000e+00 3.686745e-24 7.198130e-27 4.418887e-33], sum to 1.0000
[2019-04-10 12:07:35,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3367
[2019-04-10 12:07:35,773] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.8, 65.0, 1.0, 2.0, 0.5583092521566168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780180.358338228, 780180.358338228, 193236.4637166237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5659200.0000, 
sim time next is 5659800.0000, 
raw observation next is [31.86666666666667, 64.5, 1.0, 2.0, 0.5598682227556356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782359.6633822398, 782359.6633822398, 193507.8291327912], 
processed observation next is [0.0, 0.5217391304347826, 0.7093206951026858, 0.645, 1.0, 1.0, 0.4697207503079947, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21732212871728881, 0.21732212871728881, 0.28881765542207644], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.67130125], dtype=float32), 1.1121099]. 
=============================================
[2019-04-10 12:07:36,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6091464e-21 1.0000000e+00 6.4958593e-25 6.4073499e-28 3.9444916e-34], sum to 1.0000
[2019-04-10 12:07:36,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7690
[2019-04-10 12:07:37,073] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.25, 63.0, 1.0, 2.0, 0.5272085017348208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736705.1537804308, 736705.1537804314, 187969.3024147683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5679000.0000, 
sim time next is 5679600.0000, 
raw observation next is [31.03333333333333, 64.33333333333333, 1.0, 2.0, 0.5284826783616547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738486.2684343568, 738486.2684343574, 188179.487977749], 
processed observation next is [0.0, 0.7391304347826086, 0.669826224328594, 0.6433333333333333, 1.0, 1.0, 0.431906841399584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2051350745650991, 0.20513507456509927, 0.2808649074294761], 
reward next is 0.7191, 
noisyNet noise sample is [array([-1.7924331], dtype=float32), -0.39324996]. 
=============================================
[2019-04-10 12:07:40,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7980554e-20 1.0000000e+00 3.4999932e-24 7.9901532e-28 4.9412225e-33], sum to 1.0000
[2019-04-10 12:07:40,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4643
[2019-04-10 12:07:40,305] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.2, 57.5, 1.0, 2.0, 0.5215649810163725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728816.3638848484, 728816.3638848478, 187044.1820415007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5743800.0000, 
sim time next is 5744400.0000, 
raw observation next is [32.4, 56.66666666666667, 1.0, 2.0, 0.5211209160873153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728195.6306460764, 728195.6306460764, 186971.8663684944], 
processed observation next is [0.0, 0.4782608695652174, 0.7345971563981042, 0.5666666666666668, 1.0, 1.0, 0.4230372482979702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20227656406835456, 0.20227656406835456, 0.2790624871171558], 
reward next is 0.7209, 
noisyNet noise sample is [array([1.0476564], dtype=float32), 0.2089646]. 
=============================================
[2019-04-10 12:07:40,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4135809e-22 1.0000000e+00 4.1551537e-25 5.2822365e-28 7.9471463e-34], sum to 1.0000
[2019-04-10 12:07:40,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-10 12:07:40,559] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.86666666666667, 64.5, 1.0, 2.0, 0.5598682227556356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782359.6633822398, 782359.6633822398, 193507.8291327912], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5659800.0000, 
sim time next is 5660400.0000, 
raw observation next is [31.93333333333334, 64.0, 1.0, 2.0, 0.5579368814429562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779659.8171915929, 779659.8171915929, 193171.5707137766], 
processed observation next is [0.0, 0.5217391304347826, 0.7124802527646134, 0.64, 1.0, 1.0, 0.4673938330638026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21657217144210913, 0.21657217144210913, 0.2883157771847412], 
reward next is 0.7117, 
noisyNet noise sample is [array([0.655967], dtype=float32), 0.023161847]. 
=============================================
[2019-04-10 12:07:41,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0780960e-20 1.0000000e+00 9.9850164e-24 3.6048293e-26 1.2823891e-32], sum to 1.0000
[2019-04-10 12:07:41,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0219
[2019-04-10 12:07:41,907] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5168852551501861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722274.8581099614, 722274.8581099614, 186283.3548427695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5706000.0000, 
sim time next is 5706600.0000, 
raw observation next is [26.48333333333333, 87.0, 1.0, 2.0, 0.5148031346994155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719364.4008000082, 719364.4008000082, 185947.4449789666], 
processed observation next is [0.0, 0.043478260869565216, 0.4541864139020536, 0.87, 1.0, 1.0, 0.4154254634932717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19982344466666896, 0.19982344466666896, 0.2775334999686069], 
reward next is 0.7225, 
noisyNet noise sample is [array([-0.6072084], dtype=float32), 0.62787145]. 
=============================================
[2019-04-10 12:07:43,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7840361e-19 1.0000000e+00 5.0938011e-24 4.1569115e-26 2.4271870e-32], sum to 1.0000
[2019-04-10 12:07:43,866] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9721
[2019-04-10 12:07:43,915] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 87.0, 1.0, 2.0, 0.5122068554158608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715735.2463210775, 715735.2463210775, 185530.3552951152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5707800.0000, 
sim time next is 5708400.0000, 
raw observation next is [26.43333333333333, 87.0, 1.0, 2.0, 0.5113840476243214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714585.1043146537, 714585.1043146544, 185398.5746482973], 
processed observation next is [0.0, 0.043478260869565216, 0.4518167456556081, 0.87, 1.0, 1.0, 0.411306081475086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19849586230962601, 0.1984958623096262, 0.27671429051984675], 
reward next is 0.7233, 
noisyNet noise sample is [array([2.4011471], dtype=float32), 0.8733428]. 
=============================================
[2019-04-10 12:07:48,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5397481e-21 1.0000000e+00 2.7225954e-25 1.7764355e-28 4.4601562e-34], sum to 1.0000
[2019-04-10 12:07:48,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4434
[2019-04-10 12:07:48,867] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.73333333333333, 59.16666666666667, 1.0, 2.0, 0.5643201405193947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788583.0810784464, 788583.0810784464, 194286.3640795134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5760600.0000, 
sim time next is 5761200.0000, 
raw observation next is [32.56666666666666, 60.33333333333334, 1.0, 2.0, 0.5521515385808276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771572.4550705901, 771572.4550705901, 192170.8571111246], 
processed observation next is [0.0, 0.6956521739130435, 0.7424960505529224, 0.6033333333333334, 1.0, 1.0, 0.46042354045882833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21432568196405283, 0.21432568196405283, 0.2868221747927233], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.16941698], dtype=float32), 0.76422703]. 
=============================================
[2019-04-10 12:07:50,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.40568312e-21 1.00000000e+00 2.01378243e-25 1.61080765e-27
 1.55437400e-33], sum to 1.0000
[2019-04-10 12:07:50,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5833
[2019-04-10 12:07:50,473] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.5461227528156555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763144.8483131959, 763144.8483131966, 191137.5523005457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5781000.0000, 
sim time next is 5781600.0000, 
raw observation next is [27.5, 86.0, 1.0, 2.0, 0.5443056818944126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760604.7878911024, 760604.7878911024, 190828.5858248018], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.86, 1.0, 1.0, 0.45097070107760556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21127910774752845, 0.21127910774752845, 0.284818784813137], 
reward next is 0.7152, 
noisyNet noise sample is [array([0.44781905], dtype=float32), 0.079311125]. 
=============================================
[2019-04-10 12:07:57,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2626376e-17 1.0000000e+00 8.3781433e-21 4.5422773e-21 8.0288957e-28], sum to 1.0000
[2019-04-10 12:07:57,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7514
[2019-04-10 12:07:57,033] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.81666666666667, 89.0, 1.0, 2.0, 0.740013288627387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1034216.884668962, 1034216.884668962, 229718.7650340575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5809800.0000, 
sim time next is 5810400.0000, 
raw observation next is [27.0, 88.0, 1.0, 2.0, 0.7542076622200108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1054064.29386378, 1054064.293863781, 232980.7288102433], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.88, 1.0, 1.0, 0.7038646532771214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29279563718438334, 0.2927956371843836, 0.34773243106006463], 
reward next is 0.6523, 
noisyNet noise sample is [array([0.5962193], dtype=float32), -0.6685586]. 
=============================================
[2019-04-10 12:07:59,348] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7487770e-09 9.9385768e-01 8.2426638e-10 6.1423671e-03 1.7105046e-15], sum to 1.0000
[2019-04-10 12:07:59,348] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8441
[2019-04-10 12:07:59,349] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2446832.387256789 W.
[2019-04-10 12:07:59,373] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.85, 70.66666666666667, 1.0, 2.0, 0.8748325010279762, 1.0, 2.0, 0.8748325010279762, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2446832.387256789, 2446832.387256789, 457947.5995297941], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5824200.0000, 
sim time next is 5824800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.396212677056896, 6.9112, 168.9101458154023, 2628766.992730651, 2284688.032313855, 475061.2369672721], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.7, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.04850126770568961, 0.0, 0.82942614335297, 0.730213053536292, 0.6346355645316264, 0.7090466223392121], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23138063], dtype=float32), 0.7207303]. 
=============================================
[2019-04-10 12:08:04,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5101588e-09 7.1425714e-02 1.0817184e-09 9.2857426e-01 1.5640475e-14], sum to 1.0000
[2019-04-10 12:08:04,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4384
[2019-04-10 12:08:04,327] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.6, 72.0, 1.0, 2.0, 0.8308546995577386, 1.0, 2.0, 0.8308546995577386, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2323715.8398455, 2323715.8398455, 435180.3162769026], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5913000.0000, 
sim time next is 5913600.0000, 
raw observation next is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 0.8503434410066634, 1.0, 2.0, 0.8503434410066634, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2378273.38167162, 2378273.38167162, 445132.6523353131], 
processed observation next is [1.0, 0.43478260869565216, 0.7045813586097948, 0.7133333333333333, 1.0, 1.0, 0.8196908927791124, 1.0, 1.0, 0.8196908927791124, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6606314949087834, 0.6606314949087834, 0.6643770930377807], 
reward next is 0.3356, 
noisyNet noise sample is [array([-1.0445085], dtype=float32), -0.68923783]. 
=============================================
[2019-04-10 12:08:19,851] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9262778e-21 1.0000000e+00 3.6826951e-25 1.3082947e-26 2.7904678e-33], sum to 1.0000
[2019-04-10 12:08:19,852] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5965
[2019-04-10 12:08:19,858] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.21666666666667, 86.16666666666667, 1.0, 2.0, 0.5281266562988742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737988.600777555, 737988.6007775556, 188120.4078167687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6124200.0000, 
sim time next is 6124800.0000, 
raw observation next is [27.23333333333333, 86.33333333333334, 1.0, 2.0, 0.5303451222734418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741089.7010509837, 741089.7010509837, 188487.1810775904], 
processed observation next is [1.0, 0.9130434782608695, 0.4897314375987361, 0.8633333333333334, 1.0, 1.0, 0.43415074972703827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20585825029193994, 0.20585825029193994, 0.2813241508620752], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.37443194], dtype=float32), -0.799468]. 
=============================================
[2019-04-10 12:08:29,386] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5448257e-20 1.0000000e+00 4.3776648e-25 1.9981080e-28 1.5595343e-34], sum to 1.0000
[2019-04-10 12:08:29,392] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3957
[2019-04-10 12:08:29,399] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 73.5, 1.0, 2.0, 0.5153618136472595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720145.3400570109, 720145.3400570103, 186037.6931160903], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6370200.0000, 
sim time next is 6370800.0000, 
raw observation next is [28.63333333333333, 74.0, 1.0, 2.0, 0.5156643631530057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720568.2536481174, 720568.2536481174, 186086.5075482793], 
processed observation next is [0.0, 0.7391304347826086, 0.55608214849921, 0.74, 1.0, 1.0, 0.4164630881361514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20015784823558816, 0.20015784823558816, 0.2777410560422079], 
reward next is 0.7223, 
noisyNet noise sample is [array([0.03878732], dtype=float32), 0.25308678]. 
=============================================
[2019-04-10 12:08:32,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1840677e-17 1.0000000e+00 8.5748628e-21 3.3270951e-21 5.1449163e-28], sum to 1.0000
[2019-04-10 12:08:32,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-10 12:08:32,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2827282.90406878 W.
[2019-04-10 12:08:32,385] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.1, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.846174690813985, 6.9112, 168.9021258788283, 2827282.90406878, 1454633.281107895, 309664.5672815763], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6423600.0000, 
sim time next is 6424200.0000, 
raw observation next is [28.2, 78.5, 1.0, 2.0, 0.5524831754024473, 1.0, 1.0, 0.5524831754024473, 1.0, 1.0, 0.9481692942445578, 6.9112, 6.9112, 170.5573041426782, 2317753.357161385, 2317753.357161385, 451042.2801689178], 
processed observation next is [1.0, 0.34782608695652173, 0.5355450236966824, 0.785, 1.0, 1.0, 0.4608231028945148, 1.0, 0.5, 0.4608231028945148, 1.0, 0.5, 0.9367918222494606, 0.0, 0.0, 0.8375144448122397, 0.6438203769892736, 0.6438203769892736, 0.673197433087937], 
reward next is 0.3268, 
noisyNet noise sample is [array([-2.108595], dtype=float32), -0.7479253]. 
=============================================
[2019-04-10 12:08:37,567] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3849879e-15 1.0000000e+00 3.2532656e-18 4.1689043e-17 4.2140723e-25], sum to 1.0000
[2019-04-10 12:08:37,573] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2970
[2019-04-10 12:08:37,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1845001.203879803 W.
[2019-04-10 12:08:37,585] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.85, 81.5, 1.0, 2.0, 0.4398870615512408, 1.0, 1.0, 0.4398870615512408, 1.0, 1.0, 0.7549027969083013, 6.9112, 6.9112, 170.5573041426782, 1845001.203879803, 1845001.203879803, 374649.2977226173], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6510600.0000, 
sim time next is 6511200.0000, 
raw observation next is [28.06666666666667, 79.66666666666667, 1.0, 2.0, 0.4413252583302257, 1.0, 2.0, 0.4413252583302257, 1.0, 2.0, 0.7575316994938076, 6.9112, 6.9112, 170.5573041426782, 1851038.587240945, 1851038.587240945, 375541.5186463497], 
processed observation next is [1.0, 0.34782608695652173, 0.529225908372828, 0.7966666666666667, 1.0, 1.0, 0.32689790160268156, 1.0, 1.0, 0.32689790160268156, 1.0, 1.0, 0.7043069506022043, 0.0, 0.0, 0.8375144448122397, 0.5141773853447069, 0.5141773853447069, 0.56050972932291], 
reward next is 0.4395, 
noisyNet noise sample is [array([-0.92779315], dtype=float32), 0.77927995]. 
=============================================
[2019-04-10 12:08:37,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2224207e-19 1.0000000e+00 1.0819925e-22 2.3518840e-23 1.2577787e-30], sum to 1.0000
[2019-04-10 12:08:37,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9653
[2019-04-10 12:08:37,851] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 92.33333333333334, 1.0, 2.0, 0.5688200332436789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794873.6009094777, 794873.6009094783, 195076.6319416119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6582000.0000, 
sim time next is 6582600.0000, 
raw observation next is [25.8, 92.5, 1.0, 2.0, 0.5607225865789942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783553.9914521124, 783553.9914521124, 193653.0624439428], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.925, 1.0, 1.0, 0.4707501043120412, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21765388651447568, 0.21765388651447568, 0.2890344215581236], 
reward next is 0.7110, 
noisyNet noise sample is [array([0.33840066], dtype=float32), 0.1875204]. 
=============================================
[2019-04-10 12:08:40,110] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-10 12:08:40,110] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:08:40,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:40,111] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:08:40,112] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:08:40,112] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:40,113] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:40,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:08:40,114] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:08:40,116] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:40,117] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:40,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run25
[2019-04-10 12:08:40,157] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run25
[2019-04-10 12:08:40,181] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run25
[2019-04-10 12:08:40,200] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run25
[2019-04-10 12:08:40,255] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run25
[2019-04-10 12:09:00,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12082464], dtype=float32), 0.09488732]
[2019-04-10 12:09:00,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.30949723333334, 71.26570649166666, 1.0, 2.0, 0.5348261198768339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815708.7752498986, 815708.7752498986, 197524.3696065637]
[2019-04-10 12:09:00,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:09:00,152] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9594355e-20 1.0000000e+00 1.4324335e-23 3.1560242e-26 1.0528911e-31], sampled 0.998435914771608
[2019-04-10 12:09:52,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12082464], dtype=float32), 0.09488732]
[2019-04-10 12:09:52,238] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.83912213, 90.42172841333334, 1.0, 2.0, 0.7747878146125853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1082841.333930949, 1082841.333930949, 237813.9460955729]
[2019-04-10 12:09:52,239] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:09:52,241] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.6150092e-19 1.0000000e+00 1.4967752e-22 1.2288147e-24 1.3862774e-30], sampled 0.2767247795858766
[2019-04-10 12:09:59,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12082464], dtype=float32), 0.09488732]
[2019-04-10 12:09:59,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.85, 63.0, 1.0, 2.0, 0.3368815115637155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527814.0641355963, 527814.0641355963, 169069.2215191835]
[2019-04-10 12:09:59,425] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:09:59,429] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.0186639e-21 1.0000000e+00 2.9613419e-25 2.2268493e-28 5.5122659e-34], sampled 0.05501961340950168
[2019-04-10 12:10:00,481] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.0488 3164159564.1172 1777.0000
[2019-04-10 12:10:00,902] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3098 2927393583.6156 1338.0000
[2019-04-10 12:10:01,068] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.4158 2842250142.9897 1131.0000
[2019-04-10 12:10:01,146] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4768 3007723564.7470 1766.0000
[2019-04-10 12:10:01,147] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6185 2779263580.8788 933.0000
[2019-04-10 12:10:02,162] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 600000, evaluation results [600000.0, 7883.048806120166, 3164159564.117182, 1777.0, 8254.30981308413, 2927393583.615589, 1338.0, 8660.61847634223, 2779263580.878825, 933.0, 7997.476797878408, 3007723564.746969, 1766.0, 8496.415766865775, 2842250142.989735, 1131.0]
[2019-04-10 12:10:14,093] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.1618508e-18 1.0000000e+00 4.4238750e-20 6.7827642e-19 6.2593093e-27], sum to 1.0000
[2019-04-10 12:10:14,110] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2607
[2019-04-10 12:10:14,113] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 45.66666666666667, 1.0, 2.0, 0.9824697540796304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1540358.846843579, 1540358.846843579, 317208.1901931292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6790800.0000, 
sim time next is 6791400.0000, 
raw observation next is [29.35, 46.0, 1.0, 2.0, 0.9997152701477372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1565343.841780062, 1565343.841780062, 322826.0142518128], 
processed observation next is [1.0, 0.6086956521739131, 0.590047393364929, 0.46, 1.0, 1.0, 0.9996569519852255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.43481773382779504, 0.43481773382779504, 0.4818298720176311], 
reward next is 0.5182, 
noisyNet noise sample is [array([-0.590299], dtype=float32), 0.40067446]. 
=============================================
[2019-04-10 12:10:24,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.53692381e-20 1.00000000e+00 1.22885695e-24 6.57600999e-27
 9.66193633e-34], sum to 1.0000
[2019-04-10 12:10:24,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2252
[2019-04-10 12:10:24,188] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 74.0, 1.0, 2.0, 0.3322379970240978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519787.4987548686, 519787.4987548686, 168418.6717349846], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6824400.0000, 
sim time next is 6825000.0000, 
raw observation next is [24.06666666666667, 74.5, 1.0, 2.0, 0.3323537443047103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 519854.7091726194, 519854.7091726201, 168421.1193799338], 
processed observation next is [1.0, 1.0, 0.3396524486571882, 0.745, 1.0, 1.0, 0.19560692084904857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14440408588128317, 0.14440408588128337, 0.2513748050446773], 
reward next is 0.7486, 
noisyNet noise sample is [array([-1.9712029], dtype=float32), 0.42139637]. 
=============================================
[2019-04-10 12:10:24,256] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.10418 ]
 [75.121895]
 [75.140755]
 [75.15601 ]
 [75.14703 ]], R is [[75.08066559]
 [75.07848358]
 [75.07627106]
 [75.07409668]
 [75.07206726]].
[2019-04-10 12:10:25,950] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5702724e-22 1.0000000e+00 6.7869586e-26 7.7548123e-29 2.8737675e-34], sum to 1.0000
[2019-04-10 12:10:25,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-10 12:10:25,999] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.66666666666667, 1.0, 2.0, 0.4149436429970368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611070.153209249, 611070.153209249, 175341.5404123613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6930600.0000, 
sim time next is 6931200.0000, 
raw observation next is [24.0, 89.33333333333334, 1.0, 2.0, 0.4156428271130365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611545.8753499115, 611545.8753499115, 175370.8192671171], 
processed observation next is [0.0, 0.21739130434782608, 0.3364928909952607, 0.8933333333333334, 1.0, 1.0, 0.2959552133892006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1698738542638643, 0.1698738542638643, 0.2617474914434584], 
reward next is 0.7383, 
noisyNet noise sample is [array([1.7447702], dtype=float32), -0.16536102]. 
=============================================
[2019-04-10 12:10:26,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5598898e-21 1.0000000e+00 4.5110629e-26 3.1177279e-29 8.2606631e-35], sum to 1.0000
[2019-04-10 12:10:26,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9687
[2019-04-10 12:10:26,201] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 67.33333333333334, 1.0, 2.0, 0.379459517283768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573201.4065560342, 573201.4065560342, 172320.4791446481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6900000.0000, 
sim time next is 6900600.0000, 
raw observation next is [26.46666666666667, 68.16666666666666, 1.0, 2.0, 0.3826662405196189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 576728.5518387468, 576728.5518387475, 172593.8361468513], 
processed observation next is [0.0, 0.8695652173913043, 0.45339652448657203, 0.6816666666666665, 1.0, 1.0, 0.2562243861682155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.160202375510763, 0.1602023755107632, 0.2576027405176885], 
reward next is 0.7424, 
noisyNet noise sample is [array([0.44068104], dtype=float32), -0.81304866]. 
=============================================
[2019-04-10 12:10:42,588] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1063968e-15 1.0000000e+00 2.8298319e-17 3.9515760e-16 5.8058415e-24], sum to 1.0000
[2019-04-10 12:10:42,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6786
[2019-04-10 12:10:42,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1679502.828748956 W.
[2019-04-10 12:10:42,604] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.8, 71.0, 1.0, 2.0, 0.4004596691585105, 1.0, 2.0, 0.4004596691585105, 1.0, 1.0, 0.6713802916003843, 6.9112, 6.9112, 170.5573041426782, 1679502.828748956, 1679502.828748956, 349667.6220424977], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7119000.0000, 
sim time next is 7119600.0000, 
raw observation next is [27.9, 70.66666666666667, 1.0, 2.0, 0.4047606078630022, 1.0, 2.0, 0.4047606078630022, 1.0, 2.0, 0.6792687877898442, 6.9112, 6.9112, 170.5573041426782, 1697554.983745339, 1697554.983745339, 352085.9421253865], 
processed observation next is [1.0, 0.391304347826087, 0.5213270142180094, 0.7066666666666667, 1.0, 1.0, 0.2828441058590388, 1.0, 1.0, 0.2828441058590388, 1.0, 1.0, 0.6088643753534685, 0.0, 0.0, 0.8375144448122397, 0.471543051040372, 0.471543051040372, 0.5255014061572932], 
reward next is 0.4745, 
noisyNet noise sample is [array([-2.1106787], dtype=float32), 1.4751831]. 
=============================================
[2019-04-10 12:10:43,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8235516e-20 1.0000000e+00 3.0840019e-23 1.9356838e-24 1.4342820e-30], sum to 1.0000
[2019-04-10 12:10:43,063] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8573
[2019-04-10 12:10:43,067] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 92.0, 1.0, 2.0, 0.4715724515352644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661588.0312807639, 661588.0312807639, 179604.2947353525], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7092000.0000, 
sim time next is 7092600.0000, 
raw observation next is [24.75, 92.16666666666667, 1.0, 2.0, 0.772374062045634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1084727.429762709, 1084727.429762708, 237972.5326901757], 
processed observation next is [1.0, 0.08695652173913043, 0.3720379146919432, 0.9216666666666667, 1.0, 1.0, 0.7257518819826915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3013131749340858, 0.30131317493408555, 0.35518288461220254], 
reward next is 0.6448, 
noisyNet noise sample is [array([1.5607283], dtype=float32), -1.3010525]. 
=============================================
[2019-04-10 12:10:43,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.4374959e-21 1.0000000e+00 3.3479937e-25 1.5334826e-27 1.5042079e-33], sum to 1.0000
[2019-04-10 12:10:43,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1351
[2019-04-10 12:10:43,749] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 83.33333333333334, 1.0, 2.0, 0.4754627256011617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665447.246026144, 665447.2460261446, 179979.2249451161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7158000.0000, 
sim time next is 7158600.0000, 
raw observation next is [26.05, 83.5, 1.0, 2.0, 0.4759218724960754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 665866.1073400227, 665866.1073400221, 180019.0258235217], 
processed observation next is [1.0, 0.8695652173913043, 0.43364928909952616, 0.835, 1.0, 1.0, 0.36858056927238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18496280759445075, 0.18496280759445058, 0.2686851131694354], 
reward next is 0.7313, 
noisyNet noise sample is [array([2.5825558], dtype=float32), -0.5777802]. 
=============================================
[2019-04-10 12:10:47,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.870231e-21 1.000000e+00 3.204308e-25 7.177565e-27 2.917143e-33], sum to 1.0000
[2019-04-10 12:10:47,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3115
[2019-04-10 12:10:47,251] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 92.0, 1.0, 2.0, 0.3669486473806905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560136.9997718777, 560136.999771877, 171360.1716835419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7245000.0000, 
sim time next is 7245600.0000, 
raw observation next is [22.53333333333333, 92.0, 1.0, 2.0, 0.3660153972335382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558996.2606867763, 558996.2606867768, 171270.7980575553], 
processed observation next is [1.0, 0.8695652173913043, 0.26698262243285936, 0.92, 1.0, 1.0, 0.23616312919703397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15527673907966008, 0.15527673907966022, 0.25562805680232137], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.44673917], dtype=float32), -0.105280496]. 
=============================================
[2019-04-10 12:10:50,047] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0988271e-20 1.0000000e+00 4.7898377e-25 1.0394688e-26 4.3690054e-33], sum to 1.0000
[2019-04-10 12:10:50,056] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-10 12:10:50,064] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 73.83333333333334, 1.0, 2.0, 0.3854389143278537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581655.8441599511, 581655.8441599518, 173056.399844698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7332600.0000, 
sim time next is 7333200.0000, 
raw observation next is [25.4, 74.0, 1.0, 2.0, 0.3807836591491494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575343.7028946707, 575343.7028946707, 172514.4012383298], 
processed observation next is [1.0, 0.9130434782608695, 0.4028436018957346, 0.74, 1.0, 1.0, 0.2539562158423487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15981769524851963, 0.15981769524851963, 0.257484180952731], 
reward next is 0.7425, 
noisyNet noise sample is [array([2.578274], dtype=float32), -1.5830075]. 
=============================================
[2019-04-10 12:10:51,148] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5562705e-20 1.0000000e+00 8.2371404e-24 3.0812337e-26 3.9909614e-32], sum to 1.0000
[2019-04-10 12:10:51,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9788
[2019-04-10 12:10:51,158] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 82.0, 1.0, 2.0, 0.3771318744214461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.9689907632, 571243.9689907638, 172194.9392325905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7357800.0000, 
sim time next is 7358400.0000, 
raw observation next is [24.1, 83.0, 1.0, 2.0, 0.3776804476273991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570742.1987594216, 570742.1987594216, 172110.4719952524], 
processed observation next is [1.0, 0.17391304347826086, 0.3412322274881518, 0.83, 1.0, 1.0, 0.25021740677999893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15853949965539488, 0.15853949965539488, 0.25688130148545135], 
reward next is 0.7431, 
noisyNet noise sample is [array([0.16236159], dtype=float32), 1.2494045]. 
=============================================
[2019-04-10 12:10:55,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9946784e-21 1.0000000e+00 1.2162088e-25 4.0731726e-31 8.0565100e-35], sum to 1.0000
[2019-04-10 12:10:55,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9136
[2019-04-10 12:10:55,344] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 95.0, 1.0, 2.0, 0.3359611040407641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521705.286226209, 521705.2862262096, 168465.0910193421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7459200.0000, 
sim time next is 7459800.0000, 
raw observation next is [21.7, 94.66666666666667, 1.0, 2.0, 0.3381506709639861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524440.2747357157, 524440.2747357157, 168662.9221081334], 
processed observation next is [0.0, 0.34782608695652173, 0.2274881516587678, 0.9466666666666668, 1.0, 1.0, 0.20259116983612782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14567785409325437, 0.14567785409325437, 0.2517357046390051], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.2342599], dtype=float32), -2.3102021]. 
=============================================
[2019-04-10 12:11:00,222] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-10 12:11:00,223] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:11:00,224] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:00,225] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:11:00,226] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:11:00,226] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:00,226] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:11:00,227] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:00,228] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:00,228] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:11:00,230] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:00,246] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run26
[2019-04-10 12:11:00,246] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run26
[2019-04-10 12:11:00,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run26
[2019-04-10 12:11:00,297] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run26
[2019-04-10 12:11:00,298] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run26
[2019-04-10 12:11:03,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:03,432] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.45, 52.5, 1.0, 2.0, 0.3215416507102594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508403.7319723128, 508403.7319723135, 167657.9690922488]
[2019-04-10 12:11:03,433] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:03,436] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5595210e-21 1.0000000e+00 1.0662407e-25 7.9214356e-30 1.2003066e-34], sampled 0.7366661737503813
[2019-04-10 12:11:03,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:03,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.33181988333333, 90.39907544333334, 1.0, 2.0, 0.3519516092343092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543118.341228168, 543118.341228168, 170100.8386489795]
[2019-04-10 12:11:03,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:11:03,789] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.0490261e-21 1.0000000e+00 2.0362149e-25 1.7657666e-29 2.6636874e-34], sampled 0.5792231549385534
[2019-04-10 12:11:06,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:06,812] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.93333333333334, 62.00000000000001, 1.0, 2.0, 0.2573484980919545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424412.083650167, 424412.083650167, 161611.0935667595]
[2019-04-10 12:11:06,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:06,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.2592362e-21 1.0000000e+00 2.0935870e-25 1.0078244e-29 2.9558432e-34], sampled 0.5665577567960397
[2019-04-10 12:11:16,193] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:16,194] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.33333333333334, 83.33333333333334, 1.0, 2.0, 0.2879557566817844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464933.0057404468, 464933.0057404474, 164607.5044064838]
[2019-04-10 12:11:16,194] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:11:16,197] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1102159e-20 1.0000000e+00 8.2318592e-25 2.7324028e-28 1.9701366e-33], sampled 0.5504245706040696
[2019-04-10 12:11:21,096] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:21,097] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.4, 94.0, 1.0, 2.0, 0.4345920707625124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638205.5416204906, 638205.5416204906, 177916.4551400688]
[2019-04-10 12:11:21,097] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:21,099] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.9593335e-21 1.0000000e+00 5.8339454e-25 1.6206878e-27 8.3051238e-34], sampled 0.9477109658773153
[2019-04-10 12:11:29,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:29,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.03333333333333, 90.66666666666667, 1.0, 2.0, 0.9115302520775497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565101188, 1274066.850492621, 1274066.850492622, 273129.8339025305]
[2019-04-10 12:11:29,363] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:11:29,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4190282e-19 1.0000000e+00 1.9033727e-22 6.1662744e-24 1.8087777e-30], sampled 0.46791910619055777
[2019-04-10 12:11:33,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:33,357] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4824658461015852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674163.2854537941, 674163.2854537941, 180894.6835960273]
[2019-04-10 12:11:33,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:33,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1705390e-20 1.0000000e+00 9.8471732e-25 3.3394171e-28 1.8570662e-33], sampled 0.784064045411554
[2019-04-10 12:11:46,753] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:46,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.54083837333333, 94.34551646333333, 1.0, 2.0, 0.5350825950537156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747712.0452943261, 747712.0452943267, 189273.5679428883]
[2019-04-10 12:11:46,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:11:46,758] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.2454023e-20 1.0000000e+00 1.1416348e-23 7.2479227e-26 4.6188006e-32], sampled 0.05028371758540795
[2019-04-10 12:11:49,266] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:49,267] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 84.0, 1.0, 2.0, 0.9793588156964964, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992255305599, 6.9112, 168.9123159483817, 2266111.277928011, 2198862.644541927, 457011.0687618355]
[2019-04-10 12:11:49,268] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:49,270] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4721162e-10 9.9955696e-01 1.3958236e-10 4.4303239e-04 1.3828796e-16], sampled 0.31601244575998133
[2019-04-10 12:11:49,271] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2266111.277928011 W.
[2019-04-10 12:11:52,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:11:52,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.33333333333334, 75.66666666666667, 1.0, 2.0, 0.6359914710542098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888778.7710656532, 888778.7710656532, 207641.8183919041]
[2019-04-10 12:11:52,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:52,426] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2231630e-20 1.0000000e+00 1.6133806e-24 1.1200944e-26 2.8553781e-33], sampled 0.0531142946082207
[2019-04-10 12:12:20,888] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12286074], dtype=float32), 0.09304623]
[2019-04-10 12:12:20,889] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.46666666666667, 58.0, 1.0, 2.0, 0.4339306067462285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684189.0145917134, 684189.014591714, 182969.7953383106]
[2019-04-10 12:12:20,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:12:20,892] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.3015839e-21 1.0000000e+00 5.3045172e-25 1.4535171e-28 8.7423618e-34], sampled 0.6208795898156887
[2019-04-10 12:12:20,979] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.0709 3163892471.3588 1776.0000
[2019-04-10 12:12:21,275] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3098 2927393583.6156 1338.0000
[2019-04-10 12:12:21,453] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3823 3007801364.9131 1766.0000
[2019-04-10 12:12:21,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3928 2842574156.6246 1131.0000
[2019-04-10 12:12:21,495] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-04-10 12:12:22,508] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 625000, evaluation results [625000.0, 7885.070892373251, 3163892471.35884, 1776.0, 8254.30981308413, 2927393583.615589, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7997.382315553678, 3007801364.91306, 1766.0, 8497.392829644317, 2842574156.6245975, 1131.0]
[2019-04-10 12:12:30,602] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5471860e-20 1.0000000e+00 4.5102264e-23 1.1010409e-23 2.9693302e-31], sum to 1.0000
[2019-04-10 12:12:30,607] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4200
[2019-04-10 12:12:30,617] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 90.0, 1.0, 2.0, 0.4780594260313317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668004.1331067557, 668004.1331067551, 180229.4805068943], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7689600.0000, 
sim time next is 7690200.0000, 
raw observation next is [25.15, 90.33333333333333, 1.0, 2.0, 0.4766447056817076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666026.6900457671, 666026.6900457671, 180017.383691852], 
processed observation next is [1.0, 0.0, 0.3909952606635071, 0.9033333333333333, 1.0, 1.0, 0.3694514526285633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18500741390160197, 0.18500741390160197, 0.2686826622266448], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.1098424], dtype=float32), 0.12794141]. 
=============================================
[2019-04-10 12:12:33,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3314402e-20 1.0000000e+00 7.6755748e-24 3.0621141e-24 7.9763110e-32], sum to 1.0000
[2019-04-10 12:12:33,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3370
[2019-04-10 12:12:33,748] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666666, 74.66666666666666, 1.0, 2.0, 0.5007804407418393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699763.2198669618, 699763.2198669618, 183719.2481085293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7756800.0000, 
sim time next is 7757400.0000, 
raw observation next is [28.38333333333333, 75.83333333333334, 1.0, 2.0, 0.5038040579441986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703989.6573216347, 703989.6573216347, 184194.8833086832], 
processed observation next is [1.0, 0.782608695652174, 0.5442338072669825, 0.7583333333333334, 1.0, 1.0, 0.402173563788191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19555268258934297, 0.19555268258934297, 0.2749177362816167], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.7639119], dtype=float32), 1.6903143]. 
=============================================
[2019-04-10 12:12:34,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7598851e-10 6.2654489e-01 4.6051921e-10 3.7345508e-01 5.6086930e-15], sum to 1.0000
[2019-04-10 12:12:34,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2756
[2019-04-10 12:12:34,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2294626.468720231 W.
[2019-04-10 12:12:34,679] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.3, 64.0, 1.0, 2.0, 0.8204632085344796, 1.0, 2.0, 0.8204632085344796, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2294626.468720231, 2294626.468720231, 429953.2130944829], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7731000.0000, 
sim time next is 7731600.0000, 
raw observation next is [31.36666666666667, 63.66666666666667, 1.0, 2.0, 0.8171000238369834, 1.0, 2.0, 0.8171000238369834, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2285211.898616864, 2285211.898616865, 428278.9183829559], 
processed observation next is [1.0, 0.4782608695652174, 0.6856240126382308, 0.6366666666666667, 1.0, 1.0, 0.7796385829361246, 1.0, 1.0, 0.7796385829361246, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6347810829491288, 0.6347810829491292, 0.6392222662432178], 
reward next is 0.3608, 
noisyNet noise sample is [array([-0.47388205], dtype=float32), 1.2226737]. 
=============================================
[2019-04-10 12:12:42,043] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1272150e-18 1.0000000e+00 2.1822620e-21 8.7601082e-22 2.2009058e-28], sum to 1.0000
[2019-04-10 12:12:42,044] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1470
[2019-04-10 12:12:42,064] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 92.33333333333334, 1.0, 2.0, 0.8849227226535293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1236855.230653404, 1236855.230653404, 265822.6574180614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7870800.0000, 
sim time next is 7871400.0000, 
raw observation next is [26.05, 92.0, 1.0, 2.0, 0.8139185253152822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137559.602306956, 1137559.602306956, 247354.6019092652], 
processed observation next is [1.0, 0.08695652173913043, 0.43364928909952616, 0.92, 1.0, 1.0, 0.775805452187087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3159887784185989, 0.3159887784185989, 0.3691859729989033], 
reward next is 0.6308, 
noisyNet noise sample is [array([0.23458411], dtype=float32), 0.9277953]. 
=============================================
[2019-04-10 12:12:43,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1659763e-10 9.8855853e-01 2.4452240e-10 1.1441476e-02 8.8669950e-16], sum to 1.0000
[2019-04-10 12:12:43,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3174
[2019-04-10 12:12:43,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2353980.989222294 W.
[2019-04-10 12:12:43,829] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.33333333333334, 1.0, 2.0, 0.5611106352372068, 1.0, 2.0, 0.5611106352372068, 1.0, 1.0, 0.971284257304611, 6.911199999999999, 6.9112, 170.5573041426782, 2353980.989222294, 2353980.989222294, 459325.4581505079], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7832400.0000, 
sim time next is 7833000.0000, 
raw observation next is [31.0, 64.16666666666666, 1.0, 2.0, 0.837358602933398, 1.0, 2.0, 0.837358602933398, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2341922.855453411, 2341922.855453411, 438463.3600346174], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.6416666666666666, 1.0, 1.0, 0.8040465095583109, 1.0, 1.0, 0.8040465095583109, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6505341265148364, 0.6505341265148364, 0.6544229254248021], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7357547], dtype=float32), 0.2891003]. 
=============================================
[2019-04-10 12:12:43,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[47.562042]
 [48.998028]
 [48.237507]
 [48.451195]
 [49.40618 ]], R is [[47.59453964]
 [47.43303299]
 [47.31401825]
 [46.84087753]
 [46.37247086]].
[2019-04-10 12:12:46,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:46,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:46,158] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-04-10 12:12:47,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:47,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:47,404] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-04-10 12:12:47,534] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:47,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:47,536] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-04-10 12:12:47,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:47,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:47,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-04-10 12:12:48,564] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:48,564] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:48,565] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-04-10 12:12:48,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:48,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:48,688] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-04-10 12:12:48,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:48,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:48,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-04-10 12:12:49,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,014] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-04-10 12:12:49,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-04-10 12:12:49,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-04-10 12:12:49,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-04-10 12:12:49,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-04-10 12:12:49,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-04-10 12:12:49,955] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:49,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:49,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-04-10 12:12:50,253] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:50,253] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:50,254] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run4
[2019-04-10 12:12:50,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:12:50,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:12:50,508] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-04-10 12:12:52,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.6842705e-18 1.0000000e+00 4.6510944e-21 3.4537190e-24 8.0041516e-29], sum to 1.0000
[2019-04-10 12:12:52,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7235
[2019-04-10 12:12:52,959] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 85.0, 1.0, 2.0, 0.3107733800305775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501288.572051924, 501288.572051924, 167191.282826826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 13800.0000, 
sim time next is 14400.0000, 
raw observation next is [21.2, 85.0, 1.0, 2.0, 0.3098742515674184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499538.4435189567, 499538.4435189574, 167065.2828317583], 
processed observation next is [1.0, 0.17391304347826086, 0.20379146919431282, 0.85, 1.0, 1.0, 0.16852319465954022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13876067875526577, 0.13876067875526596, 0.2493511684056094], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.7861483], dtype=float32), -0.40192455]. 
=============================================
[2019-04-10 12:13:15,256] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 12:13:15,258] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:13:15,259] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:13:15,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:15,261] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:13:15,261] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:15,262] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:15,262] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:13:15,263] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:13:15,265] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:15,267] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:15,275] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run27
[2019-04-10 12:13:15,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run27
[2019-04-10 12:13:15,293] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run27
[2019-04-10 12:13:15,293] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run27
[2019-04-10 12:13:15,343] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run27
[2019-04-10 12:13:33,144] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12460577], dtype=float32), 0.09111756]
[2019-04-10 12:13:33,145] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.2, 72.33333333333333, 1.0, 2.0, 0.9312153378483143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1301598.014751764, 1301598.014751764, 278665.2702844763]
[2019-04-10 12:13:33,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:13:33,149] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6385314e-19 1.0000000e+00 3.9402895e-22 3.2203148e-23 2.3324074e-30], sampled 0.04032548621257359
[2019-04-10 12:14:09,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12460577], dtype=float32), 0.09111756]
[2019-04-10 12:14:09,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 79.0, 1.0, 2.0, 0.6238121772107247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 871751.5926885338, 871751.5926885332, 205266.4086801062]
[2019-04-10 12:14:09,758] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:14:09,758] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1515612e-23 1.0000000e+00 3.3495361e-27 1.9774356e-30 1.9757105e-36], sampled 0.029165050098622913
[2019-04-10 12:14:14,316] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12460577], dtype=float32), 0.09111756]
[2019-04-10 12:14:14,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.01166311, 49.31993996, 1.0, 2.0, 0.5641543040559405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788351.254583219, 788351.254583219, 194254.8468621274]
[2019-04-10 12:14:14,319] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:14:14,322] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.9034730e-21 1.0000000e+00 6.8114727e-25 1.4830287e-27 5.8477106e-34], sampled 0.790317217185902
[2019-04-10 12:14:20,429] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12460577], dtype=float32), 0.09111756]
[2019-04-10 12:14:20,429] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.88609202333333, 68.15286209, 1.0, 2.0, 0.5693139809949268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795564.1058894746, 795564.1058894746, 195167.2068809414]
[2019-04-10 12:14:20,430] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:14:20,431] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.0070127e-21 1.0000000e+00 7.5149759e-25 6.8337666e-28 7.5360989e-34], sampled 0.837499279114681
[2019-04-10 12:14:26,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12460577], dtype=float32), 0.09111756]
[2019-04-10 12:14:26,076] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.463180405, 76.860595765, 1.0, 2.0, 0.3634236251417403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557207.6677519004, 557207.6677519011, 171181.2250012064]
[2019-04-10 12:14:26,077] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:14:26,079] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5354515e-21 1.0000000e+00 2.7107605e-25 1.4734073e-29 2.2742393e-34], sampled 0.0033151440132872523
[2019-04-10 12:14:35,759] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12460577], dtype=float32), 0.09111756]
[2019-04-10 12:14:35,760] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.17078584333333, 93.04944754333333, 1.0, 2.0, 0.7343514177944086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026300.226881347, 1026300.226881347, 228439.5303471409]
[2019-04-10 12:14:35,761] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:14:35,762] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2406166e-19 1.0000000e+00 3.4906218e-23 1.2165985e-25 1.3755988e-31], sampled 0.6499116732180567
[2019-04-10 12:14:35,891] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2879 2927342514.0358 1338.0000
[2019-04-10 12:14:36,075] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.2445 3164044324.1527 1777.0000
[2019-04-10 12:14:36,281] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0444 2842348130.8811 1130.0000
[2019-04-10 12:14:36,295] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-04-10 12:14:36,300] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9509 2779214995.4704 933.0000
[2019-04-10 12:14:37,312] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 650000, evaluation results [650000.0, 7882.244499250992, 3164044324.1526756, 1777.0, 8254.287911454629, 2927342514.035813, 1338.0, 8659.950915139985, 2779214995.470401, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8498.04442055128, 2842348130.8810706, 1130.0]
[2019-04-10 12:14:40,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9911331e-21 1.0000000e+00 1.2420150e-24 2.3519230e-28 2.0192897e-33], sum to 1.0000
[2019-04-10 12:14:40,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7610
[2019-04-10 12:14:40,756] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 80.0, 1.0, 2.0, 0.2208335492003436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 366683.8036039399, 366683.8036039399, 157923.2939653766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 630000.0000, 
sim time next is 630600.0000, 
raw observation next is [19.83333333333334, 79.0, 1.0, 2.0, 0.2216664275007999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367760.7904005787, 367760.7904005787, 158041.4913121994], 
processed observation next is [1.0, 0.30434782608695654, 0.13902053712480286, 0.79, 1.0, 1.0, 0.06224870783228902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10215577511127186, 0.10215577511127186, 0.23588282285402895], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.45631683], dtype=float32), 0.5083241]. 
=============================================
[2019-04-10 12:14:47,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3213340e-19 1.0000000e+00 6.0981125e-23 2.8167363e-24 9.5606076e-32], sum to 1.0000
[2019-04-10 12:14:47,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6648
[2019-04-10 12:14:47,506] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 57.0, 1.0, 2.0, 0.5367944882323397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872424.6141691782, 872424.6141691782, 202478.6110542036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 732000.0000, 
sim time next is 732600.0000, 
raw observation next is [25.0, 56.5, 1.0, 2.0, 0.5711455446654399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928565.8693541674, 928565.8693541674, 209314.956994964], 
processed observation next is [1.0, 0.4782608695652174, 0.38388625592417064, 0.565, 1.0, 1.0, 0.4833078851390842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2579349637094909, 0.2579349637094909, 0.3124103835745732], 
reward next is 0.6876, 
noisyNet noise sample is [array([2.2142406], dtype=float32), 2.0312889]. 
=============================================
[2019-04-10 12:14:49,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6281531e-21 1.0000000e+00 1.6722824e-25 1.9781176e-27 1.4776419e-34], sum to 1.0000
[2019-04-10 12:14:49,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3819
[2019-04-10 12:14:49,991] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 63.5, 1.0, 2.0, 0.2440252528498042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402372.2842304306, 402372.28423043, 160297.3331151968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 757800.0000, 
sim time next is 758400.0000, 
raw observation next is [22.5, 65.0, 1.0, 2.0, 0.2450190389611186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403835.3042693184, 403835.3042693177, 160399.4502776505], 
processed observation next is [1.0, 0.782608695652174, 0.2654028436018958, 0.65, 1.0, 1.0, 0.09038438429050433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.112176473408144, 0.11217647340814381, 0.23940216459350822], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.45067734], dtype=float32), 1.1743149]. 
=============================================
[2019-04-10 12:14:54,697] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.19592295e-20 1.00000000e+00 9.23307209e-26 3.17312381e-30
 9.16218638e-36], sum to 1.0000
[2019-04-10 12:14:54,700] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-10 12:14:54,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 71.0, 1.0, 2.0, 0.2884861582285086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462428.4785197963, 462428.4785197969, 164428.0509224132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 811200.0000, 
sim time next is 811800.0000, 
raw observation next is [23.65, 70.0, 1.0, 2.0, 0.2890485833937786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463072.8546722764, 463072.8546722764, 164469.9306947154], 
processed observation next is [0.0, 0.391304347826087, 0.31990521327014215, 0.7, 1.0, 1.0, 0.1434320281852754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1286313485200768, 0.1286313485200768, 0.24547750849957525], 
reward next is 0.7545, 
noisyNet noise sample is [array([-1.6201063], dtype=float32), -0.76590383]. 
=============================================
[2019-04-10 12:15:01,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0611302e-21 1.0000000e+00 6.0615283e-26 4.4311850e-29 1.4066067e-35], sum to 1.0000
[2019-04-10 12:15:01,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6452
[2019-04-10 12:15:01,786] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 71.0, 1.0, 2.0, 0.3134652038833068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493740.0374303367, 493740.0374303367, 166515.7331143028], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 921600.0000, 
sim time next is 922200.0000, 
raw observation next is [24.25, 71.5, 1.0, 2.0, 0.31413214871874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494627.0220118128, 494627.0220118128, 166577.6945062599], 
processed observation next is [0.0, 0.6956521739130435, 0.3483412322274882, 0.715, 1.0, 1.0, 0.17365319122739759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13739639500328132, 0.13739639500328132, 0.2486234246362088], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.2746925], dtype=float32), 0.5422717]. 
=============================================
[2019-04-10 12:15:21,750] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.5085405e-21 1.0000000e+00 1.8602654e-24 1.9962130e-26 1.7376448e-33], sum to 1.0000
[2019-04-10 12:15:21,755] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6617
[2019-04-10 12:15:21,774] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 84.5, 1.0, 2.0, 0.354696437771207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546798.935816469, 546798.935816469, 170390.5290413429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1204200.0000, 
sim time next is 1204800.0000, 
raw observation next is [23.1, 85.33333333333334, 1.0, 2.0, 0.3554413071659422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547751.2515474142, 547751.2515474149, 170464.3484644801], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.8533333333333334, 1.0, 1.0, 0.22342326164571347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15215312542983728, 0.15215312542983747, 0.2544244006932539], 
reward next is 0.7456, 
noisyNet noise sample is [array([1.9820588], dtype=float32), -1.8196266]. 
=============================================
[2019-04-10 12:15:25,040] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3800137e-20 1.0000000e+00 3.2694555e-23 3.1862453e-24 9.2425230e-32], sum to 1.0000
[2019-04-10 12:15:25,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3493
[2019-04-10 12:15:25,054] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4583264292921354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649845.9318804906, 649845.9318804899, 178539.1992395844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1296000.0000, 
sim time next is 1296600.0000, 
raw observation next is [24.3, 94.00000000000001, 1.0, 2.0, 0.4584754386161387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650208.60854035, 650208.60854035, 178580.432649693], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.9400000000000002, 1.0, 1.0, 0.34756076941703457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18061350237231943, 0.18061350237231943, 0.26653795917864626], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.71394986], dtype=float32), 0.5646564]. 
=============================================
[2019-04-10 12:15:30,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5173048e-21 1.0000000e+00 1.0942905e-24 2.7910497e-28 2.1641845e-33], sum to 1.0000
[2019-04-10 12:15:30,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5216
[2019-04-10 12:15:30,054] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.3987073290933262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589124.01544938, 589124.01544938, 173357.6767942892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1418400.0000, 
sim time next is 1419000.0000, 
raw observation next is [23.86666666666667, 88.83333333333334, 1.0, 2.0, 0.3984034084893297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589359.5372362196, 589359.5372362196, 173400.918414338], 
processed observation next is [0.0, 0.43478260869565216, 0.33017377567140627, 0.8883333333333334, 1.0, 1.0, 0.2751848295052165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16371098256561656, 0.16371098256561656, 0.25880734091692237], 
reward next is 0.7412, 
noisyNet noise sample is [array([-0.34973952], dtype=float32), -0.270273]. 
=============================================
[2019-04-10 12:15:30,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.04525 ]
 [76.04399 ]
 [76.0908  ]
 [76.130295]
 [76.161   ]], R is [[76.03689575]
 [76.01778412]
 [75.99990082]
 [75.98322296]
 [75.967659  ]].
[2019-04-10 12:15:30,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5573180e-21 1.0000000e+00 1.8047169e-24 4.7295844e-29 9.9745728e-34], sum to 1.0000
[2019-04-10 12:15:30,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7099
[2019-04-10 12:15:30,590] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 93.0, 1.0, 2.0, 0.3300264527944478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 515928.093286766, 515928.0932867654, 168107.1509076998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1407600.0000, 
sim time next is 1408200.0000, 
raw observation next is [21.68333333333334, 92.5, 1.0, 2.0, 0.3308154636927673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516777.1966169412, 516777.1966169412, 168163.2932813919], 
processed observation next is [0.0, 0.30434782608695654, 0.22669826224328635, 0.925, 1.0, 1.0, 0.19375357071417748, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14354922128248365, 0.14354922128248365, 0.2509899899722267], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.2204156], dtype=float32), 1.3537296]. 
=============================================
[2019-04-10 12:15:31,964] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-10 12:15:31,966] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:15:31,967] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:15:31,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:31,969] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:31,968] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:15:31,970] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:15:31,972] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:31,973] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:31,971] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:15:31,976] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:31,988] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run28
[2019-04-10 12:15:31,989] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run28
[2019-04-10 12:15:32,010] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run28
[2019-04-10 12:15:32,011] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run28
[2019-04-10 12:15:32,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run28
[2019-04-10 12:15:52,711] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12405269], dtype=float32), 0.09172681]
[2019-04-10 12:15:52,712] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.7, 87.0, 1.0, 2.0, 0.6380042621710216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 891592.7661912874, 891592.7661912867, 208029.0132338809]
[2019-04-10 12:15:52,715] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:15:52,717] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1749597e-19 1.0000000e+00 4.9611169e-23 3.8638062e-25 1.1635342e-31], sampled 0.9382397698310734
[2019-04-10 12:16:17,342] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12405269], dtype=float32), 0.09172681]
[2019-04-10 12:16:17,342] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.016659375, 84.257856045, 1.0, 2.0, 0.6160765955111605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 860937.0573990844, 860937.0573990839, 203779.7427170428]
[2019-04-10 12:16:17,343] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:16:17,348] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.0696233e-20 1.0000000e+00 1.0775887e-23 6.0239551e-26 1.7923190e-32], sampled 0.3731007771707122
[2019-04-10 12:16:17,971] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12405269], dtype=float32), 0.09172681]
[2019-04-10 12:16:17,971] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.21614821166667, 74.95212895000002, 1.0, 2.0, 0.6231933924078005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870886.5121177881, 870886.5121177881, 205145.5940643171]
[2019-04-10 12:16:17,972] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:16:17,974] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9835426e-20 1.0000000e+00 4.9201223e-24 2.5195541e-26 5.4650966e-33], sampled 0.8057827886266176
[2019-04-10 12:16:40,344] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12405269], dtype=float32), 0.09172681]
[2019-04-10 12:16:40,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.41835225333333, 69.99767238333334, 1.0, 2.0, 0.5191513125371655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725442.4376575081, 725442.4376575075, 186650.8113576467]
[2019-04-10 12:16:40,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:16:40,348] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6102113e-20 1.0000000e+00 4.6637015e-24 3.1677360e-26 4.2640870e-33], sampled 0.2789472594930559
[2019-04-10 12:16:46,654] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12405269], dtype=float32), 0.09172681]
[2019-04-10 12:16:46,655] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.066327315, 71.534062765, 1.0, 2.0, 0.4318710595912179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 628288.5272968095, 628288.5272968088, 176791.1345398859]
[2019-04-10 12:16:46,656] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:16:46,657] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.8065587e-21 1.0000000e+00 7.7243889e-25 1.0374676e-27 5.1073558e-34], sampled 0.7432207855665451
[2019-04-10 12:16:52,334] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2118 2927459259.5787 1338.0000
[2019-04-10 12:16:52,465] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.0903 3164247787.9300 1776.0000
[2019-04-10 12:16:52,512] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5202 2779263176.1304 933.0000
[2019-04-10 12:16:52,623] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6912 2842517602.1483 1129.0000
[2019-04-10 12:16:52,667] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6740 3007829833.3403 1765.0000
[2019-04-10 12:16:53,682] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 675000, evaluation results [675000.0, 7883.090296756506, 3164247787.9299717, 1776.0, 8254.211789258557, 2927459259.5787225, 1338.0, 8660.520172588904, 2779263176.1304064, 933.0, 7997.674047326066, 3007829833.3403144, 1765.0, 8498.69120766137, 2842517602.1483355, 1129.0]
[2019-04-10 12:17:04,172] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2760490e-19 1.0000000e+00 1.0540011e-22 4.0957015e-24 1.3638483e-30], sum to 1.0000
[2019-04-10 12:17:04,176] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4207
[2019-04-10 12:17:04,271] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 99.00000000000001, 1.0, 2.0, 0.5172756652396943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747466.6913259211, 747466.6913259217, 189472.1891914051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1650000.0000, 
sim time next is 1650600.0000, 
raw observation next is [23.25, 99.0, 1.0, 2.0, 0.4681029145885947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675971.529121182, 675971.5291211813, 181536.7750607812], 
processed observation next is [1.0, 0.08695652173913043, 0.30094786729857825, 0.99, 1.0, 1.0, 0.3591601380585479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18776986920032834, 0.18776986920032815, 0.2709504105384794], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.3036948], dtype=float32), 0.113963194]. 
=============================================
[2019-04-10 12:17:07,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8128119e-16 1.0000000e+00 5.7999932e-18 7.0031268e-17 2.3861588e-25], sum to 1.0000
[2019-04-10 12:17:07,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7591
[2019-04-10 12:17:07,660] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 85.0, 1.0, 2.0, 0.8176992240782671, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129563573426, 1142846.476253308, 1142846.476253309, 248299.8620626072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1686600.0000, 
sim time next is 1687200.0000, 
raw observation next is [26.96666666666667, 84.66666666666667, 1.0, 2.0, 0.8939736225011075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103927, 1249513.104464605, 1249513.104464605, 268283.6770599128], 
processed observation next is [1.0, 0.5217391304347826, 0.47709320695102697, 0.8466666666666667, 1.0, 1.0, 0.8722573765073584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451521146, 0.34708697346239026, 0.34708697346239026, 0.4004233985968848], 
reward next is 0.5996, 
noisyNet noise sample is [array([1.5502735], dtype=float32), -2.2116828]. 
=============================================
[2019-04-10 12:17:19,939] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3672623e-15 1.0000000e+00 1.2930000e-16 3.8202973e-15 1.0816206e-23], sum to 1.0000
[2019-04-10 12:17:19,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2556
[2019-04-10 12:17:19,979] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 86.66666666666666, 1.0, 2.0, 0.5779554883924458, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9667409273744382, 6.911199999999999, 6.9112, 168.9129564104247, 1615903.406832011, 1615903.406832011, 345573.4100126667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [25.8, 86.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.67467636098535, 6.9112, 168.9087467111892, 1995748.843627454, 1454125.935461304, 311346.7977911989], 
processed observation next is [1.0, 0.4782608695652174, 0.42180094786729866, 0.8633333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.07634763609853498, 0.0, 0.829419273110771, 0.5543746787854039, 0.4039238709614733, 0.4646967131211924], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7257801], dtype=float32), 0.09521118]. 
=============================================
[2019-04-10 12:17:19,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[57.18239 ]
 [58.702137]
 [63.59241 ]
 [64.560295]
 [65.136566]], R is [[57.49632263]
 [56.92136002]
 [56.82370377]
 [56.25546646]
 [56.24207687]].
[2019-04-10 12:17:20,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2864384e-16 1.0000000e+00 5.1859453e-19 5.9611188e-17 3.6851506e-27], sum to 1.0000
[2019-04-10 12:17:20,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5016
[2019-04-10 12:17:20,551] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 85.0, 1.0, 2.0, 0.7470040219412213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043991.693378904, 1043991.693378904, 231315.588364542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1861200.0000, 
sim time next is 1861800.0000, 
raw observation next is [26.6, 84.66666666666667, 1.0, 2.0, 0.6201111546737215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866577.4575470068, 866577.4575470068, 204544.0130438117], 
processed observation next is [1.0, 0.5652173913043478, 0.4597156398104266, 0.8466666666666667, 1.0, 1.0, 0.5423025959924355, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2407159604297241, 0.2407159604297241, 0.30528957170718163], 
reward next is 0.6947, 
noisyNet noise sample is [array([-0.9333744], dtype=float32), 1.9290129]. 
=============================================
[2019-04-10 12:17:37,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1840129e-18 1.0000000e+00 1.7328356e-22 7.3324394e-25 2.3128090e-30], sum to 1.0000
[2019-04-10 12:17:37,846] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6335
[2019-04-10 12:17:37,851] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 74.83333333333334, 1.0, 2.0, 0.5460826286472423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763088.7591808831, 763088.7591808836, 191132.0399709794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [30.0, 74.0, 1.0, 2.0, 0.5477626437469091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765437.2362863581, 765437.2362863588, 191418.6536162527], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.74, 1.0, 1.0, 0.45513571535772174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21262145452398837, 0.21262145452398856, 0.2856994830093324], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.3721347], dtype=float32), 0.80322284]. 
=============================================
[2019-04-10 12:17:39,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0456688e-11 9.9979752e-01 4.5834092e-12 2.0250620e-04 2.3859037e-17], sum to 1.0000
[2019-04-10 12:17:39,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1798
[2019-04-10 12:17:39,727] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3100113.510040225 W.
[2019-04-10 12:17:39,729] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.8, 66.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 8.061009248676445, 6.9112, 168.9064937157545, 3100113.510040225, 2284430.46576951, 473355.7039939382], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [31.9, 65.66666666666666, 1.0, 2.0, 0.6341724131276409, 1.0, 1.0, 0.6341724131276409, 1.0, 2.0, 1.03, 6.991410284259087, 6.9112, 170.5573041426782, 2660817.371763602, 2603359.482403061, 500701.2119801512], 
processed observation next is [1.0, 0.5652173913043478, 0.7109004739336492, 0.6566666666666666, 1.0, 1.0, 0.5592438712381216, 1.0, 0.5, 0.5592438712381216, 1.0, 1.0, 1.0365853658536586, 0.00802102842590866, 0.0, 0.8375144448122397, 0.7391159366010005, 0.7231554117786281, 0.7473152417614197], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36794773], dtype=float32), 0.43135515]. 
=============================================
[2019-04-10 12:17:40,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3084226e-10 8.8067389e-01 7.0209910e-10 1.1932614e-01 4.2878724e-16], sum to 1.0000
[2019-04-10 12:17:40,149] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7607
[2019-04-10 12:17:40,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2608086.63073593 W.
[2019-04-10 12:17:40,159] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.9, 67.0, 1.0, 2.0, 0.9324267104048418, 1.0, 1.0, 0.9324267104048418, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2608086.63073593, 2608086.63073593, 489492.7877146461], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2217600.0000, 
sim time next is 2218200.0000, 
raw observation next is [31.88333333333333, 67.16666666666667, 1.0, 2.0, 0.6386744980108953, 1.0, 2.0, 0.6386744980108953, 1.0, 1.0, 1.03, 7.00020078823041, 6.9112, 170.5573041426782, 2679727.170681049, 2615972.28575101, 502427.0879082956], 
processed observation next is [1.0, 0.6956521739130435, 0.7101105845181673, 0.6716666666666667, 1.0, 1.0, 0.5646680698926448, 1.0, 1.0, 0.5646680698926448, 1.0, 0.5, 1.0365853658536586, 0.008900078823041025, 0.0, 0.8375144448122397, 0.7443686585225137, 0.7266589682641694, 0.7498911759825307], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1513595], dtype=float32), -0.39164457]. 
=============================================
[2019-04-10 12:17:42,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2885110e-19 1.0000000e+00 1.4599455e-22 1.4740646e-22 2.4122179e-30], sum to 1.0000
[2019-04-10 12:17:42,766] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-10 12:17:42,770] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 85.0, 1.0, 2.0, 0.5202554624139636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726985.8629665411, 726985.8629665418, 186830.5868944517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2244600.0000, 
sim time next is 2245200.0000, 
raw observation next is [27.1, 85.0, 1.0, 2.0, 0.5183004901568588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724253.125288148, 724253.1252881474, 186513.1048810808], 
processed observation next is [1.0, 1.0, 0.4834123222748816, 0.85, 1.0, 1.0, 0.4196391447672997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20118142369115222, 0.20118142369115205, 0.2783777684792251], 
reward next is 0.7216, 
noisyNet noise sample is [array([1.2741829], dtype=float32), -0.16040492]. 
=============================================
[2019-04-10 12:17:43,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5055201e-18 1.0000000e+00 1.0989403e-20 6.1758415e-21 5.5875429e-29], sum to 1.0000
[2019-04-10 12:17:43,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0854
[2019-04-10 12:17:43,409] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 86.0, 1.0, 2.0, 0.507832736013975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709621.0034056521, 709621.0034056515, 184832.4162623897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2251800.0000, 
sim time next is 2252400.0000, 
raw observation next is [26.6, 86.0, 1.0, 2.0, 0.5063659425956416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707570.6943086644, 707570.6943086638, 184599.488787411], 
processed observation next is [1.0, 0.043478260869565216, 0.4597156398104266, 0.86, 1.0, 1.0, 0.4052601718019778, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1965474150857401, 0.19654741508573995, 0.2755216250558373], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.7675744], dtype=float32), -0.52285355]. 
=============================================
[2019-04-10 12:17:44,798] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3835391e-10 9.9753881e-01 1.7230117e-10 2.4612609e-03 3.9030516e-16], sum to 1.0000
[2019-04-10 12:17:44,804] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6414
[2019-04-10 12:17:44,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1964479.316916527 W.
[2019-04-10 12:17:44,813] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.9, 62.5, 1.0, 2.0, 0.7025205949417138, 1.0, 2.0, 0.7025205949417138, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1964479.316916527, 1964479.316916528, 375206.4342809258], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2285400.0000, 
sim time next is 2286000.0000, 
raw observation next is [32.1, 62.0, 1.0, 2.0, 0.4754241697191621, 1.0, 2.0, 0.4754241697191621, 1.0, 1.0, 0.8254753783115181, 6.911199999999999, 6.9112, 170.5573041426782, 1994191.83077738, 1994191.83077738, 398536.4391133412], 
processed observation next is [1.0, 0.4782608695652174, 0.7203791469194314, 0.62, 1.0, 1.0, 0.36798092737248445, 1.0, 1.0, 0.36798092737248445, 1.0, 0.5, 0.7871650955018513, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5539421752159389, 0.5539421752159389, 0.5948305061393152], 
reward next is 0.4052, 
noisyNet noise sample is [array([-0.12850869], dtype=float32), -0.8593178]. 
=============================================
[2019-04-10 12:17:44,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[52.786884]
 [52.79994 ]
 [52.75762 ]
 [54.34344 ]
 [51.438408]], R is [[50.89052963]
 [50.82161713]
 [50.31340027]
 [49.8102684 ]
 [49.31216431]].
[2019-04-10 12:17:46,561] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3367791e-17 1.0000000e+00 5.1955460e-20 7.0222160e-20 2.4397902e-27], sum to 1.0000
[2019-04-10 12:17:46,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0780
[2019-04-10 12:17:46,568] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 81.5, 1.0, 2.0, 0.8924187544643202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1247338.578074932, 1247338.578074931, 267859.7291019847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2341800.0000, 
sim time next is 2342400.0000, 
raw observation next is [27.66666666666666, 81.66666666666667, 1.0, 2.0, 0.8003493707167393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1118584.911423456, 1118584.911423456, 243995.5322233662], 
processed observation next is [1.0, 0.08695652173913043, 0.5102685624012636, 0.8166666666666668, 1.0, 1.0, 0.759457073152698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31071803095095996, 0.31071803095095996, 0.3641724361542779], 
reward next is 0.6358, 
noisyNet noise sample is [array([-1.851439], dtype=float32), -2.2634592]. 
=============================================
[2019-04-10 12:17:46,603] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.23583640e-19 1.00000000e+00 7.03832302e-23 2.42544878e-23
 1.22154815e-30], sum to 1.0000
[2019-04-10 12:17:46,610] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9915
[2019-04-10 12:17:46,615] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 79.33333333333333, 1.0, 2.0, 0.5631109198253339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786892.6856301264, 786892.6856301264, 194074.9688327717], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2324400.0000, 
sim time next is 2325000.0000, 
raw observation next is [29.2, 79.66666666666667, 1.0, 2.0, 0.5618134814509129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 785078.9717929928, 785078.9717929922, 193847.6444563118], 
processed observation next is [1.0, 0.9130434782608695, 0.5829383886255924, 0.7966666666666667, 1.0, 1.0, 0.47206443548302757, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21807749216472022, 0.21807749216472005, 0.28932484247210716], 
reward next is 0.7107, 
noisyNet noise sample is [array([1.4078138], dtype=float32), -0.4920626]. 
=============================================
[2019-04-10 12:17:46,626] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.05415 ]
 [69.967964]
 [69.950386]
 [70.03788 ]
 [70.02166 ]], R is [[69.98512268]
 [69.99560547]
 [70.00533295]
 [70.01441193]
 [70.02308655]].
[2019-04-10 12:17:48,697] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6003130e-10 1.7196657e-01 6.2399924e-10 8.2803345e-01 2.3915929e-15], sum to 1.0000
[2019-04-10 12:17:48,704] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6317
[2019-04-10 12:17:48,708] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.06666666666667, 61.16666666666666, 1.0, 2.0, 0.9520770703412637, 1.0, 2.0, 0.9520770703412637, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2663109.144676618, 2663109.144676618, 500700.260565953], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [33.1, 61.0, 1.0, 2.0, 0.9496478659779094, 1.0, 2.0, 0.9496478659779094, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2656307.05500587, 2656307.055005871, 499302.447758615], 
processed observation next is [1.0, 0.6956521739130435, 0.7677725118483413, 0.61, 1.0, 1.0, 0.9393347782866378, 1.0, 1.0, 0.9393347782866378, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7378630708349638, 0.7378630708349643, 0.7452275339680822], 
reward next is 0.2548, 
noisyNet noise sample is [array([2.0733464], dtype=float32), 1.1675566]. 
=============================================
[2019-04-10 12:17:49,120] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 12:17:49,121] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:17:49,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:17:49,123] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:17:49,123] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:17:49,124] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:17:49,124] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:17:49,124] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:17:49,125] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:17:49,125] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:17:49,126] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:17:49,144] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run29
[2019-04-10 12:17:49,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run29
[2019-04-10 12:17:49,171] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run29
[2019-04-10 12:17:49,187] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run29
[2019-04-10 12:17:49,229] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run29
[2019-04-10 12:18:09,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12029572], dtype=float32), 0.09567583]
[2019-04-10 12:18:09,867] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.1, 90.5, 1.0, 2.0, 0.4275561303757981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 623101.0690679227, 623101.0690679227, 176314.3168440952]
[2019-04-10 12:18:09,869] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:18:09,872] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6998092e-19 1.0000000e+00 8.0601554e-23 8.1671706e-26 2.5057167e-31], sampled 0.26825356455017957
[2019-04-10 12:18:19,004] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12029572], dtype=float32), 0.09567583]
[2019-04-10 12:18:19,004] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.94372278333333, 91.655468455, 1.0, 2.0, 0.3904177562622753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590535.6868281501, 590535.6868281501, 173896.1281036013]
[2019-04-10 12:18:19,005] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:18:19,009] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2966890e-19 1.0000000e+00 1.1434790e-22 1.4493724e-25 3.7620714e-31], sampled 0.2274184923037017
[2019-04-10 12:18:42,364] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12029572], dtype=float32), 0.09567583]
[2019-04-10 12:18:42,365] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.0, 75.0, 1.0, 2.0, 0.5746147316975855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 802974.2190155948, 802974.2190155942, 196111.2417573527]
[2019-04-10 12:18:42,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:18:42,369] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8997543e-20 1.0000000e+00 1.4708061e-23 3.9056548e-25 4.6583660e-32], sampled 0.4782633974963181
[2019-04-10 12:18:51,922] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12029572], dtype=float32), 0.09567583]
[2019-04-10 12:18:51,922] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.76098505833333, 60.530971635, 1.0, 2.0, 0.5933635835726637, 0.0, 2.0, 0.0, 1.0, 1.0, 1.012448127152387, 6.911200000000001, 6.9112, 168.9124356731488, 1659016.511468141, 1659016.51146814, 359479.8342600444]
[2019-04-10 12:18:51,923] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:18:51,926] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8339391e-16 1.0000000e+00 4.5476761e-18 4.4347800e-17 1.7018535e-25], sampled 0.11822631691974705
[2019-04-10 12:18:51,927] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1659016.511468141 W.
[2019-04-10 12:19:04,170] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12029572], dtype=float32), 0.09567583]
[2019-04-10 12:19:04,172] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.7, 83.33333333333334, 1.0, 2.0, 0.5107821978147504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713743.8238773372, 713743.8238773372, 185301.7372166766]
[2019-04-10 12:19:04,173] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:19:04,174] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4510289e-19 1.0000000e+00 1.7714199e-22 3.5311312e-25 6.2736632e-31], sampled 0.9635394312261524
[2019-04-10 12:19:09,040] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9729 2779235900.9067 931.0000
[2019-04-10 12:19:09,095] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8004.1869 3006995477.3226 1751.0000
[2019-04-10 12:19:09,856] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8503.4415 2841865520.8146 1119.0000
[2019-04-10 12:19:09,898] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7907.3518 3161669991.0679 1716.0000
[2019-04-10 12:19:09,914] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.1557 2927022422.4520 1329.0000
[2019-04-10 12:19:10,928] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 700000, evaluation results [700000.0, 7907.351834707436, 3161669991.067928, 1716.0, 8258.155722411222, 2927022422.452023, 1329.0, 8659.972851363173, 2779235900.906661, 931.0, 8004.186853596081, 3006995477.3226213, 1751.0, 8503.44145626686, 2841865520.8146467, 1119.0]
[2019-04-10 12:19:12,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8188275e-19 1.0000000e+00 5.2387473e-22 2.1194004e-22 3.3309658e-30], sum to 1.0000
[2019-04-10 12:19:12,960] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8941
[2019-04-10 12:19:12,963] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 78.0, 1.0, 2.0, 0.5741745914193467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802358.9288674291, 802358.9288674291, 196033.3951373182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
processed observation next is [1.0, 0.9565217391304348, 0.6074249605055293, 0.7833333333333333, 1.0, 1.0, 0.4869236659138897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22286662728617787, 0.22286662728617787, 0.29257959931056793], 
reward next is 0.7074, 
noisyNet noise sample is [array([-1.1322361], dtype=float32), -1.0166781]. 
=============================================
[2019-04-10 12:19:18,220] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2261127e-17 1.0000000e+00 6.6214942e-20 6.2545814e-20 9.6361266e-28], sum to 1.0000
[2019-04-10 12:19:18,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1323
[2019-04-10 12:19:18,234] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.33333333333333, 1.0, 2.0, 0.7834872276045254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095005.878563655, 1095005.878563655, 239898.1733106222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2535000.0000, 
sim time next is 2535600.0000, 
raw observation next is [26.7, 91.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.976737658534601, 6.9112, 168.9073340352205, 2210174.842893007, 1454270.916980723, 311350.9248962943], 
processed observation next is [1.0, 0.34782608695652173, 0.46445497630331756, 0.9166666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.1065537658534601, 0.0, 0.8294123362250387, 0.6139374563591686, 0.4039641436057564, 0.46470287297954377], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8240539], dtype=float32), -0.424718]. 
=============================================
[2019-04-10 12:19:18,841] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.2486905e-10 9.8008609e-01 1.0581066e-10 1.9913869e-02 5.8528971e-16], sum to 1.0000
[2019-04-10 12:19:18,844] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0095
[2019-04-10 12:19:18,848] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2132152.484648907 W.
[2019-04-10 12:19:18,853] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.1, 76.0, 1.0, 2.0, 0.8836508634388401, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988416776539614, 6.9112, 168.9124331386289, 2132152.484648907, 2077372.415510309, 430212.7467995001], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2566800.0000, 
sim time next is 2567400.0000, 
raw observation next is [29.03333333333334, 76.5, 1.0, 2.0, 0.2391284697815141, 1.0, 1.0, 0.2391284697815141, 1.0, 2.0, 0.4115994521958894, 6.911199999999999, 6.9112, 170.5573041426782, 1002573.45544519, 1002573.455445191, 281470.9081613107], 
processed observation next is [1.0, 0.7391304347826086, 0.5750394944707744, 0.765, 1.0, 1.0, 0.08328731298977603, 1.0, 0.5, 0.08328731298977603, 1.0, 1.0, 0.2824383563364505, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.27849262651255274, 0.2784926265125531, 0.42010583307658317], 
reward next is 0.5799, 
noisyNet noise sample is [array([-0.23930682], dtype=float32), 0.42504987]. 
=============================================
[2019-04-10 12:19:23,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1859586e-19 1.0000000e+00 1.7402948e-22 1.4445765e-24 9.9591897e-31], sum to 1.0000
[2019-04-10 12:19:23,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6214
[2019-04-10 12:19:23,518] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 98.0, 1.0, 2.0, 0.4122433452441483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 605038.3928123197, 605038.3928123191, 174713.2652325692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2680800.0000, 
sim time next is 2681400.0000, 
raw observation next is [23.0, 99.0, 1.0, 2.0, 0.4180163733061256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610637.2915071364, 610637.2915071371, 175157.7326906635], 
processed observation next is [0.0, 0.0, 0.28909952606635075, 0.99, 1.0, 1.0, 0.2988149075977417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16962146986309345, 0.16962146986309365, 0.26142945177710974], 
reward next is 0.7386, 
noisyNet noise sample is [array([2.5641873], dtype=float32), -0.43872973]. 
=============================================
[2019-04-10 12:19:23,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2895395e-20 1.0000000e+00 4.5413523e-24 2.3680657e-27 2.5597201e-33], sum to 1.0000
[2019-04-10 12:19:23,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7514
[2019-04-10 12:19:23,569] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3854612839127774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580578.2330739391, 580578.2330739397, 172927.0904247991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2727600.0000, 
sim time next is 2728200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3859103808522213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581254.2257732699, 581254.2257732705, 172987.745629178], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2601329889785799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16145950715924165, 0.1614595071592418, 0.2581906651181761], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.9841163], dtype=float32), 0.62287414]. 
=============================================
[2019-04-10 12:19:30,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2469927e-20 1.0000000e+00 3.0734084e-23 7.9496040e-26 1.4276973e-32], sum to 1.0000
[2019-04-10 12:19:30,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1751
[2019-04-10 12:19:30,120] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3387719845445776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521876.3759363471, 521876.3759363471, 168348.8799461964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2785800.0000, 
sim time next is 2786400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3395761230537678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523113.9895834465, 523113.9895834465, 168447.6071190568], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2043085819924913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14530944155095737, 0.14530944155095737, 0.2514143389836669], 
reward next is 0.7486, 
noisyNet noise sample is [array([-0.20700438], dtype=float32), -2.1775491]. 
=============================================
[2019-04-10 12:19:31,328] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6206578e-19 1.0000000e+00 1.6080551e-22 1.1303807e-23 7.0050271e-31], sum to 1.0000
[2019-04-10 12:19:31,353] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5254
[2019-04-10 12:19:31,361] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 85.5, 1.0, 2.0, 0.5874426808244066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 919498.511964977, 919498.511964977, 210048.3238018114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2802600.0000, 
sim time next is 2803200.0000, 
raw observation next is [22.66666666666666, 84.66666666666666, 1.0, 2.0, 0.6201661007753495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969315.0156245605, 969315.0156245598, 216795.5202452995], 
processed observation next is [1.0, 0.43478260869565216, 0.27330173775671385, 0.8466666666666666, 1.0, 1.0, 0.5423687961148789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26925417100682236, 0.2692541710068222, 0.3235754033511933], 
reward next is 0.6764, 
noisyNet noise sample is [array([0.89392424], dtype=float32), -1.0200864]. 
=============================================
[2019-04-10 12:19:31,400] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4793283e-19 1.0000000e+00 1.3432318e-22 4.9771483e-25 9.7957723e-32], sum to 1.0000
[2019-04-10 12:19:31,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9659
[2019-04-10 12:19:31,411] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3456146005598426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532415.1094333327, 532415.109433332, 169196.7591309555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2864400.0000, 
sim time next is 2865000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.347442041362454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535227.7222545915, 535227.7222545922, 169425.6412734822], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2137855920029566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1486743672929421, 0.14867436729294228, 0.2528740914529585], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.8577151], dtype=float32), 0.3600672]. 
=============================================
[2019-04-10 12:19:31,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.84675 ]
 [72.91447 ]
 [72.945335]
 [72.949005]
 [72.942245]], R is [[72.79489899]
 [72.81441498]
 [72.83493805]
 [72.85520172]
 [72.87274933]].
[2019-04-10 12:19:35,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9004943e-21 1.0000000e+00 1.3400836e-23 4.4801938e-26 8.4823496e-33], sum to 1.0000
[2019-04-10 12:19:35,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3294
[2019-04-10 12:19:35,803] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3528675361465913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543578.0494211183, 543578.0494211189, 170111.7734432093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2862600.0000, 
sim time next is 2863200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3394237821965167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522871.2346575569, 522871.2346575569, 168427.9518024827], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20412503879098398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14524200962709916, 0.14524200962709916, 0.2513850026902727], 
reward next is 0.7486, 
noisyNet noise sample is [array([-1.2258614], dtype=float32), 0.22628465]. 
=============================================
[2019-04-10 12:19:37,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3847600e-21 1.0000000e+00 2.8434671e-24 2.5241057e-25 2.2862355e-33], sum to 1.0000
[2019-04-10 12:19:37,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3007
[2019-04-10 12:19:37,096] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.3994368544729122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 593040.6507495865, 593040.6507495872, 173807.9887622953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2839200.0000, 
sim time next is 2839800.0000, 
raw observation next is [23.16666666666667, 93.16666666666667, 1.0, 2.0, 0.3977676784717663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591824.6310228049, 591824.6310228043, 173734.0849522283], 
processed observation next is [1.0, 0.8695652173913043, 0.2969984202211693, 0.9316666666666668, 1.0, 1.0, 0.27441888972501965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16439573083966802, 0.16439573083966785, 0.25930460440631087], 
reward next is 0.7407, 
noisyNet noise sample is [array([0.3210202], dtype=float32), 1.4835551]. 
=============================================
[2019-04-10 12:19:40,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3193892e-20 1.0000000e+00 1.1865329e-23 3.8905469e-25 5.6202383e-33], sum to 1.0000
[2019-04-10 12:19:40,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1809
[2019-04-10 12:19:40,212] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 99.00000000000001, 1.0, 2.0, 0.3431875476310522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534162.7870504323, 534162.7870504316, 169493.8753872201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2916600.0000, 
sim time next is 2917200.0000, 
raw observation next is [21.0, 98.0, 1.0, 2.0, 0.3380407253222273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527771.1459022759, 527771.1459022759, 169022.2824738064], 
processed observation next is [1.0, 0.782608695652174, 0.19431279620853087, 0.98, 1.0, 1.0, 0.20245870520750275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14660309608396555, 0.14660309608396555, 0.2522720633937409], 
reward next is 0.7477, 
noisyNet noise sample is [array([-0.59424937], dtype=float32), 0.0944279]. 
=============================================
[2019-04-10 12:19:45,150] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2180551e-20 1.0000000e+00 4.4792074e-24 2.2898135e-26 7.2277144e-33], sum to 1.0000
[2019-04-10 12:19:45,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6623
[2019-04-10 12:19:45,161] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3539231406670067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563599.6850036548, 563599.6850036555, 172083.1644948465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3031800.0000, 
sim time next is 3032400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3275383449927264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521563.783305758, 521563.783305758, 168714.6045830214], 
processed observation next is [1.0, 0.08695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.18980523493099563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1448788286960439, 0.1448788286960439, 0.25181284266122594], 
reward next is 0.7482, 
noisyNet noise sample is [array([-2.1478906], dtype=float32), -0.49787995]. 
=============================================
[2019-04-10 12:19:58,014] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0656533e-21 1.0000000e+00 6.0167273e-24 2.5881262e-25 5.0168530e-33], sum to 1.0000
[2019-04-10 12:19:58,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7554
[2019-04-10 12:19:58,024] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 92.66666666666666, 1.0, 2.0, 0.4810841055832136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672231.9291245964, 672231.9291245958, 180685.2682598107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3202800.0000, 
sim time next is 3203400.0000, 
raw observation next is [25.0, 92.33333333333333, 1.0, 2.0, 0.4791888750308868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669582.8372130482, 669582.8372130488, 180399.4825859996], 
processed observation next is [0.0, 0.043478260869565216, 0.38388625592417064, 0.9233333333333333, 1.0, 1.0, 0.37251671690468285, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18599523255918007, 0.18599523255918024, 0.2692529590835815], 
reward next is 0.7307, 
noisyNet noise sample is [array([0.6168195], dtype=float32), 1.0916058]. 
=============================================
[2019-04-10 12:20:00,511] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5749970e-19 1.0000000e+00 4.4955059e-23 1.8491846e-25 1.2206950e-31], sum to 1.0000
[2019-04-10 12:20:00,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1460
[2019-04-10 12:20:00,525] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.5983974642588359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836221.6063354497, 836221.6063354504, 200448.1885635025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5975090512203506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834979.618799215, 834979.618799215, 200283.1510616608], 
processed observation next is [0.0, 0.6086956521739131, 0.6761453396524489, 0.7433333333333333, 1.0, 1.0, 0.5150711460486151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23193878299978193, 0.23193878299978193, 0.298930076211434], 
reward next is 0.7011, 
noisyNet noise sample is [array([-1.2319973], dtype=float32), 0.14836124]. 
=============================================
[2019-04-10 12:20:01,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2572543e-20 1.0000000e+00 3.9282660e-23 1.3409516e-25 2.3637289e-31], sum to 1.0000
[2019-04-10 12:20:01,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6500
[2019-04-10 12:20:01,685] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 83.66666666666667, 1.0, 2.0, 0.4643756041318868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658046.0924876723, 658046.092487673, 179383.2837849105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3295200.0000, 
sim time next is 3295800.0000, 
raw observation next is [25.5, 83.5, 1.0, 2.0, 0.4553574347421961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650414.1350752919, 650414.1350752924, 178714.4579361897], 
processed observation next is [0.0, 0.13043478260869565, 0.40758293838862564, 0.835, 1.0, 1.0, 0.3438041382436098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18067059307646996, 0.18067059307647013, 0.2667379969196861], 
reward next is 0.7333, 
noisyNet noise sample is [array([1.902299], dtype=float32), -0.3720065]. 
=============================================
[2019-04-10 12:20:02,958] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 12:20:02,960] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:20:02,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:02,961] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:20:02,963] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:20:02,963] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:02,965] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:20:02,964] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:20:02,966] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:02,966] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:02,967] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:02,976] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run30
[2019-04-10 12:20:02,976] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run30
[2019-04-10 12:20:02,991] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run30
[2019-04-10 12:20:03,007] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run30
[2019-04-10 12:20:03,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run30
[2019-04-10 12:20:07,927] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:20:07,929] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.10196711, 91.01089283, 1.0, 2.0, 0.4392905661844721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619275.1922485793, 619275.1922485793, 175341.6094145535]
[2019-04-10 12:20:07,930] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:20:07,933] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.3108506e-19 1.0000000e+00 1.4798622e-22 5.0812419e-25 8.5200414e-31], sampled 0.6067302541072442
[2019-04-10 12:20:25,529] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:20:25,530] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 94.00000000000001, 1.0, 2.0, 0.4660959969776622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702572.896145645, 702572.896145645, 184775.6724167685]
[2019-04-10 12:20:25,531] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:20:25,535] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4767077e-19 1.0000000e+00 1.6354919e-22 1.0063248e-24 6.6941441e-31], sampled 0.7924916791487455
[2019-04-10 12:20:27,145] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:20:27,146] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.14469994, 68.13564068666668, 1.0, 2.0, 0.684600268896247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 956738.687149238, 956738.6871492387, 217558.3992808431]
[2019-04-10 12:20:27,147] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:20:27,149] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6153007e-18 1.0000000e+00 1.4859439e-21 1.2006757e-22 7.8739547e-30], sampled 0.1460541470632264
[2019-04-10 12:20:50,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:20:50,389] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.81246663, 80.45636846, 1.0, 2.0, 0.552227056627872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771678.021760929, 771678.021760929, 192183.6671368148]
[2019-04-10 12:20:50,391] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:20:50,393] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.04023060e-18 1.00000000e+00 4.79105091e-22 1.07066805e-23
 2.93618252e-30], sampled 0.06962836486502122
[2019-04-10 12:20:54,126] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:20:54,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 81.0, 1.0, 2.0, 0.7115941611653405, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000877734866209, 6.9112, 168.9123499690566, 1891353.255353981, 1827733.012222137, 387376.7388818548]
[2019-04-10 12:20:54,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:20:54,131] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9933233e-10 9.9836141e-01 2.0126083e-10 1.6385971e-03 3.9532490e-17], sampled 0.9773981510885461
[2019-04-10 12:20:54,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1891353.255353981 W.
[2019-04-10 12:20:56,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:20:56,248] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.56954613, 61.47633218999999, 1.0, 2.0, 0.5261387791349995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735209.8385944327, 735209.8385944327, 187793.3097928005]
[2019-04-10 12:20:56,248] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:20:56,253] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4154959e-20 1.0000000e+00 2.1016870e-23 1.4228796e-25 2.4778930e-32], sampled 0.5118426063466059
[2019-04-10 12:21:14,266] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12265655], dtype=float32), 0.09358874]
[2019-04-10 12:21:14,266] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.86723276333333, 71.49529317666668, 1.0, 2.0, 0.3834722642785531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580631.4440731104, 580631.4440731104, 173020.9425405155]
[2019-04-10 12:21:14,267] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:21:14,269] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7577159e-20 1.0000000e+00 2.2706717e-23 9.3009751e-27 3.9084879e-32], sampled 0.11696765164803391
[2019-04-10 12:21:23,873] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.5452 3162953072.2745 1751.0000
[2019-04-10 12:21:23,968] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8003.0789 3007173575.2538 1756.0000
[2019-04-10 12:21:24,020] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.9589 2842201176.6021 1120.0000
[2019-04-10 12:21:24,125] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.6079 2779295756.3282 928.0000
[2019-04-10 12:21:24,127] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.4700 2927273081.5220 1337.0000
[2019-04-10 12:21:25,143] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 725000, evaluation results [725000.0, 7894.545237351943, 3162953072.274464, 1751.0, 8255.4699626884, 2927273081.5220466, 1337.0, 8661.60785417641, 2779295756.3281875, 928.0, 8003.078856538997, 3007173575.2537847, 1756.0, 8498.958927435951, 2842201176.602124, 1120.0]
[2019-04-10 12:21:25,560] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.6959413e-19 1.0000000e+00 7.0460133e-21 2.8981383e-21 2.1521126e-28], sum to 1.0000
[2019-04-10 12:21:25,570] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8259
[2019-04-10 12:21:25,575] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 90.66666666666667, 1.0, 2.0, 0.5371298264495912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750573.8098263108, 750573.8098263108, 189617.9473488054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3372600.0000, 
sim time next is 3373200.0000, 
raw observation next is [26.7, 91.0, 1.0, 2.0, 0.5368904837319607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750239.2392855887, 750239.2392855887, 189577.8286554313], 
processed observation next is [1.0, 0.043478260869565216, 0.46445497630331756, 0.91, 1.0, 1.0, 0.4420367273879044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20839978869044132, 0.20839978869044132, 0.28295198306780794], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.22455913], dtype=float32), -0.25459754]. 
=============================================
[2019-04-10 12:21:28,190] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.2352226e-18 1.0000000e+00 9.9753671e-20 1.5029912e-19 2.7974148e-27], sum to 1.0000
[2019-04-10 12:21:28,197] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5279
[2019-04-10 12:21:28,200] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.5, 1.0, 2.0, 0.8171736454974532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1142111.5132643, 1142111.5132643, 248169.615476196], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [26.66666666666667, 90.66666666666666, 1.0, 2.0, 0.8117824857550356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1134572.607413059, 1134572.60741306, 246823.3154878099], 
processed observation next is [1.0, 0.21739130434782608, 0.4628751974723541, 0.9066666666666666, 1.0, 1.0, 0.7732319105482357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3151590576147386, 0.3151590576147389, 0.36839300819076104], 
reward next is 0.6316, 
noisyNet noise sample is [array([-0.40907347], dtype=float32), 0.23925282]. 
=============================================
[2019-04-10 12:21:28,217] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[62.598236]
 [62.779533]
 [62.874947]
 [63.239357]
 [63.188683]], R is [[62.70035553]
 [62.70294952]
 [62.70370865]
 [62.69276047]
 [62.72600555]].
[2019-04-10 12:21:37,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0304884e-10 6.6487044e-03 5.9657612e-11 9.9335128e-01 4.9479472e-16], sum to 1.0000
[2019-04-10 12:21:37,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8148
[2019-04-10 12:21:37,047] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.7533817565010612, 1.0, 2.0, 0.6972809177647932, 1.0, 2.0, 1.03, 7.005101941635396, 6.9112, 170.5573041426782, 2925913.822262781, 2858648.041782577, 538719.4274917376], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 1.030449546377266, 1.0, 2.0, 1.030449546377266, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2882582.194200814, 2882582.194200814, 547658.6319303802], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.63, 1.0, 1.0, 1.0366862004545374, 1.0, 1.0, 1.0366862004545374, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.8007172761668927, 0.8007172761668927, 0.817400943179672], 
reward next is 0.1826, 
noisyNet noise sample is [array([-1.6737077], dtype=float32), 1.4707257]. 
=============================================
[2019-04-10 12:21:38,688] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6328100e-10 9.9809891e-01 5.1976347e-11 1.9011627e-03 3.0831311e-17], sum to 1.0000
[2019-04-10 12:21:38,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0469
[2019-04-10 12:21:38,700] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2332508.907219906 W.
[2019-04-10 12:21:38,708] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.5, 63.0, 1.0, 2.0, 0.8339957676611354, 1.0, 2.0, 0.8339957676611354, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2332508.907219906, 2332508.907219906, 436763.7214637095], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3673800.0000, 
sim time next is 3674400.0000, 
raw observation next is [32.66666666666667, 63.0, 1.0, 2.0, 0.7340153728707591, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005977364109727, 6.9112, 168.9123932236006, 1922729.409648122, 1855491.309788288, 392186.5777542969], 
processed observation next is [1.0, 0.5217391304347826, 0.7472353870458138, 0.63, 1.0, 1.0, 0.6795365938201917, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009477736410972692, 0.0, 0.8294371791560915, 0.5340915026800339, 0.5154142527189689, 0.5853531011258163], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47761303], dtype=float32), -0.28028014]. 
=============================================
[2019-04-10 12:21:42,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.4028640e-10 9.9960285e-01 7.0549250e-10 3.9719572e-04 1.5183329e-16], sum to 1.0000
[2019-04-10 12:21:42,602] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7278
[2019-04-10 12:21:42,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1894906.321891 W.
[2019-04-10 12:21:42,615] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 66.5, 1.0, 2.0, 0.4517749782895262, 1.0, 2.0, 0.4517749782895262, 1.0, 1.0, 0.7714614749805597, 6.911199999999999, 6.9112, 170.5573041426782, 1894906.321891, 1894906.321891001, 381266.8024411526], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3749400.0000, 
sim time next is 3750000.0000, 
raw observation next is [30.33333333333333, 65.33333333333334, 1.0, 2.0, 0.6771052689960037, 1.0, 2.0, 0.6771052689960037, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1893346.885150939, 1893346.885150939, 364472.2351870395], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783569, 0.6533333333333334, 1.0, 1.0, 0.6109702036096429, 1.0, 1.0, 0.6109702036096429, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5259296903197053, 0.5259296903197053, 0.5439884107269246], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5735089], dtype=float32), 0.7251077]. 
=============================================
[2019-04-10 12:21:42,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[47.043056]
 [48.296295]
 [48.0339  ]
 [46.830315]
 [46.957405]], R is [[47.38447952]
 [46.9106369 ]
 [46.44153214]
 [45.97711563]
 [45.51734543]].
[2019-04-10 12:21:47,818] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1409728e-20 1.0000000e+00 4.8400750e-24 3.6295977e-26 1.8125256e-33], sum to 1.0000
[2019-04-10 12:21:47,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4308
[2019-04-10 12:21:47,829] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 68.5, 1.0, 2.0, 0.5615181716793312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 784666.1530663765, 784666.1530663759, 193795.6428661484], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3871800.0000, 
sim time next is 3872400.0000, 
raw observation next is [30.66666666666667, 69.0, 1.0, 2.0, 0.5548712087925728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775374.2891201531, 775374.2891201531, 192639.4390450597], 
processed observation next is [0.0, 0.8260869565217391, 0.6524486571879939, 0.69, 1.0, 1.0, 0.4637002515573166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21538174697782028, 0.21538174697782028, 0.28752155081352193], 
reward next is 0.7125, 
noisyNet noise sample is [array([1.8183521], dtype=float32), -0.025612684]. 
=============================================
[2019-04-10 12:21:47,900] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2032269e-19 1.0000000e+00 7.1651870e-23 2.7549815e-26 1.8391613e-31], sum to 1.0000
[2019-04-10 12:21:47,907] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4375
[2019-04-10 12:21:47,911] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.5, 1.0, 2.0, 0.5714166052600571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798503.4350052854, 798503.4350052854, 195542.0834691583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3834600.0000, 
sim time next is 3835200.0000, 
raw observation next is [31.66666666666667, 69.0, 1.0, 2.0, 0.5746693473168272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803050.5684547573, 803050.5684547573, 196122.4919113029], 
processed observation next is [0.0, 0.391304347826087, 0.6998420221169038, 0.69, 1.0, 1.0, 0.4875534305022014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2230696023485437, 0.2230696023485437, 0.2927201371810491], 
reward next is 0.7073, 
noisyNet noise sample is [array([1.2548085], dtype=float32), 0.5771751]. 
=============================================
[2019-04-10 12:21:48,721] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2726050e-20 1.0000000e+00 6.0009111e-24 7.9021356e-26 1.8844346e-32], sum to 1.0000
[2019-04-10 12:21:48,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8102
[2019-04-10 12:21:48,730] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5780382386054245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807760.0919121156, 807760.0919121163, 196726.0384655843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3884400.0000, 
sim time next is 3885000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5821277182919646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813476.9861038653, 813476.9861038653, 197463.5189598995], 
processed observation next is [0.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49653941962887305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259658294732959, 0.2259658294732959, 0.2947216700894022], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.7365101], dtype=float32), 0.68559754]. 
=============================================
[2019-04-10 12:21:48,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.13605 ]
 [74.17434 ]
 [74.257545]
 [74.334175]
 [74.40678 ]], R is [[74.04142761]
 [74.00740051]
 [73.97501373]
 [73.94421387]
 [73.91479492]].
[2019-04-10 12:21:59,012] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3537607e-16 1.0000000e+00 2.4784200e-19 3.0232307e-17 2.5488453e-26], sum to 1.0000
[2019-04-10 12:21:59,019] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1758
[2019-04-10 12:21:59,032] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 84.0, 1.0, 2.0, 0.9836277700152309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1374904.467449562, 1374904.467449563, 293981.0077860345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3996600.0000, 
sim time next is 3997200.0000, 
raw observation next is [29.33333333333334, 84.0, 1.0, 2.0, 0.9560641707630189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336352.111159913, 1336352.111159913, 285829.2553981227], 
processed observation next is [1.0, 0.2608695652173913, 0.5892575039494474, 0.84, 1.0, 1.0, 0.9470652659795408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3712089197666425, 0.3712089197666425, 0.42661082895242197], 
reward next is 0.5734, 
noisyNet noise sample is [array([0.7380578], dtype=float32), 0.198523]. 
=============================================
[2019-04-10 12:21:59,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0932353e-18 1.0000000e+00 2.5462960e-21 4.2818802e-21 4.0040667e-29], sum to 1.0000
[2019-04-10 12:21:59,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4348
[2019-04-10 12:21:59,089] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 84.0, 1.0, 2.0, 1.003204192902753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1402286.256768923, 1402286.256768923, 299908.4481120039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3997800.0000, 
sim time next is 3998400.0000, 
raw observation next is [29.66666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.929289861679488, 6.9112, 168.9126525651582, 1466597.264269796, 1453763.716882355, 311355.5335554229], 
processed observation next is [1.0, 0.2608695652173913, 0.6050552922590839, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0018089861679487563, 0.0, 0.8294384526418664, 0.4073881289638322, 0.40382325468954305, 0.46470975157525807], 
reward next is 0.4448, 
noisyNet noise sample is [array([-0.12738909], dtype=float32), 0.16417146]. 
=============================================
[2019-04-10 12:22:14,764] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.20435032e-14 1.94204546e-08 1.11536045e-13 1.00000000e+00
 8.83132040e-19], sum to 1.0000
[2019-04-10 12:22:14,764] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1779
[2019-04-10 12:22:14,766] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [38.0, 48.0, 1.0, 2.0, 0.9356499626550296, 1.0, 2.0, 0.7884150208417774, 1.0, 1.0, 1.03, 7.005116320297601, 6.9112, 170.5573041426782, 3308835.191978823, 3241559.111477995, 606100.0758635656], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4289400.0000, 
sim time next is 4290000.0000, 
raw observation next is [38.0, 47.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.838185613984203, 6.9112, 170.5573041426782, 3574140.792923941, 2910103.290263139, 548356.3161468708], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.47, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09269856139842032, 0.0, 0.8375144448122397, 0.9928168869233169, 0.8083620250730942, 0.8184422629057774], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82519406], dtype=float32), -1.46762]. 
=============================================
[2019-04-10 12:22:14,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[44.434265]
 [44.87677 ]
 [42.9019  ]
 [41.82051 ]
 [41.313637]], R is [[45.63571167]
 [45.17935562]
 [44.72756195]
 [44.28028488]
 [43.83748245]].
[2019-04-10 12:22:14,939] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3504061e-19 1.0000000e+00 7.0763095e-23 2.6525079e-23 6.8899556e-31], sum to 1.0000
[2019-04-10 12:22:14,971] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7901
[2019-04-10 12:22:14,998] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 73.0, 1.0, 2.0, 0.6022286390928935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 841577.5454832029, 841577.5454832029, 201162.1590519047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4228200.0000, 
sim time next is 4228800.0000, 
raw observation next is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.6003760123385354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 838987.5908322779, 838987.5908322772, 200816.3024063902], 
processed observation next is [1.0, 0.9565217391304348, 0.6840442338072668, 0.7366666666666667, 1.0, 1.0, 0.5185253160705245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23305210856452163, 0.23305210856452144, 0.2997258244871496], 
reward next is 0.7003, 
noisyNet noise sample is [array([1.2571458], dtype=float32), -0.11189855]. 
=============================================
[2019-04-10 12:22:17,329] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 12:22:17,333] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:22:17,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:17,336] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:22:17,336] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:17,342] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run31
[2019-04-10 12:22:17,343] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:22:17,363] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:17,374] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:22:17,377] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:22:17,386] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:17,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run31
[2019-04-10 12:22:17,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run31
[2019-04-10 12:22:17,407] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:17,441] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run31
[2019-04-10 12:22:17,464] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run31
[2019-04-10 12:22:20,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:22:20,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.96666666666667, 52.0, 1.0, 2.0, 0.2792023820835609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 453662.8390093106, 453662.8390093106, 163811.323553348]
[2019-04-10 12:22:20,543] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:22:20,545] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.7361744e-20 1.0000000e+00 8.6278727e-24 1.6023746e-27 3.1585319e-32], sampled 0.7539605669573679
[2019-04-10 12:22:37,739] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:22:38,026] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.888081475, 89.47317248, 1.0, 2.0, 0.4827618101688409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681255.4262152286, 681255.4262152286, 181797.6630724437]
[2019-04-10 12:22:38,027] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:22:38,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0090850e-19 1.0000000e+00 5.5829365e-23 1.1306130e-24 2.8667314e-31], sampled 0.6495818018615444
[2019-04-10 12:22:44,524] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:22:44,525] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.11666666666667, 92.16666666666666, 1.0, 2.0, 0.4485928756151633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647410.329114645, 647410.3291146456, 178569.3164935917]
[2019-04-10 12:22:44,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:22:44,528] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1434982e-19 1.0000000e+00 3.5676787e-23 6.5253894e-25 1.1479185e-31], sampled 0.746089198797249
[2019-04-10 12:22:55,015] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:22:55,016] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.5, 71.0, 1.0, 2.0, 0.5993928898817227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837613.1975166285, 837613.1975166292, 200632.447364543]
[2019-04-10 12:22:55,017] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:22:55,019] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.9451369e-21 1.0000000e+00 6.0249695e-25 3.0359048e-27 1.8528206e-33], sampled 0.24847347632309047
[2019-04-10 12:22:57,248] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:22:57,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.87333309833333, 74.26601601166666, 1.0, 2.0, 0.5354389579521942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748210.1939505028, 748210.1939505034, 189333.9199520967]
[2019-04-10 12:22:57,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:22:57,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.5631670e-20 1.0000000e+00 1.6761480e-23 2.7557815e-25 4.8330812e-32], sampled 0.103519407225102
[2019-04-10 12:23:03,733] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:23:03,734] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.35338550333334, 77.61566574, 1.0, 2.0, 0.5696522007014121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796036.914033578, 796036.914033578, 195227.4729511808]
[2019-04-10 12:23:03,736] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:23:03,739] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8713128e-19 1.0000000e+00 1.0413918e-22 3.9241134e-25 6.2576012e-31], sampled 0.02500456750927671
[2019-04-10 12:23:18,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11990532], dtype=float32), 0.09660589]
[2019-04-10 12:23:18,535] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.03333333333334, 72.33333333333334, 1.0, 2.0, 0.7974872070673678, 1.0, 2.0, 0.7974872070673678, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2230311.111459408, 2230311.111459408, 418655.6760605196]
[2019-04-10 12:23:18,535] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:23:18,536] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.0685443e-10 9.9944717e-01 4.5956690e-11 5.5280101e-04 1.0101329e-16], sampled 0.5928881255713404
[2019-04-10 12:23:18,538] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2230311.111459408 W.
[2019-04-10 12:23:37,985] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4872 2927442749.4156 1337.0000
[2019-04-10 12:23:38,222] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.2672 2841993438.1922 1119.0000
[2019-04-10 12:23:38,458] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.3421 3007462805.7070 1759.0000
[2019-04-10 12:23:38,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7896.6519 3162448242.0833 1745.0000
[2019-04-10 12:23:38,609] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.1109 2779329425.2862 930.0000
[2019-04-10 12:23:39,622] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 750000, evaluation results [750000.0, 7896.651916943583, 3162448242.083264, 1745.0, 8254.487197211422, 2927442749.4155893, 1337.0, 8660.11092830215, 2779329425.2862372, 930.0, 8000.342134383308, 3007462805.706954, 1759.0, 8500.267168466797, 2841993438.1921506, 1119.0]
[2019-04-10 12:23:42,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8017729e-22 1.0000000e+00 5.6074520e-26 1.2371857e-26 2.3171922e-35], sum to 1.0000
[2019-04-10 12:23:42,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2726
[2019-04-10 12:23:42,264] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.0, 50.0, 1.0, 2.0, 0.5702680169259311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796897.7837683539, 796897.7837683539, 195338.9931550255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.5713963860043857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798475.1697896112, 798475.169789612, 195539.38747306], 
processed observation next is [1.0, 0.8260869565217391, 0.9052132701421801, 0.5, 1.0, 1.0, 0.48361010361974177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.221798658274892, 0.2217986582748922, 0.2918498320493433], 
reward next is 0.7082, 
noisyNet noise sample is [array([-2.7784934], dtype=float32), 1.0835861]. 
=============================================
[2019-04-10 12:23:42,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.465355]
 [71.87847 ]
 [72.01785 ]
 [72.06765 ]
 [72.37069 ]], R is [[71.2366333 ]
 [71.23271942]
 [71.22897339]
 [71.22494507]
 [71.22053528]].
[2019-04-10 12:23:52,330] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3103494e-19 1.0000000e+00 1.4819580e-22 1.8006126e-23 5.0422441e-30], sum to 1.0000
[2019-04-10 12:23:52,335] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7983
[2019-04-10 12:23:52,340] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 91.5, 1.0, 2.0, 0.9952515552379048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1391162.717099196, 1391162.717099196, 297487.5177597047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4602600.0000, 
sim time next is 4603200.0000, 
raw observation next is [28.66666666666666, 90.66666666666666, 1.0, 2.0, 0.9771414880796578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1365832.178960124, 1365832.178960124, 292043.7306130881], 
processed observation next is [1.0, 0.2608695652173913, 0.5576619273301735, 0.9066666666666666, 1.0, 1.0, 0.9724596241923589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37939782748892337, 0.37939782748892337, 0.4358861650941614], 
reward next is 0.5641, 
noisyNet noise sample is [array([0.85927236], dtype=float32), 0.8239114]. 
=============================================
[2019-04-10 12:23:56,148] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6969392e-18 1.0000000e+00 1.3286727e-21 7.8406230e-23 1.1345720e-29], sum to 1.0000
[2019-04-10 12:23:56,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8852
[2019-04-10 12:23:56,158] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6540673159982022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914050.085566055, 914050.085566055, 211243.3200539501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4687200.0000, 
sim time next is 4687800.0000, 
raw observation next is [27.16666666666666, 88.16666666666667, 1.0, 2.0, 0.8850632591315151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1237051.77272048, 1237051.772720481, 265862.1609175757], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.8816666666666667, 1.0, 1.0, 0.8615219989536326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34362549242235557, 0.34362549242235585, 0.3968091953993667], 
reward next is 0.6032, 
noisyNet noise sample is [array([-0.2884612], dtype=float32), 1.1889642]. 
=============================================
[2019-04-10 12:23:58,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1271479e-18 1.0000000e+00 3.7398548e-21 4.4639112e-22 1.1107663e-28], sum to 1.0000
[2019-04-10 12:23:58,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2082
[2019-04-10 12:23:58,878] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.634153649034275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 886209.3990846409, 886209.3990846416, 207274.5878739175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4685400.0000, 
sim time next is 4686000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6436963845000729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 899550.7135327054, 899550.7135327049, 209161.7714681792], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.5707185355422565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2498751982035293, 0.24987519820352913, 0.312181748459969], 
reward next is 0.6878, 
noisyNet noise sample is [array([-0.52720445], dtype=float32), 2.9643397]. 
=============================================
[2019-04-10 12:23:58,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[61.82171 ]
 [61.540104]
 [61.122166]
 [60.642105]
 [60.53258 ]], R is [[62.13007736]
 [62.1994133 ]
 [62.26728821]
 [62.32316208]
 [62.35594177]].
[2019-04-10 12:24:03,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0399094e-15 1.0000000e+00 1.8180881e-16 1.8634389e-14 1.3264437e-22], sum to 1.0000
[2019-04-10 12:24:03,284] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4034
[2019-04-10 12:24:03,290] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1847355.016535267 W.
[2019-04-10 12:24:03,293] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.6801521681174151, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.983271664237221, 6.9112, 168.9124757867812, 1847355.016535267, 1796225.04292566, 381227.1859888562], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.920763830324158, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.981724369952408, 6.9112, 168.9124792001829, 2184100.515484812, 2134068.241559505, 440748.5477170491], 
processed observation next is [1.0, 0.391304347826087, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.9045347353303108, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007052436995240807, 0.0, 0.829437601340471, 0.6066945876346701, 0.5927967337665292, 0.6578336533090284], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41424996], dtype=float32), 0.27419975]. 
=============================================
[2019-04-10 12:24:04,491] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9594220e-10 9.3836868e-01 1.2386073e-10 6.1631311e-02 6.9999359e-16], sum to 1.0000
[2019-04-10 12:24:04,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8572
[2019-04-10 12:24:04,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2086553.621114996 W.
[2019-04-10 12:24:04,509] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 66.0, 1.0, 2.0, 0.7461333100739324, 1.0, 2.0, 0.7461333100739324, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2086553.621114996, 2086553.621114996, 394495.961035923], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4897200.0000, 
sim time next is 4897800.0000, 
raw observation next is [30.5, 66.0, 1.0, 2.0, 0.507808152211936, 1.0, 2.0, 0.507808152211936, 1.0, 1.0, 0.8733483432455982, 6.9112, 6.9112, 170.5573041426782, 2130163.212353949, 2130163.212353949, 419011.6961789262], 
processed observation next is [1.0, 0.6956521739130435, 0.6445497630331753, 0.66, 1.0, 1.0, 0.40699777374932045, 1.0, 1.0, 0.40699777374932045, 1.0, 0.5, 0.8455467600556076, 0.0, 0.0, 0.8375144448122397, 0.5917120034316525, 0.5917120034316525, 0.6253905913118302], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5487431], dtype=float32), 0.7745439]. 
=============================================
[2019-04-10 12:24:06,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6824756e-15 1.0000000e+00 3.0227103e-18 5.4230635e-18 1.4141468e-24], sum to 1.0000
[2019-04-10 12:24:06,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7878
[2019-04-10 12:24:06,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1835863.648738508 W.
[2019-04-10 12:24:06,129] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.66666666666666, 75.66666666666666, 1.0, 2.0, 0.4377103411664891, 1.0, 1.0, 0.4377103411664891, 1.0, 1.0, 0.7496888873756252, 6.911200000000001, 6.9112, 170.5573041426782, 1835863.648738508, 1835863.648738508, 373109.158087089], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4869600.0000, 
sim time next is 4870200.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.6424179511760737, 1.0, 2.0, 0.6424179511760737, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1796271.616554048, 1796271.616554048, 350444.3024575792], 
processed observation next is [1.0, 0.34782608695652173, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.5691782544290044, 1.0, 1.0, 0.5691782544290044, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.49896433793168, 0.49896433793168, 0.5230511976978793], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9472921], dtype=float32), 0.2061516]. 
=============================================
[2019-04-10 12:24:07,329] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1496153e-12 9.9999809e-01 2.8350510e-13 1.9404442e-06 1.0278856e-19], sum to 1.0000
[2019-04-10 12:24:07,338] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7159
[2019-04-10 12:24:07,343] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2138692.869344925 W.
[2019-04-10 12:24:07,348] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.16666666666666, 66.0, 1.0, 2.0, 0.8883235781793507, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979011179751583, 6.9112, 168.9125525678371, 2138692.869344925, 2090585.399594124, 431787.1809211841], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4899000.0000, 
sim time next is 4899600.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.865828618903589, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.974835135768316, 6.9112, 168.9125222564224, 2107207.106127935, 2062062.266723559, 425827.0895173026], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.66, 1.0, 1.0, 0.8383477336187819, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006363513576831625, 0.0, 0.8294378127663181, 0.5853353072577597, 0.5727950740898775, 0.6355628201750785], 
reward next is 0.0463, 
noisyNet noise sample is [array([-0.43096012], dtype=float32), 0.67074716]. 
=============================================
[2019-04-10 12:24:24,378] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-10 12:24:24,379] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:24:24,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:24:24,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:24:24,383] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:24:24,384] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:24:24,387] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:24:24,391] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run32
[2019-04-10 12:24:24,391] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run32
[2019-04-10 12:24:24,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run32
[2019-04-10 12:24:24,457] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:24:24,458] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:24:24,458] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:24:24,459] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:24:24,461] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run32
[2019-04-10 12:24:24,480] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run32
[2019-04-10 12:24:33,481] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12224288], dtype=float32), 0.09451696]
[2019-04-10 12:24:33,483] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.30095909, 95.14623495000001, 1.0, 2.0, 0.242651846698883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 400697.1049453298, 400697.1049453298, 160138.7227049823]
[2019-04-10 12:24:33,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:24:33,492] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.4435495e-21 1.0000000e+00 5.3807798e-25 1.4168322e-29 9.1939858e-34], sampled 0.5268449371129513
[2019-04-10 12:25:21,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12224288], dtype=float32), 0.09451696]
[2019-04-10 12:25:21,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.0, 49.0, 1.0, 2.0, 0.609992296329296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852431.1334352818, 852431.1334352818, 202620.3267000657]
[2019-04-10 12:25:21,035] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:25:21,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6402130e-21 1.0000000e+00 5.8416728e-25 6.5390599e-28 7.6655286e-34], sampled 0.05133519430249733
[2019-04-10 12:25:27,404] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12224288], dtype=float32), 0.09451696]
[2019-04-10 12:25:27,404] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.16666666666667, 93.16666666666667, 1.0, 2.0, 0.9919741131948946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564686264, 1386578.516435549, 1386578.516435549, 296494.8538506472]
[2019-04-10 12:25:27,405] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:25:27,407] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8128368e-18 1.0000000e+00 1.3300216e-21 5.4922099e-23 1.3837685e-29], sampled 0.307639210751943
[2019-04-10 12:25:44,866] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12224288], dtype=float32), 0.09451696]
[2019-04-10 12:25:44,868] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 88.0, 1.0, 2.0, 0.5939456352597255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 829998.0336000351, 829998.0336000345, 199622.8344593488]
[2019-04-10 12:25:44,869] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:25:44,872] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7870370e-20 1.0000000e+00 9.4537320e-24 1.3002149e-26 3.2571538e-32], sampled 0.4450045809305869
[2019-04-10 12:26:00,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12224288], dtype=float32), 0.09451696]
[2019-04-10 12:26:00,441] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.0, 67.0, 1.0, 2.0, 0.3331526024609719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524371.7260712241, 524371.7260712241, 168847.690166316]
[2019-04-10 12:26:00,442] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:26:00,444] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3920329e-21 1.0000000e+00 1.8817690e-25 4.1020863e-29 2.1514823e-34], sampled 0.20335801703003564
[2019-04-10 12:26:01,122] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7906 2779329859.0617 933.0000
[2019-04-10 12:26:01,264] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9141 3164223878.4413 1778.0000
[2019-04-10 12:26:01,265] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7647 3007775956.8125 1766.0000
[2019-04-10 12:26:01,287] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 12:26:01,297] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.5779 2842695196.5357 1131.0000
[2019-04-10 12:26:02,310] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 775000, evaluation results [775000.0, 7881.914118921734, 3164223878.441265, 1778.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8659.79062523386, 2779329859.0616684, 933.0, 7998.764734894051, 3007775956.812482, 1766.0, 8496.577922422102, 2842695196.535731, 1131.0]
[2019-04-10 12:26:03,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0231095e-10 1.6837725e-01 2.8272735e-09 8.3162266e-01 8.3390716e-15], sum to 1.0000
[2019-04-10 12:26:03,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8182
[2019-04-10 12:26:03,204] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 69.0, 1.0, 2.0, 0.6132025838181987, 1.0, 2.0, 0.6132025838181987, 1.0, 1.0, 1.03, 6.950467590312816, 6.9112, 170.5573041426782, 2572743.004330986, 2544614.032086467, 492815.1657481309], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5224800.0000, 
sim time next is 5225400.0000, 
raw observation next is [31.5, 68.5, 1.0, 2.0, 0.9165969101859813, 1.0, 2.0, 0.9165969101859813, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2563763.739017916, 2563763.739017917, 480628.2536483991], 
processed observation next is [1.0, 0.4782608695652174, 0.6919431279620853, 0.685, 1.0, 1.0, 0.8995143496216642, 1.0, 1.0, 0.8995143496216642, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7121565941716433, 0.7121565941716436, 0.7173556024602972], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.641524], dtype=float32), 0.42245555]. 
=============================================
[2019-04-10 12:26:12,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7123693e-11 9.9339181e-01 6.8016369e-12 6.6081579e-03 1.2715407e-17], sum to 1.0000
[2019-04-10 12:26:12,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-10 12:26:12,047] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2995447.431281497 W.
[2019-04-10 12:26:12,052] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.31666666666667, 56.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.031279017444057, 6.9112, 170.5573041426782, 2995447.431281497, 2909429.946584185, 553018.2704612114], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5415000.0000, 
sim time next is 5415600.0000, 
raw observation next is [33.63333333333333, 59.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.475153117340208, 6.9112, 170.5573041426782, 3313782.868075198, 2909800.309978295, 550575.6319165813], 
processed observation next is [1.0, 0.6956521739130435, 0.7930489731437599, 0.59, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.056395311734020834, 0.0, 0.8375144448122397, 0.9204952411319994, 0.8082778638828597, 0.8217546745023602], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2805115], dtype=float32), 0.36414236]. 
=============================================
[2019-04-10 12:26:14,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3913577e-21 1.0000000e+00 1.3376868e-25 6.2973624e-28 2.2606593e-34], sum to 1.0000
[2019-04-10 12:26:14,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5656
[2019-04-10 12:26:14,200] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 81.0, 1.0, 2.0, 0.5759460982098975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804835.393041449, 804835.393041449, 196350.2243232327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5518800.0000, 
sim time next is 5519400.0000, 
raw observation next is [29.15, 81.5, 1.0, 2.0, 0.5729850744184317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800696.0560447715, 800696.0560447715, 195820.9859783708], 
processed observation next is [1.0, 0.9130434782608695, 0.5805687203791469, 0.815, 1.0, 1.0, 0.48552418604630326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22241557112354762, 0.22241557112354762, 0.2922701283259266], 
reward next is 0.7077, 
noisyNet noise sample is [array([0.14131898], dtype=float32), -0.5480854]. 
=============================================
[2019-04-10 12:26:14,746] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.88829255e-10 9.65906680e-01 1.12752314e-10 3.40932608e-02
 8.48234993e-16], sum to 1.0000
[2019-04-10 12:26:14,753] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9540
[2019-04-10 12:26:14,758] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2703474.065176213 W.
[2019-04-10 12:26:14,765] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.09999999999999, 54.0, 1.0, 2.0, 0.9664921684684185, 1.0, 2.0, 0.9664921684684185, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2703474.065176213, 2703474.065176213, 509075.03703464], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5488800.0000, 
sim time next is 5489400.0000, 
raw observation next is [36.2, 53.0, 1.0, 2.0, 0.6519097862972327, 1.0, 2.0, 0.6465449326628788, 1.0, 1.0, 1.03, 7.005093940593786, 6.9112, 170.5573041426782, 2712785.506082917, 2645525.457074279, 506569.7868411154], 
processed observation next is [1.0, 0.5217391304347826, 0.9146919431279622, 0.53, 1.0, 1.0, 0.5806142003581117, 1.0, 1.0, 0.5741505212805769, 1.0, 0.5, 1.0365853658536586, 0.009389394059378641, 0.0, 0.8375144448122397, 0.7535515294674769, 0.734868182520633, 0.7560743087180827], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.64186203], dtype=float32), -0.6458505]. 
=============================================
[2019-04-10 12:26:20,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6080254e-20 1.0000000e+00 1.0116673e-24 1.5389846e-28 2.1713789e-33], sum to 1.0000
[2019-04-10 12:26:20,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4958
[2019-04-10 12:26:20,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 70.66666666666667, 1.0, 2.0, 0.5440075490657256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760188.0323666892, 760188.0323666892, 190778.8687070981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5651400.0000, 
sim time next is 5652000.0000, 
raw observation next is [30.5, 70.0, 1.0, 2.0, 0.5451095698350268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761728.5319104239, 761728.5319104233, 190966.1246960469], 
processed observation next is [0.0, 0.43478260869565216, 0.6445497630331753, 0.7, 1.0, 1.0, 0.4519392407650925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21159125886400665, 0.21159125886400648, 0.2850240667105178], 
reward next is 0.7150, 
noisyNet noise sample is [array([-0.8285301], dtype=float32), -3.4899526]. 
=============================================
[2019-04-10 12:26:20,553] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.60023]
 [70.56981]
 [70.53683]
 [70.50632]
 [70.4771 ]], R is [[70.6891861 ]
 [70.69754791]
 [70.70610046]
 [70.71481323]
 [70.72361755]].
[2019-04-10 12:26:27,239] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7436801e-21 1.0000000e+00 1.1396973e-25 5.1022304e-28 7.4773384e-34], sum to 1.0000
[2019-04-10 12:26:27,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3348
[2019-04-10 12:26:27,250] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 77.0, 1.0, 2.0, 0.557313716052515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778788.6879580602, 778788.6879580602, 193062.4076944206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5771400.0000, 
sim time next is 5772000.0000, 
raw observation next is [29.0, 78.0, 1.0, 2.0, 0.5512923579204319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770371.4070233355, 770371.4070233355, 192022.4183767498], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.78, 1.0, 1.0, 0.45938838303666485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21399205750648206, 0.21399205750648206, 0.2866006244429101], 
reward next is 0.7134, 
noisyNet noise sample is [array([-1.9867585], dtype=float32), 0.77906096]. 
=============================================
[2019-04-10 12:26:27,262] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.154594]
 [74.16408 ]
 [74.119804]
 [74.11943 ]
 [74.10767 ]], R is [[74.118927  ]
 [74.08958435]
 [74.06181335]
 [74.03394318]
 [74.00606537]].
[2019-04-10 12:26:29,675] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.2973808e-19 1.0000000e+00 2.0008294e-22 3.6357956e-24 3.2710032e-30], sum to 1.0000
[2019-04-10 12:26:29,685] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2562
[2019-04-10 12:26:29,691] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 92.66666666666667, 1.0, 2.0, 0.5262040121936707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735301.0247756034, 735301.0247756029, 187803.544846052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5880000.0000, 
sim time next is 5880600.0000, 
raw observation next is [26.15, 93.0, 1.0, 2.0, 0.5257780160128267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734705.5451753035, 734705.545175304, 187733.5330950924], 
processed observation next is [1.0, 0.043478260869565216, 0.43838862559241704, 0.93, 1.0, 1.0, 0.4286482120636466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20408487365980651, 0.20408487365980668, 0.2801993031270036], 
reward next is 0.7198, 
noisyNet noise sample is [array([-1.0823971], dtype=float32), 1.8379409]. 
=============================================
[2019-04-10 12:26:31,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3041482e-18 1.0000000e+00 1.1929658e-20 1.9955633e-21 2.9614955e-28], sum to 1.0000
[2019-04-10 12:26:31,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5970
[2019-04-10 12:26:31,947] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 84.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.162334949921681, 6.9112, 168.9114930119914, 1632039.177974247, 1453876.944751443, 311352.3946409515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5901600.0000, 
sim time next is 5902200.0000, 
raw observation next is [28.6, 83.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.629502893913581, 6.9112, 168.9087034292206, 1963679.944062809, 1454103.98004513, 311352.6526017103], 
processed observation next is [1.0, 0.30434782608695654, 0.5545023696682465, 0.835, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.07183028939135809, 0.0, 0.8294190605764906, 0.5454666511285581, 0.40391777223475833, 0.46470545164434374], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2298489], dtype=float32), -0.26704988]. 
=============================================
[2019-04-10 12:26:33,002] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0151592e-19 1.0000000e+00 3.6000128e-22 5.0466304e-23 4.3881558e-29], sum to 1.0000
[2019-04-10 12:26:33,008] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3000
[2019-04-10 12:26:33,012] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 93.16666666666667, 1.0, 2.0, 0.7470131953901096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044004.520233728, 1044004.520233727, 231319.4086159989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5976600.0000, 
sim time next is 5977200.0000, 
raw observation next is [26.03333333333333, 93.33333333333334, 1.0, 2.0, 0.6938547687521955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969677.8877956612, 969677.8877956606, 219523.5685585835], 
processed observation next is [1.0, 0.17391304347826086, 0.4328593996840442, 0.9333333333333335, 1.0, 1.0, 0.631150323797826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2693549688321281, 0.26935496883212795, 0.32764711725161716], 
reward next is 0.6724, 
noisyNet noise sample is [array([-0.71801853], dtype=float32), 0.052324276]. 
=============================================
[2019-04-10 12:26:42,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1711375e-11 9.9981636e-01 1.2634959e-11 1.8371432e-04 4.1615771e-17], sum to 1.0000
[2019-04-10 12:26:42,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6465
[2019-04-10 12:26:42,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2350037.37356018 W.
[2019-04-10 12:26:42,604] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.8402572375610423, 1.0, 2.0, 0.8402572375610423, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2350037.37356018, 2350037.37356018, 439941.3754641316], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6088200.0000, 
sim time next is 6088800.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.5926409578927495, 1.0, 2.0, 0.5926409578927495, 1.0, 1.0, 1.02132426345292, 6.9112, 6.9112, 170.5573041426782, 2486389.140135819, 2486389.140135819, 483394.22486107], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.65, 1.0, 1.0, 0.5092059733647584, 1.0, 1.0, 0.5092059733647584, 1.0, 0.5, 1.0260051993328294, 0.0, 0.0, 0.8375144448122397, 0.6906636500377276, 0.6906636500377276, 0.7214839177030895], 
reward next is 0.2785, 
noisyNet noise sample is [array([0.534578], dtype=float32), -0.14954811]. 
=============================================
[2019-04-10 12:26:43,008] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:26:43,010] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:26:43,011] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:26:43,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:26:43,012] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:26:43,012] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:26:43,013] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:26:43,014] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:26:43,014] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:26:43,012] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:26:43,018] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:26:43,031] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run33
[2019-04-10 12:26:43,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run33
[2019-04-10 12:26:43,062] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run33
[2019-04-10 12:26:43,062] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run33
[2019-04-10 12:26:43,102] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run33
[2019-04-10 12:26:47,308] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:26:47,309] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.43333333333333, 44.66666666666667, 1.0, 2.0, 0.2176515670175083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 363202.6012993614, 363202.6012993614, 157177.7204126029]
[2019-04-10 12:26:47,311] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:26:47,313] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4615423e-21 1.0000000e+00 1.3422105e-25 3.9256757e-30 1.5726056e-34], sampled 0.7200521705444529
[2019-04-10 12:27:37,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:37,346] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.48528577333333, 97.16378346333333, 1.0, 2.0, 0.4073034104638502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 607310.9578986963, 607310.9578986957, 175199.0202013614]
[2019-04-10 12:27:37,346] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:27:37,348] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5150905e-20 1.0000000e+00 2.1143190e-24 2.1576542e-27 5.5654078e-33], sampled 0.8625078506225774
[2019-04-10 12:27:44,906] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:44,907] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.774039745, 62.92569448, 1.0, 2.0, 0.624231886793013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872338.3602552846, 872338.3602552846, 205346.8914406849]
[2019-04-10 12:27:44,910] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:27:44,912] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.7535178e-20 1.0000000e+00 9.8517241e-24 2.0675380e-26 4.1594527e-32], sampled 0.37408486155969867
[2019-04-10 12:27:50,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:50,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.00093156333333, 59.57509354333334, 1.0, 2.0, 0.5369207160458916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750281.5001921565, 750281.5001921572, 189582.7386417212]
[2019-04-10 12:27:50,247] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:27:50,249] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.3819544e-20 1.0000000e+00 7.5800769e-24 3.4850849e-24 9.0572883e-33], sampled 0.5610131193790249
[2019-04-10 12:27:52,771] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:52,771] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 75.0, 1.0, 2.0, 0.9643571076472812, 1.0, 2.0, 0.9643571076472812, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2697495.420731256, 2697495.420731256, 507830.0166037648]
[2019-04-10 12:27:52,772] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:27:52,774] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1695227e-10 9.9968219e-01 1.6582603e-10 3.1780981e-04 3.8790896e-16], sampled 0.04153868454196408
[2019-04-10 12:27:52,775] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2697495.420731256 W.
[2019-04-10 12:27:54,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:54,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.22564466166666, 81.91693161500001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.479080961779024, 6.9112, 168.9097634815619, 1859258.275612176, 1456391.592065114, 311727.1951696569]
[2019-04-10 12:27:54,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:27:54,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0563685e-13 1.0000000e+00 3.7947723e-14 9.7933228e-10 5.5287706e-21], sampled 0.6106145718889364
[2019-04-10 12:27:54,221] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1859258.275612176 W.
[2019-04-10 12:27:56,122] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:56,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.45, 79.16666666666667, 1.0, 2.0, 0.9514887268967726, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.004075276328494, 6.9112, 168.9123283900718, 2227103.547223224, 2161214.874690252, 448799.5012173572]
[2019-04-10 12:27:56,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:27:56,128] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2558731e-11 1.0000000e+00 3.6663402e-12 3.2468908e-08 1.1957462e-18], sampled 0.8114581664548615
[2019-04-10 12:27:56,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2227103.547223224 W.
[2019-04-10 12:27:57,005] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:27:57,006] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.37337929333334, 87.95881523666667, 1.0, 2.0, 0.3892948385440283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 590587.8037508847, 590587.8037508841, 173947.416058783]
[2019-04-10 12:27:57,008] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:27:57,010] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4537200e-20 1.0000000e+00 1.1530537e-23 4.2484992e-26 5.3558179e-32], sampled 0.8438470773138507
[2019-04-10 12:28:02,724] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:28:02,725] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 61.0, 1.0, 2.0, 0.7118402900635233, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976022777542, 6.9112, 168.9123160397445, 1891697.682460131, 1824460.564908092, 387250.4559719515]
[2019-04-10 12:28:02,725] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:28:02,728] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.4708844e-11 9.9961287e-01 3.5617852e-11 3.8714198e-04 7.3874806e-18], sampled 0.307509463598872
[2019-04-10 12:28:02,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1891697.682460131 W.
[2019-04-10 12:28:03,542] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:28:03,543] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.68333333333333, 60.33333333333334, 1.0, 2.0, 0.5257630584048668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734684.6366596281, 734684.6366596281, 187731.3094257401]
[2019-04-10 12:28:03,544] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:28:03,554] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.02789486e-20 1.00000000e+00 1.33104481e-24 1.12929666e-27
 2.65349630e-33], sampled 0.6436559237201728
[2019-04-10 12:28:22,422] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12190022], dtype=float32), 0.09588621]
[2019-04-10 12:28:22,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.66407176, 92.417515705, 1.0, 2.0, 0.3797607987382187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575415.895907586, 575415.895907586, 172568.3871606958]
[2019-04-10 12:28:22,424] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:28:22,428] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2063928e-20 1.0000000e+00 1.1604133e-24 7.4606325e-29 2.7599155e-33], sampled 0.6934031709699333
[2019-04-10 12:28:26,359] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2325 2779426324.6545 934.0000
[2019-04-10 12:28:26,368] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.7217 3163959461.2656 1775.0000
[2019-04-10 12:28:26,433] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-04-10 12:28:26,466] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3900 2842356437.4759 1130.0000
[2019-04-10 12:28:26,498] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7647 3007775956.8125 1766.0000
[2019-04-10 12:28:27,515] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 800000, evaluation results [800000.0, 7884.721715444188, 3163959461.265646, 1775.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8660.232518334204, 2779426324.6544776, 934.0, 7998.764734894051, 3007775956.812482, 1766.0, 8497.389968733638, 2842356437.475869, 1130.0]
[2019-04-10 12:28:35,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4353120e-21 1.0000000e+00 3.0312557e-25 8.6016268e-29 1.4742862e-34], sum to 1.0000
[2019-04-10 12:28:35,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0631
[2019-04-10 12:28:35,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.43333333333333, 75.66666666666667, 1.0, 2.0, 0.5180286488138364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723873.1352450701, 723873.1352450701, 186468.7692289273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6372600.0000, 
sim time next is 6373200.0000, 
raw observation next is [28.36666666666667, 76.33333333333334, 1.0, 2.0, 0.5198048218945126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726355.9391702501, 726355.9391702507, 186756.9989314036], 
processed observation next is [0.0, 0.782608695652174, 0.543443917851501, 0.7633333333333334, 1.0, 1.0, 0.42145159264399107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017655386584028, 0.20176553865840297, 0.2787417894498561], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.65921795], dtype=float32), -0.6779925]. 
=============================================
[2019-04-10 12:28:36,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7753072e-21 1.0000000e+00 1.9661054e-25 1.2125149e-28 1.0982207e-33], sum to 1.0000
[2019-04-10 12:28:36,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5651
[2019-04-10 12:28:36,421] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 76.33333333333334, 1.0, 2.0, 0.5230728038745603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730924.0668513494, 730924.06685135, 187290.0561081764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6336600.0000, 
sim time next is 6337200.0000, 
raw observation next is [28.7, 75.66666666666667, 1.0, 2.0, 0.5220087075708716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729436.6244912896, 729436.6244912896, 187116.3963388392], 
processed observation next is [0.0, 0.34782608695652173, 0.5592417061611374, 0.7566666666666667, 1.0, 1.0, 0.42410687659141155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20262128458091377, 0.20262128458091377, 0.2792782034908048], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.09843453], dtype=float32), -0.2872386]. 
=============================================
[2019-04-10 12:28:38,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5386305e-22 1.0000000e+00 3.8168399e-25 8.5637787e-29 1.6797047e-33], sum to 1.0000
[2019-04-10 12:28:38,122] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8808
[2019-04-10 12:28:38,125] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 64.0, 1.0, 2.0, 0.5296479157171096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740115.104356034, 740115.104356034, 188371.7588722059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6363600.0000, 
sim time next is 6364200.0000, 
raw observation next is [31.0, 63.5, 1.0, 2.0, 0.527451184571821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737044.3890597225, 737044.3890597232, 188008.8912373442], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.635, 1.0, 1.0, 0.43066407779737476, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2047345525165896, 0.20473455251658979, 0.28061028542887195], 
reward next is 0.7194, 
noisyNet noise sample is [array([1.2454911], dtype=float32), -0.41354337]. 
=============================================
[2019-04-10 12:28:43,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1162546e-20 1.0000000e+00 6.7714755e-24 9.9033785e-25 1.3703508e-31], sum to 1.0000
[2019-04-10 12:28:43,149] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3364
[2019-04-10 12:28:43,152] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.33333333333334, 1.0, 2.0, 0.5178707242791181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723652.3824592204, 723652.3824592197, 186443.6304068216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6562200.0000, 
sim time next is 6562800.0000, 
raw observation next is [27.2, 85.0, 1.0, 2.0, 0.5191763705832839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725477.46478193, 725477.4647819294, 186655.4129660442], 
processed observation next is [1.0, 1.0, 0.4881516587677725, 0.85, 1.0, 1.0, 0.42069442238949867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20152151799498055, 0.2015215179949804, 0.27859016860603614], 
reward next is 0.7214, 
noisyNet noise sample is [array([-1.3928823], dtype=float32), 0.116327]. 
=============================================
[2019-04-10 12:28:59,017] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0216296e-21 1.0000000e+00 1.2797213e-24 6.2737958e-27 5.9851320e-33], sum to 1.0000
[2019-04-10 12:28:59,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3607
[2019-04-10 12:28:59,026] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.51666666666667, 71.16666666666666, 1.0, 2.0, 0.3309378349317248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518444.9361335237, 518444.9361335237, 168330.5553564526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6821400.0000, 
sim time next is 6822000.0000, 
raw observation next is [24.4, 72.0, 1.0, 2.0, 0.3309746863278688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518330.183853464, 518330.183853464, 168317.4379391805], 
processed observation next is [1.0, 1.0, 0.3554502369668246, 0.72, 1.0, 1.0, 0.19394540521429976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14398060662596224, 0.14398060662596224, 0.2512200566256425], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.35420948], dtype=float32), -0.15224613]. 
=============================================
[2019-04-10 12:28:59,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.44774 ]
 [74.4815  ]
 [74.51358 ]
 [74.54728 ]
 [74.578545]], R is [[74.47190094]
 [74.47594452]
 [74.47993469]
 [74.48392487]
 [74.48795319]].
[2019-04-10 12:29:00,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4609777e-22 1.0000000e+00 6.7218998e-26 2.9109335e-30 1.9278323e-35], sum to 1.0000
[2019-04-10 12:29:00,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5292
[2019-04-10 12:29:00,026] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.93333333333333, 50.66666666666667, 1.0, 2.0, 0.3428301808668725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529361.4641478624, 529361.4641478618, 168987.8760007616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6888000.0000, 
sim time next is 6888600.0000, 
raw observation next is [28.8, 51.5, 1.0, 2.0, 0.3473424583430342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535688.6631004182, 535688.6631004182, 169481.8156276213], 
processed observation next is [0.0, 0.7391304347826086, 0.5639810426540285, 0.515, 1.0, 1.0, 0.21366561246148696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14880240641678283, 0.14880240641678283, 0.25295793377256914], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.06140308], dtype=float32), 0.32066303]. 
=============================================
[2019-04-10 12:29:01,715] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6537870e-21 1.0000000e+00 2.7938024e-25 2.0345879e-29 8.4058435e-34], sum to 1.0000
[2019-04-10 12:29:01,724] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7535
[2019-04-10 12:29:01,731] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 84.0, 1.0, 2.0, 0.4177037662227866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612920.5840332492, 612920.5840332485, 175454.1703259616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6933600.0000, 
sim time next is 6934200.0000, 
raw observation next is [25.01666666666667, 82.83333333333333, 1.0, 2.0, 0.4190303621525407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613990.9822505518, 613990.9822505518, 175531.1922872106], 
processed observation next is [0.0, 0.2608695652173913, 0.3846761453396526, 0.8283333333333333, 1.0, 1.0, 0.3000365809066755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17055305062515327, 0.17055305062515327, 0.2619868541600158], 
reward next is 0.7380, 
noisyNet noise sample is [array([-1.0472528], dtype=float32), 1.3833374]. 
=============================================
[2019-04-10 12:29:02,326] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4421458e-20 1.0000000e+00 1.3908317e-24 1.8557043e-28 3.2080130e-33], sum to 1.0000
[2019-04-10 12:29:02,333] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1366
[2019-04-10 12:29:02,336] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 88.0, 1.0, 2.0, 0.4163090947651873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612018.797796959, 612018.7977969585, 175401.2052912403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6931800.0000, 
sim time next is 6932400.0000, 
raw observation next is [24.4, 86.66666666666667, 1.0, 2.0, 0.4168761409181803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612405.9088191295, 612405.9088191288, 175425.2297648134], 
processed observation next is [0.0, 0.21739130434782608, 0.3554502369668246, 0.8666666666666667, 1.0, 1.0, 0.2974411336363618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17011275244975818, 0.170112752449758, 0.2618287011415126], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.20400333], dtype=float32), 0.6309651]. 
=============================================
[2019-04-10 12:29:07,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7817567e-20 1.0000000e+00 2.8179737e-23 3.0623725e-23 1.3258103e-31], sum to 1.0000
[2019-04-10 12:29:07,735] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4133
[2019-04-10 12:29:07,740] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 77.33333333333334, 1.0, 2.0, 0.4763694024420677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665641.8819275071, 665641.8819275071, 179976.8854039479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7064400.0000, 
sim time next is 7065000.0000, 
raw observation next is [27.25, 78.0, 1.0, 2.0, 0.4790616709608624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669405.035718534, 669405.0357185346, 180380.7580126614], 
processed observation next is [1.0, 0.782608695652174, 0.490521327014218, 0.78, 1.0, 1.0, 0.3723634589889908, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18594584325514835, 0.1859458432551485, 0.2692250119591961], 
reward next is 0.7308, 
noisyNet noise sample is [array([-0.7115388], dtype=float32), 0.6024346]. 
=============================================
[2019-04-10 12:29:07,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.05193 ]
 [70.293304]
 [69.95741 ]
 [69.00693 ]
 [67.88815 ]], R is [[70.2197113 ]
 [70.24889374]
 [70.27823639]
 [70.30809021]
 [70.33839417]].
[2019-04-10 12:29:07,952] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 12:29:07,954] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:29:07,954] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:29:07,954] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:07,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:07,955] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:29:07,956] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:29:07,957] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:29:07,957] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:07,958] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:07,957] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:07,975] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run34
[2019-04-10 12:29:07,975] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run34
[2019-04-10 12:29:07,976] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run34
[2019-04-10 12:29:08,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run34
[2019-04-10 12:29:08,051] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run34
[2019-04-10 12:29:17,671] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12345459], dtype=float32), 0.094649255]
[2019-04-10 12:29:17,671] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.26666666666667, 54.5, 1.0, 2.0, 0.4705730174011961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766204.2374903259, 766204.2374903265, 190516.0469403185]
[2019-04-10 12:29:17,672] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:29:17,673] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3747574e-19 1.0000000e+00 5.0782435e-23 4.6217934e-26 1.7223756e-31], sampled 0.3122082411356423
[2019-04-10 12:29:24,510] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12345459], dtype=float32), 0.094649255]
[2019-04-10 12:29:24,511] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.76666666666667, 78.83333333333333, 1.0, 2.0, 0.2293346518298193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380857.9473852812, 380857.9473852806, 158678.2351256814]
[2019-04-10 12:29:24,512] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:29:24,516] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4822938e-21 1.0000000e+00 5.1931946e-25 3.8925707e-30 5.3719775e-34], sampled 0.13041357265581244
[2019-04-10 12:29:29,264] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12345459], dtype=float32), 0.094649255]
[2019-04-10 12:29:29,265] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.30135908, 86.53061567, 1.0, 2.0, 0.4009002312902765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606139.711374938, 606139.711374938, 175308.0411949753]
[2019-04-10 12:29:29,265] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:29:29,267] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5414115e-20 1.0000000e+00 3.8529020e-24 6.4089668e-27 5.7327930e-33], sampled 0.6435274026180456
[2019-04-10 12:29:31,552] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12345459], dtype=float32), 0.094649255]
[2019-04-10 12:29:31,553] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.37550707666667, 94.34121869999998, 1.0, 2.0, 0.4812563824483178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 676926.4679552651, 676926.4679552657, 181284.7182711499]
[2019-04-10 12:29:31,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:29:31,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.9047286e-20 1.0000000e+00 1.8890207e-23 1.2705751e-26 6.3568924e-32], sampled 0.06646098124567368
[2019-04-10 12:30:15,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12345459], dtype=float32), 0.094649255]
[2019-04-10 12:30:15,528] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.10063646333334, 82.04792521333333, 1.0, 2.0, 0.5389419970769102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753106.9962591572, 753106.9962591578, 189922.4179290982]
[2019-04-10 12:30:15,528] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:30:15,529] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7302736e-21 1.0000000e+00 2.4659545e-25 1.2577799e-28 2.3655563e-34], sampled 0.21852930769578838
[2019-04-10 12:30:17,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12345459], dtype=float32), 0.094649255]
[2019-04-10 12:30:17,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.06666666666667, 50.0, 1.0, 2.0, 0.5866999734578495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819868.813426668, 819868.813426668, 198294.6221999544]
[2019-04-10 12:30:17,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:30:17,131] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4365452e-20 1.0000000e+00 3.3510748e-24 2.8207428e-27 4.2293221e-33], sampled 0.8833653301827687
[2019-04-10 12:30:38,634] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 12:30:38,643] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7793 2779329268.8772 933.0000
[2019-04-10 12:30:38,826] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.2754 3163668860.2921 1768.0000
[2019-04-10 12:30:38,957] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0444 2842348130.8811 1130.0000
[2019-04-10 12:30:38,972] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-10 12:30:39,987] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 825000, evaluation results [825000.0, 7885.275426018458, 3163668860.292074, 1768.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8659.779274702447, 2779329268.877203, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8498.04442055128, 2842348130.8810706, 1130.0]
[2019-04-10 12:30:49,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7523368e-21 1.0000000e+00 1.9340674e-24 4.9461357e-29 4.4392294e-33], sum to 1.0000
[2019-04-10 12:30:49,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8031
[2019-04-10 12:30:49,518] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 88.66666666666667, 1.0, 2.0, 0.3514389348955866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554537.0874032846, 554537.0874032853, 171297.7065912789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7279800.0000, 
sim time next is 7280400.0000, 
raw observation next is [21.8, 88.33333333333334, 1.0, 2.0, 0.3245502997702647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512005.7534928402, 512005.7534928408, 167911.230375331], 
processed observation next is [1.0, 0.2608695652173913, 0.23222748815165886, 0.8833333333333334, 1.0, 1.0, 0.18620518044610207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14222382041467785, 0.142223820414678, 0.2506137766795985], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.12476537], dtype=float32), -0.97442853]. 
=============================================
[2019-04-10 12:30:52,922] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5391358e-21 1.0000000e+00 1.8859309e-24 1.1865485e-27 2.4000889e-33], sum to 1.0000
[2019-04-10 12:30:52,924] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1669
[2019-04-10 12:30:52,933] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 82.0, 1.0, 2.0, 0.3771318744214461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.9689907632, 571243.9689907638, 172194.9392325905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7357800.0000, 
sim time next is 7358400.0000, 
raw observation next is [24.1, 83.0, 1.0, 2.0, 0.3776804476273991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570742.1987594216, 570742.1987594216, 172110.4719952524], 
processed observation next is [1.0, 0.17391304347826086, 0.3412322274881518, 0.83, 1.0, 1.0, 0.25021740677999893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15853949965539488, 0.15853949965539488, 0.25688130148545135], 
reward next is 0.7431, 
noisyNet noise sample is [array([0.5935683], dtype=float32), -0.8147364]. 
=============================================
[2019-04-10 12:30:56,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7638621e-21 1.0000000e+00 2.8754584e-25 5.7615876e-30 3.8475360e-34], sum to 1.0000
[2019-04-10 12:30:56,177] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6127
[2019-04-10 12:30:56,180] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.25, 95.0, 1.0, 2.0, 0.3245182230692261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508622.2419579999, 508622.2419580005, 167576.5750021531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7450200.0000, 
sim time next is 7450800.0000, 
raw observation next is [21.26666666666667, 95.0, 1.0, 2.0, 0.324788456360997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508829.2329260107, 508829.2329260114, 167586.9011929026], 
processed observation next is [0.0, 0.21739130434782608, 0.2069510268562403, 0.95, 1.0, 1.0, 0.18649211609758676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14134145359055852, 0.14134145359055872, 0.25012970327298895], 
reward next is 0.7499, 
noisyNet noise sample is [array([-1.2652642], dtype=float32), -2.0141108]. 
=============================================
[2019-04-10 12:30:59,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3220149e-20 1.0000000e+00 2.0721355e-24 1.0233015e-28 9.0202174e-34], sum to 1.0000
[2019-04-10 12:30:59,237] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7134
[2019-04-10 12:30:59,242] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 83.0, 1.0, 2.0, 0.3911180894396441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581906.0985251336, 581906.0985251341, 172825.8553227844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7476000.0000, 
sim time next is 7476600.0000, 
raw observation next is [24.68333333333333, 82.5, 1.0, 2.0, 0.3929669905831121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583741.7616279752, 583741.7616279759, 172963.5841207946], 
processed observation next is [0.0, 0.5217391304347826, 0.3688783570300157, 0.825, 1.0, 1.0, 0.26863492841338804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1621504893411042, 0.1621504893411044, 0.2581546031653651], 
reward next is 0.7418, 
noisyNet noise sample is [array([-1.3826658], dtype=float32), -0.39759722]. 
=============================================
[2019-04-10 12:31:02,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3843664e-20 1.0000000e+00 2.0868584e-24 1.1789777e-27 3.8605788e-33], sum to 1.0000
[2019-04-10 12:31:02,384] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0091
[2019-04-10 12:31:02,388] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 89.0, 1.0, 2.0, 0.484058537974898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676389.5077499435, 676389.5077499435, 181136.0137621213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7594200.0000, 
sim time next is 7594800.0000, 
raw observation next is [25.53333333333333, 89.33333333333334, 1.0, 2.0, 0.4829224890743254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674801.5683662131, 674801.5683662125, 180963.5920637619], 
processed observation next is [0.0, 0.9130434782608695, 0.4091627172195892, 0.8933333333333334, 1.0, 1.0, 0.3770150470775005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18744488010172586, 0.1874448801017257, 0.27009491352800286], 
reward next is 0.7299, 
noisyNet noise sample is [array([1.3463899], dtype=float32), -0.6254679]. 
=============================================
[2019-04-10 12:31:03,696] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.3141805e-22 1.0000000e+00 7.5168362e-26 5.3801793e-27 6.1103935e-35], sum to 1.0000
[2019-04-10 12:31:03,700] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-10 12:31:03,705] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 84.83333333333333, 1.0, 2.0, 0.5105080214043148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713360.5735815713, 713360.5735815706, 185258.7629883675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7672200.0000, 
sim time next is 7672800.0000, 
raw observation next is [26.7, 85.66666666666667, 1.0, 2.0, 0.5095597186048926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712035.0144809416, 712035.0144809422, 185107.3575488465], 
processed observation next is [1.0, 0.8260869565217391, 0.46445497630331756, 0.8566666666666667, 1.0, 1.0, 0.40910809470468984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19778750402248377, 0.19778750402248393, 0.27627963813260675], 
reward next is 0.7237, 
noisyNet noise sample is [array([0.5713436], dtype=float32), -0.641155]. 
=============================================
[2019-04-10 12:31:05,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1058913e-13 1.0000000e+00 8.5073353e-14 4.4635833e-09 8.4581358e-21], sum to 1.0000
[2019-04-10 12:31:05,154] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5398
[2019-04-10 12:31:05,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2850915.604252574 W.
[2019-04-10 12:31:05,165] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.96666666666667, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 8.879474719867023, 6.9112, 168.9016594941036, 2850915.604252574, 1454647.164844079, 309580.9420870952], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7658400.0000, 
sim time next is 7659000.0000, 
raw observation next is [29.85, 66.5, 1.0, 2.0, 0.5200988357606138, 1.0, 1.0, 0.5200988357606138, 1.0, 1.0, 0.8872531196832434, 6.9112, 6.9112, 170.5573041426782, 2181772.910186344, 2181772.910186344, 426274.647652061], 
processed observation next is [1.0, 0.6521739130434783, 0.613744075829384, 0.665, 1.0, 1.0, 0.4218058262176069, 1.0, 0.5, 0.4218058262176069, 1.0, 0.5, 0.8625038044917602, 0.0, 0.0, 0.8375144448122397, 0.6060480306073177, 0.6060480306073177, 0.6362308173911358], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5974314], dtype=float32), -0.414994]. 
=============================================
[2019-04-10 12:31:05,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[53.212425]
 [51.52926 ]
 [49.519905]
 [47.826485]
 [46.95045 ]], R is [[49.61257935]
 [49.11645508]
 [49.13804245]
 [48.64666367]
 [48.21880722]].
[2019-04-10 12:31:06,035] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2853798e-19 1.0000000e+00 2.2763971e-22 1.1483043e-23 4.1515131e-31], sum to 1.0000
[2019-04-10 12:31:06,041] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-10 12:31:06,051] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 88.0, 1.0, 2.0, 0.4853202563005086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678153.1073449855, 678153.1073449855, 181327.514845985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7684800.0000, 
sim time next is 7685400.0000, 
raw observation next is [25.53333333333333, 88.0, 1.0, 2.0, 0.4831288606916362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675090.0290286097, 675090.0290286097, 180994.4590735345], 
processed observation next is [1.0, 0.9565217391304348, 0.4091627172195892, 0.88, 1.0, 1.0, 0.37726368758028456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1875250080635027, 0.1875250080635027, 0.27014098369184253], 
reward next is 0.7299, 
noisyNet noise sample is [array([0.7269], dtype=float32), -0.24323948]. 
=============================================
[2019-04-10 12:31:07,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3697153e-21 1.0000000e+00 3.0176878e-25 3.2104099e-27 3.4083119e-34], sum to 1.0000
[2019-04-10 12:31:07,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1567
[2019-04-10 12:31:07,923] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 88.16666666666667, 1.0, 2.0, 0.5049603361901578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705605.917185019, 705605.9171850183, 184376.9016848402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7674600.0000, 
sim time next is 7675200.0000, 
raw observation next is [26.1, 89.0, 1.0, 2.0, 0.5042556198532036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704620.8557840367, 704620.8557840367, 184265.4910945388], 
processed observation next is [1.0, 0.8695652173913043, 0.4360189573459717, 0.89, 1.0, 1.0, 0.4027176142809681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19572801549556573, 0.19572801549556573, 0.2750231210366251], 
reward next is 0.7250, 
noisyNet noise sample is [array([3.1516721], dtype=float32), -0.80069476]. 
=============================================
[2019-04-10 12:31:08,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0493381e-12 1.0000000e+00 6.2222157e-13 2.7297777e-09 1.6810436e-19], sum to 1.0000
[2019-04-10 12:31:08,990] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7168
[2019-04-10 12:31:08,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1743362.445580987 W.
[2019-04-10 12:31:09,002] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.65, 77.0, 1.0, 2.0, 0.6235059799736856, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.950973477727713, 6.9112, 168.912685318565, 1743362.445580987, 1715145.816885713, 369732.3268343509], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7810200.0000, 
sim time next is 7810800.0000, 
raw observation next is [28.76666666666667, 76.33333333333334, 1.0, 2.0, 0.5851466765764494, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9970616269701263, 6.9112, 6.9112, 168.9129241251938, 1636024.717210978, 1636024.717210978, 354039.5149948676], 
processed observation next is [1.0, 0.391304347826087, 0.5624012638230649, 0.7633333333333334, 1.0, 1.0, 0.5001767187668065, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9964166182562515, 0.0, 0.0, 0.829439786125962, 0.4544513103363828, 0.4544513103363828, 0.5284171865595039], 
reward next is 0.4716, 
noisyNet noise sample is [array([0.2738877], dtype=float32), -2.0081909]. 
=============================================
[2019-04-10 12:31:11,900] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6677743e-20 1.0000000e+00 5.0345956e-22 1.1313427e-23 1.7854830e-30], sum to 1.0000
[2019-04-10 12:31:11,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8995
[2019-04-10 12:31:11,912] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666666, 88.5, 1.0, 2.0, 0.5782649213866234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808076.9826366856, 808076.9826366856, 196760.9204017919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7797000.0000, 
sim time next is 7797600.0000, 
raw observation next is [26.2, 88.0, 1.0, 2.0, 0.6261083496733422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874961.7199992255, 874961.7199992255, 205702.1248106308], 
processed observation next is [1.0, 0.2608695652173913, 0.44075829383886256, 0.88, 1.0, 1.0, 0.5495281321365568, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24304492222200708, 0.24304492222200708, 0.3070180967322848], 
reward next is 0.6930, 
noisyNet noise sample is [array([-1.8617268], dtype=float32), -0.6248441]. 
=============================================
[2019-04-10 12:31:15,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:15,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:15,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-04-10 12:31:16,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.6877255e-10 8.6450988e-01 1.6005247e-09 1.3549015e-01 2.6790375e-15], sum to 1.0000
[2019-04-10 12:31:16,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6663
[2019-04-10 12:31:16,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2166390.758244282 W.
[2019-04-10 12:31:16,065] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.7, 72.0, 1.0, 2.0, 0.7746535315426415, 1.0, 2.0, 0.7746535315426415, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2166390.758244282, 2166390.758244282, 407717.483984904], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7903800.0000, 
sim time next is 7904400.0000, 
raw observation next is [29.73333333333333, 72.0, 1.0, 2.0, 0.7179270921961691, 1.0, 2.0, 0.7179270921961691, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2007601.331349029, 2007601.331349029, 381892.0036964489], 
processed observation next is [1.0, 0.4782608695652174, 0.6082148499210109, 0.72, 1.0, 1.0, 0.6601531231279146, 1.0, 1.0, 0.6601531231279146, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5576670364858414, 0.5576670364858414, 0.5699880652185805], 
reward next is 0.4300, 
noisyNet noise sample is [array([-0.24449861], dtype=float32), 0.92634493]. 
=============================================
[2019-04-10 12:31:16,127] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:16,128] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:16,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-04-10 12:31:16,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:16,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:16,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-04-10 12:31:16,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:16,514] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:16,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-04-10 12:31:16,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:16,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:16,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-04-10 12:31:16,822] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:16,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:16,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-04-10 12:31:17,106] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:17,107] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:17,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-04-10 12:31:17,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:17,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:17,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-04-10 12:31:17,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:17,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:17,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run5
[2019-04-10 12:31:17,739] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6662838e-13 1.0000000e+00 4.4364219e-13 1.3022166e-09 8.8992905e-20], sum to 1.0000
[2019-04-10 12:31:17,739] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3660
[2019-04-10 12:31:17,739] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1867581.460468683 W.
[2019-04-10 12:31:17,743] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.58333333333334, 64.5, 1.0, 2.0, 0.6646309084338591, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.938306279159991, 6.9112, 168.9127397388051, 1867581.460468683, 1848351.357785119, 384065.5506900233], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 53400.0000, 
sim time next is 54000.0000, 
raw observation next is [27.5, 65.0, 1.0, 2.0, 0.4225322516127769, 1.0, 1.0, 0.4225322516127769, 1.0, 2.0, 0.7207297914087466, 6.9112, 6.9112, 170.5573041426782, 1836450.03306294, 1836450.03306294, 369146.125003136], 
processed observation next is [1.0, 0.6521739130434783, 0.5023696682464456, 0.65, 1.0, 1.0, 0.304255724834671, 1.0, 0.5, 0.304255724834671, 1.0, 1.0, 0.6594265748887153, 0.0, 0.0, 0.8375144448122397, 0.51012500918415, 0.51012500918415, 0.5509643656763223], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8819786], dtype=float32), 0.17567222]. 
=============================================
[2019-04-10 12:31:17,748] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[49.70526 ]
 [49.80598 ]
 [52.599564]
 [54.873444]
 [58.63018 ]], R is [[48.24435043]
 [47.76190567]
 [47.2842865 ]
 [46.81144333]
 [46.75225449]].
[2019-04-10 12:31:17,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:17,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:17,750] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-04-10 12:31:17,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:17,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:17,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-04-10 12:31:18,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:18,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:18,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-04-10 12:31:18,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:18,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:18,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-04-10 12:31:18,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:18,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:18,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-04-10 12:31:18,218] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:18,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:18,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-04-10 12:31:18,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:31:18,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:18,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-04-10 12:31:21,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4930567e-17 1.0000000e+00 1.4765825e-19 1.7912645e-19 1.5731092e-27], sum to 1.0000
[2019-04-10 12:31:21,169] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4601
[2019-04-10 12:31:21,175] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 62.5, 1.0, 2.0, 0.883599305018736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1322282.517607203, 1322282.517607203, 277843.0599871189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 48600.0000, 
sim time next is 49200.0000, 
raw observation next is [27.8, 62.33333333333333, 1.0, 2.0, 0.9097782116964279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1358494.337185492, 1358494.337185492, 285163.0603203893], 
processed observation next is [1.0, 0.5652173913043478, 0.5165876777251186, 0.6233333333333333, 1.0, 1.0, 0.8912990502366601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3773595381070811, 0.3773595381070811, 0.4256165079408795], 
reward next is 0.5744, 
noisyNet noise sample is [array([0.15815757], dtype=float32), 0.31375197]. 
=============================================
[2019-04-10 12:31:21,907] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-10 12:31:21,908] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:31:21,909] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:31:21,909] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:31:21,910] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:21,910] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:21,911] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:31:21,912] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:31:21,913] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:21,913] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:21,911] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:21,929] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run35
[2019-04-10 12:31:21,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run35
[2019-04-10 12:31:21,969] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run35
[2019-04-10 12:31:21,986] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run35
[2019-04-10 12:31:22,004] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run35
[2019-04-10 12:31:32,120] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12432214], dtype=float32), 0.09321228]
[2019-04-10 12:31:32,121] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.6, 87.0, 1.0, 2.0, 0.3431871569859712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532871.1752988446, 532871.1752988439, 169355.9569176638]
[2019-04-10 12:31:32,123] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:31:32,124] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3758921e-20 1.0000000e+00 6.0611879e-24 4.9146540e-28 5.7303443e-33], sampled 0.6093220490802406
[2019-04-10 12:31:44,426] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12432214], dtype=float32), 0.09321228]
[2019-04-10 12:31:44,427] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.19487643, 83.81931551, 1.0, 2.0, 0.4117542913253303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 614101.1034539838, 614101.1034539844, 175837.2074300173]
[2019-04-10 12:31:44,428] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:31:44,431] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4983320e-20 1.0000000e+00 1.5147010e-23 5.5983252e-26 1.3621623e-32], sampled 0.6587782777471418
[2019-04-10 12:32:43,354] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12432214], dtype=float32), 0.09321228]
[2019-04-10 12:32:43,355] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.13333333333334, 77.33333333333334, 1.0, 2.0, 0.5907353025666356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 825510.0718481813, 825510.0718481807, 199032.1163648024]
[2019-04-10 12:32:43,356] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:32:43,357] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.1513221e-20 1.0000000e+00 4.3000123e-23 9.5146486e-26 5.9892231e-32], sampled 0.8087568509700238
[2019-04-10 12:32:53,336] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12432214], dtype=float32), 0.09321228]
[2019-04-10 12:32:53,338] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.532833595, 90.749898495, 1.0, 2.0, 0.3738423605643421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571778.4401103669, 571778.4401103675, 172398.6542205903]
[2019-04-10 12:32:53,340] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:32:53,342] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.6760384e-20 1.0000000e+00 2.4408193e-23 6.6916852e-26 2.3938634e-32], sampled 0.6535503275210804
[2019-04-10 12:33:00,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.2811 3163352996.7710 1752.0000
[2019-04-10 12:33:00,515] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8632 2779302024.3122 932.0000
[2019-04-10 12:33:00,533] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.1307 2842019846.4842 1124.0000
[2019-04-10 12:33:00,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.7886 3007560236.4953 1760.0000
[2019-04-10 12:33:00,602] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.9687 2927415753.3846 1337.0000
[2019-04-10 12:33:01,615] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 850000, evaluation results [850000.0, 7894.281133236639, 3163352996.770995, 1752.0, 8253.968746332052, 2927415753.38457, 1337.0, 8659.863162898208, 2779302024.312172, 932.0, 8000.788585642043, 3007560236.495273, 1760.0, 8499.130665624494, 2842019846.4841886, 1124.0]
[2019-04-10 12:33:01,903] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2204548e-20 1.0000000e+00 3.3619728e-23 2.6691515e-25 3.5436547e-32], sum to 1.0000
[2019-04-10 12:33:01,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8971
[2019-04-10 12:33:01,917] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 89.0, 1.0, 2.0, 0.341691003898236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532156.9568278198, 532156.9568278198, 169340.608502679], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 84000.0000, 
sim time next is 84600.0000, 
raw observation next is [22.25, 89.0, 1.0, 2.0, 0.3419217873529487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532283.6402900005, 532283.6402900012, 169344.8744007905], 
processed observation next is [1.0, 1.0, 0.2535545023696683, 0.89, 1.0, 1.0, 0.2071346835577695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14785656674722236, 0.14785656674722256, 0.25275354388177684], 
reward next is 0.7472, 
noisyNet noise sample is [array([0.67842907], dtype=float32), -0.6147876]. 
=============================================
[2019-04-10 12:33:06,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9510985e-21 1.0000000e+00 6.0542554e-24 1.3534292e-25 9.1822492e-33], sum to 1.0000
[2019-04-10 12:33:06,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4108
[2019-04-10 12:33:06,031] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 96.0, 1.0, 2.0, 0.3849870078901698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581361.768807654, 581361.7688076535, 173041.3148942467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 155400.0000, 
sim time next is 156000.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3837643577247886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579520.9411866262, 579520.9411866268, 172876.7395139545], 
processed observation next is [1.0, 0.8260869565217391, 0.2606635071090047, 0.96, 1.0, 1.0, 0.25754741894552846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16097803921850729, 0.16097803921850745, 0.2580249843491858], 
reward next is 0.7420, 
noisyNet noise sample is [array([-1.5073248], dtype=float32), -0.74244493]. 
=============================================
[2019-04-10 12:33:06,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.493256]
 [73.27689 ]
 [73.06549 ]
 [72.88452 ]
 [72.6762  ]], R is [[73.68560028]
 [73.69047546]
 [73.69523621]
 [73.70018005]
 [73.70562744]].
[2019-04-10 12:33:06,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4637365e-22 1.0000000e+00 3.3388380e-24 3.8300214e-27 3.4388345e-33], sum to 1.0000
[2019-04-10 12:33:06,905] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1141
[2019-04-10 12:33:06,908] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 96.0, 1.0, 2.0, 0.3012043923353655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481151.2677636385, 481151.2677636392, 165724.1129445849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 174600.0000, 
sim time next is 175200.0000, 
raw observation next is [20.26666666666667, 96.0, 1.0, 2.0, 0.2998768415748062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479368.807101741, 479368.807101741, 165600.5459385222], 
processed observation next is [0.0, 0.0, 0.15955766192733034, 0.96, 1.0, 1.0, 0.15647812237928457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13315800197270583, 0.13315800197270583, 0.24716499393809285], 
reward next is 0.7528, 
noisyNet noise sample is [array([1.4921519], dtype=float32), -1.5980529]. 
=============================================
[2019-04-10 12:33:08,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3340734e-22 1.0000000e+00 2.0405850e-25 1.1955525e-29 4.3960840e-35], sum to 1.0000
[2019-04-10 12:33:08,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0666
[2019-04-10 12:33:08,072] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 86.0, 1.0, 2.0, 0.3147507185463219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 497559.3291075058, 497559.3291075052, 166839.7974174898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 226800.0000, 
sim time next is 227400.0000, 
raw observation next is [21.96666666666667, 86.16666666666667, 1.0, 2.0, 0.3143532581937268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497142.6088022541, 497142.6088022548, 166813.0386222106], 
processed observation next is [0.0, 0.6521739130434783, 0.24012638230647723, 0.8616666666666667, 1.0, 1.0, 0.17391958818521297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13809516911173725, 0.13809516911173744, 0.24897468451076207], 
reward next is 0.7510, 
noisyNet noise sample is [array([1.0844249], dtype=float32), -0.3111625]. 
=============================================
[2019-04-10 12:33:08,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5385565e-21 1.0000000e+00 3.8133472e-25 2.3942415e-29 2.2555083e-34], sum to 1.0000
[2019-04-10 12:33:08,595] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-10 12:33:08,600] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.78333333333333, 92.16666666666667, 1.0, 2.0, 0.3010305345907621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480659.1345238702, 480659.1345238708, 165686.503816592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 211800.0000, 
sim time next is 212400.0000, 
raw observation next is [20.8, 92.0, 1.0, 2.0, 0.3002782339955276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479492.7533179404, 479492.753317941, 165603.6485975491], 
processed observation next is [0.0, 0.4782608695652174, 0.1848341232227489, 0.92, 1.0, 1.0, 0.15696172770545494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13319243147720566, 0.13319243147720583, 0.24716962477246132], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.06114525], dtype=float32), -0.6512401]. 
=============================================
[2019-04-10 12:33:12,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2157515e-21 1.0000000e+00 5.0802337e-25 8.9562947e-29 1.0071877e-34], sum to 1.0000
[2019-04-10 12:33:12,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0912
[2019-04-10 12:33:12,826] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 86.0, 1.0, 2.0, 0.2733448314107534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442603.5072770095, 442603.5072770101, 163104.8118955987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 340200.0000, 
sim time next is 340800.0000, 
raw observation next is [20.76666666666667, 86.0, 1.0, 2.0, 0.2702746393103583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 437882.283578461, 437882.283578461, 162795.8371069399], 
processed observation next is [0.0, 0.9565217391304348, 0.18325434439178534, 0.86, 1.0, 1.0, 0.12081281844621479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1216339676606836, 0.1216339676606836, 0.24297886135364163], 
reward next is 0.7570, 
noisyNet noise sample is [array([0.36970457], dtype=float32), 1.3559909]. 
=============================================
[2019-04-10 12:33:17,856] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0281262e-20 1.0000000e+00 1.8232759e-24 8.3861393e-28 4.0206939e-34], sum to 1.0000
[2019-04-10 12:33:17,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-10 12:33:17,868] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.35, 81.0, 1.0, 2.0, 0.2483863533733219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408670.3053262848, 408670.3053262854, 160744.8091226011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 419400.0000, 
sim time next is 420000.0000, 
raw observation next is [20.3, 81.33333333333334, 1.0, 2.0, 0.2477303762414592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 407624.7265960263, 407624.7265960256, 160680.0559784349], 
processed observation next is [1.0, 0.8695652173913043, 0.16113744075829392, 0.8133333333333335, 1.0, 1.0, 0.09365105571260142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1132290907211184, 0.11322909072111821, 0.23982097907229089], 
reward next is 0.7602, 
noisyNet noise sample is [array([0.57687813], dtype=float32), 0.71620053]. 
=============================================
[2019-04-10 12:33:17,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.69984 ]
 [76.64629 ]
 [76.574036]
 [76.5022  ]
 [76.320816]], R is [[76.70368958]
 [76.6967392 ]
 [76.6897583 ]
 [76.68275452]
 [76.6756134 ]].
[2019-04-10 12:33:18,769] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.469772e-21 1.000000e+00 3.394653e-24 9.487130e-28 3.952022e-34], sum to 1.0000
[2019-04-10 12:33:18,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2888
[2019-04-10 12:33:18,781] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.86666666666666, 82.0, 1.0, 2.0, 0.229338830578283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378904.9235685278, 378904.9235685278, 158880.38700392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [19.9, 82.0, 1.0, 2.0, 0.2303264801330011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380402.9795722726, 380402.9795722726, 158978.7952441557], 
processed observation next is [1.0, 0.2608695652173913, 0.14218009478672985, 0.82, 1.0, 1.0, 0.07268250618433868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10566749432563129, 0.10566749432563129, 0.23728178394650107], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.8309453], dtype=float32), -0.020980285]. 
=============================================
[2019-04-10 12:33:30,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2917451e-20 1.0000000e+00 7.4401359e-23 3.3516373e-24 1.3885789e-31], sum to 1.0000
[2019-04-10 12:33:30,343] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8807
[2019-04-10 12:33:30,346] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 49.66666666666666, 1.0, 2.0, 0.6109809502398384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1006422.373508957, 1006422.373508958, 217980.0518838072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 747600.0000, 
sim time next is 748200.0000, 
raw observation next is [25.16666666666667, 49.83333333333334, 1.0, 2.0, 0.6262301618591973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031855.432564975, 1031855.432564975, 221349.0490915797], 
processed observation next is [1.0, 0.6521739130434783, 0.39178515007898923, 0.4983333333333334, 1.0, 1.0, 0.5496748938062618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28662650904582637, 0.28662650904582637, 0.33037171506205926], 
reward next is 0.6696, 
noisyNet noise sample is [array([0.0021733], dtype=float32), 0.5440374]. 
=============================================
[2019-04-10 12:33:30,719] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0305798e-21 1.0000000e+00 8.4885023e-25 9.6394720e-28 3.3790948e-33], sum to 1.0000
[2019-04-10 12:33:30,727] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0710
[2019-04-10 12:33:30,732] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 82.0, 1.0, 2.0, 0.2567924316637124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421572.8536829458, 421572.8536829464, 161587.8803266313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 766800.0000, 
sim time next is 767400.0000, 
raw observation next is [20.33333333333333, 82.50000000000001, 1.0, 2.0, 0.2573163386722432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 422445.1565407084, 422445.1565407078, 161640.7203193426], 
processed observation next is [1.0, 0.9130434782608695, 0.16271721958925733, 0.8250000000000002, 1.0, 1.0, 0.1052004080388472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11734587681686344, 0.11734587681686327, 0.24125480644678], 
reward next is 0.7587, 
noisyNet noise sample is [array([0.87283605], dtype=float32), 0.17121622]. 
=============================================
[2019-04-10 12:33:30,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4842417e-20 1.0000000e+00 5.8600476e-23 1.9426808e-25 6.6328050e-32], sum to 1.0000
[2019-04-10 12:33:30,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3245
[2019-04-10 12:33:30,888] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 50.66666666666667, 1.0, 2.0, 0.6062070162912763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 990598.8902239722, 990598.8902239722, 216869.7082294722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 739200.0000, 
sim time next is 739800.0000, 
raw observation next is [25.8, 50.0, 1.0, 2.0, 0.6286001612381256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1027981.185747482, 1027981.185747482, 221830.122651141], 
processed observation next is [1.0, 0.5652173913043478, 0.42180094786729866, 0.5, 1.0, 1.0, 0.5525303147447296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2855503293743006, 0.2855503293743006, 0.3310897353002104], 
reward next is 0.6689, 
noisyNet noise sample is [array([0.23261647], dtype=float32), 1.3078468]. 
=============================================
[2019-04-10 12:33:31,674] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.8320919e-22 1.0000000e+00 3.5582969e-25 4.0451058e-28 1.2208602e-35], sum to 1.0000
[2019-04-10 12:33:31,682] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8126
[2019-04-10 12:33:31,686] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 77.16666666666667, 1.0, 2.0, 0.2526953037791749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415038.7934701332, 415038.7934701332, 161177.4105638153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 762600.0000, 
sim time next is 763200.0000, 
raw observation next is [20.8, 79.0, 1.0, 2.0, 0.2538892108566228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416831.1548951771, 416831.1548951764, 161296.7499757389], 
processed observation next is [1.0, 0.8695652173913043, 0.1848341232227489, 0.79, 1.0, 1.0, 0.10107133838147325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11578643191532698, 0.11578643191532678, 0.24074141787423717], 
reward next is 0.7593, 
noisyNet noise sample is [array([1.4996612], dtype=float32), 1.7350391]. 
=============================================
[2019-04-10 12:33:34,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1634730e-22 1.0000000e+00 4.8167359e-25 1.7755336e-29 1.9252748e-35], sum to 1.0000
[2019-04-10 12:33:34,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-10 12:33:34,213] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 76.0, 1.0, 2.0, 0.3223766121397979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504887.1789225037, 504887.1789225037, 167281.2224847983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 927000.0000, 
sim time next is 927600.0000, 
raw observation next is [23.73333333333333, 76.66666666666667, 1.0, 2.0, 0.3226985744614804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505003.2668212441, 505003.2668212448, 167279.5888668344], 
processed observation next is [0.0, 0.7391304347826086, 0.3238546603475513, 0.7666666666666667, 1.0, 1.0, 0.18397418609816918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14027868522812337, 0.14027868522812356, 0.24967102815945436], 
reward next is 0.7503, 
noisyNet noise sample is [array([1.4154419], dtype=float32), -1.4672914]. 
=============================================
[2019-04-10 12:33:39,531] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1704175e-20 1.0000000e+00 4.8613755e-23 1.6247936e-25 9.9980063e-33], sum to 1.0000
[2019-04-10 12:33:39,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1320
[2019-04-10 12:33:39,543] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.16666666666667, 1.0, 2.0, 0.3442852464206104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532953.7233094486, 532953.7233094486, 169317.6355052818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 960600.0000, 
sim time next is 961200.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3433815576369807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531856.691001625, 531856.6910016245, 169237.5690268535], 
processed observation next is [1.0, 0.13043478260869565, 0.23222748815165886, 0.94, 1.0, 1.0, 0.2088934429361213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1477379697226736, 0.14773796972267345, 0.25259338660724406], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.10648756], dtype=float32), -0.074164614]. 
=============================================
[2019-04-10 12:33:39,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5157425e-20 1.0000000e+00 7.5514796e-23 6.7103999e-25 6.0853039e-32], sum to 1.0000
[2019-04-10 12:33:39,733] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0215
[2019-04-10 12:33:39,740] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.5, 1.0, 2.0, 0.3553740872370029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545365.865114062, 545365.8651140614, 170199.1461675087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1013400.0000, 
sim time next is 1014000.0000, 
raw observation next is [21.7, 97.33333333333333, 1.0, 2.0, 0.3578481240162013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549494.6115644863, 549494.6115644869, 170554.0564070382], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.9733333333333333, 1.0, 1.0, 0.22632304098337508, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1526373921012462, 0.15263739210124636, 0.25455829314483314], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.88463837], dtype=float32), -0.92987055]. 
=============================================
[2019-04-10 12:33:39,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.65646 ]
 [71.01419 ]
 [70.127335]
 [69.017624]
 [68.89806 ]], R is [[72.11701202]
 [72.14181519]
 [72.16603851]
 [72.1800766 ]
 [72.13558197]].
[2019-04-10 12:33:42,284] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 12:33:42,285] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:33:42,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:33:42,286] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:33:42,287] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:33:42,287] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:33:42,287] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:33:42,288] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:33:42,288] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:33:42,290] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:33:42,290] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:33:42,304] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run36
[2019-04-10 12:33:42,304] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run36
[2019-04-10 12:33:42,319] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run36
[2019-04-10 12:33:42,359] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run36
[2019-04-10 12:33:42,376] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run36
[2019-04-10 12:33:52,998] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12670434], dtype=float32), 0.09209491]
[2019-04-10 12:33:52,999] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 93.0, 1.0, 2.0, 0.3350384879186246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519354.9549688383, 519354.9549688377, 168252.030569168]
[2019-04-10 12:33:53,000] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:33:53,002] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.1237836e-21 1.0000000e+00 1.8602228e-24 1.3998509e-28 1.1775358e-33], sampled 0.008364866831384044
[2019-04-10 12:34:35,108] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12670434], dtype=float32), 0.09209491]
[2019-04-10 12:34:35,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [38.25, 40.66666666666667, 1.0, 2.0, 1.021185146485922, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00599449512408, 6.9112, 168.9123160916299, 2324654.849595763, 2257404.627154354, 469878.1332617268]
[2019-04-10 12:34:35,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:34:35,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3920174e-12 9.9999726e-01 6.4315019e-13 2.7077433e-06 1.2850427e-20], sampled 0.8352323344591965
[2019-04-10 12:34:35,137] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2324654.849595763 W.
[2019-04-10 12:34:44,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12670434], dtype=float32), 0.09209491]
[2019-04-10 12:34:44,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.994134225, 81.487548815, 1.0, 2.0, 0.6461153588281974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 902932.6108249053, 902932.6108249059, 209652.0265990945]
[2019-04-10 12:34:44,218] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:34:44,233] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8097007e-21 1.0000000e+00 7.1935807e-25 2.5096533e-27 5.8445437e-34], sampled 0.26483629335845615
[2019-04-10 12:34:45,845] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12670434], dtype=float32), 0.09209491]
[2019-04-10 12:34:45,845] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.76666666666667, 66.33333333333333, 1.0, 2.0, 0.5764166793863574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805493.2393167956, 805493.2393167956, 196434.2077977959]
[2019-04-10 12:34:45,846] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:34:45,850] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.5201370e-21 1.0000000e+00 1.0178259e-24 2.0334737e-26 4.6615252e-34], sampled 0.7720760862732744
[2019-04-10 12:35:05,142] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12670434], dtype=float32), 0.09209491]
[2019-04-10 12:35:05,142] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.4, 88.0, 1.0, 2.0, 0.4750357433376128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664685.6372309809, 664685.6372309809, 179894.1851140492]
[2019-04-10 12:35:05,143] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:35:05,146] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4673785e-20 1.0000000e+00 5.9743245e-24 1.7408821e-26 4.1424917e-33], sampled 0.42846630759063997
[2019-04-10 12:35:13,778] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3053 2842396260.8320 1129.0000
[2019-04-10 12:35:13,828] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12670434], dtype=float32), 0.09209491]
[2019-04-10 12:35:13,829] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.31666666666667, 71.83333333333333, 1.0, 2.0, 0.3215009498043824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504727.5628631852, 504727.5628631845, 167300.4529017632]
[2019-04-10 12:35:13,830] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:35:13,831] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0754139e-20 1.0000000e+00 2.2339413e-24 1.7068571e-28 1.6472917e-33], sampled 0.7720247599952746
[2019-04-10 12:35:13,916] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.7095 3163786488.1072 1771.0000
[2019-04-10 12:35:14,250] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5757 3007562549.7634 1766.0000
[2019-04-10 12:35:14,317] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5866 2927393683.0658 1338.0000
[2019-04-10 12:35:14,339] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5927 2779281048.9555 933.0000
[2019-04-10 12:35:15,354] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 875000, evaluation results [875000.0, 7885.709468479401, 3163786488.107172, 1771.0, 8253.586568547773, 2927393683.06578, 1338.0, 8660.592666056795, 2779281048.95547, 933.0, 7997.57569756011, 3007562549.763409, 1766.0, 8496.305342948124, 2842396260.83197, 1129.0]
[2019-04-10 12:35:17,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1791072e-21 1.0000000e+00 1.9298372e-24 4.6810544e-27 8.7433619e-34], sum to 1.0000
[2019-04-10 12:35:17,319] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8050
[2019-04-10 12:35:17,321] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 97.66666666666666, 1.0, 2.0, 0.3647809916978575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555219.2460212935, 555219.2460212935, 170890.4417163264], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1032000.0000, 
sim time next is 1032600.0000, 
raw observation next is [21.98333333333333, 97.83333333333334, 1.0, 2.0, 0.3643513155949491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553924.9319534462, 553924.9319534457, 170759.8298645259], 
processed observation next is [1.0, 0.9565217391304348, 0.24091627172195884, 0.9783333333333334, 1.0, 1.0, 0.23415821156017963, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15386803665373508, 0.1538680366537349, 0.2548654177082476], 
reward next is 0.7451, 
noisyNet noise sample is [array([1.3641773], dtype=float32), -0.11804081]. 
=============================================
[2019-04-10 12:35:19,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1078721e-20 1.0000000e+00 6.5925974e-24 5.3225542e-26 1.1554564e-33], sum to 1.0000
[2019-04-10 12:35:19,290] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7544
[2019-04-10 12:35:19,296] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 71.5, 1.0, 2.0, 0.3334198661261245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520393.1444944203, 520393.1444944209, 168434.5801737338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [24.46666666666667, 72.0, 1.0, 2.0, 0.3296099892819256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515132.1048108037, 515132.1048108043, 168041.358064677], 
processed observation next is [1.0, 0.782608695652174, 0.3586097946287521, 0.72, 1.0, 1.0, 0.19230119190593445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14309225133633438, 0.14309225133633452, 0.25080799711145824], 
reward next is 0.7492, 
noisyNet noise sample is [array([-0.17222632], dtype=float32), 0.8174795]. 
=============================================
[2019-04-10 12:35:19,304] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.93552 ]
 [73.609566]
 [73.377464]
 [73.05516 ]
 [72.6324  ]], R is [[74.20296478]
 [74.20954132]
 [74.21555328]
 [74.22168732]
 [74.22820282]].
[2019-04-10 12:35:19,928] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1119155e-20 1.0000000e+00 3.5164406e-24 6.7113112e-28 1.3632065e-33], sum to 1.0000
[2019-04-10 12:35:19,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9573
[2019-04-10 12:35:19,936] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 94.33333333333333, 1.0, 2.0, 0.2725155493758764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 440664.0383251179, 440664.0383251173, 162982.4112130433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [20.05, 93.5, 1.0, 2.0, 0.2755818247337372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445192.6284017785, 445192.6284017785, 163280.6944681586], 
processed observation next is [1.0, 0.21739130434782608, 0.14928909952606645, 0.935, 1.0, 1.0, 0.1272070177514906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12366461900049404, 0.12366461900049404, 0.2437025290569531], 
reward next is 0.7563, 
noisyNet noise sample is [array([-0.94473803], dtype=float32), -0.7018271]. 
=============================================
[2019-04-10 12:35:19,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.78201 ]
 [72.778015]
 [72.72908 ]
 [72.673065]
 [72.67114 ]], R is [[72.8034668 ]
 [72.83217621]
 [72.86064148]
 [72.88855743]
 [72.91611481]].
[2019-04-10 12:35:22,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.7127816e-18 1.0000000e+00 5.9660240e-20 5.1026407e-20 5.5087933e-28], sum to 1.0000
[2019-04-10 12:35:22,651] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1735
[2019-04-10 12:35:22,654] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 59.0, 1.0, 2.0, 0.99189320344214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1514832.984715675, 1514832.984715675, 315390.8894443341], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1177200.0000, 
sim time next is 1177800.0000, 
raw observation next is [27.6, 58.83333333333334, 1.0, 2.0, 0.9342209316084197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1428344.877944785, 1428344.877944785, 296995.1814569918], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.5883333333333334, 1.0, 1.0, 0.92074811037159, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3967624660957736, 0.3967624660957736, 0.44327639023431614], 
reward next is 0.5567, 
noisyNet noise sample is [array([0.9694877], dtype=float32), 1.6442782]. 
=============================================
[2019-04-10 12:35:23,977] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1256841e-19 1.0000000e+00 2.1733208e-23 3.1231965e-26 5.5630546e-32], sum to 1.0000
[2019-04-10 12:35:23,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2102
[2019-04-10 12:35:23,988] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 84.66666666666666, 1.0, 2.0, 0.4420523263281333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661778.3851932376, 661778.3851932376, 180518.3583214897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1237800.0000, 
sim time next is 1238400.0000, 
raw observation next is [24.3, 84.0, 1.0, 2.0, 0.4335734377921851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647834.1596891858, 647834.1596891858, 179108.9664061305], 
processed observation next is [1.0, 0.34782608695652173, 0.3507109004739337, 0.84, 1.0, 1.0, 0.3175583587857652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17995393324699605, 0.17995393324699605, 0.26732681553153803], 
reward next is 0.7327, 
noisyNet noise sample is [array([-0.04706847], dtype=float32), 0.8434895]. 
=============================================
[2019-04-10 12:35:26,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2605698e-22 1.0000000e+00 1.3587516e-26 9.3110437e-30 2.0295189e-36], sum to 1.0000
[2019-04-10 12:35:26,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1517
[2019-04-10 12:35:26,965] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 87.33333333333333, 1.0, 2.0, 0.4677127682766395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657033.2627142441, 657033.2627142441, 179143.0367438367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1280400.0000, 
sim time next is 1281000.0000, 
raw observation next is [25.25, 88.16666666666667, 1.0, 2.0, 0.4689321822145334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659733.1560217439, 659733.1560217432, 179450.6952276578], 
processed observation next is [1.0, 0.8260869565217391, 0.39573459715639814, 0.8816666666666667, 1.0, 1.0, 0.3601592556801608, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18325921000603998, 0.18325921000603979, 0.26783685854874295], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.00626084], dtype=float32), -0.26788345]. 
=============================================
[2019-04-10 12:35:26,976] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.79174]
 [74.11144]
 [74.20536]
 [74.45306]
 [74.5124 ]], R is [[73.60701752]
 [73.60356903]
 [73.59999847]
 [73.59634399]
 [73.59264374]].
[2019-04-10 12:35:28,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5905665e-19 1.0000000e+00 6.2018785e-22 5.7283813e-24 4.3527258e-30], sum to 1.0000
[2019-04-10 12:35:28,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7363
[2019-04-10 12:35:28,234] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 92.83333333333333, 1.0, 2.0, 0.5427297502722095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773743.6861823428, 773743.6861823434, 192522.4575212592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1307400.0000, 
sim time next is 1308000.0000, 
raw observation next is [24.36666666666667, 92.66666666666667, 1.0, 2.0, 0.5058709776170744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720844.650781309, 720844.6507813084, 186305.1775416525], 
processed observation next is [1.0, 0.13043478260869565, 0.3538704581358612, 0.9266666666666667, 1.0, 1.0, 0.40466382845430654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20023462521703028, 0.20023462521703012, 0.2780674291666455], 
reward next is 0.7219, 
noisyNet noise sample is [array([1.972846], dtype=float32), 0.24492562]. 
=============================================
[2019-04-10 12:35:28,243] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[69.23629 ]
 [69.2501  ]
 [69.19445 ]
 [69.13162 ]
 [69.092735]], R is [[69.33727264]
 [69.35655212]
 [69.37922668]
 [69.40001678]
 [69.41712952]].
[2019-04-10 12:35:35,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5609927e-21 1.0000000e+00 5.5351116e-25 4.8586566e-28 4.1223274e-34], sum to 1.0000
[2019-04-10 12:35:35,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9453
[2019-04-10 12:35:35,383] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 96.0, 1.0, 2.0, 0.3584876783872445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549730.4218779436, 549730.4218779436, 170551.8880829734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1467000.0000, 
sim time next is 1467600.0000, 
raw observation next is [21.86666666666667, 96.0, 1.0, 2.0, 0.3577710798312892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549178.7174393281, 549178.7174393281, 170521.7732477924], 
processed observation next is [0.0, 1.0, 0.23538704581358633, 0.96, 1.0, 1.0, 0.22623021666420384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1525496437331467, 0.1525496437331467, 0.25451010932506324], 
reward next is 0.7455, 
noisyNet noise sample is [array([-1.2813764], dtype=float32), 1.5071892]. 
=============================================
[2019-04-10 12:35:38,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9168441e-22 1.0000000e+00 6.5910852e-25 1.0719644e-29 3.2914239e-34], sum to 1.0000
[2019-04-10 12:35:38,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2010
[2019-04-10 12:35:38,045] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 51.0, 1.0, 2.0, 0.3592854886153388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546012.8346006966, 546012.8346006966, 170083.2172065805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1515600.0000, 
sim time next is 1516200.0000, 
raw observation next is [29.55, 51.16666666666666, 1.0, 2.0, 0.357107558748814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 541535.631387054, 541535.6313870547, 169667.6832419138], 
processed observation next is [0.0, 0.5652173913043478, 0.5995260663507109, 0.5116666666666666, 1.0, 1.0, 0.22543079367326987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15042656427418166, 0.15042656427418186, 0.2532353481222594], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.6256433], dtype=float32), 0.32714927]. 
=============================================
[2019-04-10 12:35:40,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5992430e-13 1.0000000e+00 8.4902497e-14 5.2206450e-10 2.2008968e-21], sum to 1.0000
[2019-04-10 12:35:40,618] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8937
[2019-04-10 12:35:40,625] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2455947.102314014 W.
[2019-04-10 12:35:40,629] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.25, 78.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 8.322997409240834, 6.9112, 168.9052358941019, 2455947.102314014, 1454415.209655492, 310847.7258418417], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1696200.0000, 
sim time next is 1696800.0000, 
raw observation next is [28.3, 78.33333333333334, 1.0, 2.0, 0.7394467560146165, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.980713228111492, 6.9112, 168.9113928550239, 1930330.215679329, 1881015.595714252, 394338.5844383045], 
processed observation next is [1.0, 0.6521739130434783, 0.5402843601895735, 0.7833333333333334, 1.0, 1.0, 0.6860804289332729, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006951322811149208, 0.0, 0.8294322668884093, 0.5362028376887025, 0.5225043321428477, 0.5885650514004545], 
reward next is 0.0639, 
noisyNet noise sample is [array([0.5942267], dtype=float32), 0.19627403]. 
=============================================
[2019-04-10 12:35:43,199] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2320425e-19 1.0000000e+00 2.1914372e-22 1.8314097e-25 7.2479007e-32], sum to 1.0000
[2019-04-10 12:35:43,206] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4778
[2019-04-10 12:35:43,208] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 96.0, 1.0, 2.0, 0.8629951862666918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223988.652060425, 1223988.652060425, 262545.9603633804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [24.16666666666666, 95.33333333333334, 1.0, 2.0, 0.8906065988511283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1261283.914661627, 1261283.914661628, 269797.2266853297], 
processed observation next is [1.0, 0.34782608695652173, 0.34439178515007873, 0.9533333333333335, 1.0, 1.0, 0.8682007215073835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35035664296156305, 0.3503566429615633, 0.4026824278885518], 
reward next is 0.5973, 
noisyNet noise sample is [array([-0.4376634], dtype=float32), -0.675121]. 
=============================================
[2019-04-10 12:35:46,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.3921746e-20 1.0000000e+00 5.5170723e-23 5.7000442e-24 2.8798501e-32], sum to 1.0000
[2019-04-10 12:35:46,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6433
[2019-04-10 12:35:46,729] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 91.0, 1.0, 2.0, 0.5097164316008096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712254.0713368938, 712254.0713368938, 185132.4658874664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1720800.0000, 
sim time next is 1721400.0000, 
raw observation next is [25.95, 91.50000000000001, 1.0, 2.0, 0.50972278043252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712262.9458737795, 712262.9458737788, 185133.5208410282], 
processed observation next is [1.0, 0.9565217391304348, 0.42890995260663506, 0.9150000000000001, 1.0, 1.0, 0.40930455473797583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19785081829827209, 0.1978508182982719, 0.27631868782243013], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.0260791], dtype=float32), 0.38914073]. 
=============================================
[2019-04-10 12:35:47,687] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0648990e-18 1.0000000e+00 1.5147661e-21 1.6449901e-22 2.5572742e-30], sum to 1.0000
[2019-04-10 12:35:47,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4081
[2019-04-10 12:35:47,698] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 90.66666666666667, 1.0, 2.0, 0.6415110078483643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1012854.986478552, 1012854.986478552, 222429.9052460256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1786800.0000, 
sim time next is 1787400.0000, 
raw observation next is [21.7, 89.0, 1.0, 2.0, 0.6847610812767095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080473.910355778, 1080473.910355779, 232388.5543588489], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.89, 1.0, 1.0, 0.6201940738273609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3001316417654939, 0.3001316417654942, 0.3468485885952969], 
reward next is 0.6532, 
noisyNet noise sample is [array([-2.185769], dtype=float32), -1.8401108]. 
=============================================
[2019-04-10 12:35:48,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8849466e-20 1.0000000e+00 1.4118742e-21 1.5464692e-22 1.4147629e-30], sum to 1.0000
[2019-04-10 12:35:49,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7887
[2019-04-10 12:35:49,009] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 89.0, 1.0, 2.0, 0.6847610812767095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080473.910355778, 1080473.910355779, 232388.5543588489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1787400.0000, 
sim time next is 1788000.0000, 
raw observation next is [21.93333333333333, 87.33333333333334, 1.0, 2.0, 0.686824532390248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083132.043582678, 1083132.043582678, 232826.8951584674], 
processed observation next is [1.0, 0.6956521739130435, 0.23854660347551332, 0.8733333333333334, 1.0, 1.0, 0.6226801595063228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3008700121062995, 0.3008700121062995, 0.3475028285947275], 
reward next is 0.6525, 
noisyNet noise sample is [array([-0.7235934], dtype=float32), -1.1696792]. 
=============================================
[2019-04-10 12:35:49,019] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.343  ]
 [68.70367]
 [68.8785 ]
 [69.34002]
 [69.42412]], R is [[68.0871582 ]
 [68.05944061]
 [68.04685974]
 [68.02625275]
 [68.03256989]].
[2019-04-10 12:35:50,007] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9979255e-20 1.0000000e+00 1.4129720e-23 3.1959683e-26 3.9030475e-33], sum to 1.0000
[2019-04-10 12:35:50,013] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0557
[2019-04-10 12:35:50,016] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 95.33333333333333, 1.0, 2.0, 0.3451864844258361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534911.974563992, 534911.9745639926, 169492.160138009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1813200.0000, 
sim time next is 1813800.0000, 
raw observation next is [21.66666666666666, 95.16666666666667, 1.0, 2.0, 0.3453853068329361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535022.2491416008, 535022.2491416014, 169495.6252996505], 
processed observation next is [1.0, 1.0, 0.22590837282780388, 0.9516666666666667, 1.0, 1.0, 0.21130759859389892, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14861729142822244, 0.1486172914282226, 0.25297854522335894], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.9037861], dtype=float32), -0.88137054]. 
=============================================
[2019-04-10 12:35:55,725] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 12:35:55,726] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:35:55,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:35:55,727] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:35:55,728] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:35:55,731] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:35:55,729] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:35:55,729] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:35:55,730] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:35:55,734] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:35:55,735] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:35:55,736] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run37
[2019-04-10 12:35:55,763] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run37
[2019-04-10 12:35:55,786] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run37
[2019-04-10 12:35:55,803] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run37
[2019-04-10 12:35:55,823] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run37
[2019-04-10 12:36:12,716] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12609883], dtype=float32), 0.09180655]
[2019-04-10 12:36:12,719] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.55, 85.0, 1.0, 2.0, 0.7709525610057512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1173762.643976374, 1173762.643976374, 249564.6657224942]
[2019-04-10 12:36:12,720] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:36:12,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2409684e-19 1.0000000e+00 4.5029054e-22 9.3489720e-25 8.5099067e-31], sampled 0.40787705883602776
[2019-04-10 12:36:12,740] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12609883], dtype=float32), 0.09180655]
[2019-04-10 12:36:12,740] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.44355079, 83.8421134, 1.0, 2.0, 0.3818546107329768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 572732.1306483229, 572732.1306483235, 172152.1593679514]
[2019-04-10 12:36:12,741] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:36:12,744] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5311966e-20 1.0000000e+00 6.4520519e-24 6.4793175e-28 2.7612212e-33], sampled 0.6664583776428761
[2019-04-10 12:36:38,469] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12609883], dtype=float32), 0.09180655]
[2019-04-10 12:36:38,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.0, 77.0, 1.0, 2.0, 0.5781499641394804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 807916.2783792759, 807916.2783792753, 196745.7918536853]
[2019-04-10 12:36:38,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:36:38,473] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1017464e-20 1.0000000e+00 1.6778945e-23 1.1689520e-25 6.7650927e-33], sampled 0.6367852736591334
[2019-04-10 12:37:06,613] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12609883], dtype=float32), 0.09180655]
[2019-04-10 12:37:06,615] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.04272362, 85.79638198, 1.0, 2.0, 0.5255147383239034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734337.5221062832, 734337.5221062832, 187689.9770336103]
[2019-04-10 12:37:06,628] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:37:06,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.5268671e-20 1.0000000e+00 2.7616039e-23 1.3648713e-26 1.9652961e-32], sampled 0.9527781934028542
[2019-04-10 12:37:27,732] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.3161 3007646295.1366 1762.0000
[2019-04-10 12:37:28,032] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5310 2779331587.3780 933.0000
[2019-04-10 12:37:28,136] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.3077 3163720196.2920 1764.0000
[2019-04-10 12:37:28,194] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.2433 2842296446.3679 1128.0000
[2019-04-10 12:37:28,222] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.5308 2927439879.8552 1337.0000
[2019-04-10 12:37:29,236] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 900000, evaluation results [900000.0, 7890.307711974619, 3163720196.2919774, 1764.0, 8254.53077933479, 2927439879.8552184, 1337.0, 8660.53097408367, 2779331587.377975, 933.0, 7999.316126172706, 3007646295.1366467, 1762.0, 8498.24327935657, 2842296446.3679, 1128.0]
[2019-04-10 12:37:31,317] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3645898e-20 1.0000000e+00 5.7655439e-23 1.3793225e-26 4.1347106e-32], sum to 1.0000
[2019-04-10 12:37:31,323] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7587
[2019-04-10 12:37:31,327] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 95.16666666666667, 1.0, 2.0, 0.4854643778384652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678354.5571582321, 678354.5571582327, 181350.1802974536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2011800.0000, 
sim time next is 2012400.0000, 
raw observation next is [25.0, 95.0, 1.0, 2.0, 0.4874043180397513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681066.1652401134, 681066.165240114, 181646.3651873764], 
processed observation next is [0.0, 0.30434782608695654, 0.38388625592417064, 0.95, 1.0, 1.0, 0.3824148410117486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1891850459000315, 0.18918504590003166, 0.2711139778916066], 
reward next is 0.7289, 
noisyNet noise sample is [array([-1.2683984], dtype=float32), 0.9168254]. 
=============================================
[2019-04-10 12:37:32,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7405671e-21 1.0000000e+00 1.8724264e-23 2.0132744e-26 1.3942843e-32], sum to 1.0000
[2019-04-10 12:37:32,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2965
[2019-04-10 12:37:32,392] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.5, 1.0, 2.0, 0.5597742409293968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782228.2848523678, 782228.2848523678, 193491.8349713187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2118600.0000, 
sim time next is 2119200.0000, 
raw observation next is [30.0, 75.66666666666667, 1.0, 2.0, 0.5612681575784264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784316.6540803378, 784316.6540803378, 193752.661080726], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.7566666666666667, 1.0, 1.0, 0.4714074187691884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21786573724453828, 0.21786573724453828, 0.28918307623988954], 
reward next is 0.7108, 
noisyNet noise sample is [array([1.0003988], dtype=float32), -0.53369784]. 
=============================================
[2019-04-10 12:37:34,274] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.475161e-20 1.000000e+00 1.508065e-23 7.499422e-26 2.022922e-32], sum to 1.0000
[2019-04-10 12:37:34,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0543
[2019-04-10 12:37:34,284] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 82.66666666666666, 1.0, 2.0, 0.5139542107435818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718177.7489100752, 718177.7489100752, 185811.3459137381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2040000.0000, 
sim time next is 2040600.0000, 
raw observation next is [27.26666666666667, 82.83333333333334, 1.0, 2.0, 0.5107027243003914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713632.7338979434, 713632.7338979434, 185290.1323114938], 
processed observation next is [0.0, 0.6086956521739131, 0.4913112164297, 0.8283333333333335, 1.0, 1.0, 0.41048521000047156, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19823131497165095, 0.19823131497165095, 0.27655243628581166], 
reward next is 0.7234, 
noisyNet noise sample is [array([-1.0674655], dtype=float32), 0.5519695]. 
=============================================
[2019-04-10 12:37:34,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9636886e-20 1.0000000e+00 6.5232802e-23 4.8014885e-26 3.2093241e-32], sum to 1.0000
[2019-04-10 12:37:34,428] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1091
[2019-04-10 12:37:34,436] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 97.0, 1.0, 2.0, 0.460795891596776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651176.1322495863, 651176.1322495863, 178624.0099571674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2088000.0000, 
sim time next is 2088600.0000, 
raw observation next is [23.96666666666667, 97.16666666666667, 1.0, 2.0, 0.4599709408056562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650217.9598427864, 650217.9598427871, 178529.635563173], 
processed observation next is [0.0, 0.17391304347826086, 0.33491311216429714, 0.9716666666666667, 1.0, 1.0, 0.34936257928392317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18061609995632957, 0.18061609995632977, 0.2664621426316015], 
reward next is 0.7335, 
noisyNet noise sample is [array([1.0075439], dtype=float32), 0.31750187]. 
=============================================
[2019-04-10 12:37:34,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1286018e-20 1.0000000e+00 2.0444917e-22 1.0121459e-24 9.0990921e-32], sum to 1.0000
[2019-04-10 12:37:34,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9500
[2019-04-10 12:37:34,493] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 94.5, 1.0, 2.0, 0.4668428552344496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655833.8261731579, 655833.8261731579, 179017.4397863802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2075400.0000, 
sim time next is 2076000.0000, 
raw observation next is [24.43333333333333, 94.66666666666666, 1.0, 2.0, 0.46682209422433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655661.740848962, 655661.7408489614, 178995.9478892546], 
processed observation next is [0.0, 0.0, 0.3570300157977882, 0.9466666666666665, 1.0, 1.0, 0.35761698099316874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18212826134693388, 0.18212826134693372, 0.2671581311779919], 
reward next is 0.7328, 
noisyNet noise sample is [array([1.2260805], dtype=float32), -0.93433607]. 
=============================================
[2019-04-10 12:37:34,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.062195]
 [73.26787 ]
 [73.83509 ]
 [74.77187 ]
 [76.15896 ]], R is [[73.06132507]
 [73.06352234]
 [73.06562042]
 [73.06761169]
 [73.06951141]].
[2019-04-10 12:37:34,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0908513e-18 1.0000000e+00 2.1514544e-22 3.2451480e-25 3.7652589e-31], sum to 1.0000
[2019-04-10 12:37:34,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6916
[2019-04-10 12:37:34,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 96.0, 1.0, 2.0, 0.4652106779754475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653890.3115176935, 653890.3115176935, 178821.8851558273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2082000.0000, 
sim time next is 2082600.0000, 
raw observation next is [24.2, 96.0, 1.0, 2.0, 0.4635811655984104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 652580.5756895425, 652580.575689543, 178708.7037050725], 
processed observation next is [0.0, 0.08695652173913043, 0.3459715639810427, 0.96, 1.0, 1.0, 0.35371224770892823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.181272382135984, 0.18127238213598418, 0.2667294085150336], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.72236735], dtype=float32), 0.35172328]. 
=============================================
[2019-04-10 12:37:36,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3951021e-19 1.0000000e+00 9.5271565e-23 1.0848964e-25 4.0065075e-31], sum to 1.0000
[2019-04-10 12:37:36,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7507
[2019-04-10 12:37:36,558] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 94.0, 1.0, 2.0, 0.4797412250532267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670354.8935108938, 670354.8935108938, 180482.9102815486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2096400.0000, 
sim time next is 2097000.0000, 
raw observation next is [25.05, 93.5, 1.0, 2.0, 0.4820010316194018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673513.5810254433, 673513.5810254439, 180824.2370530177], 
processed observation next is [0.0, 0.2608695652173913, 0.3862559241706162, 0.935, 1.0, 1.0, 0.3759048573727732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18708710584040092, 0.18708710584040106, 0.2698869209746533], 
reward next is 0.7301, 
noisyNet noise sample is [array([-0.68377626], dtype=float32), -1.0915878]. 
=============================================
[2019-04-10 12:37:36,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.53812]
 [72.55581]
 [72.5524 ]
 [72.53468]
 [72.58276]], R is [[72.51711273]
 [72.52256012]
 [72.52828979]
 [72.53400421]
 [72.53956604]].
[2019-04-10 12:37:38,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2434024e-19 1.0000000e+00 1.5406105e-22 4.1073266e-24 4.1627543e-32], sum to 1.0000
[2019-04-10 12:37:38,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2061
[2019-04-10 12:37:38,963] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 89.0, 1.0, 2.0, 0.5387283967521289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752808.4095776491, 752808.4095776498, 189886.5806052274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2149200.0000, 
sim time next is 2149800.0000, 
raw observation next is [27.01666666666667, 89.33333333333334, 1.0, 2.0, 0.5373715115108973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750911.6547711656, 750911.654771165, 189658.679064698], 
processed observation next is [0.0, 0.9130434782608695, 0.4794628751974725, 0.8933333333333334, 1.0, 1.0, 0.44261627892879196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20858657076976822, 0.20858657076976805, 0.2830726553204448], 
reward next is 0.7169, 
noisyNet noise sample is [array([1.1990056], dtype=float32), 1.7180854]. 
=============================================
[2019-04-10 12:37:43,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1068347e-17 1.0000000e+00 9.6859877e-21 2.9589539e-21 1.3639063e-29], sum to 1.0000
[2019-04-10 12:37:43,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7107
[2019-04-10 12:37:43,298] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 86.0, 1.0, 2.0, 0.5047112053019159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705257.678776894, 705257.6787768946, 184337.4951966176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2253000.0000, 
sim time next is 2253600.0000, 
raw observation next is [26.5, 86.0, 1.0, 2.0, 0.502926540079341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702763.0537670986, 702763.0537670986, 184055.8480308753], 
processed observation next is [1.0, 0.08695652173913043, 0.4549763033175356, 0.86, 1.0, 1.0, 0.4011163133486036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1952119593797496, 0.1952119593797496, 0.2747102209416049], 
reward next is 0.7253, 
noisyNet noise sample is [array([0.37606013], dtype=float32), -0.8254055]. 
=============================================
[2019-04-10 12:37:56,468] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4618249e-20 1.0000000e+00 1.7886331e-22 4.8274417e-25 5.3351004e-32], sum to 1.0000
[2019-04-10 12:37:56,476] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-10 12:37:56,480] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3950281865701634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 589440.5759568586, 589440.5759568592, 173566.3096669186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2676000.0000, 
sim time next is 2676600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3949073917860469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 589260.5812168231, 589260.5812168224, 173549.8204274147], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2709727611880083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16368349478245087, 0.16368349478245067, 0.2590295827274846], 
reward next is 0.7410, 
noisyNet noise sample is [array([0.01103233], dtype=float32), -0.238144]. 
=============================================
[2019-04-10 12:38:00,735] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0787178e-19 1.0000000e+00 1.6632699e-22 1.1563038e-24 4.3382193e-31], sum to 1.0000
[2019-04-10 12:38:00,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4855
[2019-04-10 12:38:00,749] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.4020926291463947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595566.2606575809, 595566.2606575809, 173999.560208433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.4070037782581065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600112.9647688577, 600112.9647688577, 174337.0890746021], 
processed observation next is [0.0, 0.0, 0.28909952606635075, 0.97, 1.0, 1.0, 0.28554672079289944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16669804576912714, 0.16669804576912714, 0.2602046105591076], 
reward next is 0.7398, 
noisyNet noise sample is [array([0.781179], dtype=float32), 0.22679932]. 
=============================================
[2019-04-10 12:38:01,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9823561e-20 1.0000000e+00 8.0487869e-23 3.3991070e-26 2.4143696e-32], sum to 1.0000
[2019-04-10 12:38:01,380] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0585
[2019-04-10 12:38:01,385] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3868157276255282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582555.5111790922, 582555.5111790922, 173102.8784746511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2718600.0000, 
sim time next is 2719200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3852189403496105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580207.7230043663, 580207.7230043663, 172893.702197987], 
processed observation next is [0.0, 0.4782608695652174, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2592999281320609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1611688119456573, 0.1611688119456573, 0.2580503017880403], 
reward next is 0.7419, 
noisyNet noise sample is [array([0.32071945], dtype=float32), -0.23975688]. 
=============================================
[2019-04-10 12:38:02,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2873649e-20 1.0000000e+00 2.1440630e-23 1.8640629e-27 1.7847322e-32], sum to 1.0000
[2019-04-10 12:38:02,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9351
[2019-04-10 12:38:02,786] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 98.66666666666667, 1.0, 2.0, 0.4684998212375665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656228.650162088, 656228.6501620887, 179013.0580089379], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2697600.0000, 
sim time next is 2698200.0000, 
raw observation next is [24.0, 99.0, 1.0, 2.0, 0.4710489935766886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658538.7930215286, 658538.7930215293, 179227.4332090337], 
processed observation next is [0.0, 0.21739130434782608, 0.3364928909952607, 0.99, 1.0, 1.0, 0.36270963081528745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18292744250598017, 0.18292744250598036, 0.2675036316552742], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.6595195], dtype=float32), -0.80441415]. 
=============================================
[2019-04-10 12:38:04,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5375813e-19 1.0000000e+00 1.8284527e-22 1.1790739e-24 1.6896418e-31], sum to 1.0000
[2019-04-10 12:38:04,702] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5247
[2019-04-10 12:38:04,707] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.95, 1.0, 1.0, 0.21611542952280008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14963343786470854, 0.14963343786470834, 0.2533167744344758], 
reward next is 0.7467, 
noisyNet noise sample is [array([1.2457536], dtype=float32), 1.3485152]. 
=============================================
[2019-04-10 12:38:09,590] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:38:09,592] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:38:09,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:09,593] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:38:09,594] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:38:09,595] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:38:09,596] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:38:09,595] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:09,597] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:09,599] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:09,598] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:09,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run38
[2019-04-10 12:38:09,626] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run38
[2019-04-10 12:38:09,643] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run38
[2019-04-10 12:38:09,663] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run38
[2019-04-10 12:38:09,684] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run38
[2019-04-10 12:38:25,568] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:38:25,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.6, 83.0, 1.0, 2.0, 0.3586410818911617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549815.2703252132, 549815.2703252139, 170554.578030716]
[2019-04-10 12:38:25,570] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:38:25,571] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.4078795e-21 1.0000000e+00 1.1536515e-24 2.1357314e-28 2.1957752e-34], sampled 0.29200198053827897
[2019-04-10 12:38:26,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:38:26,539] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.2, 83.66666666666667, 1.0, 2.0, 0.3532610940427609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 546442.272052549, 546442.2720525496, 170411.5794200069]
[2019-04-10 12:38:26,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:38:26,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.3592619e-20 1.0000000e+00 2.8452370e-23 1.5765162e-26 2.0651355e-32], sampled 0.7561552080382439
[2019-04-10 12:38:43,589] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:38:43,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5489947093149598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 767159.5323086148, 767159.5323086142, 191628.2826581257]
[2019-04-10 12:38:43,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:38:43,592] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.04824819e-20 1.00000000e+00 6.79349341e-24 1.28380764e-26
 1.84203822e-33], sampled 0.37660773039175754
[2019-04-10 12:38:48,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:38:48,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.94705636333333, 63.67774440666667, 1.0, 2.0, 0.7986720077903872, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598144529593, 6.9112, 168.9123160046146, 2013214.845059122, 1945973.880614481, 407522.8986745699]
[2019-04-10 12:38:48,664] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:38:48,666] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7857867e-13 9.9999988e-01 3.2748768e-13 8.9228507e-08 2.4652351e-21], sampled 0.22572690492037828
[2019-04-10 12:38:48,668] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2013214.845059122 W.
[2019-04-10 12:39:25,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:39:25,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.49907594, 66.442478275, 1.0, 2.0, 0.4044185626594984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599818.5447583556, 599818.5447583556, 174415.511428668]
[2019-04-10 12:39:25,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:39:25,300] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8757934e-21 1.0000000e+00 8.7839534e-25 9.7330980e-29 1.6779299e-34], sampled 0.19732682135702329
[2019-04-10 12:39:26,753] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:39:26,753] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.83333333333333, 92.33333333333333, 1.0, 2.0, 0.678520416044222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 948238.212449525, 948238.2124495243, 216280.9189372825]
[2019-04-10 12:39:26,754] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:39:26,756] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.55425147e-20 1.00000000e+00 1.01263354e-22 2.00959246e-25
 8.49851618e-32], sampled 0.22175759740687262
[2019-04-10 12:39:31,949] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12548251], dtype=float32), 0.09131388]
[2019-04-10 12:39:31,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.74140197, 93.36595391, 1.0, 2.0, 0.3445197612874448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536401.2841234354, 536401.2841234347, 169679.195765193]
[2019-04-10 12:39:31,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:39:31,954] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4818341e-20 1.0000000e+00 9.9170308e-24 2.1133641e-27 6.7429353e-33], sampled 0.016945471485512287
[2019-04-10 12:39:38,334] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9196 3163394431.2052 1771.0000
[2019-04-10 12:39:38,981] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.2718 3007426626.1547 1763.0000
[2019-04-10 12:39:39,183] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1919 2927472788.5469 1338.0000
[2019-04-10 12:39:39,319] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.5795 2779433981.0969 934.0000
[2019-04-10 12:39:39,410] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.1429 2842292995.1936 1128.0000
[2019-04-10 12:39:40,429] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 925000, evaluation results [925000.0, 7884.919598055997, 3163394431.2051897, 1771.0, 8254.191905559404, 2927472788.546885, 1338.0, 8659.579528040395, 2779433981.096936, 934.0, 8000.271837749032, 3007426626.154748, 1763.0, 8497.142925209022, 2842292995.193615, 1128.0]
[2019-04-10 12:39:47,331] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.010790e-20 1.000000e+00 6.552935e-23 9.954359e-25 7.166412e-32], sum to 1.0000
[2019-04-10 12:39:47,337] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1035
[2019-04-10 12:39:47,344] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.5159348702410322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820788.1294222798, 820788.1294222791, 197403.8990551119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2984400.0000, 
sim time next is 2985000.0000, 
raw observation next is [20.16666666666667, 99.00000000000001, 1.0, 2.0, 0.4749872591779489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755568.6569742446, 755568.6569742446, 190168.9957770277], 
processed observation next is [1.0, 0.5652173913043478, 0.15481832543443946, 0.9900000000000001, 1.0, 1.0, 0.36745452913005894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20988018249284573, 0.20988018249284573, 0.28383432205526526], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.7696868], dtype=float32), 1.2080133]. 
=============================================
[2019-04-10 12:39:47,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.77724 ]
 [71.77009 ]
 [71.462006]
 [71.07639 ]
 [70.57548 ]], R is [[72.00992584]
 [71.99519348]
 [71.9943924 ]
 [71.99223328]
 [71.98919678]].
[2019-04-10 12:39:50,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1814340e-20 1.0000000e+00 1.4441150e-22 1.7921555e-24 2.8898203e-32], sum to 1.0000
[2019-04-10 12:39:50,474] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9696
[2019-04-10 12:39:50,479] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3930914692769988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586550.9406073227, 586550.9406073234, 173302.0483535628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3112200.0000, 
sim time next is 3112800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3899718470832311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581896.8657064831, 581896.8657064831, 172879.0009968217], 
processed observation next is [1.0, 0.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26502632178702545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16163801825180088, 0.16163801825180088, 0.2580283596967488], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.4166534], dtype=float32), 1.7479317]. 
=============================================
[2019-04-10 12:39:51,929] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9966230e-20 1.0000000e+00 2.0774870e-23 1.9342056e-26 7.4892785e-32], sum to 1.0000
[2019-04-10 12:39:51,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7418
[2019-04-10 12:39:51,939] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.3742363454517803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571630.6657486432, 571630.6657486426, 172365.7004848244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3123600.0000, 
sim time next is 3124200.0000, 
raw observation next is [22.0, 95.0, 1.0, 2.0, 0.3673401473100638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563160.8548989792, 563160.8548989792, 171688.6325612299], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.95, 1.0, 1.0, 0.2377592136265829, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.156433570805272, 0.156433570805272, 0.25625169038989537], 
reward next is 0.7437, 
noisyNet noise sample is [array([-1.8489977], dtype=float32), 0.28651085]. 
=============================================
[2019-04-10 12:39:58,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0201608e-19 1.0000000e+00 1.9310174e-22 3.3882214e-25 3.6002987e-31], sum to 1.0000
[2019-04-10 12:39:59,000] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8106
[2019-04-10 12:39:59,004] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.47835643680427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 668545.7355885248, 668545.7355885255, 180290.3378336026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3292200.0000, 
sim time next is 3292800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4777042438026083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667634.6350104392, 667634.6350104392, 180192.5017852487], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37072800458145583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18545406528067757, 0.18545406528067757, 0.2689440325152966], 
reward next is 0.7311, 
noisyNet noise sample is [array([1.2495277], dtype=float32), -0.7609585]. 
=============================================
[2019-04-10 12:40:02,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6017648e-19 1.0000000e+00 2.1654353e-22 1.6068884e-24 3.6494139e-31], sum to 1.0000
[2019-04-10 12:40:02,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7678
[2019-04-10 12:40:02,896] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5387947595640994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752901.1765240583, 752901.176524059, 189897.467371552], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3368400.0000, 
sim time next is 3369000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5388179412332851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752933.5816124796, 752933.5816124802, 189901.3645148258], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4443589653413073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20914821711457768, 0.20914821711457784, 0.2834348724101878], 
reward next is 0.7166, 
noisyNet noise sample is [array([1.434904], dtype=float32), -2.4687042]. 
=============================================
[2019-04-10 12:40:02,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.10825 ]
 [74.14095 ]
 [74.17163 ]
 [74.195274]
 [74.19997 ]], R is [[74.05258179]
 [74.02862549]
 [74.00523376]
 [73.98227692]
 [73.95948029]].
[2019-04-10 12:40:06,582] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.1945117e-18 1.0000000e+00 2.0173859e-20 7.6936440e-21 9.2655184e-29], sum to 1.0000
[2019-04-10 12:40:06,592] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6470
[2019-04-10 12:40:06,595] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.6556928344327676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916322.7052083366, 916322.705208336, 211569.2633407649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [27.33333333333334, 79.0, 1.0, 2.0, 0.7490208551332743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046811.75168703, 1046811.75168703, 231779.4291621183], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.79, 1.0, 1.0, 0.6976154881123787, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2907810421352861, 0.2907810421352861, 0.3459394465106243], 
reward next is 0.6541, 
noisyNet noise sample is [array([-0.20894317], dtype=float32), -1.8120165]. 
=============================================
[2019-04-10 12:40:09,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9293976e-10 9.6760356e-01 2.3078518e-10 3.2396428e-02 7.6707140e-16], sum to 1.0000
[2019-04-10 12:40:09,084] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7660
[2019-04-10 12:40:09,094] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3019620.997615928 W.
[2019-04-10 12:40:09,096] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.947645179801011, 6.9112, 168.9071910575588, 3019620.997615928, 2284356.21279352, 473674.7847231004], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3509400.0000, 
sim time next is 3510000.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.8994133201291126, 1.0, 1.0, 0.8994133201291126, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2515652.062379776, 2515652.062379776, 471175.8254852348], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.63, 1.0, 1.0, 0.87881122907122, 1.0, 0.5, 0.87881122907122, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6987922395499377, 0.6987922395499377, 0.703247500724231], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24175064], dtype=float32), -0.95652884]. 
=============================================
[2019-04-10 12:40:09,104] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[47.40738 ]
 [48.2008  ]
 [47.67639 ]
 [46.922913]
 [45.821022]], R is [[46.98474884]
 [46.51490021]
 [46.35884476]
 [46.20900726]
 [46.06567764]].
[2019-04-10 12:40:11,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5044027e-12 9.9999988e-01 2.4034849e-13 1.4768308e-07 3.8391475e-20], sum to 1.0000
[2019-04-10 12:40:11,582] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7129
[2019-04-10 12:40:11,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1875908.879677713 W.
[2019-04-10 12:40:11,591] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.6708744836693119, 1.0, 2.0, 0.6708744836693119, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1875908.879677713, 1875908.879677713, 361913.1855994056], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3589200.0000, 
sim time next is 3589800.0000, 
raw observation next is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.4521433767691777, 1.0, 2.0, 0.4521433767691777, 1.0, 1.0, 0.7852235193484713, 6.911200000000001, 6.9112, 170.5573041426782, 1896452.884795868, 1896452.884795867, 383630.8267144794], 
processed observation next is [1.0, 0.5652173913043478, 0.7235387045813588, 0.6633333333333334, 1.0, 1.0, 0.3399317792399731, 1.0, 1.0, 0.3399317792399731, 1.0, 0.5, 0.738077462620087, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5267924679988523, 0.5267924679988519, 0.5725833234544468], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1005105], dtype=float32), 0.11368797]. 
=============================================
[2019-04-10 12:40:19,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6206583e-21 1.0000000e+00 4.2255014e-25 8.2375635e-28 1.4083799e-33], sum to 1.0000
[2019-04-10 12:40:19,545] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6167
[2019-04-10 12:40:19,550] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5544707328078563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774814.4618470911, 774814.4618470911, 192571.2591240697], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3787200.0000, 
sim time next is 3787800.0000, 
raw observation next is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.5566760394165543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777897.274046499, 777897.274046499, 192952.8135621715], 
processed observation next is [1.0, 0.8695652173913043, 0.6129541864139019, 0.7566666666666667, 1.0, 1.0, 0.46587474628500514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2160825761240275, 0.2160825761240275, 0.2879892739733903], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.5685191], dtype=float32), -0.7482207]. 
=============================================
[2019-04-10 12:40:21,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0112512e-22 1.0000000e+00 3.6853651e-25 2.6984597e-27 3.5871737e-34], sum to 1.0000
[2019-04-10 12:40:21,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2480
[2019-04-10 12:40:21,348] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 77.0, 1.0, 2.0, 0.5563579084719945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777452.5562140992, 777452.5562140992, 192897.3829382654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3789000.0000, 
sim time next is 3789600.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5542721353531355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774536.8416353217, 774536.8416353212, 192536.3966542327], 
processed observation next is [1.0, 0.8695652173913043, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 0.46297847632907885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21514912267647826, 0.2151491226764781, 0.2873677562003473], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.9389248], dtype=float32), -1.2616982]. 
=============================================
[2019-04-10 12:40:22,042] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:40:22,043] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:40:22,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:22,046] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:40:22,047] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:40:22,047] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:22,048] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:40:22,048] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:22,050] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:40:22,050] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:22,052] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:22,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run39
[2019-04-10 12:40:22,087] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run39
[2019-04-10 12:40:22,087] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run39
[2019-04-10 12:40:22,119] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run39
[2019-04-10 12:40:22,120] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run39
[2019-04-10 12:40:34,583] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12955068], dtype=float32), 0.09576375]
[2019-04-10 12:40:34,584] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.13446699666667, 96.95081463833334, 1.0, 2.0, 0.3124872013155473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491814.8634425409, 491814.8634425403, 166364.0836192166]
[2019-04-10 12:40:34,585] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:40:34,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.5956686e-20 1.0000000e+00 3.4415685e-23 4.7889924e-27 6.5317666e-32], sampled 0.3286163805017047
[2019-04-10 12:40:52,796] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12955068], dtype=float32), 0.09576375]
[2019-04-10 12:40:52,797] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.8, 56.16666666666667, 1.0, 2.0, 0.7677875937242764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073052.901160582, 1073052.901160582, 236159.9391139732]
[2019-04-10 12:40:52,797] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:40:52,799] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7760720e-18 1.0000000e+00 2.6752935e-21 7.8559515e-23 1.5419043e-29], sampled 0.029640404220999228
[2019-04-10 12:40:59,544] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12955068], dtype=float32), 0.09576375]
[2019-04-10 12:40:59,545] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.758548635, 75.27035244000001, 1.0, 2.0, 0.7822036276952332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1093210.989110497, 1093210.989110497, 239591.0741711905]
[2019-04-10 12:40:59,545] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:40:59,546] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5692943e-18 1.0000000e+00 5.7898156e-21 7.3762685e-23 4.2999335e-29], sampled 0.8772392530345444
[2019-04-10 12:41:45,870] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.6819 2927358253.7736 1334.0000
[2019-04-10 12:41:45,953] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8003.4921 3007151314.3106 1749.0000
[2019-04-10 12:41:46,065] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.8870 2842136134.4977 1119.0000
[2019-04-10 12:41:46,197] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.5168 2779241707.1616 930.0000
[2019-04-10 12:41:46,254] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7898.2723 3162138857.4165 1732.0000
[2019-04-10 12:41:47,270] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 950000, evaluation results [950000.0, 7898.272299597328, 3162138857.4165025, 1732.0, 8255.681883468118, 2927358253.7736125, 1334.0, 8661.51677822446, 2779241707.161594, 930.0, 8003.492075286366, 3007151314.31059, 1749.0, 8499.886978794124, 2842136134.497694, 1119.0]
[2019-04-10 12:41:49,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0860472e-19 1.0000000e+00 6.3516619e-23 2.0902913e-25 7.1733044e-32], sum to 1.0000
[2019-04-10 12:41:49,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9465
[2019-04-10 12:41:49,147] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.83333333333334, 60.33333333333334, 1.0, 2.0, 0.62162509133096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868693.9811581243, 868693.9811581243, 204844.9359646072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3850800.0000, 
sim time next is 3851400.0000, 
raw observation next is [34.91666666666666, 60.16666666666666, 1.0, 2.0, 0.6229797056396114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870587.7714744635, 870587.7714744635, 205106.270703865], 
processed observation next is [0.0, 0.5652173913043478, 0.8538704581358605, 0.6016666666666666, 1.0, 1.0, 0.5457586814935076, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2418299365206843, 0.2418299365206843, 0.30612876224457464], 
reward next is 0.6939, 
noisyNet noise sample is [array([1.7873971], dtype=float32), 0.5746594]. 
=============================================
[2019-04-10 12:41:53,533] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0680560e-18 1.0000000e+00 1.1546615e-21 1.6135292e-23 5.6571792e-30], sum to 1.0000
[2019-04-10 12:41:53,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5600
[2019-04-10 12:41:53,568] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 93.0, 1.0, 2.0, 0.5736046835603705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801562.2318250238, 801562.2318250238, 195930.9415085911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3897000.0000, 
sim time next is 3897600.0000, 
raw observation next is [27.16666666666667, 93.33333333333334, 1.0, 2.0, 0.5725396724857381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800073.4117516486, 800073.4117516486, 195740.9913087986], 
processed observation next is [0.0, 0.08695652173913043, 0.4865718799368091, 0.9333333333333335, 1.0, 1.0, 0.48498755721173264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22224261437545795, 0.22224261437545795, 0.2921507332967143], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.37349826], dtype=float32), 0.12176621]. 
=============================================
[2019-04-10 12:41:54,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3068336e-18 1.0000000e+00 1.9784028e-21 1.5125769e-22 2.3479505e-29], sum to 1.0000
[2019-04-10 12:41:54,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6616
[2019-04-10 12:41:54,144] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 84.0, 1.0, 2.0, 0.6367628637425055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889857.220524771, 889857.220524771, 207793.4828310144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3975000.0000, 
sim time next is 3975600.0000, 
raw observation next is [29.66666666666667, 84.0, 1.0, 2.0, 0.6130253333528325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 856671.347422672, 856671.347422672, 203197.3067250598], 
processed observation next is [1.0, 0.0, 0.6050552922590839, 0.84, 1.0, 1.0, 0.5337654618708825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23796426317296443, 0.23796426317296443, 0.30327956227620867], 
reward next is 0.6967, 
noisyNet noise sample is [array([0.16959172], dtype=float32), 0.33962923]. 
=============================================
[2019-04-10 12:42:08,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7552466e-11 5.9572409e-04 1.4184732e-09 9.9940431e-01 1.5451533e-14], sum to 1.0000
[2019-04-10 12:42:08,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8558
[2019-04-10 12:42:08,849] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.941292037881312, 1.0, 2.0, 0.941292037881312, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2632909.927154378, 2632909.927154378, 494529.3812576864], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4096800.0000, 
sim time next is 4097400.0000, 
raw observation next is [31.16666666666667, 77.66666666666667, 1.0, 2.0, 0.9264819622845728, 1.0, 2.0, 0.9264819622845728, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2591441.35776736, 2591441.35776736, 486154.7503510349], 
processed observation next is [1.0, 0.43478260869565216, 0.6761453396524489, 0.7766666666666667, 1.0, 1.0, 0.9114240509452685, 1.0, 1.0, 0.9114240509452685, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7198448216020444, 0.7198448216020444, 0.7256041050015447], 
reward next is 0.2744, 
noisyNet noise sample is [array([0.00872283], dtype=float32), 0.21237794]. 
=============================================
[2019-04-10 12:42:16,372] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6144755e-12 1.5353224e-04 1.0194254e-10 9.9984646e-01 1.9303733e-16], sum to 1.0000
[2019-04-10 12:42:16,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2574
[2019-04-10 12:42:16,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3446496.607029837 W.
[2019-04-10 12:42:16,444] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.83333333333334, 64.5, 1.0, 2.0, 1.001162288985849, 1.0, 2.0, 0.8211711840071869, 1.0, 2.0, 1.03, 7.005121490547213, 6.9112, 170.5573041426782, 3446496.607029837, 3379216.822868899, 633351.5225711435], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [35.66666666666667, 65.0, 1.0, 2.0, 1.011571726253293, 1.0, 2.0, 0.826375902640909, 1.0, 2.0, 1.03, 7.005122312167575, 6.9112, 170.5573041426782, 3468371.412805902, 3401091.040084877, 637829.0759938265], 
processed observation next is [1.0, 0.6521739130434783, 0.8894154818325437, 0.65, 1.0, 1.0, 1.0139418388593893, 1.0, 1.0, 0.7908143405312157, 1.0, 1.0, 1.0365853658536586, 0.00939223121675754, 0.0, 0.8375144448122397, 0.9634365035571949, 0.944747511134688, 0.9519836955131739], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.77580154], dtype=float32), -0.12513387]. 
=============================================
[2019-04-10 12:42:16,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[40.18468 ]
 [41.912067]
 [44.3863  ]
 [44.79609 ]
 [44.618576]], R is [[38.50782013]
 [38.1227417 ]
 [37.74151611]
 [37.36410141]
 [36.99045944]].
[2019-04-10 12:42:35,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5015470e-19 1.0000000e+00 1.4329971e-22 4.0062119e-26 2.7134216e-31], sum to 1.0000
[2019-04-10 12:42:35,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9859
[2019-04-10 12:42:35,872] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 83.16666666666667, 1.0, 2.0, 0.5815724154656606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 812700.6978889741, 812700.6978889747, 197363.1852121624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4432200.0000, 
sim time next is 4432800.0000, 
raw observation next is [29.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5826276356725464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814175.8484580095, 814175.8484580095, 197554.2035287762], 
processed observation next is [0.0, 0.30434782608695654, 0.5892575039494474, 0.8233333333333335, 1.0, 1.0, 0.4971417297259595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22615995790500265, 0.22615995790500265, 0.2948570201922033], 
reward next is 0.7051, 
noisyNet noise sample is [array([-0.04035423], dtype=float32), -1.2597536]. 
=============================================
[2019-04-10 12:42:35,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.8044715e-20 1.0000000e+00 6.0670434e-23 1.0964538e-26 6.1311599e-32], sum to 1.0000
[2019-04-10 12:42:35,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1210
[2019-04-10 12:42:35,988] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5817448407078488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812941.7405118359, 812941.7405118353, 197394.2623503959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5820091056976335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813311.1710184453, 813311.1710184453, 197442.0592235731], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49639651288871506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259197697273459, 0.2259197697273459, 0.29468964063219866], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.03107359], dtype=float32), -1.8490788]. 
=============================================
[2019-04-10 12:42:36,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.72422]
 [67.7329 ]
 [67.73764]
 [67.73775]
 [67.72792]], R is [[67.79193878]
 [67.8194046 ]
 [67.84671783]
 [67.87394714]
 [67.90106201]].
[2019-04-10 12:42:37,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5868900e-20 1.0000000e+00 2.1286686e-23 1.3901407e-26 1.2464954e-32], sum to 1.0000
[2019-04-10 12:42:37,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9827
[2019-04-10 12:42:37,040] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5284096911494777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738384.2427864469, 738384.2427864476, 188166.5361804091], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.529300894799023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739630.018258668, 739630.0182586674, 188313.6533800974], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.43289264433617225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20545278284963, 0.20545278284962984, 0.28106515429865286], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.3116148], dtype=float32), -0.2556337]. 
=============================================
[2019-04-10 12:42:40,702] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4067335e-20 1.0000000e+00 1.9599510e-24 1.6498625e-27 1.0124850e-33], sum to 1.0000
[2019-04-10 12:42:40,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3177
[2019-04-10 12:42:40,713] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5221957909900145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729698.1380788796, 729698.1380788796, 187146.5020171929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4567200.0000, 
sim time next is 4567800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5221095641334648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729577.6063064608, 729577.6063064614, 187132.431466196], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4242283905222467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2026604461962391, 0.20266044619623927, 0.27930213651671043], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.72903204], dtype=float32), 0.6567541]. 
=============================================
[2019-04-10 12:42:48,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2899436e-19 1.0000000e+00 1.1384352e-21 1.0033087e-23 7.1159139e-30], sum to 1.0000
[2019-04-10 12:42:48,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6519
[2019-04-10 12:42:48,387] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.5970820639323688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 834382.6977638899, 834382.6977638893, 200195.5653108609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4770000.0000, 
sim time next is 4770600.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.6504004919059506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908923.556545309, 908923.5565453096, 210500.1410584522], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.5787957733806633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25247876570703026, 0.25247876570703043, 0.3141793150126152], 
reward next is 0.6858, 
noisyNet noise sample is [array([0.3703917], dtype=float32), -1.6796274]. 
=============================================
[2019-04-10 12:42:50,036] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 12:42:50,037] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:42:50,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:42:50,039] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:42:50,040] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:42:50,040] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:42:50,040] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:42:50,041] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:42:50,042] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:42:50,041] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:42:50,045] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:42:50,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run40
[2019-04-10 12:42:50,084] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run40
[2019-04-10 12:42:50,084] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run40
[2019-04-10 12:42:50,084] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run40
[2019-04-10 12:42:50,116] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run40
[2019-04-10 12:42:56,130] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13012937], dtype=float32), 0.09633534]
[2019-04-10 12:42:56,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [16.9833765, 86.98377725, 1.0, 2.0, 0.1977581353704448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 330788.9464503482, 330788.9464503482, 143663.6914571478]
[2019-04-10 12:42:56,132] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:42:56,133] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.6032035e-21 1.0000000e+00 1.3865990e-24 1.5195637e-29 1.2202841e-33], sampled 0.9623723848131197
[2019-04-10 12:43:31,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13012937], dtype=float32), 0.09633534]
[2019-04-10 12:43:31,248] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.85262833333334, 80.90637165666666, 1.0, 2.0, 0.767988379661143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9127986864862, 1073333.660071631, 1073333.660071631, 236204.0271228557]
[2019-04-10 12:43:31,248] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:43:31,253] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.6028166e-17 1.0000000e+00 4.1081519e-19 7.6732602e-17 9.1479092e-27], sampled 0.7382917659327074
[2019-04-10 12:43:32,660] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13012937], dtype=float32), 0.09633534]
[2019-04-10 12:43:32,661] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.83820682333333, 86.85998343666665, 1.0, 2.0, 0.66636671722115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 931245.8654439936, 931245.8654439936, 213762.6834183444]
[2019-04-10 12:43:32,662] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:43:32,663] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.3850750e-20 1.0000000e+00 4.8685762e-23 3.0436539e-25 8.7116098e-32], sampled 0.06928876988858135
[2019-04-10 12:43:35,015] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13012937], dtype=float32), 0.09633534]
[2019-04-10 12:43:35,016] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.66666666666666, 68.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.879271546632492, 6.9112, 170.5573041426782, 3603606.611296185, 2910137.583810527, 548095.1205418754]
[2019-04-10 12:43:35,018] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:43:35,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7379935e-10 9.9272728e-01 4.6665533e-11 7.2726789e-03 7.4494318e-17], sampled 0.28396068797046337
[2019-04-10 12:43:35,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3603606.611296185 W.
[2019-04-10 12:44:10,916] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.4903 2927455121.1122 1338.0000
[2019-04-10 12:44:11,015] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6679 3007840824.3263 1766.0000
[2019-04-10 12:44:11,097] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1488 2779280987.4023 933.0000
[2019-04-10 12:44:11,252] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.5730 3164112910.9075 1769.0000
[2019-04-10 12:44:11,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.0868 2842291645.7667 1130.0000
[2019-04-10 12:44:12,366] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 975000, evaluation results [975000.0, 7886.572956227018, 3164112910.90749, 1769.0, 8253.49033987512, 2927455121.1121645, 1338.0, 8659.148803224978, 2779280987.402282, 933.0, 7998.667917709291, 3007840824.32627, 1766.0, 8497.086839422922, 2842291645.7667413, 1130.0]
[2019-04-10 12:44:19,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8216447e-11 9.9866641e-01 4.3473444e-11 1.3335847e-03 1.1745134e-17], sum to 1.0000
[2019-04-10 12:44:19,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4648
[2019-04-10 12:44:19,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2088597.615538081 W.
[2019-04-10 12:44:19,141] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666667, 66.0, 1.0, 2.0, 0.4979090084936313, 1.0, 2.0, 0.4979090084936313, 1.0, 2.0, 0.8586624979691028, 6.9112, 6.9112, 170.5573041426782, 2088597.615538081, 2088597.615538081, 412615.8492631185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4897200.0000, 
sim time next is 4897800.0000, 
raw observation next is [30.5, 66.0, 1.0, 2.0, 0.761712408687576, 1.0, 2.0, 0.761712408687576, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2130163.717267555, 2130163.717267555, 401657.3974058814], 
processed observation next is [1.0, 0.6956521739130435, 0.6445497630331753, 0.66, 1.0, 1.0, 0.7129065164910553, 1.0, 1.0, 0.7129065164910553, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.591712143685432, 0.591712143685432, 0.5994886528445991], 
reward next is 0.4005, 
noisyNet noise sample is [array([-0.48375428], dtype=float32), -0.73789436]. 
=============================================
[2019-04-10 12:44:24,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0490363e-20 1.0000000e+00 6.1732910e-23 1.0522324e-24 7.0120548e-32], sum to 1.0000
[2019-04-10 12:44:24,188] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2682
[2019-04-10 12:44:24,205] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.91666666666666, 84.0, 1.0, 2.0, 0.5113504572866356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714538.1508985457, 714538.1508985457, 185393.2993440054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5008200.0000, 
sim time next is 5008800.0000, 
raw observation next is [26.83333333333334, 84.0, 1.0, 2.0, 0.5094181981704583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711837.194158794, 711837.1941587933, 185084.5378598327], 
processed observation next is [1.0, 1.0, 0.4707740916271725, 0.84, 1.0, 1.0, 0.4089375881571786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19773255393299832, 0.19773255393299813, 0.2762455788952727], 
reward next is 0.7238, 
noisyNet noise sample is [array([0.6585562], dtype=float32), 0.18644449]. 
=============================================
[2019-04-10 12:44:25,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6230014e-20 1.0000000e+00 6.1463391e-23 1.6035939e-24 1.7007905e-31], sum to 1.0000
[2019-04-10 12:44:25,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6319
[2019-04-10 12:44:25,629] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5091015903215216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711394.6329976249, 711394.6329976243, 185034.4587508654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5005800.0000, 
sim time next is 5006400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5104633257084197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713298.0968898302, 713298.0968898295, 185251.679123224], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4101967779619514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19813836024717504, 0.19813836024717485, 0.2764950434674985], 
reward next is 0.7235, 
noisyNet noise sample is [array([2.2592912], dtype=float32), -1.5042586]. 
=============================================
[2019-04-10 12:44:26,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9338575e-20 1.0000000e+00 3.9504746e-24 1.0491281e-28 1.4571006e-32], sum to 1.0000
[2019-04-10 12:44:26,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5559
[2019-04-10 12:44:26,546] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 87.33333333333334, 1.0, 2.0, 0.5031349410749865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703054.358707691, 703054.3587076915, 184088.7353093144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5034000.0000, 
sim time next is 5034600.0000, 
raw observation next is [26.5, 86.5, 1.0, 2.0, 0.50525111726647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706012.3749141812, 706012.3749141812, 184422.9075002007], 
processed observation next is [0.0, 0.2608695652173913, 0.4549763033175356, 0.865, 1.0, 1.0, 0.4039170087547831, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19611454858727256, 0.19611454858727256, 0.27525807089582194], 
reward next is 0.7247, 
noisyNet noise sample is [array([-1.717408], dtype=float32), 0.23894794]. 
=============================================
[2019-04-10 12:44:28,220] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.5896931e-21 1.0000000e+00 3.4170516e-24 7.7178089e-28 9.5851058e-33], sum to 1.0000
[2019-04-10 12:44:28,248] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6745
[2019-04-10 12:44:28,276] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5364795520307343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749664.8095138671, 749664.8095138664, 189509.7604375454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5040000.0000, 
sim time next is 5040600.0000, 
raw observation next is [28.33333333333334, 81.0, 1.0, 2.0, 0.5360444949806962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749056.655939734, 749056.6559397334, 189436.6097146941], 
processed observation next is [0.0, 0.34782608695652173, 0.5418641390205374, 0.81, 1.0, 1.0, 0.44101746383216406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20807129331659277, 0.2080712933165926, 0.28274120852939416], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.20982414], dtype=float32), 1.4800811]. 
=============================================
[2019-04-10 12:44:41,340] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9461772e-20 1.0000000e+00 2.7410002e-24 4.7459430e-28 6.1564452e-33], sum to 1.0000
[2019-04-10 12:44:41,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8749
[2019-04-10 12:44:41,405] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5524797156795057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772031.213966883, 772031.213966883, 192227.284804925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5146200.0000, 
sim time next is 5146800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5515039770462621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770667.2291260133, 770667.2291260128, 192059.4410722862], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45964334583886995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2140742303127815, 0.21407423031278133, 0.2866558821974421], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.49901024], dtype=float32), -0.69362324]. 
=============================================
[2019-04-10 12:44:44,221] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.8747441e-19 1.0000000e+00 1.0164849e-21 2.1756837e-24 1.6472753e-30], sum to 1.0000
[2019-04-10 12:44:44,227] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9233
[2019-04-10 12:44:44,229] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5171155593673222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722596.7855046365, 722596.7855046365, 186320.7193050761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5186400.0000, 
sim time next is 5187000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5168586989980881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722237.7369859146, 722237.7369859153, 186279.1976108113], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4179020469856483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006215936071985, 0.2006215936071987, 0.27802865315046466], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.30221394], dtype=float32), -1.6366215]. 
=============================================
[2019-04-10 12:44:44,240] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[68.51615]
 [69.04728]
 [69.81757]
 [71.1238 ]
 [72.78847]], R is [[68.12961578]
 [68.17022705]
 [68.21060181]
 [68.25082397]
 [68.291008  ]].
[2019-04-10 12:44:48,006] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.6527840e-18 1.0000000e+00 4.8835574e-20 4.1481149e-21 1.2126721e-28], sum to 1.0000
[2019-04-10 12:44:48,012] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4574
[2019-04-10 12:44:48,023] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.8040646589605409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123780.223190856, 1123780.223190856, 244908.5317874186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5199000.0000, 
sim time next is 5199600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7720453625760281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079006.543133964, 1079006.543133964, 237162.2853708779], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.725355858525335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29972403975943446, 0.29972403975943446, 0.3539735602550416], 
reward next is 0.6460, 
noisyNet noise sample is [array([1.0201038], dtype=float32), 0.60774434]. 
=============================================
[2019-04-10 12:45:05,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7626139e-19 1.0000000e+00 7.9828049e-22 5.6656625e-24 8.7859081e-30], sum to 1.0000
[2019-04-10 12:45:05,586] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2330
[2019-04-10 12:45:05,592] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.8164064319881933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1141038.651709217, 1141038.651709217, 247977.6576206631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5551800.0000, 
sim time next is 5552400.0000, 
raw observation next is [27.03333333333333, 88.66666666666667, 1.0, 2.0, 0.7850647749198626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1097211.806089695, 1097211.806089695, 240277.8492756816], 
processed observation next is [1.0, 0.2608695652173913, 0.48025276461295413, 0.8866666666666667, 1.0, 1.0, 0.7410418974938103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3047810572471375, 0.3047810572471375, 0.35862365563534565], 
reward next is 0.6414, 
noisyNet noise sample is [array([0.823647], dtype=float32), 1.9052513]. 
=============================================
[2019-04-10 12:45:11,852] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:45:11,852] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:45:11,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:11,853] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:45:11,854] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:45:11,854] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:11,854] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:11,855] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:45:11,856] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:45:11,859] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:11,860] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:12,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run41
[2019-04-10 12:45:12,248] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run41
[2019-04-10 12:45:12,271] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run41
[2019-04-10 12:45:12,308] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run41
[2019-04-10 12:45:12,456] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run41
[2019-04-10 12:45:17,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:17,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.68408012666667, 86.19071168333333, 1.0, 2.0, 0.3788249808648405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571277.4073560556, 571277.4073560549, 172120.7594605197]
[2019-04-10 12:45:17,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:45:17,301] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8302750e-21 1.0000000e+00 3.4631502e-25 5.5348650e-30 2.1884165e-34], sampled 0.6284544715489746
[2019-04-10 12:45:19,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:19,709] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.71654667333333, 52.43752498666666, 1.0, 2.0, 0.337082467275816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 554454.2085948013, 554454.2085948013, 170817.86058049]
[2019-04-10 12:45:19,709] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:45:19,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6544060e-21 1.0000000e+00 8.1513323e-25 9.1395938e-30 6.9470835e-34], sampled 0.44409947761295576
[2019-04-10 12:45:21,534] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:21,534] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.630366795, 56.39757033333333, 1.0, 2.0, 0.502181892074714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822057.8877722424, 822057.8877722424, 196196.2378851489]
[2019-04-10 12:45:21,535] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:45:21,538] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4168730e-21 1.0000000e+00 1.4920478e-24 4.7677702e-29 1.4512143e-33], sampled 0.5557767778253517
[2019-04-10 12:45:24,183] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:24,185] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.92304559666667, 92.11633132, 1.0, 2.0, 0.2595779258004804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426984.8859263958, 426984.8859263958, 161863.9708110259]
[2019-04-10 12:45:24,187] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:45:24,188] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1530154e-21 1.0000000e+00 1.9000690e-25 4.2925870e-31 1.0508311e-34], sampled 0.7574780287163004
[2019-04-10 12:45:24,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:24,428] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.38333333333333, 91.16666666666667, 1.0, 2.0, 0.3504776690119179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553640.093699951, 553640.093699951, 171230.9683853386]
[2019-04-10 12:45:24,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:45:24,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6570976e-21 1.0000000e+00 9.0333093e-25 1.5449660e-29 7.9167065e-34], sampled 0.9051379887964067
[2019-04-10 12:45:37,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:37,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.08333333333333, 90.5, 1.0, 2.0, 0.3861487727766609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583063.9210667113, 583063.9210667113, 173192.5667469175]
[2019-04-10 12:45:37,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:45:37,245] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1839037e-21 1.0000000e+00 1.7679211e-24 5.9708234e-29 1.9112418e-33], sampled 0.16838977910903763
[2019-04-10 12:45:55,779] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:55,780] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.26666666666667, 61.66666666666667, 1.0, 2.0, 0.591261322169543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 826245.4322964352, 826245.4322964358, 199129.1170499689]
[2019-04-10 12:45:55,781] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:45:55,783] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8636908e-22 1.0000000e+00 3.6157091e-26 1.6768600e-28 7.7142749e-36], sampled 0.5842040959169649
[2019-04-10 12:45:58,987] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:45:58,988] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.03333333333333, 46.0, 1.0, 2.0, 0.5958166335597674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832613.6493914969, 832613.6493914969, 199968.6159585647]
[2019-04-10 12:45:58,989] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:45:58,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4682843e-21 1.0000000e+00 6.5983304e-25 2.7975294e-29 4.4689099e-34], sampled 0.2082807816953719
[2019-04-10 12:46:16,991] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:46:16,993] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.5, 88.0, 1.0, 2.0, 0.5934647647470511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829325.7879011242, 829325.7879011242, 199534.1763663082]
[2019-04-10 12:46:16,993] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:46:16,995] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5485369e-20 1.0000000e+00 6.3464996e-24 9.4735319e-28 8.7732014e-33], sampled 0.8196059445595637
[2019-04-10 12:46:25,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:46:25,487] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.86666666666667, 88.0, 1.0, 2.0, 0.5673907862701171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792875.6144799187, 792875.6144799181, 194825.0522412113]
[2019-04-10 12:46:25,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:25,491] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.5028464e-21 1.0000000e+00 2.7603493e-24 1.1353910e-28 3.1743659e-33], sampled 0.4564472238344567
[2019-04-10 12:46:26,105] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12798426], dtype=float32), 0.09503878]
[2019-04-10 12:46:26,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 83.33333333333334, 1.0, 2.0, 0.7987027285087922, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598144721452, 6.9112, 168.9123160047743, 2013257.839813735, 1946016.874007922, 407509.0864454412]
[2019-04-10 12:46:26,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:26,108] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5829316e-14 1.0000000e+00 1.3722537e-14 6.3024759e-11 3.2158724e-22], sampled 0.737279536326608
[2019-04-10 12:46:26,108] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2013257.839813735 W.
[2019-04-10 12:46:33,081] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-04-10 12:46:33,256] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.8340 3007783761.7950 1766.0000
[2019-04-10 12:46:33,386] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-04-10 12:46:33,418] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-04-10 12:46:33,562] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6764 2842629191.6753 1131.0000
[2019-04-10 12:46:34,576] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1000000, evaluation results [1000000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7996.833990763685, 3007783761.794972, 1766.0, 8496.676437139186, 2842629191.6752853, 1131.0]
[2019-04-10 12:46:35,397] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.4421058e-20 1.0000000e+00 1.1885442e-23 8.4430550e-27 3.5369291e-32], sum to 1.0000
[2019-04-10 12:46:35,403] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8890
[2019-04-10 12:46:35,408] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5194312625950179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725833.7629470523, 725833.7629470523, 186695.802563423], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5704800.0000, 
sim time next is 5705400.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5186722328170297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724772.7613659343, 724772.761365935, 186572.6404854767], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.42008702749039717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20132576704609284, 0.20132576704609303, 0.27846662759026375], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.20859581], dtype=float32), 1.9095771]. 
=============================================
[2019-04-10 12:46:36,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2271291e-21 1.0000000e+00 8.2920877e-25 3.9078546e-28 4.6492009e-34], sum to 1.0000
[2019-04-10 12:46:36,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-10 12:46:36,848] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.05, 83.0, 1.0, 2.0, 0.546436695655818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763583.7056478023, 763583.7056478023, 191191.241292817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5776200.0000, 
sim time next is 5776800.0000, 
raw observation next is [28.0, 83.33333333333334, 1.0, 2.0, 0.5460980681589057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763110.3419035353, 763110.3419035347, 191133.5679550873], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8333333333333335, 1.0, 1.0, 0.4531302026010912, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21197509497320424, 0.21197509497320408, 0.28527398202251836], 
reward next is 0.7147, 
noisyNet noise sample is [array([1.0071071], dtype=float32), 1.6802375]. 
=============================================
[2019-04-10 12:46:40,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2354363e-15 1.0000000e+00 2.8089256e-16 1.9943634e-11 3.0242365e-23], sum to 1.0000
[2019-04-10 12:46:40,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6944
[2019-04-10 12:46:40,678] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.6, 67.0, 1.0, 2.0, 0.5249805169895906, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564928207, 733590.7604926385, 733590.760492639, 187605.9427035319], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5851200.0000, 
sim time next is 5851800.0000, 
raw observation next is [31.4, 68.0, 1.0, 2.0, 0.5145617171270578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104266, 719026.9397929979, 719026.9397929985, 185913.1239523695], 
processed observation next is [1.0, 0.7391304347826086, 0.6872037914691943, 0.68, 1.0, 1.0, 0.4151345989482624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439945152281, 0.19972970549805497, 0.19972970549805513, 0.27748227455577534], 
reward next is 0.7225, 
noisyNet noise sample is [array([-1.2844663], dtype=float32), 1.0456376]. 
=============================================
[2019-04-10 12:46:40,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0216207e-20 1.0000000e+00 1.0102057e-24 7.3361007e-26 2.8039339e-33], sum to 1.0000
[2019-04-10 12:46:40,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7647
[2019-04-10 12:46:40,949] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 81.33333333333334, 1.0, 2.0, 0.5666369186222883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791821.7622108033, 791821.7622108033, 194695.9060357982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5942400.0000, 
sim time next is 5943000.0000, 
raw observation next is [29.21666666666667, 81.66666666666667, 1.0, 2.0, 0.566209917345607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 791224.845874145, 791224.8458741456, 194620.4623591429], 
processed observation next is [1.0, 0.782608695652174, 0.5837282780410744, 0.8166666666666668, 1.0, 1.0, 0.47736134619952647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21978467940948473, 0.2197846794094849, 0.2904783020285715], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.5642009], dtype=float32), -1.7109612]. 
=============================================
[2019-04-10 12:46:40,962] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[65.99809 ]
 [66.138016]
 [66.0249  ]
 [65.700775]
 [65.61623 ]], R is [[65.91290283]
 [65.96318817]
 [66.01239014]
 [66.06060791]
 [66.10823822]].
[2019-04-10 12:46:42,824] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6961099e-19 1.0000000e+00 3.9983419e-22 2.3325568e-24 2.1151595e-30], sum to 1.0000
[2019-04-10 12:46:42,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0622
[2019-04-10 12:46:42,833] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 86.5, 1.0, 2.0, 0.7215532050365911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008405.463406446, 1008405.463406446, 225567.9730546769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5987400.0000, 
sim time next is 5988000.0000, 
raw observation next is [28.1, 86.0, 1.0, 2.0, 0.7260829362993917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1014739.006210879, 1014739.00621088, 226577.8577497597], 
processed observation next is [1.0, 0.30434782608695654, 0.5308056872037916, 0.86, 1.0, 1.0, 0.6699794413245683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2818719461696886, 0.2818719461696889, 0.3381759070891936], 
reward next is 0.6618, 
noisyNet noise sample is [array([0.07146273], dtype=float32), -1.153059]. 
=============================================
[2019-04-10 12:46:42,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.871006]
 [61.735123]
 [61.671787]
 [61.882027]
 [62.10175 ]], R is [[61.9225235 ]
 [61.96662903]
 [62.00387573]
 [62.02971268]
 [62.06225586]].
[2019-04-10 12:46:43,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8592564e-09 1.4685553e-01 5.3021045e-09 8.5314447e-01 1.3559412e-14], sum to 1.0000
[2019-04-10 12:46:43,983] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8411
[2019-04-10 12:46:43,987] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 0.8528708368065053, 1.0, 2.0, 0.8528708368065053, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2385348.846385958, 2385348.846385958, 446439.6021957427], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5913600.0000, 
sim time next is 5914200.0000, 
raw observation next is [31.93333333333334, 70.66666666666667, 1.0, 2.0, 0.8612803119347249, 1.0, 2.0, 0.8612803119347249, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2408891.527546187, 2408891.527546187, 450815.6362906778], 
processed observation next is [1.0, 0.43478260869565216, 0.7124802527646134, 0.7066666666666667, 1.0, 1.0, 0.8328678457044878, 1.0, 1.0, 0.8328678457044878, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6691365354294964, 0.6691365354294964, 0.6728591586428028], 
reward next is 0.3271, 
noisyNet noise sample is [array([0.17763041], dtype=float32), 2.390546]. 
=============================================
[2019-04-10 12:46:46,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0630236e-11 1.0000000e+00 1.7974280e-12 4.3555128e-09 1.3059814e-18], sum to 1.0000
[2019-04-10 12:46:46,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7974
[2019-04-10 12:46:46,860] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2388276.69602869 W.
[2019-04-10 12:46:46,864] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.85, 78.33333333333334, 1.0, 2.0, 0.569277784655915, 1.0, 2.0, 0.569277784655915, 1.0, 1.0, 0.9886472489071109, 6.9112, 6.9112, 170.5573041426782, 2388276.69602869, 2388276.69602869, 466358.5664150786], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5994600.0000, 
sim time next is 5995200.0000, 
raw observation next is [30.0, 77.66666666666667, 1.0, 2.0, 0.5802287620206563, 1.0, 2.0, 0.5802287620206563, 1.0, 2.0, 1.007665475046111, 6.911199999999999, 6.9112, 170.5573041426782, 2434263.802946579, 2434263.802946579, 475058.4898176566], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.7766666666666667, 1.0, 1.0, 0.4942515205068148, 1.0, 1.0, 0.4942515205068148, 1.0, 1.0, 1.0093481403001352, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6761843897073831, 0.6761843897073831, 0.7090425221159053], 
reward next is 0.2910, 
noisyNet noise sample is [array([-1.5124801], dtype=float32), -0.7041957]. 
=============================================
[2019-04-10 12:46:48,732] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.01742735e-14 1.00000000e+00 4.22212096e-16 1.18466033e-12
 2.20575259e-23], sum to 1.0000
[2019-04-10 12:46:48,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5241
[2019-04-10 12:46:48,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1744620.118522008 W.
[2019-04-10 12:46:48,750] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.73333333333333, 83.66666666666667, 1.0, 2.0, 0.4159735688466771, 1.0, 1.0, 0.4159735688466771, 1.0, 2.0, 0.7050459233488067, 6.911199999999999, 6.9112, 170.5573041426782, 1744620.118522008, 1744620.118522009, 359321.4916842105], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6013200.0000, 
sim time next is 6013800.0000, 
raw observation next is [27.3, 82.5, 1.0, 2.0, 0.6630615054507011, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.976198587516626, 6.9112, 168.9125698612905, 1823440.02490718, 1777327.895397074, 377883.4744277534], 
processed observation next is [1.0, 0.6086956521739131, 0.4928909952606636, 0.825, 1.0, 1.0, 0.5940500065671097, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006499858751662568, 0.0, 0.8294380465280141, 0.5065111180297722, 0.4937021931658539, 0.5640051857130648], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4370083], dtype=float32), 0.8558008]. 
=============================================
[2019-04-10 12:46:52,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.7882600e-10 9.8997855e-01 6.5027317e-10 1.0021464e-02 1.2143637e-15], sum to 1.0000
[2019-04-10 12:46:52,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6459
[2019-04-10 12:46:52,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2594866.146808613 W.
[2019-04-10 12:46:52,988] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.95, 65.5, 1.0, 2.0, 0.6184700728657909, 1.0, 2.0, 0.6184700728657909, 1.0, 1.0, 1.03, 6.95712383879067, 6.9112, 170.5573041426782, 2594866.146808613, 2561969.033020531, 494924.944852425], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6100200.0000, 
sim time next is 6100800.0000, 
raw observation next is [30.9, 65.66666666666667, 1.0, 2.0, 0.5561401313592396, 1.0, 2.0, 0.5561401313592396, 1.0, 2.0, 0.9626958505885095, 6.911200000000001, 6.9112, 170.5573041426782, 2333109.180435748, 2333109.180435747, 455502.1674633721], 
processed observation next is [1.0, 0.6086956521739131, 0.6635071090047393, 0.6566666666666667, 1.0, 1.0, 0.46522907392679463, 1.0, 1.0, 0.46522907392679463, 1.0, 1.0, 0.9545071348640359, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6480858834543743, 0.6480858834543741, 0.6798539812886151], 
reward next is 0.3201, 
noisyNet noise sample is [array([0.34216413], dtype=float32), 1.3521392]. 
=============================================
[2019-04-10 12:46:54,464] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.7869053e-19 1.0000000e+00 1.0245785e-21 1.0903700e-23 8.0130986e-30], sum to 1.0000
[2019-04-10 12:46:54,464] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2806
[2019-04-10 12:46:54,469] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 92.0, 1.0, 2.0, 0.7094215103613662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 991442.9196514131, 991442.9196514137, 222892.1227399357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6150000.0000, 
sim time next is 6150600.0000, 
raw observation next is [26.55, 92.0, 1.0, 2.0, 0.7472737070761235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044368.783032849, 1044368.783032848, 231380.1548623836], 
processed observation next is [1.0, 0.17391304347826086, 0.4573459715639811, 0.92, 1.0, 1.0, 0.6955104904531608, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2901024397313469, 0.2901024397313467, 0.34534351471997554], 
reward next is 0.6547, 
noisyNet noise sample is [array([-0.69445133], dtype=float32), -0.42184457]. 
=============================================
[2019-04-10 12:46:54,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2118117e-21 1.0000000e+00 9.6052769e-25 8.2931545e-27 6.5443797e-34], sum to 1.0000
[2019-04-10 12:46:54,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7172
[2019-04-10 12:46:54,554] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.38333333333333, 79.0, 1.0, 2.0, 0.5254278565227833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734216.0742925534, 734216.074292554, 187676.5075501182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6115800.0000, 
sim time next is 6116400.0000, 
raw observation next is [28.2, 80.0, 1.0, 2.0, 0.5251802323519601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733869.9326706474, 733869.9326706467, 187635.8053387474], 
processed observation next is [1.0, 0.8260869565217391, 0.5355450236966824, 0.8, 1.0, 1.0, 0.42792799078549404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20385275907517983, 0.20385275907517963, 0.2800534408041006], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.7076449], dtype=float32), -0.7992738]. 
=============================================
[2019-04-10 12:47:00,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7606559e-21 1.0000000e+00 1.2725610e-23 7.4281288e-26 3.7189635e-32], sum to 1.0000
[2019-04-10 12:47:00,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6551
[2019-04-10 12:47:00,401] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 88.66666666666666, 1.0, 2.0, 0.521727724133706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729043.8533712581, 729043.8533712574, 187070.6405458976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6219600.0000, 
sim time next is 6220200.0000, 
raw observation next is [26.71666666666667, 88.83333333333334, 1.0, 2.0, 0.5217213575510007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729034.9538810509, 729034.9538810509, 187069.6210340689], 
processed observation next is [1.0, 1.0, 0.46524486571879947, 0.8883333333333334, 1.0, 1.0, 0.4237606717481936, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20250970941140303, 0.20250970941140303, 0.2792083896030879], 
reward next is 0.7208, 
noisyNet noise sample is [array([-0.36475173], dtype=float32), 0.022057783]. 
=============================================
[2019-04-10 12:47:08,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1155769e-22 1.0000000e+00 2.4581497e-25 5.6467871e-30 4.4606475e-35], sum to 1.0000
[2019-04-10 12:47:08,538] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1968
[2019-04-10 12:47:08,545] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 68.66666666666667, 1.0, 2.0, 0.5390066517151064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753197.3754166679, 753197.3754166673, 189933.3915772869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6344400.0000, 
sim time next is 6345000.0000, 
raw observation next is [30.55, 68.0, 1.0, 2.0, 0.5387501720062388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752838.8486701441, 752838.8486701441, 189890.3342784654], 
processed observation next is [0.0, 0.43478260869565216, 0.6469194312796209, 0.68, 1.0, 1.0, 0.44427731567016715, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20912190240837336, 0.20912190240837336, 0.2834184093708439], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.0865294], dtype=float32), -0.25635728]. 
=============================================
[2019-04-10 12:47:08,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.8997 ]
 [73.87589]
 [73.83205]
 [73.76104]
 [73.74068]], R is [[73.90519714]
 [73.88265991]
 [73.86070251]
 [73.83956146]
 [73.8190918 ]].
[2019-04-10 12:47:14,179] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7578765e-14 1.0000000e+00 5.0081416e-15 2.1066093e-10 4.9731897e-22], sum to 1.0000
[2019-04-10 12:47:14,188] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5209
[2019-04-10 12:47:14,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1856009.374398193 W.
[2019-04-10 12:47:14,200] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.91666666666667, 68.83333333333333, 1.0, 2.0, 0.6637640543196757, 1.0, 1.0, 0.6637640543196757, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1856009.374398193, 1856009.374398193, 358994.5257756373], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6441000.0000, 
sim time next is 6441600.0000, 
raw observation next is [29.93333333333333, 68.66666666666667, 1.0, 2.0, 0.50643315021063, 1.0, 2.0, 0.50643315021063, 1.0, 1.0, 0.8694708366894838, 6.9112, 6.9112, 170.5573041426782, 2124389.608568877, 2124389.608568877, 417775.6655765723], 
processed observation next is [1.0, 0.5652173913043478, 0.6176935229067929, 0.6866666666666668, 1.0, 1.0, 0.4053411448320843, 1.0, 1.0, 0.4053411448320843, 1.0, 0.5, 0.8408180935237606, 0.0, 0.0, 0.8375144448122397, 0.5901082246024658, 0.5901082246024658, 0.623545769517272], 
reward next is 0.3765, 
noisyNet noise sample is [array([1.4923228], dtype=float32), 0.04466428]. 
=============================================
[2019-04-10 12:47:15,380] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2187143e-16 1.0000000e+00 1.2555859e-18 1.6679115e-18 5.6942143e-26], sum to 1.0000
[2019-04-10 12:47:15,389] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7510
[2019-04-10 12:47:15,389] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2317753.357161385 W.
[2019-04-10 12:47:15,393] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.2, 78.5, 1.0, 2.0, 0.8287247631036709, 1.0, 1.0, 0.8287247631036709, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2317753.357161385, 2317753.357161385, 434090.9459252271], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6424200.0000, 
sim time next is 6424800.0000, 
raw observation next is [28.3, 78.0, 1.0, 2.0, 0.8540453178009665, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.9822060345801, 6.9112, 168.9125341373766, 2090714.61809028, 2040340.619282402, 422483.8113444116], 
processed observation next is [1.0, 0.34782608695652173, 0.5402843601895735, 0.78, 1.0, 1.0, 0.8241509853023693, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0071006034580100245, 0.0, 0.8294378711072424, 0.5807540605806334, 0.5667612831340005, 0.6305728527528531], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03372499], dtype=float32), 0.56525886]. 
=============================================
[2019-04-10 12:47:21,849] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2178678e-15 1.0000000e+00 1.5023794e-17 5.2302713e-18 5.8617182e-25], sum to 1.0000
[2019-04-10 12:47:21,851] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4183
[2019-04-10 12:47:21,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1887492.590500417 W.
[2019-04-10 12:47:21,859] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.06666666666667, 79.66666666666667, 1.0, 2.0, 0.7088353087619657, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.982791261082054, 6.9112, 168.9125357437676, 1887492.590500417, 1836703.412425269, 387412.7788793617], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6511200.0000, 
sim time next is 6511800.0000, 
raw observation next is [28.28333333333333, 77.83333333333333, 1.0, 2.0, 0.4491567054770254, 1.0, 1.0, 0.4491567054770254, 1.0, 2.0, 0.769505507248884, 6.9112, 6.9112, 170.5573041426782, 1883914.691531478, 1883914.691531478, 380072.8846738521], 
processed observation next is [1.0, 0.34782608695652173, 0.5394944707740915, 0.7783333333333333, 1.0, 1.0, 0.3363333800928017, 1.0, 0.5, 0.3363333800928017, 1.0, 1.0, 0.7189091551815657, 0.0, 0.0, 0.8375144448122397, 0.5233096365365216, 0.5233096365365216, 0.5672729621997793], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.71437776], dtype=float32), 1.0113189]. 
=============================================
[2019-04-10 12:47:23,604] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6255409e-21 1.0000000e+00 9.9186953e-24 4.3826419e-26 3.2604217e-33], sum to 1.0000
[2019-04-10 12:47:23,610] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3352
[2019-04-10 12:47:23,615] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 82.33333333333334, 1.0, 2.0, 0.5121991496836313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715724.4750435421, 715724.4750435427, 185529.704757056], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6560400.0000, 
sim time next is 6561000.0000, 
raw observation next is [27.35, 83.0, 1.0, 2.0, 0.514481834648652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718915.2775985553, 718915.2775985553, 185896.3633795477], 
processed observation next is [1.0, 0.9565217391304348, 0.4952606635071091, 0.83, 1.0, 1.0, 0.41503835499837594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1996986882218209, 0.1996986882218209, 0.2774572587754443], 
reward next is 0.7225, 
noisyNet noise sample is [array([-0.36779156], dtype=float32), 1.122003]. 
=============================================
[2019-04-10 12:47:23,629] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.37192 ]
 [70.421745]
 [70.394585]
 [70.45986 ]
 [70.46488 ]], R is [[70.33934021]
 [70.35903931]
 [70.37917328]
 [70.39969635]
 [70.42053986]].
[2019-04-10 12:47:25,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0925239e-20 1.0000000e+00 8.3573868e-23 3.0838106e-25 1.5612332e-31], sum to 1.0000
[2019-04-10 12:47:25,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0795
[2019-04-10 12:47:25,533] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4989342556943306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697182.6152101486, 697182.6152101492, 183429.3100557103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6652800.0000, 
sim time next is 6653400.0000, 
raw observation next is [25.91666666666667, 89.5, 1.0, 2.0, 0.4981941303632086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696148.0668973145, 696148.0668973145, 183313.6742048307], 
processed observation next is [1.0, 0.0, 0.4273301737756717, 0.895, 1.0, 1.0, 0.395414614895432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19337446302703182, 0.19337446302703182, 0.27360249881318016], 
reward next is 0.7264, 
noisyNet noise sample is [array([0.6206229], dtype=float32), 0.4680132]. 
=============================================
[2019-04-10 12:47:25,895] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 12:47:25,896] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:47:25,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:25,897] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:47:25,898] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:25,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:47:25,899] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:47:25,899] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:25,900] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:47:25,901] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:25,902] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:25,916] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run42
[2019-04-10 12:47:25,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run42
[2019-04-10 12:47:25,933] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run42
[2019-04-10 12:47:25,971] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run42
[2019-04-10 12:47:25,988] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run42
[2019-04-10 12:47:29,084] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:29,085] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.33333333333333, 93.33333333333334, 1.0, 2.0, 0.2908920393438755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467476.7471486373, 467476.7471486373, 164783.842805928]
[2019-04-10 12:47:29,087] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:47:29,088] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1587887e-21 1.0000000e+00 4.4709254e-25 9.5182909e-31 2.4451949e-34], sampled 0.6630593421829879
[2019-04-10 12:47:29,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:29,508] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.971735895, 85.8610818, 1.0, 2.0, 0.285948641100063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462355.7049643875, 462355.7049643875, 164427.1732516894]
[2019-04-10 12:47:29,508] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:47:29,510] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7586684e-21 1.0000000e+00 4.3712237e-25 5.6318145e-30 1.9744919e-34], sampled 0.6721905593818551
[2019-04-10 12:47:37,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:37,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.66793029666667, 90.48598405000001, 1.0, 2.0, 0.3696230294850528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562678.1207357752, 562678.1207357752, 171532.9122914012]
[2019-04-10 12:47:37,019] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:47:37,020] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7422071e-21 1.0000000e+00 7.9446670e-25 2.6025952e-29 4.3383947e-34], sampled 0.06898918969644419
[2019-04-10 12:47:42,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:42,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.78146976, 60.66996579, 1.0, 2.0, 0.4660688383161575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656343.8601703227, 656343.8601703233, 179109.0131201583]
[2019-04-10 12:47:42,794] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:47:42,795] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1903730e-21 1.0000000e+00 1.7179525e-24 2.9982898e-29 1.2274178e-33], sampled 0.547341064421997
[2019-04-10 12:47:49,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:49,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.732543285, 71.87701211000001, 1.0, 2.0, 0.4877624339036443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681566.7329062284, 681566.7329062284, 181701.7566558848]
[2019-04-10 12:47:49,039] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:47:49,042] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8015613e-21 1.0000000e+00 1.4348337e-24 1.0616325e-28 8.9965753e-34], sampled 0.07228555161860506
[2019-04-10 12:47:57,636] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:57,639] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.3, 57.0, 1.0, 2.0, 0.8084228491396839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1129874.575931229, 1129874.57593123, 245988.0639053489]
[2019-04-10 12:47:57,639] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:47:57,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4300668e-19 1.0000000e+00 2.1189720e-22 9.7995822e-25 5.2065283e-31], sampled 0.7972295352971231
[2019-04-10 12:47:59,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:47:59,006] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 100.0, 1.0, 2.0, 0.594342783005197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 895015.1927884435, 895015.1927884435, 207700.950212236]
[2019-04-10 12:47:59,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:47:59,011] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.9237186e-20 1.0000000e+00 3.2020994e-23 2.9788097e-27 6.2930507e-32], sampled 0.22004606746242084
[2019-04-10 12:48:08,317] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:48:08,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.18660458, 85.47899703, 1.0, 2.0, 0.5320342129287959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743450.815902797, 743450.815902797, 188769.5208979691]
[2019-04-10 12:48:08,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:48:08,321] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.2089392e-20 1.0000000e+00 5.2332121e-23 1.7093496e-24 9.1381902e-32], sampled 0.375031843591922
[2019-04-10 12:48:22,304] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:48:22,309] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.989959435, 81.74181802666666, 1.0, 2.0, 0.6352394130103818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887727.3535585119, 887727.3535585119, 207495.6405621902]
[2019-04-10 12:48:22,309] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:48:22,311] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.5140331e-23 1.0000000e+00 1.3728083e-26 3.1290917e-31 4.5049867e-36], sampled 0.39005598045847656
[2019-04-10 12:48:26,128] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:48:26,129] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.49772079, 70.1019973, 1.0, 2.0, 0.5924954011800733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827970.6416622172, 827970.6416622172, 199355.7594031617]
[2019-04-10 12:48:26,130] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:48:26,131] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0991416e-20 1.0000000e+00 4.9531914e-24 1.4861597e-28 4.6114923e-33], sampled 0.08402976416308139
[2019-04-10 12:48:30,425] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12741362], dtype=float32), 0.095083885]
[2019-04-10 12:48:30,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 74.83333333333334, 1.0, 2.0, 0.7480839665156215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1045501.736352633, 1045501.736352633, 231567.6029763414]
[2019-04-10 12:48:30,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:48:30,429] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.8140761e-20 1.0000000e+00 5.1484243e-23 1.2583667e-26 9.8251127e-32], sampled 0.0938652479417229
[2019-04-10 12:48:47,309] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 12:48:47,945] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4219 2779328721.6444 933.0000
[2019-04-10 12:48:48,013] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.5164 3164020044.4038 1776.0000
[2019-04-10 12:48:48,042] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1803 3007736924.2222 1766.0000
[2019-04-10 12:48:48,105] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.2657 2842574521.0981 1129.0000
[2019-04-10 12:48:49,119] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1025000, evaluation results [1025000.0, 7884.516446042627, 3164020044.4037514, 1776.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8660.421891960828, 2779328721.6443815, 933.0, 7998.180259296095, 3007736924.2222114, 1766.0, 8498.265716925627, 2842574521.098103, 1129.0]
[2019-04-10 12:48:58,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6281896e-22 1.0000000e+00 1.5644752e-25 1.1883813e-30 1.9501786e-34], sum to 1.0000
[2019-04-10 12:48:58,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7148
[2019-04-10 12:48:58,681] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 76.33333333333333, 1.0, 2.0, 0.3825835284722049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 576844.8355181072, 576844.8355181077, 172611.3858539963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6855000.0000, 
sim time next is 6855600.0000, 
raw observation next is [25.3, 74.66666666666667, 1.0, 2.0, 0.3817480512519722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576521.8400158354, 576521.8400158347, 172610.7202726799], 
processed observation next is [0.0, 0.34782608695652173, 0.39810426540284366, 0.7466666666666667, 1.0, 1.0, 0.25511813403852074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16014495555995426, 0.16014495555995406, 0.2576279407054924], 
reward next is 0.7424, 
noisyNet noise sample is [array([-0.44714576], dtype=float32), 0.08658028]. 
=============================================
[2019-04-10 12:48:59,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3655197e-22 1.0000000e+00 1.2618236e-25 1.1307044e-30 3.3500849e-35], sum to 1.0000
[2019-04-10 12:48:59,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8471
[2019-04-10 12:48:59,308] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.35, 57.5, 1.0, 2.0, 0.4239957460944761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 619755.1127983677, 619755.1127983683, 176041.4733857976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6978600.0000, 
sim time next is 6979200.0000, 
raw observation next is [29.26666666666667, 57.33333333333334, 1.0, 2.0, 0.4195635980727933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 615679.3197189064, 615679.3197189071, 175717.7498376389], 
processed observation next is [0.0, 0.782608695652174, 0.5860979462875199, 0.5733333333333335, 1.0, 1.0, 0.30067903382264255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17102203325525178, 0.17102203325525198, 0.2622652982651327], 
reward next is 0.7377, 
noisyNet noise sample is [array([2.0102959], dtype=float32), 1.7276386]. 
=============================================
[2019-04-10 12:49:06,871] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7673668e-10 9.9987769e-01 7.5796917e-11 1.2231983e-04 3.5548917e-17], sum to 1.0000
[2019-04-10 12:49:06,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7907
[2019-04-10 12:49:06,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1910710.105935365 W.
[2019-04-10 12:49:06,889] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 56.5, 1.0, 2.0, 0.6821940606381801, 1.0, 1.0, 0.6821940606381801, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1910710.105935365, 1910710.105935365, 366971.8363726462], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7036200.0000, 
sim time next is 7036800.0000, 
raw observation next is [30.26666666666667, 55.66666666666667, 1.0, 2.0, 0.6716330769658803, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.94962292707229, 6.9112, 168.912726751074, 1849164.403684939, 1821905.893882479, 381959.9531954252], 
processed observation next is [1.0, 0.43478260869565216, 0.6334913112164299, 0.5566666666666668, 1.0, 1.0, 0.6043772011637112, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0038422927072289783, 0.0, 0.8294388169286755, 0.513656778801372, 0.5060849705229108, 0.5700894823812317], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49161544], dtype=float32), -0.38032216]. 
=============================================
[2019-04-10 12:49:07,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.31850018e-19 1.00000000e+00 3.94471120e-22 3.93102248e-24
 1.25370695e-30], sum to 1.0000
[2019-04-10 12:49:07,691] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-10 12:49:07,693] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 90.0, 1.0, 2.0, 0.4738830270649294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664085.7216083314, 664085.7216083308, 179853.0042522342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7086600.0000, 
sim time next is 7087200.0000, 
raw observation next is [25.06666666666666, 90.33333333333334, 1.0, 2.0, 0.4734810867919897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663170.6182680926, 663170.6182680926, 179747.5908438063], 
processed observation next is [1.0, 0.0, 0.38704581358609763, 0.9033333333333334, 1.0, 1.0, 0.3656398636048069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18421406063002574, 0.18421406063002574, 0.26827998633403927], 
reward next is 0.7317, 
noisyNet noise sample is [array([-0.17731057], dtype=float32), -0.7744626]. 
=============================================
[2019-04-10 12:49:11,835] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7623959e-19 1.0000000e+00 6.6269203e-22 8.4890850e-25 1.9838730e-30], sum to 1.0000
[2019-04-10 12:49:11,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5299
[2019-04-10 12:49:11,852] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333333, 84.66666666666667, 1.0, 2.0, 0.6808634087049914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 951514.0320012324, 951514.0320012324, 216769.8172252924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7198800.0000, 
sim time next is 7199400.0000, 
raw observation next is [28.06666666666667, 84.33333333333333, 1.0, 2.0, 0.7052206546148992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985569.34343799, 985569.34343799, 221977.3092704298], 
processed observation next is [1.0, 0.30434782608695654, 0.529225908372828, 0.8433333333333333, 1.0, 1.0, 0.6448441621866255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27376926206610835, 0.27376926206610835, 0.33130941682153703], 
reward next is 0.6687, 
noisyNet noise sample is [array([0.6702465], dtype=float32), 0.81750584]. 
=============================================
[2019-04-10 12:49:15,583] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.00177437e-17 1.00000000e+00 1.92756767e-20 1.43086349e-19
 1.00442055e-28], sum to 1.0000
[2019-04-10 12:49:15,590] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6717
[2019-04-10 12:49:15,600] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1679081.121289232 W.
[2019-04-10 12:49:15,604] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.3, 96.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.228599121237546, 6.9112, 168.9115459792073, 1679081.121289232, 1453909.140994393, 311346.3904547578], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7219200.0000, 
sim time next is 7219800.0000, 
raw observation next is [24.45, 94.5, 1.0, 2.0, 0.3997792486136503, 1.0, 1.0, 0.3997792486136503, 1.0, 1.0, 0.6728103377099788, 6.911200000000001, 6.9112, 170.5573041426782, 1676646.955145778, 1676646.955145777, 349688.0751977867], 
processed observation next is [1.0, 0.5652173913043478, 0.3578199052132702, 0.945, 1.0, 1.0, 0.2768424682092172, 1.0, 0.5, 0.2768424682092172, 1.0, 0.5, 0.6009882167194863, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.46573526531827164, 0.46573526531827136, 0.521922500295204], 
reward next is 0.4781, 
noisyNet noise sample is [array([-1.2458187], dtype=float32), 0.85579455]. 
=============================================
[2019-04-10 12:49:20,980] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4159263e-19 1.0000000e+00 5.9375240e-22 6.9235726e-24 9.8078547e-31], sum to 1.0000
[2019-04-10 12:49:20,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1474
[2019-04-10 12:49:21,022] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 58.5, 1.0, 2.0, 0.9060661147457603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1382889.644020132, 1382889.644020132, 287994.373548909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7306200.0000, 
sim time next is 7306800.0000, 
raw observation next is [27.83333333333333, 58.0, 1.0, 2.0, 0.8507066443807985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1298762.07436315, 1298762.07436315, 271718.1958380652], 
processed observation next is [1.0, 0.5652173913043478, 0.518167456556082, 0.58, 1.0, 1.0, 0.8201284872057812, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36076724287865275, 0.36076724287865275, 0.40554954602696297], 
reward next is 0.5945, 
noisyNet noise sample is [array([-1.7813121], dtype=float32), -0.43427274]. 
=============================================
[2019-04-10 12:49:37,007] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9955202e-21 1.0000000e+00 7.4585458e-24 1.0308013e-26 2.0708943e-32], sum to 1.0000
[2019-04-10 12:49:37,007] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6449
[2019-04-10 12:49:37,028] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 93.16666666666667, 1.0, 2.0, 0.4653841024394188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655571.3896151873, 655571.3896151873, 179032.4917273292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7603800.0000, 
sim time next is 7604400.0000, 
raw observation next is [24.5, 93.33333333333334, 1.0, 2.0, 0.4637460189102502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654103.4482268982, 654103.4482268982, 178898.9780278839], 
processed observation next is [1.0, 0.0, 0.3601895734597157, 0.9333333333333335, 1.0, 1.0, 0.35391086615692796, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1816954022852495, 0.1816954022852495, 0.2670134000416177], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.21522184], dtype=float32), -0.96984977]. 
=============================================
[2019-04-10 12:49:37,480] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0815227e-21 1.0000000e+00 1.6398883e-24 7.2973456e-30 8.4597573e-34], sum to 1.0000
[2019-04-10 12:49:37,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6606
[2019-04-10 12:49:37,540] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 84.0, 1.0, 2.0, 0.4237843468856708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613192.212126483, 613192.212126483, 175229.2722180709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7549200.0000, 
sim time next is 7549800.0000, 
raw observation next is [25.43333333333333, 83.16666666666667, 1.0, 2.0, 0.4283950986484798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617267.7866510127, 617267.7866510121, 175547.0529056282], 
processed observation next is [0.0, 0.391304347826087, 0.40442338072669815, 0.8316666666666667, 1.0, 1.0, 0.31131939596202385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17146327406972575, 0.17146327406972559, 0.2620105267248182], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.20270424], dtype=float32), 1.3736362]. 
=============================================
[2019-04-10 12:49:39,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9522161e-21 1.0000000e+00 1.3596759e-24 4.9076445e-29 3.3340611e-34], sum to 1.0000
[2019-04-10 12:49:40,011] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2168
[2019-04-10 12:49:40,031] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 74.5, 1.0, 2.0, 0.4594131902218633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645908.7461473537, 645908.7461473542, 177994.9319047655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7583400.0000, 
sim time next is 7584000.0000, 
raw observation next is [27.23333333333333, 75.66666666666667, 1.0, 2.0, 0.4632503953924129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649275.9558403647, 649275.9558403647, 178293.890136891], 
processed observation next is [0.0, 0.782608695652174, 0.4897314375987361, 0.7566666666666667, 1.0, 1.0, 0.35331372938844935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18035443217787908, 0.18035443217787908, 0.26611028378640444], 
reward next is 0.7339, 
noisyNet noise sample is [array([-0.18021294], dtype=float32), 0.115961894]. 
=============================================
[2019-04-10 12:49:40,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.80456]
 [77.82074]
 [77.82666]
 [77.8056 ]
 [77.75705]], R is [[77.73109436]
 [77.68811798]
 [77.64595032]
 [77.60445404]
 [77.56349945]].
[2019-04-10 12:49:40,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6702328e-22 1.0000000e+00 1.8778322e-25 1.7469990e-30 1.1099835e-34], sum to 1.0000
[2019-04-10 12:49:40,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5609
[2019-04-10 12:49:40,313] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 90.0, 1.0, 2.0, 0.3853932460323327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576839.8275490114, 576839.8275490114, 172480.00652421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7540800.0000, 
sim time next is 7541400.0000, 
raw observation next is [23.46666666666667, 90.0, 1.0, 2.0, 0.3865041213515579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577846.516870782, 577846.516870782, 172549.472001667], 
processed observation next is [0.0, 0.2608695652173913, 0.31121642969984215, 0.9, 1.0, 1.0, 0.26084833897778065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.160512921352995, 0.160512921352995, 0.2575365253756224], 
reward next is 0.7425, 
noisyNet noise sample is [array([0.46744102], dtype=float32), 0.6426665]. 
=============================================
[2019-04-10 12:49:42,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.00422537e-20 1.00000000e+00 5.22293840e-24 1.06370345e-29
 5.65186133e-34], sum to 1.0000
[2019-04-10 12:49:42,874] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-10 12:49:42,893] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 90.0, 1.0, 2.0, 0.3762866042624627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566773.4268576526, 566773.4268576526, 171703.0952544681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7537800.0000, 
sim time next is 7538400.0000, 
raw observation next is [23.3, 90.0, 1.0, 2.0, 0.3786747759208431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569430.3873835728, 569430.3873835728, 171906.6431254898], 
processed observation next is [0.0, 0.2608695652173913, 0.3033175355450238, 0.9, 1.0, 1.0, 0.25141539267571456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.158175107606548, 0.158175107606548, 0.2565770792917758], 
reward next is 0.7434, 
noisyNet noise sample is [array([0.2859716], dtype=float32), -1.4894238]. 
=============================================
[2019-04-10 12:49:43,425] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 12:49:43,429] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:49:43,436] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:49:43,439] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:49:43,439] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:49:43,440] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:49:43,441] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:49:43,441] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:49:43,442] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:49:43,442] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:49:43,443] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:49:43,446] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run43
[2019-04-10 12:49:43,543] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run43
[2019-04-10 12:49:43,611] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run43
[2019-04-10 12:49:43,662] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run43
[2019-04-10 12:49:43,716] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run43
[2019-04-10 12:49:51,555] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12537155], dtype=float32), 0.09351024]
[2019-04-10 12:49:51,557] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.75858919, 94.33933063, 1.0, 2.0, 0.2363879149368935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 392463.4967534872, 392463.4967534872, 159346.6966858807]
[2019-04-10 12:49:51,557] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:49:51,558] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3421722e-21 1.0000000e+00 8.0056030e-25 2.1443090e-30 2.2757320e-34], sampled 0.2456243345581105
[2019-04-10 12:49:57,821] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12537155], dtype=float32), 0.09351024]
[2019-04-10 12:49:57,821] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 85.0, 1.0, 2.0, 0.3024544091692027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480719.1836881607, 480719.1836881614, 165658.2429373475]
[2019-04-10 12:49:57,823] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:49:57,824] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2821889e-21 1.0000000e+00 9.3446215e-25 2.0374306e-29 2.1422606e-34], sampled 0.18810088510752598
[2019-04-10 12:50:01,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12537155], dtype=float32), 0.09351024]
[2019-04-10 12:50:01,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.15, 92.0, 1.0, 2.0, 0.2848311774800035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 460462.3160783444, 460462.316078345, 164299.1140237204]
[2019-04-10 12:50:01,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:50:01,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5463822e-20 1.0000000e+00 1.0203378e-23 5.1403653e-28 6.7575614e-33], sampled 0.6728538668792622
[2019-04-10 12:50:17,515] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12537155], dtype=float32), 0.09351024]
[2019-04-10 12:50:17,516] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.67269248, 99.54034758666667, 1.0, 2.0, 0.2964622146781304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 475770.5435901555, 475770.5435901549, 165360.0442119298]
[2019-04-10 12:50:17,517] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:50:17,519] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1480533e-20 1.0000000e+00 1.6758347e-23 9.5161990e-28 1.0997765e-32], sampled 0.362811845914203
[2019-04-10 12:51:08,627] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.4228 3163893873.9158 1773.0000
[2019-04-10 12:51:09,030] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 12:51:09,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.0735 2779388539.9568 933.0000
[2019-04-10 12:51:09,201] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0250 3007848960.6402 1766.0000
[2019-04-10 12:51:09,225] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3952 2842298536.1922 1129.0000
[2019-04-10 12:51:10,242] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1050000, evaluation results [1050000.0, 7885.422752760629, 3163893873.915839, 1773.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8661.07353302349, 2779388539.9567747, 933.0, 7998.024995752096, 3007848960.6401815, 1766.0, 8496.395220969665, 2842298536.1921883, 1129.0]
[2019-04-10 12:51:10,648] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7983294e-20 1.0000000e+00 1.2334326e-22 3.4247622e-25 3.8518448e-31], sum to 1.0000
[2019-04-10 12:51:10,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3812
[2019-04-10 12:51:10,658] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.05, 91.0, 1.0, 2.0, 0.4763874576163864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665667.1187408757, 665667.1187408757, 179978.8588216557], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7691400.0000, 
sim time next is 7692000.0000, 
raw observation next is [25.0, 91.33333333333333, 1.0, 2.0, 0.4773952216361586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667075.7326485447, 667075.7326485447, 180129.786353574], 
processed observation next is [1.0, 0.0, 0.38388625592417064, 0.9133333333333333, 1.0, 1.0, 0.3703556887182634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18529881462459574, 0.18529881462459574, 0.268850427393394], 
reward next is 0.7311, 
noisyNet noise sample is [array([-0.14170218], dtype=float32), -0.24897112]. 
=============================================
[2019-04-10 12:51:10,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.94713 ]
 [70.10324 ]
 [70.6113  ]
 [71.505844]
 [72.87665 ]], R is [[70.00444794]
 [70.03578186]
 [70.06692505]
 [70.09757233]
 [70.12760162]].
[2019-04-10 12:51:16,879] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1092081e-19 1.0000000e+00 2.1954212e-21 1.2787112e-24 5.0617859e-30], sum to 1.0000
[2019-04-10 12:51:16,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3049
[2019-04-10 12:51:16,889] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 86.5, 1.0, 2.0, 0.6148752916851967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859257.6147002022, 859257.6147002022, 203543.1267455017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7716600.0000, 
sim time next is 7717200.0000, 
raw observation next is [27.06666666666667, 86.0, 1.0, 2.0, 0.6720123608028685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 939139.1270139074, 939139.1270139068, 214919.337831165], 
processed observation next is [1.0, 0.30434782608695654, 0.48183254344391807, 0.86, 1.0, 1.0, 0.6048341696420102, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2608719797260854, 0.2608719797260852, 0.320775131091291], 
reward next is 0.6792, 
noisyNet noise sample is [array([1.160684], dtype=float32), 1.00287]. 
=============================================
[2019-04-10 12:51:22,744] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:22,745] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:22,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-04-10 12:51:25,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:25,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:25,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-04-10 12:51:25,983] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:25,983] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-04-10 12:51:26,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:26,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-04-10 12:51:26,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:26,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-04-10 12:51:26,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:26,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-04-10 12:51:26,805] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1060221: loss 0.7483
[2019-04-10 12:51:26,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1060221: learning rate 0.0000
[2019-04-10 12:51:26,910] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:26,910] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-04-10 12:51:26,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:26,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:26,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:26,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-04-10 12:51:26,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-04-10 12:51:27,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,180] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-04-10 12:51:27,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,206] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-04-10 12:51:27,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,282] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-04-10 12:51:27,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run6
[2019-04-10 12:51:27,353] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,353] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,356] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-04-10 12:51:27,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,388] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-04-10 12:51:27,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:51:27,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:27,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-04-10 12:51:28,832] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1060673: loss 0.7043
[2019-04-10 12:51:28,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1060683: learning rate 0.0000
[2019-04-10 12:51:34,068] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1063890: loss 0.5778
[2019-04-10 12:51:34,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1063890: learning rate 0.0000
[2019-04-10 12:51:34,178] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1063956: loss 0.5793
[2019-04-10 12:51:34,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1063957: learning rate 0.0000
[2019-04-10 12:51:34,541] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1064183: loss 0.5845
[2019-04-10 12:51:34,543] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1064183: learning rate 0.0000
[2019-04-10 12:51:35,012] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1064462: loss 0.5613
[2019-04-10 12:51:35,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1064463: learning rate 0.0000
[2019-04-10 12:51:35,033] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1064474: loss 0.5546
[2019-04-10 12:51:35,035] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1064475: learning rate 0.0000
[2019-04-10 12:51:35,327] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1064648: loss 0.5436
[2019-04-10 12:51:35,329] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1064648: learning rate 0.0000
[2019-04-10 12:51:35,382] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1064686: loss 0.5394
[2019-04-10 12:51:35,387] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1064687: learning rate 0.0000
[2019-04-10 12:51:35,613] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1064823: loss 0.5268
[2019-04-10 12:51:35,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1064825: learning rate 0.0000
[2019-04-10 12:51:35,643] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1064840: loss 0.5142
[2019-04-10 12:51:35,644] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1064840: learning rate 0.0000
[2019-04-10 12:51:35,655] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1064847: loss 0.5106
[2019-04-10 12:51:35,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1064847: learning rate 0.0000
[2019-04-10 12:51:35,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7198092e-20 1.0000000e+00 6.6882021e-22 2.1839169e-23 7.9460371e-31], sum to 1.0000
[2019-04-10 12:51:35,738] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9466
[2019-04-10 12:51:35,744] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.37936315630412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570915.4068064233, 570915.4068064233, 172052.029309508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 148800.0000, 
sim time next is 149400.0000, 
raw observation next is [22.5, 96.0, 1.0, 2.0, 0.3757333794794568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565510.5378395348, 565510.5378395341, 171578.450878132], 
processed observation next is [1.0, 0.7391304347826086, 0.2654028436018958, 0.96, 1.0, 1.0, 0.2478715415415142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1570862605109819, 0.1570862605109817, 0.25608724011661493], 
reward next is 0.7439, 
noisyNet noise sample is [array([1.4367685], dtype=float32), 0.89284384]. 
=============================================
[2019-04-10 12:51:35,764] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1064907: loss 0.5050
[2019-04-10 12:51:35,766] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1064908: learning rate 0.0000
[2019-04-10 12:51:35,850] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1064956: loss 0.5006
[2019-04-10 12:51:35,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1064957: learning rate 0.0000
[2019-04-10 12:51:35,930] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1065009: loss 0.4856
[2019-04-10 12:51:35,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1065010: learning rate 0.0000
[2019-04-10 12:51:36,049] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1065082: loss 0.4812
[2019-04-10 12:51:36,053] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1065082: learning rate 0.0000
[2019-04-10 12:51:38,303] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1066453: loss 0.0173
[2019-04-10 12:51:38,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1066454: learning rate 0.0000
[2019-04-10 12:51:41,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6135378e-20 1.0000000e+00 1.3692434e-23 4.6871295e-27 1.2575375e-32], sum to 1.0000
[2019-04-10 12:51:41,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-10 12:51:41,426] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.55, 87.33333333333333, 1.0, 2.0, 0.2453003539403731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 407695.6163674646, 407695.6163674646, 160130.3803430362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 526200.0000, 
sim time next is 526800.0000, 
raw observation next is [18.5, 87.66666666666667, 1.0, 2.0, 0.2270708616949041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377424.1968030478, 377424.1968030478, 158417.41702413], 
processed observation next is [1.0, 0.08695652173913043, 0.07582938388625599, 0.8766666666666667, 1.0, 1.0, 0.06876007433120973, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10484005466751327, 0.10484005466751327, 0.2364439060061642], 
reward next is 0.7636, 
noisyNet noise sample is [array([0.364135], dtype=float32), -0.85616773]. 
=============================================
[2019-04-10 12:51:41,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8046342e-21 1.0000000e+00 1.1255481e-24 1.7979713e-29 3.2265802e-34], sum to 1.0000
[2019-04-10 12:51:41,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7104
[2019-04-10 12:51:41,499] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 77.16666666666667, 1.0, 2.0, 0.3136728975625669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494428.568492173, 494428.5684921724, 166575.4995984131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 313800.0000, 
sim time next is 314400.0000, 
raw observation next is [23.26666666666667, 77.33333333333334, 1.0, 2.0, 0.3122301602290706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492632.268274088, 492632.268274088, 166453.4960338545], 
processed observation next is [0.0, 0.6521739130434783, 0.3017377567140602, 0.7733333333333334, 1.0, 1.0, 0.1713616388302055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1368422967428022, 0.1368422967428022, 0.24843805378187236], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.02370782], dtype=float32), 1.4543891]. 
=============================================
[2019-04-10 12:51:41,856] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3326686e-21 1.0000000e+00 1.0840365e-24 2.0743526e-29 7.2725666e-35], sum to 1.0000
[2019-04-10 12:51:41,863] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4223
[2019-04-10 12:51:41,868] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 80.0, 1.0, 2.0, 0.2965845280916127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473010.2810107688, 473010.2810107688, 165136.1381677796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 322200.0000, 
sim time next is 322800.0000, 
raw observation next is [22.33333333333334, 80.33333333333334, 1.0, 2.0, 0.2961812381097969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472566.5188263085, 472566.5188263085, 165107.7572076377], 
processed observation next is [0.0, 0.7391304347826086, 0.2575039494470777, 0.8033333333333335, 1.0, 1.0, 0.15202558808409264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13126847745175235, 0.13126847745175235, 0.2464294883696085], 
reward next is 0.7536, 
noisyNet noise sample is [array([-0.617321], dtype=float32), -0.3003291]. 
=============================================
[2019-04-10 12:51:41,971] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1068672: loss 0.0217
[2019-04-10 12:51:41,973] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1068674: learning rate 0.0000
[2019-04-10 12:51:42,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4785963e-22 1.0000000e+00 4.6656384e-25 7.6777182e-30 5.9412234e-35], sum to 1.0000
[2019-04-10 12:51:42,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3972
[2019-04-10 12:51:42,467] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 76.16666666666667, 1.0, 2.0, 0.3167429366759382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498449.7570176365, 498449.7570176359, 166856.1608256341], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 305400.0000, 
sim time next is 306000.0000, 
raw observation next is [23.6, 76.0, 1.0, 2.0, 0.3160590379644926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497271.3438763592, 497271.3438763592, 166765.4173147948], 
processed observation next is [0.0, 0.5652173913043478, 0.3175355450236968, 0.76, 1.0, 1.0, 0.17597474453553322, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13813092885454423, 0.13813092885454423, 0.24890360793252952], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.21068768], dtype=float32), -0.67892617]. 
=============================================
[2019-04-10 12:51:42,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.84917]
 [77.83205]
 [77.81371]
 [77.79077]
 [77.76021]], R is [[77.89769745]
 [77.86968231]
 [77.84197998]
 [77.81483459]
 [77.78813171]].
[2019-04-10 12:51:44,507] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3552254e-20 1.0000000e+00 7.6701930e-23 2.2593409e-26 3.1848244e-31], sum to 1.0000
[2019-04-10 12:51:44,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9670
[2019-04-10 12:51:44,519] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 72.83333333333333, 1.0, 2.0, 0.5086240948133629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824134.3303615414, 824134.3303615414, 197067.1723930441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 388200.0000, 
sim time next is 388800.0000, 
raw observation next is [22.6, 73.0, 1.0, 2.0, 0.5011485573053558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 811252.6944798813, 811252.6944798807, 195669.8367907342], 
processed observation next is [1.0, 0.5217391304347826, 0.27014218009478685, 0.73, 1.0, 1.0, 0.39897416542813946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22534797068885593, 0.22534797068885576, 0.2920445325234839], 
reward next is 0.7080, 
noisyNet noise sample is [array([1.1818013], dtype=float32), -0.017671887]. 
=============================================
[2019-04-10 12:51:47,380] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1071869: loss 0.0214
[2019-04-10 12:51:47,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1071869: learning rate 0.0000
[2019-04-10 12:51:47,471] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1071925: loss 0.0194
[2019-04-10 12:51:47,474] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1071925: learning rate 0.0000
[2019-04-10 12:51:47,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1072146: loss 0.0194
[2019-04-10 12:51:47,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1072147: learning rate 0.0000
[2019-04-10 12:51:48,383] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1072460: loss 0.0193
[2019-04-10 12:51:48,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1072460: learning rate 0.0000
[2019-04-10 12:51:48,398] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1072468: loss 0.0176
[2019-04-10 12:51:48,400] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1072469: learning rate 0.0000
[2019-04-10 12:51:48,618] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1072595: loss 0.0185
[2019-04-10 12:51:48,620] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1072595: learning rate 0.0000
[2019-04-10 12:51:48,709] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1072651: loss 0.0197
[2019-04-10 12:51:48,710] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1072651: learning rate 0.0000
[2019-04-10 12:51:49,043] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1072854: loss 0.0196
[2019-04-10 12:51:49,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1072855: learning rate 0.0000
[2019-04-10 12:51:49,072] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1072869: loss 0.0179
[2019-04-10 12:51:49,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1072871: learning rate 0.0000
[2019-04-10 12:51:49,145] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1072911: loss 0.0196
[2019-04-10 12:51:49,149] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1072912: learning rate 0.0000
[2019-04-10 12:51:49,160] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1072923: loss 0.0172
[2019-04-10 12:51:49,164] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1072923: learning rate 0.0000
[2019-04-10 12:51:49,196] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1072940: loss 0.0186
[2019-04-10 12:51:49,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1072940: learning rate 0.0000
[2019-04-10 12:51:49,286] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1072990: loss 0.0175
[2019-04-10 12:51:49,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1072991: learning rate 0.0000
[2019-04-10 12:51:49,372] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1073041: loss 0.0169
[2019-04-10 12:51:49,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1073041: learning rate 0.0000
[2019-04-10 12:51:51,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.096232e-22 1.000000e+00 4.344047e-24 7.855932e-28 2.234714e-34], sum to 1.0000
[2019-04-10 12:51:51,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2778
[2019-04-10 12:51:51,453] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.31666666666667, 84.5, 1.0, 2.0, 0.2362766085890909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 391222.0854305044, 391222.0854305038, 159458.5156744316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 511800.0000, 
sim time next is 512400.0000, 
raw observation next is [19.23333333333333, 85.0, 1.0, 2.0, 0.2357627921104252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390452.6277067414, 390452.6277067414, 159402.8310996984], 
processed observation next is [1.0, 0.9565217391304348, 0.11058451816745649, 0.85, 1.0, 1.0, 0.07923227965111468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10845906325187261, 0.10845906325187261, 0.23791467328313196], 
reward next is 0.7621, 
noisyNet noise sample is [array([0.24645334], dtype=float32), -1.1199574]. 
=============================================
[2019-04-10 12:51:52,410] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1074497: loss 5.0252
[2019-04-10 12:51:52,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1074497: learning rate 0.0000
[2019-04-10 12:51:53,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3569784e-21 1.0000000e+00 2.5246077e-24 2.4018342e-29 1.3081354e-33], sum to 1.0000
[2019-04-10 12:51:53,678] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1004
[2019-04-10 12:51:53,686] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 83.0, 1.0, 2.0, 0.2377729354332007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394197.699249412, 394197.699249412, 159549.0464475696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 541800.0000, 
sim time next is 542400.0000, 
raw observation next is [19.6, 82.0, 1.0, 2.0, 0.2335044230592801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 386850.9195183523, 386850.9195183523, 159178.7506157309], 
processed observation next is [1.0, 0.2608695652173913, 0.127962085308057, 0.82, 1.0, 1.0, 0.07651135308346999, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10745858875509787, 0.10745858875509787, 0.23758022479959837], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.22239304], dtype=float32), -0.98585886]. 
=============================================
[2019-04-10 12:51:53,756] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 12:51:53,757] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:51:53,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:53,758] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:51:53,762] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:53,766] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run44
[2019-04-10 12:51:53,767] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run44
[2019-04-10 12:51:53,805] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:51:53,807] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:51:53,829] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:53,831] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run44
[2019-04-10 12:51:53,874] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:53,874] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:51:53,879] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:51:53,882] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run44
[2019-04-10 12:51:53,901] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run44
[2019-04-10 12:51:56,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12381122], dtype=float32), 0.09214824]
[2019-04-10 12:51:56,308] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.96950897, 74.58479374, 1.0, 2.0, 0.3639212578500496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581117.3137601841, 581117.3137601847, 173553.6789464121]
[2019-04-10 12:51:56,310] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:51:56,311] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.4275198e-21 1.0000000e+00 3.5378073e-24 3.5259552e-29 6.5392893e-34], sampled 0.48500729439641477
[2019-04-10 12:52:09,192] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12381122], dtype=float32), 0.09214824]
[2019-04-10 12:52:09,195] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.4, 62.0, 1.0, 2.0, 0.3426895930882087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532607.0506246571, 532607.0506246571, 169348.2696185964]
[2019-04-10 12:52:09,197] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:52:09,199] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7551880e-21 1.0000000e+00 1.7986618e-24 1.0836243e-29 2.4740558e-34], sampled 0.7802483004909017
[2019-04-10 12:52:29,370] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12381122], dtype=float32), 0.09214824]
[2019-04-10 12:52:29,370] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.10739263833333, 85.03136691166667, 1.0, 2.0, 0.7782881712967258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1127605.401581455, 1127605.401581454, 244134.5914486167]
[2019-04-10 12:52:29,370] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:52:29,374] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.5627095e-19 1.0000000e+00 2.5968639e-21 1.1435963e-23 2.8698295e-30], sampled 0.7969402501153211
[2019-04-10 12:53:13,122] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12381122], dtype=float32), 0.09214824]
[2019-04-10 12:53:13,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.3, 85.33333333333334, 1.0, 2.0, 0.5381466890125759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751995.2549495173, 751995.2549495173, 189787.9723181777]
[2019-04-10 12:53:13,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:53:13,126] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6959794e-20 1.0000000e+00 4.0631796e-23 1.2574502e-26 2.0707047e-32], sampled 0.8062149232811284
[2019-04-10 12:53:24,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.9479 3163599712.1268 1769.0000
[2019-04-10 12:53:24,458] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7737 2927512725.0370 1338.0000
[2019-04-10 12:53:24,515] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0416 2842346408.1498 1130.0000
[2019-04-10 12:53:24,532] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5310 2779331587.3780 933.0000
[2019-04-10 12:53:24,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0250 3007848960.6402 1766.0000
[2019-04-10 12:53:25,647] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1075000, evaluation results [1075000.0, 7883.947937788183, 3163599712.1268196, 1769.0, 8254.773707254759, 2927512725.037018, 1338.0, 8660.53097408367, 2779331587.377975, 933.0, 7998.024995752096, 3007848960.6401815, 1766.0, 8498.041609280115, 2842346408.14978, 1130.0]
[2019-04-10 12:53:28,305] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1076652: loss 5.0411
[2019-04-10 12:53:28,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1076652: learning rate 0.0000
[2019-04-10 12:53:33,449] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1079847: loss 5.2218
[2019-04-10 12:53:33,451] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1079847: learning rate 0.0000
[2019-04-10 12:53:33,624] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1079953: loss 5.1958
[2019-04-10 12:53:33,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1079954: learning rate 0.0000
[2019-04-10 12:53:33,955] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1080155: loss 5.1503
[2019-04-10 12:53:33,957] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1080156: learning rate 0.0000
[2019-04-10 12:53:34,396] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1080429: loss 5.0872
[2019-04-10 12:53:34,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1080429: learning rate 0.0000
[2019-04-10 12:53:34,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1080470: loss 5.0585
[2019-04-10 12:53:34,472] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1080474: learning rate 0.0000
[2019-04-10 12:53:34,686] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1080605: loss 5.0611
[2019-04-10 12:53:34,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1080605: learning rate 0.0000
[2019-04-10 12:53:34,792] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1080670: loss 5.0268
[2019-04-10 12:53:34,795] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1080671: learning rate 0.0000
[2019-04-10 12:53:35,067] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1080841: loss 5.0289
[2019-04-10 12:53:35,069] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1080842: learning rate 0.0000
[2019-04-10 12:53:35,103] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1080865: loss 5.0007
[2019-04-10 12:53:35,105] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1080866: learning rate 0.0000
[2019-04-10 12:53:35,132] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1080880: loss 5.0052
[2019-04-10 12:53:35,134] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1080881: learning rate 0.0000
[2019-04-10 12:53:35,141] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1080886: loss 5.0018
[2019-04-10 12:53:35,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1080886: learning rate 0.0000
[2019-04-10 12:53:35,171] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1080904: loss 5.0146
[2019-04-10 12:53:35,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1080905: learning rate 0.0000
[2019-04-10 12:53:35,381] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1081031: loss 4.9942
[2019-04-10 12:53:35,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1081031: learning rate 0.0000
[2019-04-10 12:53:35,412] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1081052: loss 4.9962
[2019-04-10 12:53:35,414] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1081053: learning rate 0.0000
[2019-04-10 12:53:37,519] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4841999e-21 1.0000000e+00 1.1659954e-24 6.8326653e-30 2.3775705e-34], sum to 1.0000
[2019-04-10 12:53:37,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5738
[2019-04-10 12:53:37,527] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 72.0, 1.0, 2.0, 0.3088691677278808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488660.4118502186, 488660.4118502192, 166190.8060545051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 837600.0000, 
sim time next is 838200.0000, 
raw observation next is [23.93333333333333, 72.5, 1.0, 2.0, 0.3101979174747647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490296.4679412794, 490296.46794128, 166300.871713252], 
processed observation next is [0.0, 0.6956521739130435, 0.3333333333333332, 0.725, 1.0, 1.0, 0.16891315358405387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13619346331702206, 0.13619346331702223, 0.24821025628843585], 
reward next is 0.7518, 
noisyNet noise sample is [array([-0.426472], dtype=float32), 1.4313638]. 
=============================================
[2019-04-10 12:53:37,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9102038e-21 1.0000000e+00 1.2096622e-24 4.7479661e-30 4.5658561e-34], sum to 1.0000
[2019-04-10 12:53:37,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7586
[2019-04-10 12:53:37,716] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 76.5, 1.0, 2.0, 0.287851958334223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461785.5987357184, 461785.5987357184, 164387.0768557938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [22.76666666666667, 75.33333333333334, 1.0, 2.0, 0.287771708005965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461659.387396554, 461659.3873965534, 164378.4425637016], 
processed observation next is [0.0, 0.34782608695652173, 0.2780410742496052, 0.7533333333333334, 1.0, 1.0, 0.1418936241035723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.128238718721265, 0.12823871872126483, 0.2453409590503009], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.14063154], dtype=float32), 0.79608077]. 
=============================================
[2019-04-10 12:53:37,811] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1082523: loss 0.1024
[2019-04-10 12:53:37,813] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1082523: learning rate 0.0000
[2019-04-10 12:53:40,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2501533e-21 1.0000000e+00 6.6704358e-24 1.5470891e-29 1.9683036e-33], sum to 1.0000
[2019-04-10 12:53:40,298] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-10 12:53:40,302] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.292241266409421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 466670.2044195034, 466670.204419504, 164700.6656892739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 892800.0000, 
sim time next is 893400.0000, 
raw observation next is [22.5, 79.00000000000001, 1.0, 2.0, 0.2925385832152782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466982.5633899804, 466982.5633899804, 164720.0440739929], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.7900000000000001, 1.0, 1.0, 0.14763684724732312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.129717378719439, 0.129717378719439, 0.24585081205073567], 
reward next is 0.7541, 
noisyNet noise sample is [array([0.10317682], dtype=float32), -0.09809466]. 
=============================================
[2019-04-10 12:53:41,311] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1084697: loss 0.1317
[2019-04-10 12:53:41,313] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1084698: learning rate 0.0000
[2019-04-10 12:53:41,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2828709e-21 1.0000000e+00 6.0142949e-24 2.3952750e-28 3.5537226e-34], sum to 1.0000
[2019-04-10 12:53:41,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2891
[2019-04-10 12:53:41,586] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 95.0, 1.0, 2.0, 0.3003555778695772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478186.0030671549, 478186.0030671549, 165490.4903227347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1062000.0000, 
sim time next is 1062600.0000, 
raw observation next is [20.65, 95.0, 1.0, 2.0, 0.3155317453343924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501790.6406633075, 501790.6406633075, 167205.703821218], 
processed observation next is [1.0, 0.30434782608695654, 0.1777251184834123, 0.95, 1.0, 1.0, 0.1753394522101113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13938628907314096, 0.13938628907314096, 0.24956075197196714], 
reward next is 0.7504, 
noisyNet noise sample is [array([-1.2954847], dtype=float32), 0.4377704]. 
=============================================
[2019-04-10 12:53:42,649] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6849158e-21 1.0000000e+00 4.5949893e-24 1.0718300e-28 3.8628898e-34], sum to 1.0000
[2019-04-10 12:53:42,657] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3201
[2019-04-10 12:53:42,661] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 72.0, 1.0, 2.0, 0.3149666127718496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495637.203822925, 495637.2038229257, 166645.467085181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 922800.0000, 
sim time next is 923400.0000, 
raw observation next is [24.15, 72.5, 1.0, 2.0, 0.3155477402989085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496242.3324721864, 496242.3324721857, 166682.9085394677], 
processed observation next is [0.0, 0.6956521739130435, 0.34360189573459715, 0.725, 1.0, 1.0, 0.17535872325169696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13784509235338513, 0.13784509235338493, 0.2487804605066682], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.0157348], dtype=float32), -1.8883444]. 
=============================================
[2019-04-10 12:53:42,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2123777e-21 1.0000000e+00 1.3758608e-24 2.1345086e-29 1.3720906e-34], sum to 1.0000
[2019-04-10 12:53:42,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9266
[2019-04-10 12:53:42,698] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 80.66666666666667, 1.0, 2.0, 0.328202584641839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512173.1894221251, 512173.1894221251, 167791.1771287859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 931200.0000, 
sim time next is 931800.0000, 
raw observation next is [23.18333333333334, 81.33333333333334, 1.0, 2.0, 0.3290145769842875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513310.819311581, 513310.8193115816, 167875.7974355611], 
processed observation next is [0.0, 0.782608695652174, 0.29778830963665126, 0.8133333333333335, 1.0, 1.0, 0.19158382769191262, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1425863386976614, 0.14258633869766155, 0.25056089169486734], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.37688452], dtype=float32), 0.6726835]. 
=============================================
[2019-04-10 12:53:46,342] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1087805: loss 0.1491
[2019-04-10 12:53:46,345] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1087806: learning rate 0.0000
[2019-04-10 12:53:46,598] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1087964: loss 0.1384
[2019-04-10 12:53:46,602] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1087967: learning rate 0.0000
[2019-04-10 12:53:46,942] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1088179: loss 0.1211
[2019-04-10 12:53:46,944] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1088179: learning rate 0.0000
[2019-04-10 12:53:47,324] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1088413: loss 0.1138
[2019-04-10 12:53:47,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1088413: learning rate 0.0000
[2019-04-10 12:53:47,353] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1088438: loss 0.1143
[2019-04-10 12:53:47,355] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1088438: learning rate 0.0000
[2019-04-10 12:53:47,595] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1088587: loss 0.1123
[2019-04-10 12:53:47,599] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1088589: learning rate 0.0000
[2019-04-10 12:53:47,782] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1088705: loss 0.1124
[2019-04-10 12:53:47,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1088705: learning rate 0.0000
[2019-04-10 12:53:47,910] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1088786: loss 0.1106
[2019-04-10 12:53:47,914] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1088786: learning rate 0.0000
[2019-04-10 12:53:47,986] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1088831: loss 0.1068
[2019-04-10 12:53:47,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1088832: learning rate 0.0000
[2019-04-10 12:53:48,030] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1088862: loss 0.1114
[2019-04-10 12:53:48,033] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1088863: learning rate 0.0000
[2019-04-10 12:53:48,074] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1088884: loss 0.1094
[2019-04-10 12:53:48,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1088885: learning rate 0.0000
[2019-04-10 12:53:48,082] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1088890: loss 0.1106
[2019-04-10 12:53:48,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1088891: learning rate 0.0000
[2019-04-10 12:53:48,394] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1089083: loss 0.1104
[2019-04-10 12:53:48,395] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1089083: learning rate 0.0000
[2019-04-10 12:53:48,433] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1089106: loss 0.1130
[2019-04-10 12:53:48,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1089107: learning rate 0.0000
[2019-04-10 12:53:50,844] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1090589: loss 0.3601
[2019-04-10 12:53:50,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1090589: learning rate 0.0000
[2019-04-10 12:53:51,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6846153e-20 1.0000000e+00 3.4206006e-23 5.0582227e-27 1.1419824e-32], sum to 1.0000
[2019-04-10 12:53:51,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4601
[2019-04-10 12:53:51,514] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 93.33333333333334, 1.0, 2.0, 0.2942293455128591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474785.2745179156, 474785.2745179156, 165290.5602383536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1135200.0000, 
sim time next is 1135800.0000, 
raw observation next is [20.05, 93.5, 1.0, 2.0, 0.2889763351293487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466575.7557576234, 466575.7557576234, 164720.1972861002], 
processed observation next is [1.0, 0.13043478260869565, 0.14928909952606645, 0.935, 1.0, 1.0, 0.14334498208355267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12960437659933985, 0.12960437659933985, 0.24585104072552272], 
reward next is 0.7541, 
noisyNet noise sample is [array([0.62055], dtype=float32), -1.7569329]. 
=============================================
[2019-04-10 12:53:54,297] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1092742: loss 0.3748
[2019-04-10 12:53:54,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1092742: learning rate 0.0000
[2019-04-10 12:53:58,021] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5101396e-20 1.0000000e+00 3.5119668e-22 2.0479318e-24 4.7328280e-31], sum to 1.0000
[2019-04-10 12:53:58,031] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6447
[2019-04-10 12:53:58,038] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4583264292921354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649845.9318804906, 649845.9318804899, 178539.1992395844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1296000.0000, 
sim time next is 1296600.0000, 
raw observation next is [24.3, 94.00000000000001, 1.0, 2.0, 0.4584754386161387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650208.60854035, 650208.60854035, 178580.432649693], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.9400000000000002, 1.0, 1.0, 0.34756076941703457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18061350237231943, 0.18061350237231943, 0.26653795917864626], 
reward next is 0.7335, 
noisyNet noise sample is [array([2.0901358], dtype=float32), 0.12343062]. 
=============================================
[2019-04-10 12:53:58,846] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1884922e-19 1.0000000e+00 1.4417820e-21 3.5723345e-24 1.7725076e-30], sum to 1.0000
[2019-04-10 12:53:58,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2832
[2019-04-10 12:53:58,862] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.8223431819794218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1221404.104168523, 1221404.104168523, 259426.428015832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1329000.0000, 
sim time next is 1329600.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.7450749485453285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1106612.822111481, 1106612.82211148, 239564.1965244612], 
processed observation next is [1.0, 0.391304347826087, 0.28909952606635075, 0.95, 1.0, 1.0, 0.6928613837895524, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30739245058652254, 0.3073924505865222, 0.3575585022753152], 
reward next is 0.6424, 
noisyNet noise sample is [array([0.69608235], dtype=float32), -0.24276648]. 
=============================================
[2019-04-10 12:53:59,283] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1095753: loss 0.4159
[2019-04-10 12:53:59,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1095754: learning rate 0.0000
[2019-04-10 12:53:59,649] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1095976: loss 0.4120
[2019-04-10 12:53:59,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1095977: learning rate 0.0000
[2019-04-10 12:53:59,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5319341e-19 1.0000000e+00 2.9718043e-21 1.4199206e-23 1.5176337e-30], sum to 1.0000
[2019-04-10 12:53:59,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6058
[2019-04-10 12:53:59,860] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.7423227785716151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1102562.798345055, 1102562.798345055, 238896.4301853346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.8019855926611088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1191201.307154278, 1191201.307154278, 254015.6109170088], 
processed observation next is [1.0, 0.391304347826087, 0.28909952606635075, 0.95, 1.0, 1.0, 0.7614284248929022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33088925198729946, 0.33088925198729946, 0.37912777748807286], 
reward next is 0.6209, 
noisyNet noise sample is [array([-2.6508636], dtype=float32), -1.2780418]. 
=============================================
[2019-04-10 12:54:00,030] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1096206: loss 0.4073
[2019-04-10 12:54:00,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1096206: learning rate 0.0000
[2019-04-10 12:54:00,412] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1096414: loss 0.3997
[2019-04-10 12:54:00,414] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1096415: learning rate 0.0000
[2019-04-10 12:54:00,429] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1096422: loss 0.4014
[2019-04-10 12:54:00,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1096422: learning rate 0.0000
[2019-04-10 12:54:00,721] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1096573: loss 0.3968
[2019-04-10 12:54:00,724] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1096573: learning rate 0.0000
[2019-04-10 12:54:00,852] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1096646: loss 0.3866
[2019-04-10 12:54:00,853] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1096646: learning rate 0.0000
[2019-04-10 12:54:01,101] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1096779: loss 0.3730
[2019-04-10 12:54:01,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1096780: learning rate 0.0000
[2019-04-10 12:54:01,187] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1096832: loss 0.3724
[2019-04-10 12:54:01,188] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1096832: learning rate 0.0000
[2019-04-10 12:54:01,253] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1096872: loss 0.3657
[2019-04-10 12:54:01,255] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1096873: learning rate 0.0000
[2019-04-10 12:54:01,329] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1096916: loss 0.3612
[2019-04-10 12:54:01,330] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1096916: learning rate 0.0000
[2019-04-10 12:54:01,336] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1096919: loss 0.3610
[2019-04-10 12:54:01,338] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1096920: learning rate 0.0000
[2019-04-10 12:54:01,615] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1097091: loss 0.3499
[2019-04-10 12:54:01,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1097091: learning rate 0.0000
[2019-04-10 12:54:01,827] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1097222: loss 0.3364
[2019-04-10 12:54:01,834] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1097224: learning rate 0.0000
[2019-04-10 12:54:03,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4203832e-20 1.0000000e+00 6.7688407e-24 1.9634867e-28 1.2176615e-33], sum to 1.0000
[2019-04-10 12:54:03,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5831
[2019-04-10 12:54:03,618] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4029470154690746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594825.9957638703, 594825.9957638709, 173869.0295096684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1425600.0000, 
sim time next is 1426200.0000, 
raw observation next is [24.33333333333333, 87.33333333333334, 1.0, 2.0, 0.4081313560275022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599762.2408565386, 599762.2408565386, 174243.1794307405], 
processed observation next is [0.0, 0.5217391304347826, 0.35229067930489716, 0.8733333333333334, 1.0, 1.0, 0.2869052482259063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16660062246014962, 0.16660062246014962, 0.260064446911553], 
reward next is 0.7399, 
noisyNet noise sample is [array([1.1462095], dtype=float32), 1.0895371]. 
=============================================
[2019-04-10 12:54:03,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3005096e-21 1.0000000e+00 7.8163999e-24 2.5178412e-28 1.2084068e-33], sum to 1.0000
[2019-04-10 12:54:03,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0226
[2019-04-10 12:54:03,810] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 92.33333333333334, 1.0, 2.0, 0.3834233597718233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577119.6555033482, 577119.6555033482, 172605.8873872636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1455000.0000, 
sim time next is 1455600.0000, 
raw observation next is [22.9, 92.66666666666667, 1.0, 2.0, 0.3832052040680715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576963.7118294627, 576963.711829462, 172597.2331045948], 
processed observation next is [0.0, 0.8695652173913043, 0.2843601895734597, 0.9266666666666667, 1.0, 1.0, 0.25687373984105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1602676977304063, 0.1602676977304061, 0.25760781060387283], 
reward next is 0.7424, 
noisyNet noise sample is [array([-0.35197863], dtype=float32), 1.331648]. 
=============================================
[2019-04-10 12:54:03,845] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1098457: loss 0.0394
[2019-04-10 12:54:03,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1098458: learning rate 0.0000
[2019-04-10 12:54:05,718] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1895440e-20 1.0000000e+00 1.0689540e-23 2.6488003e-28 3.2231743e-33], sum to 1.0000
[2019-04-10 12:54:05,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1698
[2019-04-10 12:54:05,731] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 80.0, 1.0, 2.0, 0.4123818573780608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606327.0677499198, 606327.0677499193, 174866.3882194718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [25.03333333333333, 81.33333333333334, 1.0, 2.0, 0.4090937364745168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602988.650792751, 602988.6507927517, 174598.3464678014], 
processed observation next is [0.0, 0.782608695652174, 0.38546603475513425, 0.8133333333333335, 1.0, 1.0, 0.2880647427403817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16749684744243082, 0.167496847442431, 0.26059454696686774], 
reward next is 0.7394, 
noisyNet noise sample is [array([-0.11660664], dtype=float32), -1.0429351]. 
=============================================
[2019-04-10 12:54:06,356] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-10 12:54:06,357] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:54:06,358] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:06,359] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:54:06,361] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:06,362] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:54:06,362] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:54:06,362] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:06,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:54:06,363] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:06,364] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:06,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run45
[2019-04-10 12:54:06,402] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run45
[2019-04-10 12:54:06,420] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run45
[2019-04-10 12:54:06,438] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run45
[2019-04-10 12:54:06,460] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run45
[2019-04-10 12:54:09,118] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:54:09,119] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.05, 53.5, 1.0, 2.0, 0.2032806651477079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 340029.236551329, 340029.236551329, 155403.2863803199]
[2019-04-10 12:54:09,120] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:54:09,122] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2799614e-21 1.0000000e+00 1.7628704e-24 1.9577641e-29 1.7735322e-34], sampled 0.1978712932888972
[2019-04-10 12:54:33,701] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:54:33,702] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.649493205, 91.60048285833335, 1.0, 2.0, 0.4128149626165394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613635.7059883818, 613635.7059883812, 175740.9114550009]
[2019-04-10 12:54:33,703] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:54:33,704] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.7286745e-21 1.0000000e+00 6.0999009e-24 7.0793387e-29 8.7516372e-34], sampled 0.6349583465530033
[2019-04-10 12:54:36,467] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:54:36,469] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.36666666666667, 93.66666666666666, 1.0, 2.0, 0.5759891990431278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804895.6455888246, 804895.6455888246, 196352.5197220974]
[2019-04-10 12:54:36,483] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:54:36,485] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7817076e-20 1.0000000e+00 5.8182827e-23 4.5323456e-27 1.6435202e-32], sampled 0.2075342706521044
[2019-04-10 12:54:53,933] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:54:53,935] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.1, 55.0, 1.0, 2.0, 0.599600979594517, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.920348233461458, 6.9112, 168.9122376979667, 1676469.751318767, 1669979.707261943, 364641.4234740518]
[2019-04-10 12:54:53,936] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:54:53,938] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.32684391e-14 1.00000000e+00 3.02303772e-15 1.10286034e-13
 1.57438509e-23], sampled 0.24392452297250222
[2019-04-10 12:54:53,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1676469.751318767 W.
[2019-04-10 12:54:55,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:54:55,496] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.03430976, 69.1913658, 1.0, 2.0, 0.4638279605958357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659480.206521971, 659480.206521971, 179584.6013146403]
[2019-04-10 12:54:55,496] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:54:55,498] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.59885724e-20 1.00000000e+00 3.32692684e-23 1.09454404e-26
 5.65167031e-33], sampled 0.38514742116017087
[2019-04-10 12:55:02,401] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:55:02,402] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.12811531333333, 58.12547072333334, 1.0, 2.0, 0.5348921373837359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747445.8105133576, 747445.8105133576, 189243.0147052568]
[2019-04-10 12:55:02,403] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:55:02,405] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.77722994e-23 1.00000000e+00 8.06660319e-26 1.46730464e-29
 1.19673005e-36], sampled 0.0802463600677531
[2019-04-10 12:55:17,682] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:55:17,682] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.33333333333334, 94.66666666666666, 1.0, 2.0, 0.5172667794632713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722808.166365731, 722808.166365731, 186344.7638328526]
[2019-04-10 12:55:17,684] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:55:17,685] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0374317e-20 1.0000000e+00 2.1758851e-22 1.5881849e-25 5.7113826e-32], sampled 0.42864024416744395
[2019-04-10 12:55:19,313] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12415413], dtype=float32), 0.09256566]
[2019-04-10 12:55:19,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.43333333333333, 85.83333333333334, 1.0, 2.0, 0.5437872533677361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759880.0843348445, 759880.0843348445, 190740.3693579303]
[2019-04-10 12:55:19,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:55:19,318] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4372249e-20 1.0000000e+00 3.0164479e-23 1.2770817e-26 5.4440394e-33], sampled 0.030892523438473707
[2019-04-10 12:55:38,378] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3261 2927382873.9314 1338.0000
[2019-04-10 12:55:38,455] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.2244 3163897172.3880 1770.0000
[2019-04-10 12:55:38,479] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-04-10 12:55:38,579] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4112 3007670497.2687 1766.0000
[2019-04-10 12:55:38,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.7282 2842551279.1229 1128.0000
[2019-04-10 12:55:39,701] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1100000, evaluation results [1100000.0, 7885.224354110269, 3163897172.3880196, 1770.0, 8254.326098134321, 2927382873.931414, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7997.411155407058, 3007670497.2687254, 1766.0, 8498.728224321014, 2842551279.122919, 1128.0]
[2019-04-10 12:55:40,437] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5751318e-21 1.0000000e+00 3.8845428e-24 2.1182532e-29 4.9885496e-34], sum to 1.0000
[2019-04-10 12:55:40,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6013
[2019-04-10 12:55:40,454] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 91.66666666666667, 1.0, 2.0, 0.3455451185530622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532615.9710408411, 532615.9710408417, 169222.6397857861], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1492800.0000, 
sim time next is 1493400.0000, 
raw observation next is [22.5, 90.83333333333333, 1.0, 2.0, 0.3493363480418044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536977.74493707, 536977.74493707, 169533.0162121524], 
processed observation next is [0.0, 0.2608695652173913, 0.2654028436018958, 0.9083333333333333, 1.0, 1.0, 0.21606788920699327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14916048470474166, 0.14916048470474166, 0.25303435255545137], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.4637927], dtype=float32), 0.6382231]. 
=============================================
[2019-04-10 12:55:40,605] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1100564: loss 0.0392
[2019-04-10 12:55:40,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1100565: learning rate 0.0000
[2019-04-10 12:55:45,899] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1103825: loss 0.0503
[2019-04-10 12:55:45,901] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1103825: learning rate 0.0000
[2019-04-10 12:55:46,037] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1103910: loss 0.0466
[2019-04-10 12:55:46,039] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1103910: learning rate 0.0000
[2019-04-10 12:55:46,143] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.0108805e-20 1.0000000e+00 1.9203804e-22 2.2645105e-25 6.2982382e-32], sum to 1.0000
[2019-04-10 12:55:46,149] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8109
[2019-04-10 12:55:46,154] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 96.0, 1.0, 2.0, 0.4087115546040583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 602338.6898745953, 602338.6898745946, 174535.2351880883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1620000.0000, 
sim time next is 1620600.0000, 
raw observation next is [23.1, 95.83333333333333, 1.0, 2.0, 0.4117678222217412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607040.0174464448, 607040.0174464448, 174980.6030411718], 
processed observation next is [1.0, 0.782608695652174, 0.2938388625592418, 0.9583333333333333, 1.0, 1.0, 0.29128653279727856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1686222270684569, 0.1686222270684569, 0.26116507916592807], 
reward next is 0.7388, 
noisyNet noise sample is [array([-2.526419], dtype=float32), 0.47525364]. 
=============================================
[2019-04-10 12:55:46,528] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1104209: loss 0.0405
[2019-04-10 12:55:46,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1104209: learning rate 0.0000
[2019-04-10 12:55:46,853] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1104415: loss 0.0356
[2019-04-10 12:55:46,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1104415: learning rate 0.0000
[2019-04-10 12:55:46,866] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1104420: loss 0.0347
[2019-04-10 12:55:46,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1104421: learning rate 0.0000
[2019-04-10 12:55:47,012] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1104513: loss 0.0332
[2019-04-10 12:55:47,015] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1104513: learning rate 0.0000
[2019-04-10 12:55:47,355] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1104727: loss 0.0348
[2019-04-10 12:55:47,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1104727: learning rate 0.0000
[2019-04-10 12:55:47,435] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1104774: loss 0.0369
[2019-04-10 12:55:47,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1104774: learning rate 0.0000
[2019-04-10 12:55:47,524] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1104828: loss 0.0336
[2019-04-10 12:55:47,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1104828: learning rate 0.0000
[2019-04-10 12:55:47,576] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1104859: loss 0.0365
[2019-04-10 12:55:47,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1104859: learning rate 0.0000
[2019-04-10 12:55:47,628] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1104894: loss 0.0337
[2019-04-10 12:55:47,632] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1104894: learning rate 0.0000
[2019-04-10 12:55:47,673] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1104917: loss 0.0372
[2019-04-10 12:55:47,674] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1104917: learning rate 0.0000
[2019-04-10 12:55:47,891] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1105050: loss 0.0361
[2019-04-10 12:55:47,892] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1105050: learning rate 0.0000
[2019-04-10 12:55:48,044] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1105144: loss 0.0385
[2019-04-10 12:55:48,045] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1105144: learning rate 0.0000
[2019-04-10 12:55:48,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3601844e-19 1.0000000e+00 2.9871481e-21 2.5004084e-23 1.8778345e-30], sum to 1.0000
[2019-04-10 12:55:48,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8958
[2019-04-10 12:55:48,232] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 92.5, 1.0, 2.0, 0.844772704658518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1182885.458558733, 1182885.458558734, 255509.3238640414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [24.93333333333333, 92.0, 1.0, 2.0, 0.8680501965999639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1213259.050937398, 1213259.050937398, 261296.2513159847], 
processed observation next is [1.0, 0.391304347826087, 0.38072669826224315, 0.92, 1.0, 1.0, 0.8410243332529684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33701640303816616, 0.33701640303816616, 0.3899944049492309], 
reward next is 0.6100, 
noisyNet noise sample is [array([0.30156583], dtype=float32), 0.74377555]. 
=============================================
[2019-04-10 12:55:50,387] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1106590: loss -21.0166
[2019-04-10 12:55:50,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1106590: learning rate 0.0000
[2019-04-10 12:55:53,857] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1108732: loss -132.8715
[2019-04-10 12:55:53,859] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1108735: learning rate 0.0000
[2019-04-10 12:55:53,953] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7549237e-20 1.0000000e+00 3.4886913e-23 1.1312109e-27 1.6793916e-32], sum to 1.0000
[2019-04-10 12:55:53,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6692
[2019-04-10 12:55:53,964] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.48333333333333, 82.66666666666667, 1.0, 2.0, 0.51679997508885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722155.6506434007, 722155.6506434013, 186270.3905373753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2037000.0000, 
sim time next is 2037600.0000, 
raw observation next is [27.6, 82.0, 1.0, 2.0, 0.5167236208268372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722048.9199841464, 722048.9199841458, 186258.0923810731], 
processed observation next is [0.0, 0.6086956521739131, 0.5071090047393366, 0.82, 1.0, 1.0, 0.41773930220100863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20056914444004065, 0.2005691444400405, 0.2779971528075718], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.23527254], dtype=float32), 0.60148865]. 
=============================================
[2019-04-10 12:55:55,965] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2002032e-18 1.0000000e+00 5.7815860e-20 5.9617665e-21 2.6725810e-28], sum to 1.0000
[2019-04-10 12:55:55,972] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4002
[2019-04-10 12:55:55,976] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.18333333333334, 88.33333333333334, 1.0, 2.0, 0.880329486664621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1242475.176727639, 1242475.176727639, 266352.1982722688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1853400.0000, 
sim time next is 1854000.0000, 
raw observation next is [25.3, 88.0, 1.0, 2.0, 0.8802901067739083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238590.070452928, 1238590.070452928, 265779.7171240391], 
processed observation next is [1.0, 0.4782608695652174, 0.39810426540284366, 0.88, 1.0, 1.0, 0.8557712129806124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34405279734803557, 0.34405279734803557, 0.3966861449612524], 
reward next is 0.6033, 
noisyNet noise sample is [array([1.8197193], dtype=float32), -1.8934805]. 
=============================================
[2019-04-10 12:55:55,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.9845  ]
 [63.94661 ]
 [63.884735]
 [64.20805 ]
 [64.487335]], R is [[64.0303421 ]
 [63.99249649]
 [63.95215988]
 [63.88760757]
 [63.83491898]].
[2019-04-10 12:55:56,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6656424e-20 1.0000000e+00 4.1887220e-23 9.9553688e-28 1.9259463e-32], sum to 1.0000
[2019-04-10 12:55:56,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7087
[2019-04-10 12:55:56,397] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 94.5, 1.0, 2.0, 0.4971613187518579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694704.4029410825, 694704.4029410825, 183152.7598269352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2014200.0000, 
sim time next is 2014800.0000, 
raw observation next is [25.4, 94.33333333333334, 1.0, 2.0, 0.500182295465546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698927.1293813366, 698927.1293813366, 183624.9002663266], 
processed observation next is [0.0, 0.30434782608695654, 0.4028436018957346, 0.9433333333333335, 1.0, 1.0, 0.39780999453680244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19414642482814906, 0.19414642482814906, 0.2740670153228755], 
reward next is 0.7259, 
noisyNet noise sample is [array([-1.104628], dtype=float32), -0.09230706]. 
=============================================
[2019-04-10 12:55:56,904] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.3219710e-21 1.0000000e+00 2.6155307e-24 5.3818038e-26 9.0389555e-34], sum to 1.0000
[2019-04-10 12:55:56,910] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5473
[2019-04-10 12:55:56,913] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.98333333333333, 87.0, 1.0, 2.0, 0.4892254776866197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683611.7506314571, 683611.7506314571, 181925.0582542246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1882200.0000, 
sim time next is 1882800.0000, 
raw observation next is [25.9, 87.0, 1.0, 2.0, 0.4856036854836487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678549.2782577785, 678549.2782577785, 181371.1205841529], 
processed observation next is [1.0, 0.8260869565217391, 0.42654028436018954, 0.87, 1.0, 1.0, 0.3802454041971671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1884859106271607, 0.1884859106271607, 0.27070316505097447], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.33825505], dtype=float32), 0.99270475]. 
=============================================
[2019-04-10 12:55:58,118] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0195894e-18 1.0000000e+00 2.2488317e-20 1.4715259e-21 6.1937708e-29], sum to 1.0000
[2019-04-10 12:55:58,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1487
[2019-04-10 12:55:58,127] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 81.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.914983226132993, 6.9112, 168.9125319223211, 1486490.029086788, 1483806.085005962, 316152.0432385501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1934400.0000, 
sim time next is 1935000.0000, 
raw observation next is [25.95, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.051251240683174, 6.9112, 168.9120686360958, 1583186.047197754, 1483829.399126598, 316145.1216261043], 
processed observation next is [1.0, 0.391304347826087, 0.42890995260663506, 0.81, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.014005124068317443, 0.0, 0.8294355852828462, 0.4397739019993761, 0.4121748330907216, 0.4718583904867228], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9983262], dtype=float32), 0.53966093]. 
=============================================
[2019-04-10 12:55:58,141] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[63.960587]
 [64.46557 ]
 [65.36827 ]
 [65.978516]
 [66.84934 ]], R is [[62.76805878]
 [62.64959335]
 [62.06165695]
 [61.99978638]
 [61.9450531 ]].
[2019-04-10 12:55:58,897] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1111836: loss -117.3242
[2019-04-10 12:55:58,900] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1111837: learning rate 0.0000
[2019-04-10 12:55:58,993] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5362262e-19 1.0000000e+00 4.2047816e-22 1.1734024e-24 8.7831894e-31], sum to 1.0000
[2019-04-10 12:55:59,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0676
[2019-04-10 12:55:59,006] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 95.0, 1.0, 2.0, 0.4967422336171156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711921.751088679, 711921.7510886797, 185356.2330132807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1911600.0000, 
sim time next is 1912200.0000, 
raw observation next is [23.86666666666667, 95.0, 1.0, 2.0, 0.4686180503722527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672891.2471478183, 672891.2471478183, 181135.9230217072], 
processed observation next is [1.0, 0.13043478260869565, 0.33017377567140627, 0.95, 1.0, 1.0, 0.3597807835810273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1869142353188384, 0.1869142353188384, 0.2703521239129958], 
reward next is 0.7296, 
noisyNet noise sample is [array([0.42290792], dtype=float32), -2.4666545]. 
=============================================
[2019-04-10 12:55:59,165] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1111996: loss -132.8218
[2019-04-10 12:55:59,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1111999: learning rate 0.0000
[2019-04-10 12:55:59,643] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1112292: loss -98.9349
[2019-04-10 12:55:59,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1112293: learning rate 0.0000
[2019-04-10 12:55:59,789] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1112382: loss -101.7597
[2019-04-10 12:55:59,792] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1112384: learning rate 0.0000
[2019-04-10 12:55:59,826] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1112402: loss 47.3989
[2019-04-10 12:55:59,829] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1112403: learning rate 0.0000
[2019-04-10 12:56:00,107] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1112576: loss 15.1035
[2019-04-10 12:56:00,109] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1112577: learning rate 0.0000
[2019-04-10 12:56:00,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0446303e-15 1.0000000e+00 6.8610489e-17 1.5277124e-14 1.7297290e-25], sum to 1.0000
[2019-04-10 12:56:00,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9334
[2019-04-10 12:56:00,209] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333333, 75.33333333333333, 1.0, 2.0, 0.8720877600705063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1228875.47231064, 1228875.472310641, 263832.5898606332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1947000.0000, 
sim time next is 1947600.0000, 
raw observation next is [27.2, 75.0, 1.0, 2.0, 0.8617269414637516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1213619.04219943, 1213619.042199431, 260964.5514383021], 
processed observation next is [1.0, 0.5652173913043478, 0.4881516587677725, 0.75, 1.0, 1.0, 0.8334059535707851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33711640061095277, 0.33711640061095305, 0.3894993305049285], 
reward next is 0.6105, 
noisyNet noise sample is [array([1.0563005], dtype=float32), 0.9190933]. 
=============================================
[2019-04-10 12:56:00,391] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1112751: loss -12.8009
[2019-04-10 12:56:00,393] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1112751: learning rate 0.0000
[2019-04-10 12:56:00,478] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1112807: loss -76.0282
[2019-04-10 12:56:00,479] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1112809: learning rate 0.0000
[2019-04-10 12:56:00,496] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1112818: loss -97.8969
[2019-04-10 12:56:00,500] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1112820: learning rate 0.0000
[2019-04-10 12:56:00,584] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1112874: loss -140.8297
[2019-04-10 12:56:00,588] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1112874: learning rate 0.0000
[2019-04-10 12:56:00,648] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1112910: loss -16.4651
[2019-04-10 12:56:00,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1112911: learning rate 0.0000
[2019-04-10 12:56:00,711] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1112949: loss 58.7188
[2019-04-10 12:56:00,713] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1112951: learning rate 0.0000
[2019-04-10 12:56:00,898] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1113065: loss -24.1790
[2019-04-10 12:56:00,899] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1113065: learning rate 0.0000
[2019-04-10 12:56:01,211] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1113253: loss -107.9604
[2019-04-10 12:56:01,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1113253: learning rate 0.0000
[2019-04-10 12:56:02,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.1661823e-20 1.0000000e+00 2.4549488e-22 3.4062348e-25 1.5300837e-32], sum to 1.0000
[2019-04-10 12:56:02,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0436
[2019-04-10 12:56:02,521] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.5, 1.0, 2.0, 0.4698067786751012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658883.5471320132, 658883.5471320138, 179312.6999368303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1992600.0000, 
sim time next is 1993200.0000, 
raw observation next is [24.56666666666667, 94.33333333333334, 1.0, 2.0, 0.4714612128933924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659843.0262199509, 659843.0262199515, 179382.6308894302], 
processed observation next is [0.0, 0.043478260869565216, 0.3633491311216432, 0.9433333333333335, 1.0, 1.0, 0.3632062805944486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1832897295055419, 0.18328972950554207, 0.2677352699842242], 
reward next is 0.7323, 
noisyNet noise sample is [array([0.47948503], dtype=float32), -0.5747854]. 
=============================================
[2019-04-10 12:56:03,260] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1114519: loss 4.8716
[2019-04-10 12:56:03,264] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1114520: learning rate 0.0000
[2019-04-10 12:56:04,671] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0098294e-20 1.0000000e+00 6.6894490e-23 1.2338112e-26 4.0324158e-33], sum to 1.0000
[2019-04-10 12:56:04,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9314
[2019-04-10 12:56:04,686] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 94.0, 1.0, 2.0, 0.4687473294656851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657740.7315000216, 657740.7315000216, 179200.0333773325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2072400.0000, 
sim time next is 2073000.0000, 
raw observation next is [24.51666666666667, 94.0, 1.0, 2.0, 0.4682872902829175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657590.738664544, 657590.738664544, 179195.8874800015], 
processed observation next is [0.0, 1.0, 0.36097946287519767, 0.94, 1.0, 1.0, 0.3593822774492981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18266409407348444, 0.18266409407348444, 0.2674565484776142], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.6968748], dtype=float32), 0.074766316]. 
=============================================
[2019-04-10 12:56:04,700] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.404884]
 [76.40281 ]
 [76.40093 ]
 [76.394005]
 [76.3665  ]], R is [[76.3709259 ]
 [76.3397522 ]
 [76.3088913 ]
 [76.27828979]
 [76.24785614]].
[2019-04-10 12:56:05,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.9422206e-20 1.0000000e+00 3.2429622e-22 1.0024732e-26 4.4290658e-32], sum to 1.0000
[2019-04-10 12:56:05,084] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2517
[2019-04-10 12:56:05,089] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 94.0, 1.0, 2.0, 0.472007039567803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660407.1712579873, 660407.1712579873, 179437.8043038484], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2070000.0000, 
sim time next is 2070600.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4709198399032556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659292.5582719826, 659292.5582719819, 179329.0786444363], 
processed observation next is [0.0, 1.0, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.362554023979826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18313682174221738, 0.18313682174221718, 0.2676553412603527], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.07385069], dtype=float32), 1.0573776]. 
=============================================
[2019-04-10 12:56:06,733] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1116681: loss 4.6081
[2019-04-10 12:56:06,734] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1116681: learning rate 0.0000
[2019-04-10 12:56:11,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8068817e-22 1.0000000e+00 8.7947834e-25 2.8869175e-28 1.8671307e-34], sum to 1.0000
[2019-04-10 12:56:11,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8490
[2019-04-10 12:56:11,628] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.81666666666666, 81.16666666666667, 1.0, 2.0, 0.5550304311469174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775596.8668842987, 775596.8668842987, 192667.5603281976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2232600.0000, 
sim time next is 2233200.0000, 
raw observation next is [28.73333333333333, 81.33333333333334, 1.0, 2.0, 0.5542899613924646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774561.7607272197, 774561.7607272203, 192539.4517554183], 
processed observation next is [1.0, 0.8695652173913043, 0.560821484992101, 0.8133333333333335, 1.0, 1.0, 0.46299995348489703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21515604464644994, 0.21515604464645008, 0.2873723160528631], 
reward next is 0.7126, 
noisyNet noise sample is [array([1.7052425], dtype=float32), -0.901707]. 
=============================================
[2019-04-10 12:56:11,739] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1119783: loss 4.2993
[2019-04-10 12:56:11,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1119784: learning rate 0.0000
[2019-04-10 12:56:12,116] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1120015: loss 4.2982
[2019-04-10 12:56:12,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1120016: learning rate 0.0000
[2019-04-10 12:56:12,501] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1120253: loss 4.2966
[2019-04-10 12:56:12,504] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1120254: learning rate 0.0000
[2019-04-10 12:56:12,528] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1120266: loss 4.3415
[2019-04-10 12:56:12,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1120267: learning rate 0.0000
[2019-04-10 12:56:12,591] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1120307: loss 4.3073
[2019-04-10 12:56:12,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1120307: learning rate 0.0000
[2019-04-10 12:56:13,125] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1120631: loss 4.2544
[2019-04-10 12:56:13,128] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1120633: learning rate 0.0000
[2019-04-10 12:56:13,141] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1120641: loss 4.2717
[2019-04-10 12:56:13,146] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1120645: learning rate 0.0000
[2019-04-10 12:56:13,294] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1120736: loss 4.2718
[2019-04-10 12:56:13,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1120737: learning rate 0.0000
[2019-04-10 12:56:13,317] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1120745: loss 4.2525
[2019-04-10 12:56:13,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1120745: learning rate 0.0000
[2019-04-10 12:56:13,452] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1120830: loss 4.2370
[2019-04-10 12:56:13,455] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1120832: learning rate 0.0000
[2019-04-10 12:56:13,473] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1120846: loss 4.2780
[2019-04-10 12:56:13,475] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1120846: learning rate 0.0000
[2019-04-10 12:56:13,631] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1120944: loss 4.2311
[2019-04-10 12:56:13,634] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1120945: learning rate 0.0000
[2019-04-10 12:56:13,770] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1121028: loss 4.2064
[2019-04-10 12:56:13,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1121028: learning rate 0.0000
[2019-04-10 12:56:13,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4796708e-17 1.0000000e+00 1.4564656e-19 3.5043943e-21 7.8615078e-28], sum to 1.0000
[2019-04-10 12:56:13,990] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0770
[2019-04-10 12:56:13,996] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1951691.701059716 W.
[2019-04-10 12:56:13,999] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 72.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.612612558762084, 6.9112, 168.9094685242679, 1951691.701059716, 1454095.766489364, 311349.2136883793], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2276400.0000, 
sim time next is 2277000.0000, 
raw observation next is [29.2, 72.0, 1.0, 2.0, 0.7701609500371259, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.977583935344518, 6.9112, 168.9119908363926, 1973313.344982142, 1926218.565732034, 401598.9140287075], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.72, 1.0, 1.0, 0.7230854819724408, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006638393534451836, 0.0, 0.8294352032506866, 0.5481425958283728, 0.5350607127033428, 0.5994013642219514], 
reward next is 0.0687, 
noisyNet noise sample is [array([-0.4873968], dtype=float32), -0.6029789]. 
=============================================
[2019-04-10 12:56:14,003] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[61.50283 ]
 [63.8496  ]
 [64.23948 ]
 [64.073265]
 [64.24727 ]], R is [[56.51985168]
 [55.95465469]
 [56.04309845]
 [56.16300201]
 [56.26808167]].
[2019-04-10 12:56:14,090] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1121220: loss 4.2457
[2019-04-10 12:56:14,094] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1121220: learning rate 0.0000
[2019-04-10 12:56:16,324] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.9793118e-20 1.0000000e+00 2.0510858e-21 3.9303496e-23 1.8305878e-30], sum to 1.0000
[2019-04-10 12:56:16,329] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2123
[2019-04-10 12:56:16,333] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 80.66666666666667, 1.0, 2.0, 0.553764987147905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773827.8973694084, 773827.8973694084, 192448.6338663476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2328000.0000, 
sim time next is 2328600.0000, 
raw observation next is [28.68333333333333, 80.83333333333333, 1.0, 2.0, 0.5521788851898377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771610.6828985944, 771610.682898595, 192175.1877840316], 
processed observation next is [1.0, 0.9565217391304348, 0.5584518167456555, 0.8083333333333332, 1.0, 1.0, 0.4604564881805273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2143363008051651, 0.21433630080516528, 0.2868286384836292], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.73300475], dtype=float32), -0.02732765]. 
=============================================
[2019-04-10 12:56:16,449] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1122687: loss -288.5578
[2019-04-10 12:56:16,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1122687: learning rate 0.0000
[2019-04-10 12:56:16,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7706738e-19 1.0000000e+00 2.0045061e-20 1.8917042e-22 1.1821649e-29], sum to 1.0000
[2019-04-10 12:56:16,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6066
[2019-04-10 12:56:16,801] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 77.0, 1.0, 2.0, 0.7166497822587589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001549.461716232, 1001549.461716232, 224478.8678559963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2356200.0000, 
sim time next is 2356800.0000, 
raw observation next is [28.43333333333333, 76.33333333333334, 1.0, 2.0, 0.7052609130727916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985625.6320818511, 985625.6320818511, 221983.717802521], 
processed observation next is [1.0, 0.2608695652173913, 0.546603475513428, 0.7633333333333334, 1.0, 1.0, 0.644892666352761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2737848978005142, 0.2737848978005142, 0.33131898179480745], 
reward next is 0.6687, 
noisyNet noise sample is [array([-0.4907531], dtype=float32), -2.1730933]. 
=============================================
[2019-04-10 12:56:19,852] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1124767: loss -319.3311
[2019-04-10 12:56:19,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1124769: learning rate 0.0000
[2019-04-10 12:56:20,230] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-10 12:56:20,232] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:56:20,233] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:56:20,234] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:56:20,234] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:56:20,234] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:56:20,234] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:56:20,235] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:56:20,235] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:56:20,237] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:56:20,238] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:56:20,249] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run46
[2019-04-10 12:56:20,265] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run46
[2019-04-10 12:56:20,286] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run46
[2019-04-10 12:56:20,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run46
[2019-04-10 12:56:20,326] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run46
[2019-04-10 12:56:22,032] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:56:22,033] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.16666666666666, 89.83333333333333, 1.0, 2.0, 0.527988703186827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797045.443890673, 797045.443890673, 195362.7523189425]
[2019-04-10 12:56:22,034] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:56:22,036] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9766698e-19 1.0000000e+00 4.5908065e-22 1.2190700e-25 4.6792241e-31], sampled 0.2828444148553708
[2019-04-10 12:56:36,826] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:56:36,828] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.6, 75.5, 1.0, 2.0, 0.328702060633624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517935.096880372, 517935.0968803726, 168356.3720372412]
[2019-04-10 12:56:36,829] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:56:36,831] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9710919e-20 1.0000000e+00 4.4951800e-23 2.6396126e-27 1.3722280e-32], sampled 0.8125911013763277
[2019-04-10 12:56:37,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:56:37,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.53538628, 82.37294406, 1.0, 2.0, 0.366426346008447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 563484.1505335885, 563484.1505335878, 171762.5070480094]
[2019-04-10 12:56:37,424] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:56:37,426] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7421457e-20 1.0000000e+00 4.5967973e-23 3.7751902e-27 1.2785498e-32], sampled 0.9021963929892237
[2019-04-10 12:56:39,079] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:56:39,081] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.8, 92.0, 1.0, 2.0, 0.416919062054924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610014.4036465245, 610014.4036465245, 175127.327638525]
[2019-04-10 12:56:39,083] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:56:39,085] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4372889e-20 1.0000000e+00 9.5474213e-23 2.4659237e-27 4.8083291e-32], sampled 0.21619954410404363
[2019-04-10 12:56:55,903] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:56:55,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.41031823666667, 93.79216336666667, 1.0, 2.0, 0.3754264219616407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571287.2535821192, 571287.2535821199, 172275.5628412253]
[2019-04-10 12:56:55,905] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:56:55,907] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7969902e-20 1.0000000e+00 1.7177837e-22 6.3471194e-27 1.1573544e-31], sampled 0.16831628842280644
[2019-04-10 12:57:45,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:57:45,281] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 82.0, 1.0, 2.0, 1.008629854918774, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993822823207, 6.9112, 168.9123159368661, 2307081.140311682, 2239831.394883459, 465958.77647885]
[2019-04-10 12:57:45,282] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:57:45,284] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.1025480e-09 9.9635124e-01 3.9294239e-08 3.6487682e-03 5.6496475e-15], sampled 0.7808716711332283
[2019-04-10 12:57:45,285] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2307081.140311682 W.
[2019-04-10 12:57:45,335] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12739304], dtype=float32), 0.09599433]
[2019-04-10 12:57:45,337] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.12962351833333, 85.783176075, 1.0, 2.0, 0.6256924523179733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 874380.2806113655, 874380.2806113648, 205625.6053001141]
[2019-04-10 12:57:45,338] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:57:45,340] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.8753210e-19 1.0000000e+00 3.0083151e-21 3.3202181e-24 3.8418533e-30], sampled 0.6606208712294656
[2019-04-10 12:57:51,108] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.5154 2927257292.1386 1333.0000
[2019-04-10 12:57:51,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7898.4137 3162657762.8232 1743.0000
[2019-04-10 12:57:51,763] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.9232 3007555147.5013 1761.0000
[2019-04-10 12:57:51,832] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.6432 2842070885.0091 1124.0000
[2019-04-10 12:57:51,918] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2447 2779140419.8678 932.0000
[2019-04-10 12:57:52,929] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1125000, evaluation results [1125000.0, 7898.413693316357, 3162657762.823218, 1743.0, 8256.515379015505, 2927257292.1386466, 1333.0, 8661.244670985332, 2779140419.867784, 932.0, 8000.923182534597, 3007555147.501338, 1761.0, 8499.643235045476, 2842070885.0090866, 1124.0]
[2019-04-10 12:57:57,416] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1127784: loss -457.1583
[2019-04-10 12:57:57,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1127786: learning rate 0.0000
[2019-04-10 12:57:58,110] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1128212: loss -203.6722
[2019-04-10 12:57:58,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1128212: learning rate 0.0000
[2019-04-10 12:57:58,282] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1128320: loss -406.4478
[2019-04-10 12:57:58,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1128320: learning rate 0.0000
[2019-04-10 12:57:58,387] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1128385: loss -349.2927
[2019-04-10 12:57:58,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1128386: learning rate 0.0000
[2019-04-10 12:57:58,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4047962e-20 1.0000000e+00 2.4144372e-22 7.7814053e-26 4.8108818e-31], sum to 1.0000
[2019-04-10 12:57:58,457] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2425
[2019-04-10 12:57:58,458] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1128427: loss -278.2132
[2019-04-10 12:57:58,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1128428: learning rate 0.0000
[2019-04-10 12:57:58,461] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.358306491563518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555958.1000476147, 555958.1000476147, 171247.8310646035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2779800.0000, 
sim time next is 2780400.0000, 
raw observation next is [21.33333333333334, 98.0, 1.0, 2.0, 0.3415665411043997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529250.0020647112, 529250.0020647112, 169033.531024559], 
processed observation next is [1.0, 0.17391304347826086, 0.2101105845181678, 0.98, 1.0, 1.0, 0.20670667602939724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14701388946241978, 0.14701388946241978, 0.25228885227546116], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.14295582], dtype=float32), -0.12178042]. 
=============================================
[2019-04-10 12:57:58,751] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1128606: loss -306.6558
[2019-04-10 12:57:58,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1128608: learning rate 0.0000
[2019-04-10 12:57:58,845] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1128665: loss -479.9438
[2019-04-10 12:57:58,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1128667: learning rate 0.0000
[2019-04-10 12:57:58,886] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1128687: loss -556.6059
[2019-04-10 12:57:58,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1128687: learning rate 0.0000
[2019-04-10 12:57:59,022] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1128773: loss -316.3345
[2019-04-10 12:57:59,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1128773: learning rate 0.0000
[2019-04-10 12:57:59,218] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1128889: loss -280.0982
[2019-04-10 12:57:59,221] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1128889: learning rate 0.0000
[2019-04-10 12:57:59,294] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1128944: loss -237.6219
[2019-04-10 12:57:59,296] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1128946: learning rate 0.0000
[2019-04-10 12:57:59,339] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1128964: loss -455.2258
[2019-04-10 12:57:59,340] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1128964: learning rate 0.0000
[2019-04-10 12:57:59,432] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1129027: loss -463.0275
[2019-04-10 12:57:59,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1129027: learning rate 0.0000
[2019-04-10 12:57:59,503] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1889650e-22 1.0000000e+00 1.6069865e-24 1.4379869e-28 4.9873695e-34], sum to 1.0000
[2019-04-10 12:57:59,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7278
[2019-04-10 12:57:59,511] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.98333333333333, 87.33333333333333, 1.0, 2.0, 0.5316337481787876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742891.0211590385, 742891.0211590378, 188700.5452140872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [26.86666666666667, 87.66666666666667, 1.0, 2.0, 0.5300133249570241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740625.8949625496, 740625.8949625501, 188431.7795252927], 
processed observation next is [1.0, 0.9130434782608695, 0.4723538704581361, 0.8766666666666667, 1.0, 1.0, 0.4337509939241254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20572941526737487, 0.20572941526737504, 0.2812414619780488], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.17543836], dtype=float32), -0.41363764]. 
=============================================
[2019-04-10 12:57:59,764] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1129226: loss -381.6986
[2019-04-10 12:57:59,765] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1129226: learning rate 0.0000
[2019-04-10 12:58:01,561] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1130351: loss 0.0903
[2019-04-10 12:58:01,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1130352: learning rate 0.0000
[2019-04-10 12:58:02,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.38118057e-21 1.00000000e+00 1.03336254e-22 3.04674067e-27
 5.06728431e-33], sum to 1.0000
[2019-04-10 12:58:02,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5088
[2019-04-10 12:58:02,744] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5055479665789161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706427.3150664754, 706427.3150664754, 184470.1805756645], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2640000.0000, 
sim time next is 2640600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5058496578323471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706849.0235181857, 706849.0235181851, 184517.9407287994], 
processed observation next is [0.0, 0.5652173913043478, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4046381419666833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1963469509772738, 0.19634695097727364, 0.2753999115355215], 
reward next is 0.7246, 
noisyNet noise sample is [array([1.1418856], dtype=float32), 1.3317822]. 
=============================================
[2019-04-10 12:58:03,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.7369183e-21 1.0000000e+00 3.1102892e-23 8.4188339e-28 3.1904390e-33], sum to 1.0000
[2019-04-10 12:58:03,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7290
[2019-04-10 12:58:03,671] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3961630719637438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591133.5342768184, 591133.5342768177, 173721.6891207611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2668200.0000, 
sim time next is 2668800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3959674096291794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590841.7069712145, 590841.7069712151, 173694.8769551762], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27224989111949327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1641226963808929, 0.16412269638089308, 0.25924608500772567], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.01774757], dtype=float32), -0.061002117]. 
=============================================
[2019-04-10 12:58:05,136] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1132561: loss 0.0606
[2019-04-10 12:58:05,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1132563: learning rate 0.0000
[2019-04-10 12:58:09,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8489001e-20 1.0000000e+00 2.5475479e-22 1.3633576e-25 7.3103218e-32], sum to 1.0000
[2019-04-10 12:58:09,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5340
[2019-04-10 12:58:09,966] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.5540920708112779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861238.8819793045, 861238.8819793045, 202808.7056951553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2799000.0000, 
sim time next is 2799600.0000, 
raw observation next is [22.0, 90.0, 1.0, 2.0, 0.5349092040805988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834201.3221805632, 834201.3221805632, 199447.1048913579], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9, 1.0, 1.0, 0.439649643470601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2317225894946009, 0.2317225894946009, 0.29768224610650434], 
reward next is 0.7023, 
noisyNet noise sample is [array([1.2244233], dtype=float32), -0.44631433]. 
=============================================
[2019-04-10 12:58:10,230] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1135702: loss 0.0323
[2019-04-10 12:58:10,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1135702: learning rate 0.0000
[2019-04-10 12:58:10,921] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1136124: loss 0.0297
[2019-04-10 12:58:10,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1136125: learning rate 0.0000
[2019-04-10 12:58:11,249] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1136333: loss 0.0240
[2019-04-10 12:58:11,252] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1136335: learning rate 0.0000
[2019-04-10 12:58:11,291] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1136358: loss 0.0225
[2019-04-10 12:58:11,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1136358: learning rate 0.0000
[2019-04-10 12:58:11,348] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1136392: loss 0.0212
[2019-04-10 12:58:11,349] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1136392: learning rate 0.0000
[2019-04-10 12:58:11,771] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1136655: loss 0.0151
[2019-04-10 12:58:11,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1136657: learning rate 0.0000
[2019-04-10 12:58:11,809] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1136681: loss 0.0153
[2019-04-10 12:58:11,812] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1136682: learning rate 0.0000
[2019-04-10 12:58:11,916] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1136747: loss 0.0140
[2019-04-10 12:58:11,920] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1136748: learning rate 0.0000
[2019-04-10 12:58:12,037] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1136822: loss 0.0132
[2019-04-10 12:58:12,038] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1136822: learning rate 0.0000
[2019-04-10 12:58:12,142] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1136883: loss 0.0133
[2019-04-10 12:58:12,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1136883: learning rate 0.0000
[2019-04-10 12:58:12,248] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1136950: loss 0.0117
[2019-04-10 12:58:12,249] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1136950: learning rate 0.0000
[2019-04-10 12:58:12,354] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1137014: loss 0.0116
[2019-04-10 12:58:12,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1137017: learning rate 0.0000
[2019-04-10 12:58:12,361] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1137018: loss 0.0113
[2019-04-10 12:58:12,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1137020: learning rate 0.0000
[2019-04-10 12:58:12,729] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1137249: loss 0.0137
[2019-04-10 12:58:12,730] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1137250: learning rate 0.0000
[2019-04-10 12:58:14,597] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1138412: loss 5.2590
[2019-04-10 12:58:14,600] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1138414: learning rate 0.0000
[2019-04-10 12:58:15,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9847335e-20 1.0000000e+00 5.5392596e-22 3.1477779e-25 1.4703065e-31], sum to 1.0000
[2019-04-10 12:58:15,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0596
[2019-04-10 12:58:15,358] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.00000000000001, 1.0, 2.0, 0.6197742906219627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 973746.0532342028, 973746.0532342022, 217192.9058727489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2974200.0000, 
sim time next is 2974800.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.6031809713445242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 947686.2622197444, 947686.2622197444, 213650.3070876373], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.88, 1.0, 1.0, 0.5219047847524387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.263246183949929, 0.263246183949929, 0.31888105535468253], 
reward next is 0.6811, 
noisyNet noise sample is [array([-0.06376126], dtype=float32), -0.2402183]. 
=============================================
[2019-04-10 12:58:15,706] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.6814756e-22 1.0000000e+00 3.1467829e-24 5.6800200e-28 1.4629248e-34], sum to 1.0000
[2019-04-10 12:58:15,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8833
[2019-04-10 12:58:15,718] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.08333333333334, 99.5, 1.0, 2.0, 0.3084631333891708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490992.1554524192, 490992.1554524192, 166413.9369656206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2923800.0000, 
sim time next is 2924400.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.308800462644727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491248.2889387395, 491248.2889387395, 166428.6719962212], 
processed observation next is [1.0, 0.8695652173913043, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16722947306593614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13645785803853874, 0.13645785803853874, 0.24840100297943463], 
reward next is 0.7516, 
noisyNet noise sample is [array([-1.0114584], dtype=float32), 0.72987765]. 
=============================================
[2019-04-10 12:58:17,258] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2744894e-20 1.0000000e+00 1.1302569e-21 1.6385993e-25 5.9506707e-31], sum to 1.0000
[2019-04-10 12:58:17,266] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5954
[2019-04-10 12:58:17,270] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.6031809713445242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 947686.2622197444, 947686.2622197444, 213650.3070876373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2974800.0000, 
sim time next is 2975400.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.6098847685019685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958222.5644618089, 958222.5644618089, 215071.2350403916], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.88, 1.0, 1.0, 0.5299816487975524, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2661729345727247, 0.2661729345727247, 0.32100184334386805], 
reward next is 0.6790, 
noisyNet noise sample is [array([-0.23743317], dtype=float32), 0.49766013]. 
=============================================
[2019-04-10 12:58:18,001] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1140520: loss 5.2602
[2019-04-10 12:58:18,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1140520: learning rate 0.0000
[2019-04-10 12:58:19,397] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.6417004e-22 1.0000000e+00 2.4790376e-24 1.5583803e-27 4.8657825e-34], sum to 1.0000
[2019-04-10 12:58:19,407] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7783
[2019-04-10 12:58:19,411] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3030254175235287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482550.5332816411, 482550.5332816406, 165804.8657590378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3022800.0000, 
sim time next is 3023400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3028285612138949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482237.5283235412, 482237.5283235412, 165782.3666037986], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1600344111010782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13395486897876144, 0.13395486897876144, 0.24743636806537106], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.60271204], dtype=float32), 1.917785]. 
=============================================
[2019-04-10 12:58:20,560] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5753483e-21 1.0000000e+00 1.3784514e-23 4.3343077e-27 1.1287677e-33], sum to 1.0000
[2019-04-10 12:58:20,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1301
[2019-04-10 12:58:20,569] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.4005791927686967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598270.7071811978, 598270.7071811978, 174393.3379000188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [22.66666666666667, 96.0, 1.0, 2.0, 0.3975217528705691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594620.1196329258, 594620.1196329253, 174085.4609062469], 
processed observation next is [1.0, 0.8695652173913043, 0.27330173775671435, 0.96, 1.0, 1.0, 0.2741225938199628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16517225545359052, 0.16517225545359035, 0.2598290461287267], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.03800054], dtype=float32), -0.14105146]. 
=============================================
[2019-04-10 12:58:23,124] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1143688: loss 5.3104
[2019-04-10 12:58:23,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1143688: learning rate 0.0000
[2019-04-10 12:58:23,760] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1144083: loss 5.3342
[2019-04-10 12:58:23,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1144084: learning rate 0.0000
[2019-04-10 12:58:24,112] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1144307: loss 5.3351
[2019-04-10 12:58:24,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1144307: learning rate 0.0000
[2019-04-10 12:58:24,222] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1144375: loss 5.3487
[2019-04-10 12:58:24,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1144376: learning rate 0.0000
[2019-04-10 12:58:24,334] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1144445: loss 5.3511
[2019-04-10 12:58:24,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1144445: learning rate 0.0000
[2019-04-10 12:58:24,619] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1144621: loss 5.3773
[2019-04-10 12:58:24,621] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1144621: learning rate 0.0000
[2019-04-10 12:58:24,659] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1144643: loss 5.3916
[2019-04-10 12:58:24,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1144644: learning rate 0.0000
[2019-04-10 12:58:24,778] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1144714: loss 5.3968
[2019-04-10 12:58:24,782] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1144716: learning rate 0.0000
[2019-04-10 12:58:24,917] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1144800: loss 5.4054
[2019-04-10 12:58:24,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1144801: learning rate 0.0000
[2019-04-10 12:58:24,986] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1144844: loss 5.4219
[2019-04-10 12:58:24,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1144845: learning rate 0.0000
[2019-04-10 12:58:25,205] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1144980: loss 5.4290
[2019-04-10 12:58:25,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1144980: learning rate 0.0000
[2019-04-10 12:58:25,221] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1144988: loss 5.4298
[2019-04-10 12:58:25,222] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1144989: learning rate 0.0000
[2019-04-10 12:58:25,254] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1145010: loss 5.4347
[2019-04-10 12:58:25,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1145012: learning rate 0.0000
[2019-04-10 12:58:25,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1145236: loss 5.4570
[2019-04-10 12:58:25,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1145236: learning rate 0.0000
[2019-04-10 12:58:27,690] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1146513: loss 2.4586
[2019-04-10 12:58:27,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1146513: learning rate 0.0000
[2019-04-10 12:58:27,721] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4579768e-20 1.0000000e+00 2.2417876e-23 2.6402458e-28 6.4193460e-33], sum to 1.0000
[2019-04-10 12:58:27,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7189
[2019-04-10 12:58:27,732] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4563138439760834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646089.1724248632, 646089.1724248632, 178129.0389977989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3217800.0000, 
sim time next is 3218400.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4559155698054902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645525.3728558918, 645525.3728558918, 178071.0172771911], 
processed observation next is [0.0, 0.2608695652173913, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3444765901270966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17931260357108106, 0.17931260357108106, 0.2657776377271509], 
reward next is 0.7342, 
noisyNet noise sample is [array([-0.6068365], dtype=float32), 0.63672847]. 
=============================================
[2019-04-10 12:58:29,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1613019e-19 1.0000000e+00 7.8309376e-23 2.1770585e-27 5.1027651e-32], sum to 1.0000
[2019-04-10 12:58:29,771] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3608
[2019-04-10 12:58:29,774] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4875325137219749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681245.3547433444, 681245.3547433438, 181665.7820411038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3312000.0000, 
sim time next is 3312600.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.4878208114317547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681648.3319470783, 681648.331947079, 181710.0035530568], 
processed observation next is [0.0, 0.34782608695652173, 0.5339652448657191, 0.7333333333333334, 1.0, 1.0, 0.3829166402792225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18934675887418842, 0.18934675887418861, 0.2712089605269504], 
reward next is 0.7288, 
noisyNet noise sample is [array([2.5288377], dtype=float32), 1.9278808]. 
=============================================
[2019-04-10 12:58:30,960] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0680729e-20 1.0000000e+00 2.0173521e-23 2.2437263e-26 1.7145017e-32], sum to 1.0000
[2019-04-10 12:58:30,967] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7226
[2019-04-10 12:58:30,970] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.4883599958353328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682401.9943390093, 682401.9943390087, 181791.9972873562], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3280200.0000, 
sim time next is 3280800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4875924157084424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681329.0846189507, 681329.0846189507, 181674.5114991432], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3826414647089668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18925807906081965, 0.18925807906081965, 0.271155987312154], 
reward next is 0.7288, 
noisyNet noise sample is [array([-0.7749887], dtype=float32), -0.48273206]. 
=============================================
[2019-04-10 12:58:31,068] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1148610: loss 2.3260
[2019-04-10 12:58:31,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1148611: learning rate 0.0000
[2019-04-10 12:58:32,326] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.32204943e-20 1.00000000e+00 2.96545972e-23 1.26824105e-26
 2.77128818e-33], sum to 1.0000
[2019-04-10 12:58:32,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5135
[2019-04-10 12:58:32,336] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5855459607091553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818255.5467687334, 818255.546768734, 198084.2774686287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [30.0, 79.00000000000001, 1.0, 2.0, 0.5863806676398069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819422.435591174, 819422.435591174, 198236.2585279407], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7900000000000001, 1.0, 1.0, 0.5016634549877191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22761734321977056, 0.22761734321977056, 0.2958750127282697], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.86192507], dtype=float32), 0.87618023]. 
=============================================
[2019-04-10 12:58:32,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.337975]
 [74.26552 ]
 [74.24073 ]
 [74.210594]
 [74.175125]], R is [[74.33197784]
 [74.29301453]
 [74.25399017]
 [74.21496582]
 [74.1759491 ]].
[2019-04-10 12:58:33,363] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 12:58:33,366] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:58:33,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:58:33,367] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:58:33,368] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:58:33,368] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:58:33,369] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:58:33,369] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:58:33,371] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:58:33,368] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:58:33,373] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:58:33,389] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run47
[2019-04-10 12:58:33,390] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run47
[2019-04-10 12:58:33,406] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run47
[2019-04-10 12:58:33,451] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run47
[2019-04-10 12:58:33,473] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run47
[2019-04-10 12:58:37,408] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12575968], dtype=float32), 0.09422641]
[2019-04-10 12:58:37,408] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.60068664, 96.75031231, 1.0, 2.0, 0.2648183831894912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434253.1215824907, 434253.1215824907, 162406.1062030213]
[2019-04-10 12:58:37,409] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:58:37,411] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4228354e-20 1.0000000e+00 3.2205237e-23 2.0732224e-28 7.1465612e-33], sampled 0.3665726641869006
[2019-04-10 12:58:53,183] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12575968], dtype=float32), 0.09422641]
[2019-04-10 12:58:53,185] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.16666666666667, 89.66666666666666, 1.0, 2.0, 0.3702866739218554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 559562.3090378508, 559562.3090378502, 171134.9229773978]
[2019-04-10 12:58:53,185] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:58:53,186] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.3454472e-20 1.0000000e+00 1.5429101e-22 9.1661785e-27 5.4125050e-32], sampled 0.9757415187280551
[2019-04-10 12:58:58,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12575968], dtype=float32), 0.09422641]
[2019-04-10 12:58:58,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.54562172666667, 70.58925841666667, 1.0, 2.0, 0.4802448848361309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681768.8604103838, 681768.8604103832, 181932.2690236445]
[2019-04-10 12:58:58,273] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:58:58,274] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8029728e-20 1.0000000e+00 5.5945978e-23 2.2470840e-26 8.5452261e-33], sampled 0.15637201337506113
[2019-04-10 12:59:06,311] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12575968], dtype=float32), 0.09422641]
[2019-04-10 12:59:06,313] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.05533000333333, 94.84929449333333, 1.0, 2.0, 0.5809944477079925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839744.0514383388, 839744.0514383388, 200814.6349333061]
[2019-04-10 12:59:06,313] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:59:06,319] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.07009454e-19 1.00000000e+00 3.28396913e-22 4.51037447e-26
 1.46809817e-31], sampled 0.49279553364128004
[2019-04-10 12:59:50,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12575968], dtype=float32), 0.09422641]
[2019-04-10 12:59:50,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.45480931666667, 91.73756012666666, 1.0, 2.0, 0.5416644682575147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756912.6796382264, 756912.6796382271, 190380.6758394387]
[2019-04-10 12:59:50,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:59:50,254] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.4157789e-20 1.0000000e+00 1.5772172e-22 3.7950149e-27 6.3686891e-32], sampled 0.6447168898677202
[2019-04-10 13:00:23,175] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7891.2621 3163220555.7148 1754.0000
[2019-04-10 13:00:23,188] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1100 2927511515.8715 1338.0000
[2019-04-10 13:00:23,244] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1464 3007705609.5609 1763.0000
[2019-04-10 13:00:23,388] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6768 2842053900.0570 1125.0000
[2019-04-10 13:00:23,417] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2478 2779251156.6056 932.0000
[2019-04-10 13:00:24,429] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1150000, evaluation results [1150000.0, 7891.262083134806, 3163220555.714819, 1754.0, 8255.109997184438, 2927511515.8714986, 1338.0, 8659.247837176603, 2779251156.6056333, 932.0, 7999.146378160992, 3007705609.560879, 1763.0, 8498.676801750402, 2842053900.057007, 1125.0]
[2019-04-10 13:00:27,069] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1151637: loss 2.0033
[2019-04-10 13:00:27,074] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1151639: learning rate 0.0000
[2019-04-10 13:00:27,761] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1152067: loss 1.9993
[2019-04-10 13:00:27,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1152067: learning rate 0.0000
[2019-04-10 13:00:28,156] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1152305: loss 1.9696
[2019-04-10 13:00:28,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1152308: learning rate 0.0000
[2019-04-10 13:00:28,229] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1152355: loss 1.9604
[2019-04-10 13:00:28,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1152355: learning rate 0.0000
[2019-04-10 13:00:28,485] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1152514: loss 1.9390
[2019-04-10 13:00:28,487] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1152514: learning rate 0.0000
[2019-04-10 13:00:28,636] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1152605: loss 1.9797
[2019-04-10 13:00:28,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1152606: learning rate 0.0000
[2019-04-10 13:00:28,694] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1152639: loss 1.9455
[2019-04-10 13:00:28,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1152639: learning rate 0.0000
[2019-04-10 13:00:28,839] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1152729: loss 1.9560
[2019-04-10 13:00:28,842] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1152730: learning rate 0.0000
[2019-04-10 13:00:28,868] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1152746: loss 1.9287
[2019-04-10 13:00:28,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1152746: learning rate 0.0000
[2019-04-10 13:00:28,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2438735e-17 1.0000000e+00 5.0893053e-20 1.6980633e-21 7.4085060e-29], sum to 1.0000
[2019-04-10 13:00:28,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8898
[2019-04-10 13:00:28,996] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.7126459838344051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995951.3542207689, 995951.3542207689, 223595.1156155694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3467400.0000, 
sim time next is 3468000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6680389578119544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933583.8447510903, 933583.8447510909, 214094.3565840539], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6000469371228366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25932884576419174, 0.2593288457641919, 0.31954381579709534], 
reward next is 0.6805, 
noisyNet noise sample is [array([-2.748744], dtype=float32), -0.22117591]. 
=============================================
[2019-04-10 13:00:29,005] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.898438]
 [62.86822 ]
 [62.895798]
 [62.960262]
 [63.16341 ]], R is [[63.05512238]
 [63.09084702]
 [63.12248993]
 [63.14883804]
 [63.16679764]].
[2019-04-10 13:00:29,054] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1152859: loss 1.8997
[2019-04-10 13:00:29,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1152861: learning rate 0.0000
[2019-04-10 13:00:29,162] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1152926: loss 1.9303
[2019-04-10 13:00:29,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1152927: learning rate 0.0000
[2019-04-10 13:00:29,201] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1152950: loss 1.9249
[2019-04-10 13:00:29,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1152950: learning rate 0.0000
[2019-04-10 13:00:29,215] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1152958: loss 1.9132
[2019-04-10 13:00:29,216] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1152958: learning rate 0.0000
[2019-04-10 13:00:29,472] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1153114: loss 1.9270
[2019-04-10 13:00:29,474] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1153116: learning rate 0.0000
[2019-04-10 13:00:29,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.64074823e-09 9.85738933e-01 3.87186816e-08 1.42610455e-02
 9.14523093e-15], sum to 1.0000
[2019-04-10 13:00:29,716] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3925
[2019-04-10 13:00:29,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2110051.117380813 W.
[2019-04-10 13:00:29,721] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5030183611296488, 1.0, 2.0, 0.5030183611296488, 1.0, 1.0, 0.8590872173899181, 6.911199999999999, 6.9112, 170.5573041426782, 2110051.117380813, 2110051.117380814, 414580.1381768031], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3491400.0000, 
sim time next is 3492000.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.9165556106452262, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.977057068461649, 6.9112, 168.9125642953068, 2178210.026685609, 2131488.864180128, 439693.9766862732], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.66, 1.0, 1.0, 0.8994645911388267, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0065857068461649074, 0.0, 0.829438019196486, 0.6050583407460025, 0.5920802400500356, 0.6562596666959302], 
reward next is 0.0145, 
noisyNet noise sample is [array([0.14100228], dtype=float32), -0.5947033]. 
=============================================
[2019-04-10 13:00:29,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[44.64943 ]
 [46.076096]
 [46.311172]
 [46.720608]
 [47.01297 ]], R is [[44.76925659]
 [44.32156372]
 [43.8783493 ]
 [43.43956757]
 [43.4252243 ]].
[2019-04-10 13:00:30,279] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0519953e-19 1.0000000e+00 1.0916993e-20 4.7707502e-23 1.5637978e-29], sum to 1.0000
[2019-04-10 13:00:30,284] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3854
[2019-04-10 13:00:30,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6215309164514117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868562.3220550605, 868562.3220550605, 204816.1581560592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3475200.0000, 
sim time next is 3475800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6402951639551456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 894795.5847353708, 894795.5847353701, 208482.4780421715], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5666206794640308, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24855432909315855, 0.24855432909315836, 0.31116787767488285], 
reward next is 0.6888, 
noisyNet noise sample is [array([0.8078242], dtype=float32), 2.0141568]. 
=============================================
[2019-04-10 13:00:31,940] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1154642: loss 0.2343
[2019-04-10 13:00:31,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1154644: learning rate 0.0000
[2019-04-10 13:00:35,305] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1156673: loss -38.1452
[2019-04-10 13:00:35,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1156673: learning rate 0.0000
[2019-04-10 13:00:36,984] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7952605e-18 1.0000000e+00 6.9103638e-21 5.0819060e-23 5.3146946e-29], sum to 1.0000
[2019-04-10 13:00:36,994] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6338
[2019-04-10 13:00:36,998] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.7167963663609491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001754.416041167, 1001754.416041167, 224509.6117563216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3654600.0000, 
sim time next is 3655200.0000, 
raw observation next is [28.33333333333334, 72.66666666666667, 1.0, 2.0, 0.8337723696138105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1165323.222125652, 1165323.222125652, 252366.6801738021], 
processed observation next is [1.0, 0.30434782608695654, 0.5418641390205374, 0.7266666666666667, 1.0, 1.0, 0.7997257465226633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32370089503490335, 0.32370089503490335, 0.3766666868265703], 
reward next is 0.6233, 
noisyNet noise sample is [array([0.4324557], dtype=float32), 1.3890812]. 
=============================================
[2019-04-10 13:00:37,437] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1322171e-18 1.0000000e+00 1.7649464e-20 4.2737492e-23 5.1200294e-29], sum to 1.0000
[2019-04-10 13:00:37,443] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4653
[2019-04-10 13:00:37,448] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6450945540044076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901505.4521294803, 901505.4521294803, 209436.7986959239], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3644400.0000, 
sim time next is 3645000.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.6408547386061891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 895577.9052364004, 895577.9052364004, 208593.1511358148], 
processed observation next is [1.0, 0.17391304347826086, 0.4549763033175356, 0.815, 1.0, 1.0, 0.5672948657905893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24877164034344457, 0.24877164034344457, 0.31133306139673855], 
reward next is 0.6887, 
noisyNet noise sample is [array([-0.5180331], dtype=float32), 1.4716115]. 
=============================================
[2019-04-10 13:00:37,472] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[61.89731 ]
 [61.69674 ]
 [61.548714]
 [61.40187 ]
 [61.232517]], R is [[62.08738327]
 [62.15391922]
 [62.21071625]
 [62.26260376]
 [62.30943298]].
[2019-04-10 13:00:39,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4872921e-21 1.0000000e+00 1.7071471e-24 8.1158953e-26 1.3745168e-33], sum to 1.0000
[2019-04-10 13:00:39,439] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4769
[2019-04-10 13:00:39,445] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5538424751703588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773936.2181188411, 773936.2181188405, 192462.2249500547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3696000.0000, 
sim time next is 3696600.0000, 
raw observation next is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.5524456496319025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771983.5930162227, 771983.5930162227, 192221.322358043], 
processed observation next is [1.0, 0.782608695652174, 0.581358609794629, 0.7833333333333334, 1.0, 1.0, 0.460777891122774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21443988694895075, 0.21443988694895075, 0.2868974960567806], 
reward next is 0.7131, 
noisyNet noise sample is [array([-1.9506067], dtype=float32), -1.0237681]. 
=============================================
[2019-04-10 13:00:39,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2098250e-22 1.0000000e+00 1.6284733e-25 1.8741010e-28 4.8088903e-35], sum to 1.0000
[2019-04-10 13:00:39,759] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-10 13:00:39,762] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.16666666666667, 1.0, 2.0, 0.5485338291444588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766515.270746748, 766515.2707467487, 191549.6587717301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [29.0, 77.33333333333334, 1.0, 2.0, 0.5458690742630228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762790.2339132758, 762790.2339132765, 191094.7013062788], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7733333333333334, 1.0, 1.0, 0.4528543063409913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21188617608702104, 0.21188617608702123, 0.2852159720989236], 
reward next is 0.7148, 
noisyNet noise sample is [array([-1.38236], dtype=float32), 1.8318913]. 
=============================================
[2019-04-10 13:00:40,144] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1159651: loss -71.7850
[2019-04-10 13:00:40,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1159652: learning rate 0.0000
[2019-04-10 13:00:40,984] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1160171: loss -114.8817
[2019-04-10 13:00:40,989] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1160171: learning rate 0.0000
[2019-04-10 13:00:41,222] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1160324: loss -10.4361
[2019-04-10 13:00:41,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1160324: learning rate 0.0000
[2019-04-10 13:00:41,372] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1160414: loss -36.1420
[2019-04-10 13:00:41,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1160414: learning rate 0.0000
[2019-04-10 13:00:41,608] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1160560: loss -73.7522
[2019-04-10 13:00:41,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1160560: learning rate 0.0000
[2019-04-10 13:00:41,688] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1160611: loss -52.0593
[2019-04-10 13:00:41,691] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1160611: learning rate 0.0000
[2019-04-10 13:00:41,809] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1160686: loss -92.0927
[2019-04-10 13:00:41,812] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1160686: learning rate 0.0000
[2019-04-10 13:00:41,838] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1160699: loss -48.3938
[2019-04-10 13:00:41,841] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1160702: learning rate 0.0000
[2019-04-10 13:00:41,972] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1160782: loss -141.4247
[2019-04-10 13:00:41,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1160782: learning rate 0.0000
[2019-04-10 13:00:42,128] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1160874: loss -10.8588
[2019-04-10 13:00:42,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1160877: learning rate 0.0000
[2019-04-10 13:00:42,173] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1160903: loss -1.6155
[2019-04-10 13:00:42,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1160903: learning rate 0.0000
[2019-04-10 13:00:42,326] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1160998: loss -18.3450
[2019-04-10 13:00:42,327] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1160998: loss -59.3664
[2019-04-10 13:00:42,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1160998: learning rate 0.0000
[2019-04-10 13:00:42,329] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1160999: learning rate 0.0000
[2019-04-10 13:00:42,479] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1161094: loss 42.9641
[2019-04-10 13:00:42,482] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1161094: learning rate 0.0000
[2019-04-10 13:00:44,806] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1162537: loss 0.0092
[2019-04-10 13:00:44,807] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1162537: learning rate 0.0000
[2019-04-10 13:00:45,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3955805e-20 1.0000000e+00 2.9755562e-23 8.4633345e-28 1.9835834e-32], sum to 1.0000
[2019-04-10 13:00:45,725] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7467
[2019-04-10 13:00:45,731] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.5, 1.0, 2.0, 0.5714166052600571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798503.4350052854, 798503.4350052854, 195542.0834691583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3834600.0000, 
sim time next is 3835200.0000, 
raw observation next is [31.66666666666667, 69.0, 1.0, 2.0, 0.5746693473168272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803050.5684547573, 803050.5684547573, 196122.4919113029], 
processed observation next is [0.0, 0.391304347826087, 0.6998420221169038, 0.69, 1.0, 1.0, 0.4875534305022014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2230696023485437, 0.2230696023485437, 0.2927201371810491], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.14191002], dtype=float32), 0.20168158]. 
=============================================
[2019-04-10 13:00:48,172] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1164618: loss 0.0063
[2019-04-10 13:00:48,174] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1164618: learning rate 0.0000
[2019-04-10 13:00:48,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2170735e-23 1.0000000e+00 1.3663448e-27 1.8939330e-32 1.0281134e-37], sum to 1.0000
[2019-04-10 13:00:48,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1334
[2019-04-10 13:00:48,370] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 78.0, 1.0, 2.0, 0.5921317202988599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827462.2252508991, 827462.2252508991, 199289.4378216687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4134000.0000, 
sim time next is 4134600.0000, 
raw observation next is [30.0, 79.5, 1.0, 2.0, 0.5921353984853222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827467.3672586727, 827467.3672586727, 199289.8955047042], 
processed observation next is [1.0, 0.8695652173913043, 0.6208530805687204, 0.795, 1.0, 1.0, 0.5085968656449664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22985204646074242, 0.22985204646074242, 0.2974476052309018], 
reward next is 0.7026, 
noisyNet noise sample is [array([-0.3997652], dtype=float32), 0.8316556]. 
=============================================
[2019-04-10 13:00:48,914] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7507581e-19 1.0000000e+00 1.3521258e-22 1.5314131e-27 2.8412343e-31], sum to 1.0000
[2019-04-10 13:00:48,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7093
[2019-04-10 13:00:48,928] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 69.66666666666667, 1.0, 2.0, 0.6033263538195535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843112.1432510435, 843112.1432510435, 201367.1972354473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3921600.0000, 
sim time next is 3922200.0000, 
raw observation next is [32.0, 69.0, 1.0, 2.0, 0.5953357711821696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 831941.4134097829, 831941.4134097835, 199880.3410702076], 
processed observation next is [0.0, 0.391304347826087, 0.7156398104265403, 0.69, 1.0, 1.0, 0.5124527363640597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23109483705827302, 0.2310948370582732, 0.29832886726896657], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.81015646], dtype=float32), 0.25265932]. 
=============================================
[2019-04-10 13:00:53,066] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1167589: loss 0.0125
[2019-04-10 13:00:53,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1167589: learning rate 0.0000
[2019-04-10 13:00:53,946] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1168131: loss 0.0177
[2019-04-10 13:00:53,948] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1168132: learning rate 0.0000
[2019-04-10 13:00:54,275] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1168334: loss 0.0162
[2019-04-10 13:00:54,276] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1168334: learning rate 0.0000
[2019-04-10 13:00:54,423] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1168425: loss 0.0120
[2019-04-10 13:00:54,424] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1168425: learning rate 0.0000
[2019-04-10 13:00:54,433] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1168429: loss 0.0123
[2019-04-10 13:00:54,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1168429: learning rate 0.0000
[2019-04-10 13:00:54,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8690637e-11 2.4812014e-03 3.1953942e-10 9.9751878e-01 3.1238292e-16], sum to 1.0000
[2019-04-10 13:00:54,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9916
[2019-04-10 13:00:54,651] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.83333333333334, 50.5, 1.0, 2.0, 0.908556664423294, 1.0, 2.0, 0.7748683717259094, 1.0, 1.0, 1.03, 7.005114182420509, 6.9112, 170.5573041426782, 3251908.329730565, 3184633.780678061, 595299.3924621221], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4201800.0000, 
sim time next is 4202400.0000, 
raw observation next is [36.66666666666667, 51.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.448117754438414, 6.9112, 170.5573041426782, 3294393.777207759, 2909777.749285662, 550739.8904978733], 
processed observation next is [1.0, 0.6521739130434783, 0.9368088467614536, 0.51, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.053691775443841384, 0.0, 0.8375144448122397, 0.9151093825577109, 0.808271597023795, 0.82199983656399], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0638256], dtype=float32), 0.49199006]. 
=============================================
[2019-04-10 13:00:54,768] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1168632: loss 0.0146
[2019-04-10 13:00:54,770] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1168633: learning rate 0.0000
[2019-04-10 13:00:54,822] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1168669: loss 0.0143
[2019-04-10 13:00:54,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1168670: learning rate 0.0000
[2019-04-10 13:00:54,886] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1168706: loss 0.0160
[2019-04-10 13:00:54,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1168707: learning rate 0.0000
[2019-04-10 13:00:54,916] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1168725: loss 0.0163
[2019-04-10 13:00:54,918] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1168726: learning rate 0.0000
[2019-04-10 13:00:55,105] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1168843: loss 0.0176
[2019-04-10 13:00:55,106] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1168843: learning rate 0.0000
[2019-04-10 13:00:55,233] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1168924: loss 0.0162
[2019-04-10 13:00:55,237] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1168924: learning rate 0.0000
[2019-04-10 13:00:55,270] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1168940: loss 0.0153
[2019-04-10 13:00:55,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1168940: learning rate 0.0000
[2019-04-10 13:00:55,345] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1168994: loss 0.0108
[2019-04-10 13:00:55,347] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1168994: learning rate 0.0000
[2019-04-10 13:00:55,490] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1169083: loss 0.0152
[2019-04-10 13:00:55,491] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1169083: learning rate 0.0000
[2019-04-10 13:00:56,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2236848e-19 1.0000000e+00 6.1670838e-21 2.1009256e-23 2.0141086e-29], sum to 1.0000
[2019-04-10 13:00:56,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9845
[2019-04-10 13:00:56,245] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 88.83333333333334, 1.0, 2.0, 0.7728947693630688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1080194.273393798, 1080194.273393797, 237366.3914270941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4078200.0000, 
sim time next is 4078800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7492116210321024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047078.492507196, 1047078.492507196, 231826.2129548819], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.6978453265447017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2908551368075544, 0.2908551368075544, 0.3460092730669879], 
reward next is 0.6540, 
noisyNet noise sample is [array([1.2980065], dtype=float32), -1.2102486]. 
=============================================
[2019-04-10 13:00:56,906] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7781026e-12 1.0000000e+00 7.7700964e-13 1.1299598e-09 6.3455718e-19], sum to 1.0000
[2019-04-10 13:00:56,915] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7749
[2019-04-10 13:00:56,923] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2103248.422233628 W.
[2019-04-10 13:00:56,926] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.7520973698545431, 1.0, 2.0, 0.7520973698545431, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2103248.422233628, 2103248.422233628, 397231.9109755227], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4093200.0000, 
sim time next is 4093800.0000, 
raw observation next is [30.16666666666666, 79.00000000000001, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.445133538872499, 6.9112, 168.9104417289962, 2662812.315620938, 2284027.126020442, 474924.4072739956], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235385, 0.7900000000000001, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.05339335388724988, 0.0, 0.829427596424186, 0.7396700876724828, 0.6344519794501229, 0.7088423989164113], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1806501], dtype=float32), -0.2734842]. 
=============================================
[2019-04-10 13:00:57,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3180492e-24 1.0000000e+00 4.1761393e-28 2.5042616e-33 1.1974830e-37], sum to 1.0000
[2019-04-10 13:00:57,870] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3731
[2019-04-10 13:00:57,874] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.5, 1.0, 2.0, 0.5921353984853222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827467.3672586727, 827467.3672586727, 199289.8955047042], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4134600.0000, 
sim time next is 4135200.0000, 
raw observation next is [29.66666666666666, 81.0, 1.0, 2.0, 0.5907358342180189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825510.8150814415, 825510.8150814415, 199032.5652322701], 
processed observation next is [1.0, 0.8695652173913043, 0.6050552922590835, 0.81, 1.0, 1.0, 0.5069106436361673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22930855974484488, 0.22930855974484488, 0.29706353019741805], 
reward next is 0.7029, 
noisyNet noise sample is [array([1.5842236], dtype=float32), -0.7395892]. 
=============================================
[2019-04-10 13:00:58,189] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1170739: loss 42.1148
[2019-04-10 13:00:58,190] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1170740: learning rate 0.0000
[2019-04-10 13:01:00,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0407728e-21 1.0000000e+00 3.5449095e-26 6.6894611e-27 3.3164906e-35], sum to 1.0000
[2019-04-10 13:01:00,716] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5431
[2019-04-10 13:01:00,719] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.83333333333334, 56.66666666666667, 1.0, 2.0, 0.592812941101738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828414.5544113835, 828414.5544113835, 199415.5127693651], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4212600.0000, 
sim time next is 4213200.0000, 
raw observation next is [34.66666666666667, 57.33333333333334, 1.0, 2.0, 0.5838024596998019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815818.2022191727, 815818.2022191727, 197768.6241460458], 
processed observation next is [1.0, 0.782608695652174, 0.8420221169036337, 0.5733333333333335, 1.0, 1.0, 0.4985571803612071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22661616728310355, 0.22661616728310355, 0.29517705096424746], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.5131153], dtype=float32), 0.24683471]. 
=============================================
[2019-04-10 13:01:01,266] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1172646: loss -37.2140
[2019-04-10 13:01:01,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1172646: learning rate 0.0000
[2019-04-10 13:01:05,093] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 13:01:05,093] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:01:05,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:05,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:01:05,096] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:01:05,096] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:05,096] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:05,097] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:01:05,097] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:01:05,098] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:05,098] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:05,120] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run48
[2019-04-10 13:01:05,139] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run48
[2019-04-10 13:01:05,163] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run48
[2019-04-10 13:01:05,164] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run48
[2019-04-10 13:01:05,192] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run48
[2019-04-10 13:01:09,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:01:09,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.31998749, 95.94665175, 1.0, 2.0, 0.2551747216336133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 420443.1270660316, 420443.1270660316, 161404.127135561]
[2019-04-10 13:01:09,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:01:09,136] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2551202e-20 1.0000000e+00 8.2344701e-24 2.2249122e-29 5.5853151e-33], sampled 0.22335792513668462
[2019-04-10 13:01:18,617] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:01:18,618] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.36734545, 95.17739704333334, 1.0, 2.0, 0.5379808837254743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751763.4800201287, 751763.4800201287, 189761.3428988921]
[2019-04-10 13:01:18,619] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:01:18,620] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.3305398e-20 1.0000000e+00 1.3386434e-22 3.7542087e-27 1.8069907e-31], sampled 0.16342162618941303
[2019-04-10 13:01:19,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:01:19,328] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.8, 56.0, 1.0, 2.0, 0.2653421238836173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433204.3213332457, 433204.3213332451, 162425.6619240085]
[2019-04-10 13:01:19,329] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:01:19,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0537449e-20 1.0000000e+00 6.9514668e-24 1.1627455e-29 4.8939500e-33], sampled 0.9853779732416108
[2019-04-10 13:01:35,390] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:01:35,391] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 94.00000000000001, 1.0, 2.0, 0.3333950727343722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527203.1610932067, 527203.1610932067, 169111.7180486134]
[2019-04-10 13:01:35,392] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:01:35,395] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.09777134e-20 1.00000000e+00 8.07169026e-24 1.62698589e-29
 5.54413351e-33], sampled 0.11031303255989877
[2019-04-10 13:01:35,546] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:01:35,547] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.2, 73.0, 1.0, 2.0, 0.4999575326220291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698612.9549145472, 698612.9549145472, 183588.8618297347]
[2019-04-10 13:01:35,548] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:01:35,549] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3792912e-20 1.0000000e+00 9.2444338e-23 1.3381506e-26 1.8453738e-31], sampled 0.6274925022050096
[2019-04-10 13:01:50,284] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:01:50,285] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.46666666666667, 46.66666666666667, 1.0, 2.0, 0.602180315846313, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.92911450845935, 6.9112, 168.9127558943716, 1683687.229232414, 1670978.075456094, 364983.6271627035]
[2019-04-10 13:01:50,287] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:01:50,289] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2333152e-14 1.0000000e+00 2.0368390e-15 3.0168356e-14 3.2108696e-23], sampled 0.5612522780061145
[2019-04-10 13:01:50,290] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1683687.229232414 W.
[2019-04-10 13:02:05,674] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:02:05,674] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 68.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.496761574509529, 6.9112, 168.9096580880623, 2705071.498899957, 2289662.076472326, 475090.4977611801]
[2019-04-10 13:02:05,675] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:02:05,679] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.0144525e-10 9.9965560e-01 2.6716152e-09 3.4441112e-04 9.6123469e-16], sampled 0.898356679539027
[2019-04-10 13:02:05,680] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2705071.498899957 W.
[2019-04-10 13:02:25,132] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12868069], dtype=float32), 0.092959985]
[2019-04-10 13:02:25,133] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.25, 61.0, 1.0, 2.0, 0.8190313255636699, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994425203715002, 6.9112, 168.9123932599739, 2041708.826535972, 1982666.199511962, 413032.670430315]
[2019-04-10 13:02:25,134] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:02:25,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6616834e-12 1.0000000e+00 1.0714422e-12 9.6674002e-10 5.9008603e-20], sampled 0.3415993119288965
[2019-04-10 13:02:25,136] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2041708.826535972 W.
[2019-04-10 13:02:38,638] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.4985 3163667605.2785 1769.0000
[2019-04-10 13:02:39,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1158 3007806111.5505 1765.0000
[2019-04-10 13:02:39,216] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 13:02:39,241] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9696 2842458654.0903 1129.0000
[2019-04-10 13:02:39,249] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5276 2779228621.7687 934.0000
[2019-04-10 13:02:40,266] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1175000, evaluation results [1175000.0, 7884.498456272316, 3163667605.278512, 1769.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8660.527597268203, 2779228621.768698, 934.0, 7999.115830629687, 3007806111.5505357, 1765.0, 8496.969562132992, 2842458654.09027, 1129.0]
[2019-04-10 13:02:41,106] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1175546: loss 38.2263
[2019-04-10 13:02:41,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1175546: learning rate 0.0000
[2019-04-10 13:02:41,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1830494e-21 1.0000000e+00 3.9008306e-25 1.9311292e-28 1.0049737e-33], sum to 1.0000
[2019-04-10 13:02:41,372] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2527
[2019-04-10 13:02:41,375] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 71.0, 1.0, 2.0, 0.6122301091384381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855559.6149324746, 855559.6149324746, 203047.3737643568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4310400.0000, 
sim time next is 4311000.0000, 
raw observation next is [32.0, 73.0, 1.0, 2.0, 0.6138371803709566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857806.3199818324, 857806.3199818324, 203352.9630802807], 
processed observation next is [1.0, 0.9130434782608695, 0.7156398104265403, 0.73, 1.0, 1.0, 0.5347435908083814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23827953332828677, 0.23827953332828677, 0.30351188519444877], 
reward next is 0.6965, 
noisyNet noise sample is [array([-1.3862269], dtype=float32), 0.13676664]. 
=============================================
[2019-04-10 13:02:41,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.10506 ]
 [68.248085]
 [68.11303 ]
 [68.15213 ]
 [68.16334 ]], R is [[68.10324097]
 [68.11915588]
 [68.13522339]
 [68.15190125]
 [68.16837311]].
[2019-04-10 13:02:42,140] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1176174: loss -37.9269
[2019-04-10 13:02:42,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1176175: learning rate 0.0000
[2019-04-10 13:02:42,473] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1176377: loss 40.8956
[2019-04-10 13:02:42,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1176378: learning rate 0.0000
[2019-04-10 13:02:42,543] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1176422: loss 39.3495
[2019-04-10 13:02:42,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1176422: learning rate 0.0000
[2019-04-10 13:02:42,663] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1176496: loss 35.6537
[2019-04-10 13:02:42,663] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1176496: learning rate 0.0000
[2019-04-10 13:02:43,027] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1176722: loss 39.7740
[2019-04-10 13:02:43,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1176722: learning rate 0.0000
[2019-04-10 13:02:43,042] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1176731: loss 15.9146
[2019-04-10 13:02:43,044] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1176731: learning rate 0.0000
[2019-04-10 13:02:43,048] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1176733: loss 15.5281
[2019-04-10 13:02:43,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1176734: learning rate 0.0000
[2019-04-10 13:02:43,123] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1176775: loss 26.8426
[2019-04-10 13:02:43,126] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1176777: learning rate 0.0000
[2019-04-10 13:02:43,246] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1176855: loss 54.7806
[2019-04-10 13:02:43,247] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1176855: learning rate 0.0000
[2019-04-10 13:02:43,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1176873: loss 15.7955
[2019-04-10 13:02:43,279] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1176874: learning rate 0.0000
[2019-04-10 13:02:43,335] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1176910: loss 31.5058
[2019-04-10 13:02:43,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1176910: learning rate 0.0000
[2019-04-10 13:02:43,386] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1176942: loss -32.1759
[2019-04-10 13:02:43,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1176942: learning rate 0.0000
[2019-04-10 13:02:43,657] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1177110: loss -37.7145
[2019-04-10 13:02:43,657] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1177110: learning rate 0.0000
[2019-04-10 13:02:45,261] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7324698e-21 1.0000000e+00 1.1410906e-23 3.6790824e-27 1.5971163e-32], sum to 1.0000
[2019-04-10 13:02:45,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8679
[2019-04-10 13:02:45,272] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 87.33333333333333, 1.0, 2.0, 0.61397848311866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858003.8631620125, 858003.8631620125, 203379.1088855984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4416000.0000, 
sim time next is 4416600.0000, 
raw observation next is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.6123978910827148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855794.1759007145, 855794.1759007145, 203078.4648208303], 
processed observation next is [0.0, 0.08695652173913043, 0.581358609794629, 0.8816666666666667, 1.0, 1.0, 0.533009507328572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23772060441686513, 0.23772060441686513, 0.3031021862997467], 
reward next is 0.6969, 
noisyNet noise sample is [array([-0.39462918], dtype=float32), 0.64168113]. 
=============================================
[2019-04-10 13:02:46,028] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1178574: loss 0.0969
[2019-04-10 13:02:46,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1178575: learning rate 0.0000
[2019-04-10 13:02:47,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7084275e-20 1.0000000e+00 1.0580007e-23 7.4848956e-28 4.3881803e-33], sum to 1.0000
[2019-04-10 13:02:47,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7603
[2019-04-10 13:02:47,868] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.606296123057291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 847263.8716847828, 847263.8716847834, 201925.1362408009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4467600.0000, 
sim time next is 4468200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6465546392236101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903546.7570592399, 903546.7570592399, 209737.4679518104], 
processed observation next is [0.0, 0.7391304347826086, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5741622159320604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2509852102942333, 0.2509852102942333, 0.3130409969430006], 
reward next is 0.6870, 
noisyNet noise sample is [array([0.27381793], dtype=float32), -1.0409206]. 
=============================================
[2019-04-10 13:02:49,353] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1180619: loss 0.2035
[2019-04-10 13:02:49,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1180620: learning rate 0.0000
[2019-04-10 13:02:50,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8373745e-21 1.0000000e+00 5.3367505e-24 9.2262577e-30 2.2764016e-33], sum to 1.0000
[2019-04-10 13:02:50,909] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0815
[2019-04-10 13:02:50,915] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 64.5, 1.0, 2.0, 0.5468455069623703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764155.178867518, 764155.178867518, 191261.5018438103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4537800.0000, 
sim time next is 4538400.0000, 
raw observation next is [31.66666666666666, 64.0, 1.0, 2.0, 0.5482652458743386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766139.8200219924, 766139.8200219924, 191504.0214311893], 
processed observation next is [0.0, 0.5217391304347826, 0.6998420221169034, 0.64, 1.0, 1.0, 0.4557412600895645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21281661667277568, 0.21281661667277568, 0.2858268976584915], 
reward next is 0.7142, 
noisyNet noise sample is [array([0.6211995], dtype=float32), -2.145512]. 
=============================================
[2019-04-10 13:02:51,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5228369e-21 1.0000000e+00 4.4673795e-25 2.4275785e-30 4.4789259e-35], sum to 1.0000
[2019-04-10 13:02:51,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4663
[2019-04-10 13:02:51,686] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 66.33333333333333, 1.0, 2.0, 0.5305104047202852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741320.7427937491, 741320.7427937497, 188514.8100468623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [30.33333333333333, 68.16666666666667, 1.0, 2.0, 0.5316097653041234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742857.4964020469, 742857.4964020469, 188697.1334090805], 
processed observation next is [0.0, 0.7391304347826086, 0.6366508688783569, 0.6816666666666668, 1.0, 1.0, 0.4356744160290643, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20634930455612413, 0.20634930455612413, 0.28163751255086644], 
reward next is 0.7184, 
noisyNet noise sample is [array([1.1493924], dtype=float32), 0.68395895]. 
=============================================
[2019-04-10 13:02:51,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.29583 ]
 [75.25683 ]
 [75.209404]
 [75.147415]
 [75.05898 ]], R is [[75.27706909]
 [75.24293518]
 [75.2093811 ]
 [75.17627716]
 [75.14296722]].
[2019-04-10 13:02:54,192] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1183623: loss 0.3343
[2019-04-10 13:02:54,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1183625: learning rate 0.0000
[2019-04-10 13:02:54,987] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1184115: loss 0.3412
[2019-04-10 13:02:54,989] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1184116: learning rate 0.0000
[2019-04-10 13:02:55,406] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1184366: loss 0.2744
[2019-04-10 13:02:55,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1184367: learning rate 0.0000
[2019-04-10 13:02:55,431] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1184387: loss 0.5916
[2019-04-10 13:02:55,432] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1184388: learning rate 0.0000
[2019-04-10 13:02:55,559] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1184463: loss 0.3977
[2019-04-10 13:02:55,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1184464: learning rate 0.0000
[2019-04-10 13:02:55,847] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1184640: loss 0.0652
[2019-04-10 13:02:55,849] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1184641: learning rate 0.0000
[2019-04-10 13:02:55,935] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1184699: loss 0.3255
[2019-04-10 13:02:55,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1184699: learning rate 0.0000
[2019-04-10 13:02:55,961] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1184714: loss 0.3716
[2019-04-10 13:02:55,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1184714: learning rate 0.0000
[2019-04-10 13:02:56,101] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1184802: loss 0.4863
[2019-04-10 13:02:56,102] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1184802: learning rate 0.0000
[2019-04-10 13:02:56,203] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1184866: loss 0.0760
[2019-04-10 13:02:56,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1184867: learning rate 0.0000
[2019-04-10 13:02:56,242] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1184890: loss 0.2523
[2019-04-10 13:02:56,246] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1184892: learning rate 0.0000
[2019-04-10 13:02:56,246] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1184893: loss 0.1355
[2019-04-10 13:02:56,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1184893: learning rate 0.0000
[2019-04-10 13:02:56,383] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1184977: loss 0.2260
[2019-04-10 13:02:56,384] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1184977: learning rate 0.0000
[2019-04-10 13:02:56,580] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1185095: loss 0.4048
[2019-04-10 13:02:56,584] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1185098: learning rate 0.0000
[2019-04-10 13:02:58,530] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.52958583e-24 1.00000000e+00 1.05533866e-26 1.65047612e-31
 6.37484993e-36], sum to 1.0000
[2019-04-10 13:02:58,537] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3536
[2019-04-10 13:02:58,542] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5154406666273216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720255.5633144473, 720255.5633144473, 186050.8672658461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4737600.0000, 
sim time next is 4738200.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5159375113248671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720950.0692380589, 720950.0692380589, 186130.991501442], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41679218231911697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20026390812168302, 0.20026390812168302, 0.27780745000215223], 
reward next is 0.7222, 
noisyNet noise sample is [array([1.4221773], dtype=float32), 1.6284921]. 
=============================================
[2019-04-10 13:02:59,164] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1186689: loss -37.8459
[2019-04-10 13:02:59,169] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1186689: learning rate 0.0000
[2019-04-10 13:03:01,470] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8692330e-19 1.0000000e+00 7.9248527e-22 3.1746694e-25 7.9089953e-30], sum to 1.0000
[2019-04-10 13:03:01,477] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3707
[2019-04-10 13:03:01,487] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6945860471020369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 970700.3330538451, 970700.3330538458, 219677.5756208749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4765200.0000, 
sim time next is 4765800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.708105031886024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 989602.2348464536, 989602.2348464536, 222601.007603375], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6483193155253302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27488950967957043, 0.27488950967957043, 0.3322403098557836], 
reward next is 0.6678, 
noisyNet noise sample is [array([-1.1378473], dtype=float32), -0.31158748]. 
=============================================
[2019-04-10 13:03:01,557] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4147637e-12 1.0000000e+00 1.2707877e-12 2.1170175e-10 2.9839737e-19], sum to 1.0000
[2019-04-10 13:03:01,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4862
[2019-04-10 13:03:01,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2203308.668754146 W.
[2019-04-10 13:03:01,575] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.9344874264662594, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988291893643362, 6.9112, 168.9124975184288, 2203308.668754146, 2148617.1747205, 444452.8260715012], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4784400.0000, 
sim time next is 4785000.0000, 
raw observation next is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.8762277063367324, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986679733333421, 6.9112, 168.9124443949911, 2121762.39444264, 2068214.636152193, 428258.136853869], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235385, 0.6933333333333335, 1.0, 1.0, 0.8508767546225692, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007547973333342117, 0.0, 0.8294374304310453, 0.5893784429007333, 0.5745040655978314, 0.6391912490356254], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7191692], dtype=float32), 0.43241096]. 
=============================================
[2019-04-10 13:03:01,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[43.799313]
 [43.621696]
 [44.376606]
 [47.539795]
 [55.26387 ]], R is [[45.11327362]
 [44.66213989]
 [44.60202026]
 [44.49397659]
 [44.04903793]].
[2019-04-10 13:03:02,470] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1364197e-11 9.9995255e-01 5.0562877e-11 4.7408652e-05 7.8862250e-18], sum to 1.0000
[2019-04-10 13:03:02,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8676
[2019-04-10 13:03:02,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 64.5, 1.0, 2.0, 0.3811316410406528, 1.0, 2.0, 0.3811316410406528, 1.0, 2.0, 0.6602805764188874, 6.9112, 6.9112, 170.5573041426782, 1598381.836541656, 1598381.836541656, 342476.7667308568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4797000.0000, 
sim time next is 4797600.0000, 
raw observation next is [31.66666666666666, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.344754257871871, 6.9112, 168.9110267503123, 1761540.383532696, 1453965.585166445, 311351.819938224], 
processed observation next is [1.0, 0.5217391304347826, 0.6998420221169034, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.04335542578718714, 0.0, 0.8294304691466718, 0.4893167732035266, 0.4038793292129014, 0.4647042088630209], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5616006], dtype=float32), -1.9079931]. 
=============================================
[2019-04-10 13:03:02,492] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1188647: loss -41.9731
[2019-04-10 13:03:02,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1188648: learning rate 0.0000
[2019-04-10 13:03:07,217] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1191561: loss -41.1845
[2019-04-10 13:03:07,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1191562: learning rate 0.0000
[2019-04-10 13:03:08,032] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1192076: loss -36.4460
[2019-04-10 13:03:08,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1192077: learning rate 0.0000
[2019-04-10 13:03:08,381] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1192290: loss -59.1564
[2019-04-10 13:03:08,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1192292: learning rate 0.0000
[2019-04-10 13:03:08,553] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1192398: loss -28.6582
[2019-04-10 13:03:08,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1192400: learning rate 0.0000
[2019-04-10 13:03:08,593] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1192418: loss -28.3680
[2019-04-10 13:03:08,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1192418: learning rate 0.0000
[2019-04-10 13:03:09,028] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1192694: loss -41.0471
[2019-04-10 13:03:09,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1192694: learning rate 0.0000
[2019-04-10 13:03:09,037] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1192698: loss -58.3582
[2019-04-10 13:03:09,039] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1192698: learning rate 0.0000
[2019-04-10 13:03:09,094] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1192730: loss -30.7557
[2019-04-10 13:03:09,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1192730: learning rate 0.0000
[2019-04-10 13:03:09,184] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1192787: loss -30.8996
[2019-04-10 13:03:09,187] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1192788: learning rate 0.0000
[2019-04-10 13:03:09,203] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1192797: loss -28.1137
[2019-04-10 13:03:09,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1192797: learning rate 0.0000
[2019-04-10 13:03:09,320] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1192868: loss -36.4718
[2019-04-10 13:03:09,320] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1192868: loss -36.5706
[2019-04-10 13:03:09,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1192868: learning rate 0.0000
[2019-04-10 13:03:09,323] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1192869: learning rate 0.0000
[2019-04-10 13:03:09,506] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1192977: loss -31.8332
[2019-04-10 13:03:09,508] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1192977: learning rate 0.0000
[2019-04-10 13:03:09,633] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1193054: loss -49.3853
[2019-04-10 13:03:09,635] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1193055: learning rate 0.0000
[2019-04-10 13:03:12,145] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1194620: loss 0.1800
[2019-04-10 13:03:12,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1194621: learning rate 0.0000
[2019-04-10 13:03:13,307] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3405776e-20 1.0000000e+00 8.1403054e-24 3.0065133e-29 6.2018913e-33], sum to 1.0000
[2019-04-10 13:03:13,314] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6758
[2019-04-10 13:03:13,319] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 75.0, 1.0, 2.0, 0.5296682617982389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740143.5453028558, 740143.5453028558, 188375.121748588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5041800.0000, 
sim time next is 5042400.0000, 
raw observation next is [29.33333333333334, 72.0, 1.0, 2.0, 0.5249969805333947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733613.7740593866, 733613.774059386, 187605.231820317], 
processed observation next is [0.0, 0.34782608695652173, 0.5892575039494474, 0.72, 1.0, 1.0, 0.4277072054619213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20378160390538516, 0.203781603905385, 0.28000780868704034], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.213983], dtype=float32), -1.0753719]. 
=============================================
[2019-04-10 13:03:13,902] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6217195e-20 1.0000000e+00 2.3408313e-22 1.2007523e-24 8.7174313e-31], sum to 1.0000
[2019-04-10 13:03:13,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1181
[2019-04-10 13:03:13,914] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 83.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.112382654854306, 6.9112, 168.9119771824497, 1596577.743955647, 1453852.671929375, 311355.8802414009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5296200.0000, 
sim time next is 5296800.0000, 
raw observation next is [29.96666666666667, 83.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.220885075522989, 6.9112, 168.9108094417071, 1673603.855618813, 1453905.397148675, 311355.9461557171], 
processed observation next is [1.0, 0.30434782608695654, 0.6192733017377569, 0.83, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.030968507552298873, 0.0, 0.829429402061936, 0.46488995989411475, 0.4038626103190764, 0.46471036739659266], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4855958], dtype=float32), -1.4892961]. 
=============================================
[2019-04-10 13:03:15,299] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1196556: loss 0.3122
[2019-04-10 13:03:15,301] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1196556: learning rate 0.0000
[2019-04-10 13:03:16,578] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.8787224e-21 1.0000000e+00 4.2466236e-24 3.6803703e-29 1.2569278e-33], sum to 1.0000
[2019-04-10 13:03:16,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7407
[2019-04-10 13:03:16,591] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 65.0, 1.0, 2.0, 0.5646492078273323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789043.0917657722, 789043.0917657722, 194345.3254130156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5142600.0000, 
sim time next is 5143200.0000, 
raw observation next is [32.0, 64.33333333333334, 1.0, 2.0, 0.5609148420134713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783822.7484076903, 783822.7484076909, 193690.6884477177], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6433333333333334, 1.0, 1.0, 0.47098173736562804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21772854122435842, 0.21772854122435858, 0.28909057977271296], 
reward next is 0.7109, 
noisyNet noise sample is [array([-0.27589417], dtype=float32), 0.20982231]. 
=============================================
[2019-04-10 13:03:17,615] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.8932958e-22 1.0000000e+00 8.7614984e-25 1.2739844e-30 8.9682333e-35], sum to 1.0000
[2019-04-10 13:03:17,622] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3556
[2019-04-10 13:03:17,628] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5514719180164317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770622.4138443242, 770622.4138443236, 192053.9311437892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5147400.0000, 
sim time next is 5148000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5517852310908158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771060.3939191472, 771060.3939191466, 192107.7926408297], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.459982206133513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21418344275531867, 0.2141834427553185, 0.2867280487176563], 
reward next is 0.7133, 
noisyNet noise sample is [array([1.1434392], dtype=float32), -0.030963873]. 
=============================================
[2019-04-10 13:03:17,638] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.44252]
 [73.39212]
 [73.34027]
 [73.29246]
 [73.23446]], R is [[73.51677704]
 [73.4949646 ]
 [73.47335815]
 [73.45172119]
 [73.43011475]].
[2019-04-10 13:03:17,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3622752e-19 1.0000000e+00 9.6141304e-21 1.7276469e-23 6.6608550e-30], sum to 1.0000
[2019-04-10 13:03:17,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3355
[2019-04-10 13:03:17,778] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.8040646589605409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123780.223190856, 1123780.223190856, 244908.5317874185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5199000.0000, 
sim time next is 5199600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7720453625760281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079006.543133964, 1079006.543133964, 237162.2853708779], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.725355858525335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29972403975943446, 0.29972403975943446, 0.3539735602550416], 
reward next is 0.6460, 
noisyNet noise sample is [array([0.8299218], dtype=float32), -1.7948328]. 
=============================================
[2019-04-10 13:03:18,539] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.1084006e-22 1.0000000e+00 7.2716045e-25 1.8450096e-29 4.1741038e-34], sum to 1.0000
[2019-04-10 13:03:18,546] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4959
[2019-04-10 13:03:18,550] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5220866714079402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729545.6058289391, 729545.6058289384, 187128.7836591793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5169000.0000, 
sim time next is 5169600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5212672859450231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728400.2327483637, 728400.232748363, 186995.1039861079], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42321359752412424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2023333979856566, 0.20233339798565642, 0.27909717012851926], 
reward next is 0.7209, 
noisyNet noise sample is [array([0.22610104], dtype=float32), 1.1127216]. 
=============================================
[2019-04-10 13:03:18,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.6659867e-20 1.0000000e+00 2.2760932e-22 1.3042044e-25 2.2668891e-31], sum to 1.0000
[2019-04-10 13:03:18,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1923
[2019-04-10 13:03:18,940] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.8172105210352213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1142163.079571807, 1142163.079571807, 248178.0196909949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5209800.0000, 
sim time next is 5210400.0000, 
raw observation next is [28.33333333333334, 77.33333333333334, 1.0, 2.0, 0.7609240291581376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1063455.642995191, 1063455.642995192, 234544.1589199607], 
processed observation next is [1.0, 0.30434782608695654, 0.5418641390205374, 0.7733333333333334, 1.0, 1.0, 0.7119566616363103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29540434527644194, 0.2954043452764422, 0.3500659088357622], 
reward next is 0.6499, 
noisyNet noise sample is [array([-0.52084315], dtype=float32), 0.7392399]. 
=============================================
[2019-04-10 13:03:20,296] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1199653: loss 0.0132
[2019-04-10 13:03:20,301] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1199653: learning rate 0.0000
[2019-04-10 13:03:20,871] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 13:03:20,873] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:03:20,873] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:03:20,874] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:03:20,874] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:03:20,875] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:03:20,876] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:03:20,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:03:20,877] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:03:20,877] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:03:20,878] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:03:20,897] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run49
[2019-04-10 13:03:20,898] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run49
[2019-04-10 13:03:20,898] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run49
[2019-04-10 13:03:20,955] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run49
[2019-04-10 13:03:20,978] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run49
[2019-04-10 13:03:28,234] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:03:28,235] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.149424195, 81.73752594, 1.0, 2.0, 0.2322136488093186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 385855.7694812432, 385855.7694812425, 158907.5306347024]
[2019-04-10 13:03:28,236] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:03:28,239] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3705669e-21 1.0000000e+00 1.0346553e-24 1.8369676e-30 1.9398653e-34], sampled 0.8255256530032794
[2019-04-10 13:04:02,008] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:04:02,009] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.46157908, 88.96331751, 1.0, 2.0, 0.7153821494553083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 999777.0554338606, 999777.05543386, 224215.7288076066]
[2019-04-10 13:04:02,012] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:04:02,013] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.6962764e-23 1.0000000e+00 5.3404725e-26 5.6365617e-31 3.7242372e-36], sampled 0.35525841656835855
[2019-04-10 13:04:04,433] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:04:04,434] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.1, 46.0, 1.0, 2.0, 0.8710230735867424, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.99860025341313, 6.9112, 168.9123644399256, 2114477.603836318, 2052473.073318343, 426426.5901241553]
[2019-04-10 13:04:04,435] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:04:04,437] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2560521e-12 1.0000000e+00 1.2658479e-12 2.8554674e-09 1.5370547e-20], sampled 0.9531404461549631
[2019-04-10 13:04:04,437] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2114477.603836318 W.
[2019-04-10 13:04:08,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:04:08,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.66330852666667, 83.84036513333334, 1.0, 2.0, 0.7846490971151354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1096630.552010041, 1096630.55201004, 240177.4648180085]
[2019-04-10 13:04:08,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:04:08,038] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.3839060e-20 1.0000000e+00 2.2997714e-22 1.8238860e-26 2.3195139e-31], sampled 0.25362265525505445
[2019-04-10 13:04:15,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:04:15,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.8, 75.0, 1.0, 2.0, 0.5662175551220481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 791235.5229201498, 791235.5229201505, 194620.4546957609]
[2019-04-10 13:04:15,018] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:04:15,020] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7603639e-21 1.0000000e+00 4.4439260e-24 1.4816426e-28 9.0855556e-34], sampled 0.12230438907190821
[2019-04-10 13:04:33,207] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:04:33,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.19270014666667, 72.44633459, 1.0, 2.0, 0.7782892297888468, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005980172412436, 6.9112, 168.9123158280877, 1984688.795375933, 1917448.7340255, 402558.790210577]
[2019-04-10 13:04:33,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:04:33,210] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4836328e-14 1.0000000e+00 1.4580862e-15 5.6092297e-14 1.4711259e-23], sampled 0.817084780058108
[2019-04-10 13:04:33,210] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1984688.795375933 W.
[2019-04-10 13:04:42,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1263868], dtype=float32), 0.0909899]
[2019-04-10 13:04:42,600] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.1, 68.0, 1.0, 2.0, 0.5894892426514151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823768.1175480718, 823768.1175480718, 198804.4405053689]
[2019-04-10 13:04:42,602] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:04:42,605] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1526628e-19 1.0000000e+00 4.0845678e-21 5.0302164e-23 9.0329378e-31], sampled 0.8719064567756396
[2019-04-10 13:04:52,273] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5195 2779329887.3399 933.0000
[2019-04-10 13:04:52,339] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6674 3164200272.3547 1778.0000
[2019-04-10 13:04:52,341] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1803 3007736924.2222 1766.0000
[2019-04-10 13:04:52,373] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 13:04:52,417] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8644 2842454207.2826 1131.0000
[2019-04-10 13:04:53,432] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1200000, evaluation results [1200000.0, 7882.667391253242, 3164200272.354699, 1778.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8660.519511474935, 2779329887.339914, 933.0, 7998.180259296095, 3007736924.2222114, 1766.0, 8496.864392788735, 2842454207.2826023, 1131.0]
[2019-04-10 13:04:53,598] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1200112: loss 0.2894
[2019-04-10 13:04:53,599] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1200112: learning rate 0.0000
[2019-04-10 13:04:53,985] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1200354: loss 0.0846
[2019-04-10 13:04:53,987] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1200355: learning rate 0.0000
[2019-04-10 13:04:54,005] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1200367: loss 0.2385
[2019-04-10 13:04:54,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1200370: learning rate 0.0000
[2019-04-10 13:04:54,026] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1200381: loss 0.0664
[2019-04-10 13:04:54,027] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1200381: learning rate 0.0000
[2019-04-10 13:04:54,530] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1200702: loss 0.2454
[2019-04-10 13:04:54,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1200703: learning rate 0.0000
[2019-04-10 13:04:54,545] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1200710: loss 0.1024
[2019-04-10 13:04:54,546] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1200710: loss 0.0267
[2019-04-10 13:04:54,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1200711: learning rate 0.0000
[2019-04-10 13:04:54,547] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1200711: learning rate 0.0000
[2019-04-10 13:04:54,716] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1200816: loss 0.0062
[2019-04-10 13:04:54,717] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1200816: learning rate 0.0000
[2019-04-10 13:04:54,802] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1200875: loss 0.1080
[2019-04-10 13:04:54,806] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1200876: learning rate 0.0000
[2019-04-10 13:04:54,819] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1200881: loss 0.0061
[2019-04-10 13:04:54,821] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1200882: learning rate 0.0000
[2019-04-10 13:04:54,836] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1200891: loss 0.0068
[2019-04-10 13:04:54,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1200893: learning rate 0.0000
[2019-04-10 13:04:55,073] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1201043: loss 0.0690
[2019-04-10 13:04:55,074] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1201044: learning rate 0.0000
[2019-04-10 13:04:55,125] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1201073: loss 0.0150
[2019-04-10 13:04:55,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1201073: learning rate 0.0000
[2019-04-10 13:04:57,568] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1202669: loss 0.3662
[2019-04-10 13:04:57,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1202669: learning rate 0.0000
[2019-04-10 13:05:00,780] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1204646: loss 0.2288
[2019-04-10 13:05:00,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1204646: learning rate 0.0000
[2019-04-10 13:05:05,473] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1207535: loss 0.2018
[2019-04-10 13:05:05,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1207536: learning rate 0.0000
[2019-04-10 13:05:06,475] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1208149: loss 0.3851
[2019-04-10 13:05:06,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1208150: learning rate 0.0000
[2019-04-10 13:05:06,685] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1208280: loss 0.2055
[2019-04-10 13:05:06,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1208280: learning rate 0.0000
[2019-04-10 13:05:06,713] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1208296: loss 0.4006
[2019-04-10 13:05:06,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1208296: learning rate 0.0000
[2019-04-10 13:05:06,885] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1208406: loss 0.4618
[2019-04-10 13:05:06,894] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1208406: learning rate 0.0000
[2019-04-10 13:05:07,303] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1208661: loss 0.3245
[2019-04-10 13:05:07,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1208661: learning rate 0.0000
[2019-04-10 13:05:07,396] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1208721: loss 0.2056
[2019-04-10 13:05:07,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1208722: learning rate 0.0000
[2019-04-10 13:05:07,419] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1208734: loss 0.3433
[2019-04-10 13:05:07,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1208734: learning rate 0.0000
[2019-04-10 13:05:07,436] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1208741: loss 0.4836
[2019-04-10 13:05:07,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1208741: learning rate 0.0000
[2019-04-10 13:05:07,641] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1208870: loss 0.3762
[2019-04-10 13:05:07,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1208870: learning rate 0.0000
[2019-04-10 13:05:07,698] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1208900: loss 0.4973
[2019-04-10 13:05:07,699] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1208900: learning rate 0.0000
[2019-04-10 13:05:07,768] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1208945: loss 0.3785
[2019-04-10 13:05:07,770] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1208945: learning rate 0.0000
[2019-04-10 13:05:07,790] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1208956: loss 0.2899
[2019-04-10 13:05:07,792] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1208957: learning rate 0.0000
[2019-04-10 13:05:07,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2846610e-11 9.9997783e-01 1.0612680e-11 2.2127482e-05 3.7339045e-18], sum to 1.0000
[2019-04-10 13:05:07,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3867
[2019-04-10 13:05:07,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2147641.069455616 W.
[2019-04-10 13:05:07,851] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.51666666666667, 52.83333333333334, 1.0, 2.0, 0.5119705139819363, 1.0, 2.0, 0.5119705139819363, 1.0, 2.0, 0.8838158969643911, 6.911199999999999, 6.9112, 170.5573041426782, 2147641.069455616, 2147641.069455617, 422537.879070483], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5575800.0000, 
sim time next is 5576400.0000, 
raw observation next is [33.6, 52.0, 1.0, 2.0, 0.501370412712016, 1.0, 2.0, 0.501370412712016, 1.0, 2.0, 0.8641523562728537, 6.9112, 6.9112, 170.5573041426782, 2103131.551092723, 2103131.551092723, 414904.2031424555], 
processed observation next is [1.0, 0.5652173913043478, 0.7914691943127963, 0.52, 1.0, 1.0, 0.3992414610988144, 1.0, 1.0, 0.3992414610988144, 1.0, 1.0, 0.8343321417961631, 0.0, 0.0, 0.8375144448122397, 0.5842032086368675, 0.5842032086368675, 0.6192600046902321], 
reward next is 0.3807, 
noisyNet noise sample is [array([-0.9191674], dtype=float32), -0.8317663]. 
=============================================
[2019-04-10 13:05:08,044] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1209110: loss 0.2055
[2019-04-10 13:05:08,046] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1209111: learning rate 0.0000
[2019-04-10 13:05:10,405] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1210563: loss 0.7811
[2019-04-10 13:05:10,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1210563: learning rate 0.0000
[2019-04-10 13:05:13,693] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1212578: loss 0.8587
[2019-04-10 13:05:13,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1212579: learning rate 0.0000
[2019-04-10 13:05:18,623] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3405294e-14 1.0000000e+00 1.2796198e-15 2.5264015e-15 1.1748189e-22], sum to 1.0000
[2019-04-10 13:05:18,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9184
[2019-04-10 13:05:18,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2754769.615423486 W.
[2019-04-10 13:05:18,639] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.65, 76.0, 1.0, 2.0, 0.6719001010523652, 1.0, 1.0, 0.6565400900404451, 1.0, 1.0, 1.03, 7.005095516609717, 6.9112, 170.5573041426782, 2754769.615423486, 2687508.437450529, 512599.2672966386], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5819400.0000, 
sim time next is 5820000.0000, 
raw observation next is [29.8, 75.33333333333333, 1.0, 2.0, 0.5671374884596737, 1.0, 2.0, 0.5671374884596737, 1.0, 2.0, 0.9849302622210075, 6.911199999999999, 6.9112, 170.5573041426782, 2379289.017420859, 2379289.01742086, 464676.9598036735], 
processed observation next is [1.0, 0.34782608695652173, 0.6113744075829385, 0.7533333333333333, 1.0, 1.0, 0.478478901758643, 1.0, 1.0, 0.478478901758643, 1.0, 1.0, 0.9816222710012286, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6609136159502387, 0.6609136159502389, 0.6935477011995127], 
reward next is 0.3065, 
noisyNet noise sample is [array([0.4293189], dtype=float32), -0.06196849]. 
=============================================
[2019-04-10 13:05:18,648] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[46.541737]
 [54.05515 ]
 [57.64502 ]
 [57.8701  ]
 [57.34033 ]], R is [[42.75212097]
 [42.32460022]
 [41.90135574]
 [42.03626251]
 [42.20487976]].
[2019-04-10 13:05:18,788] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1215704: loss 0.7813
[2019-04-10 13:05:18,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1215705: learning rate 0.0000
[2019-04-10 13:05:18,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0410227e-14 1.0000000e+00 8.6998908e-16 1.6075272e-15 1.1412914e-22], sum to 1.0000
[2019-04-10 13:05:18,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8234
[2019-04-10 13:05:18,908] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2755557.115693608 W.
[2019-04-10 13:05:18,912] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.65, 76.0, 1.0, 2.0, 0.6722750550297466, 1.0, 1.0, 0.6567275670291359, 1.0, 1.0, 1.03, 7.005095546171716, 6.9112, 170.5573041426782, 2755557.115693608, 2688295.916544189, 512713.8068548669], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5819400.0000, 
sim time next is 5820000.0000, 
raw observation next is [29.8, 75.33333333333333, 1.0, 2.0, 0.850744355977784, 1.0, 2.0, 0.850744355977784, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2379395.743659821, 2379395.743659821, 445331.4442921679], 
processed observation next is [1.0, 0.34782608695652173, 0.6113744075829385, 0.7533333333333333, 1.0, 1.0, 0.8201739228648, 1.0, 1.0, 0.8201739228648, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.660943262127728, 0.660943262127728, 0.6646737974509969], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5113109], dtype=float32), 0.33919922]. 
=============================================
[2019-04-10 13:05:18,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[46.34533 ]
 [53.859726]
 [57.458015]
 [57.68751 ]
 [57.158707]], R is [[43.38634872]
 [42.95248413]
 [42.52296066]
 [42.65165329]
 [42.81411362]].
[2019-04-10 13:05:19,682] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1216251: loss 1.2572
[2019-04-10 13:05:19,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1216251: learning rate 0.0000
[2019-04-10 13:05:19,687] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1216252: loss 0.7017
[2019-04-10 13:05:19,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1216254: learning rate 0.0000
[2019-04-10 13:05:19,740] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1216285: loss 0.5698
[2019-04-10 13:05:19,741] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1216285: learning rate 0.0000
[2019-04-10 13:05:19,977] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1216430: loss 0.3830
[2019-04-10 13:05:19,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1216430: learning rate 0.0000
[2019-04-10 13:05:20,287] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1216624: loss 0.8964
[2019-04-10 13:05:20,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1216625: learning rate 0.0000
[2019-04-10 13:05:20,378] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1216678: loss 1.0545
[2019-04-10 13:05:20,380] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1216678: learning rate 0.0000
[2019-04-10 13:05:20,457] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1216727: loss 1.2917
[2019-04-10 13:05:20,458] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1216728: learning rate 0.0000
[2019-04-10 13:05:20,563] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1216795: loss 1.4142
[2019-04-10 13:05:20,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1216795: learning rate 0.0000
[2019-04-10 13:05:20,642] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1216841: loss 1.0477
[2019-04-10 13:05:20,646] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1216841: learning rate 0.0000
[2019-04-10 13:05:20,746] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1216907: loss 1.9321
[2019-04-10 13:05:20,748] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1216908: loss 0.5758
[2019-04-10 13:05:20,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1216907: learning rate 0.0000
[2019-04-10 13:05:20,750] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1216908: learning rate 0.0000
[2019-04-10 13:05:20,892] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1216995: loss 0.6800
[2019-04-10 13:05:20,895] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1216995: learning rate 0.0000
[2019-04-10 13:05:21,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1217245: loss 0.6811
[2019-04-10 13:05:21,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1217245: learning rate 0.0000
[2019-04-10 13:05:23,690] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1218727: loss 0.6786
[2019-04-10 13:05:23,692] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1218727: learning rate 0.0000
[2019-04-10 13:05:23,728] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7243585e-12 1.0000000e+00 4.0034369e-12 5.3402971e-08 3.1364365e-19], sum to 1.0000
[2019-04-10 13:05:23,734] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-10 13:05:23,740] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2353986.986746172 W.
[2019-04-10 13:05:23,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3240755e-11 9.9804544e-01 2.2862458e-10 1.9545860e-03 1.4330842e-16], sum to 1.0000
[2019-04-10 13:05:23,743] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.7, 75.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.002183901899631, 6.9112, 168.9123994497417, 2353986.986746172, 2289440.088185272, 476152.5984449629], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5925600.0000, 
sim time next is 5926200.0000, 
raw observation next is [29.73333333333333, 75.16666666666667, 1.0, 2.0, 0.7840918820961471, 1.0, 1.0, 0.7840918820961471, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2192813.002698003, 2192813.002698003, 412201.8440000325], 
processed observation next is [1.0, 0.6086956521739131, 0.6082148499210109, 0.7516666666666667, 1.0, 1.0, 0.7398697374652374, 1.0, 0.5, 0.7398697374652374, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6091147229716675, 0.6091147229716675, 0.6152266328358694], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.13503315], dtype=float32), -0.73805887]. 
=============================================
[2019-04-10 13:05:23,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9550
[2019-04-10 13:05:23,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2313568.740142677 W.
[2019-04-10 13:05:23,761] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 78.5, 1.0, 2.0, 0.8272299152371384, 1.0, 2.0, 0.8272299152371384, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2313568.740142677, 2313568.740142677, 433352.759841319], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5934600.0000, 
sim time next is 5935200.0000, 
raw observation next is [30.33333333333334, 78.66666666666667, 1.0, 2.0, 0.5488685307087079, 1.0, 2.0, 0.5488685307087079, 1.0, 1.0, 0.9532031242442298, 6.911200000000001, 6.9112, 170.5573041426782, 2302575.387371367, 2302575.387371366, 450585.513958793], 
processed observation next is [1.0, 0.6956521739130435, 0.6366508688783573, 0.7866666666666667, 1.0, 1.0, 0.45646810928759984, 1.0, 1.0, 0.45646810928759984, 1.0, 0.5, 0.9429306393222315, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6396042742698241, 0.6396042742698239, 0.6725156924758104], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5498819], dtype=float32), -1.3228048]. 
=============================================
[2019-04-10 13:05:24,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6765104e-22 1.0000000e+00 2.2052289e-25 2.5920817e-30 2.3687428e-35], sum to 1.0000
[2019-04-10 13:05:24,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7322
[2019-04-10 13:05:24,497] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.43333333333333, 83.5, 1.0, 2.0, 0.5542947447851586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774568.4474511578, 774568.4474511571, 192540.3891611593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5947800.0000, 
sim time next is 5948400.0000, 
raw observation next is [28.36666666666667, 84.0, 1.0, 2.0, 0.5538129679943189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773894.9699436781, 773894.9699436788, 192457.2471815583], 
processed observation next is [1.0, 0.8695652173913043, 0.543443917851501, 0.84, 1.0, 1.0, 0.46242526264375766, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21497082498435505, 0.21497082498435524, 0.28724962265904225], 
reward next is 0.7128, 
noisyNet noise sample is [array([-1.0471076], dtype=float32), -0.24158123]. 
=============================================
[2019-04-10 13:05:26,940] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1220735: loss 0.6310
[2019-04-10 13:05:26,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1220735: learning rate 0.0000
[2019-04-10 13:05:28,963] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.8242482e-21 1.0000000e+00 4.3816121e-23 3.1499503e-26 1.5131926e-32], sum to 1.0000
[2019-04-10 13:05:28,968] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7069
[2019-04-10 13:05:28,972] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 90.0, 1.0, 2.0, 0.53962428942875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754060.7566500064, 754060.7566500064, 190037.1389374026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6044400.0000, 
sim time next is 6045000.0000, 
raw observation next is [26.86666666666667, 90.16666666666667, 1.0, 2.0, 0.5391378269468952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753380.7421293079, 753380.7421293079, 189955.2490880575], 
processed observation next is [1.0, 1.0, 0.4723538704581361, 0.9016666666666667, 1.0, 1.0, 0.44474436981553633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2092724283692522, 0.2092724283692522, 0.2835152971463545], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.5077093], dtype=float32), 0.6206233]. 
=============================================
[2019-04-10 13:05:28,984] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[69.17052]
 [69.15102]
 [69.19154]
 [69.23589]
 [69.2782 ]], R is [[69.19178772]
 [69.2162323 ]
 [69.24018097]
 [69.26387024]
 [69.2877121 ]].
[2019-04-10 13:05:31,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1105768e-11 9.9999571e-01 1.7040865e-11 4.2546021e-06 4.3448704e-18], sum to 1.0000
[2019-04-10 13:05:31,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8728
[2019-04-10 13:05:31,091] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2139704.718505108 W.
[2019-04-10 13:05:31,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.7, 66.5, 1.0, 2.0, 0.8890464807347965, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989638118018782, 6.9112, 168.9124897423913, 2139704.718505108, 2084058.171723017, 431641.7793732912], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6103800.0000, 
sim time next is 6104400.0000, 
raw observation next is [30.66666666666666, 66.66666666666667, 1.0, 2.0, 0.7776127558890914, 1.0, 1.0, 0.7776127558890914, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2174674.905630727, 2174674.905630726, 409116.6555025143], 
processed observation next is [1.0, 0.6521739130434783, 0.6524486571879934, 0.6666666666666667, 1.0, 1.0, 0.7320635613121583, 1.0, 0.5, 0.7320635613121583, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6040763626752019, 0.6040763626752016, 0.6106218738843496], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5553945], dtype=float32), -0.45392954]. 
=============================================
[2019-04-10 13:05:31,790] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1223733: loss 0.6427
[2019-04-10 13:05:31,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1223733: learning rate 0.0000
[2019-04-10 13:05:32,478] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1224158: loss 0.6095
[2019-04-10 13:05:32,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1224160: learning rate 0.0000
[2019-04-10 13:05:32,532] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1224192: loss 0.6114
[2019-04-10 13:05:32,534] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1224192: learning rate 0.0000
[2019-04-10 13:05:32,696] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1224291: loss 0.6587
[2019-04-10 13:05:32,697] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1224291: learning rate 0.0000
[2019-04-10 13:05:32,908] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1224428: loss 0.6068
[2019-04-10 13:05:32,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1224428: learning rate 0.0000
[2019-04-10 13:05:33,219] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1224616: loss 0.6137
[2019-04-10 13:05:33,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1224616: learning rate 0.0000
[2019-04-10 13:05:33,313] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1224675: loss 0.6642
[2019-04-10 13:05:33,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1224676: learning rate 0.0000
[2019-04-10 13:05:33,412] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1224735: loss 0.6742
[2019-04-10 13:05:33,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1224738: learning rate 0.0000
[2019-04-10 13:05:33,465] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1224768: loss 0.6073
[2019-04-10 13:05:33,466] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1224768: learning rate 0.0000
[2019-04-10 13:05:33,488] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1224782: loss 0.6613
[2019-04-10 13:05:33,490] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1224784: learning rate 0.0000
[2019-04-10 13:05:33,566] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1224825: loss 0.6636
[2019-04-10 13:05:33,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1224827: learning rate 0.0000
[2019-04-10 13:05:33,738] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1224933: loss 0.6282
[2019-04-10 13:05:33,742] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1224934: learning rate 0.0000
[2019-04-10 13:05:33,848] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1225000: loss 0.5871
[2019-04-10 13:05:33,849] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 13:05:33,850] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1225000: learning rate 0.0000
[2019-04-10 13:05:33,850] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:05:33,851] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:05:33,851] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:05:33,852] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:05:33,852] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:05:33,853] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:05:33,853] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:05:33,853] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:05:33,854] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:05:33,858] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:05:33,872] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run50
[2019-04-10 13:05:33,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run50
[2019-04-10 13:05:33,897] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run50
[2019-04-10 13:05:33,919] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run50
[2019-04-10 13:05:33,939] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run50
[2019-04-10 13:05:41,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:05:41,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.55, 63.0, 1.0, 2.0, 0.6141945159866095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003097.981157165, 1003097.981157165, 218597.144372374]
[2019-04-10 13:05:41,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:05:41,123] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2706378e-19 1.0000000e+00 3.9738607e-22 3.6685878e-26 4.2151530e-31], sampled 0.23721292298189
[2019-04-10 13:05:58,852] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:05:58,854] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 88.0, 1.0, 2.0, 0.3565213025314977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 550268.8231643542, 550268.8231643542, 170697.8565741236]
[2019-04-10 13:05:58,855] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:05:58,857] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4567393e-21 1.0000000e+00 2.8845346e-24 1.9085790e-29 5.5451246e-34], sampled 0.7322262875453919
[2019-04-10 13:06:12,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:06:12,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4904130667426979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685271.7453175467, 685271.7453175472, 182107.3996066342]
[2019-04-10 13:06:12,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:06:12,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.1509482e-21 1.0000000e+00 6.8669159e-24 1.9031273e-28 1.4658583e-33], sampled 0.7872270160277984
[2019-04-10 13:06:21,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:06:21,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [38.36666666666667, 41.66666666666667, 1.0, 2.0, 0.9939914060076241, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993059579688, 6.9112, 168.9122791177017, 2286591.939992985, 2219342.750692775, 461454.247220885]
[2019-04-10 13:06:21,026] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:06:21,029] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4700623e-12 9.9999869e-01 7.0708626e-12 1.3058171e-06 9.3049715e-20], sampled 0.8419804234791878
[2019-04-10 13:06:21,029] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2286591.939992985 W.
[2019-04-10 13:06:24,267] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:06:24,268] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 70.0, 1.0, 2.0, 0.492898536860043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688745.9076437948, 688745.9076437955, 182491.3422056786]
[2019-04-10 13:06:24,270] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:06:24,274] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.0608223e-21 1.0000000e+00 3.3869049e-24 9.7947882e-27 1.5006316e-33], sampled 0.8146653889974648
[2019-04-10 13:06:42,141] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:06:42,141] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.98389500166667, 81.24049792, 1.0, 2.0, 0.5458558154556865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 762771.6995786533, 762771.6995786527, 191094.0442596162]
[2019-04-10 13:06:42,142] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:06:42,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0123122e-21 1.0000000e+00 8.9035173e-24 2.4496837e-28 2.0510889e-33], sampled 0.07544108507516412
[2019-04-10 13:06:53,798] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1254094], dtype=float32), 0.09098818]
[2019-04-10 13:06:53,814] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.85764481, 88.74540913, 1.0, 2.0, 0.566306992299852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791360.5494812609, 791360.5494812609, 194634.1139757028]
[2019-04-10 13:06:53,816] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:06:53,826] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1735066e-20 1.0000000e+00 3.9893662e-23 3.6976389e-28 3.0035397e-32], sampled 0.02097463304154623
[2019-04-10 13:07:05,242] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1747 2927484194.5974 1338.0000
[2019-04-10 13:07:05,455] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0250 3007848960.6402 1766.0000
[2019-04-10 13:07:06,355] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7531 2779356355.4790 933.0000
[2019-04-10 13:07:06,973] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1185 3164063619.1281 1775.0000
[2019-04-10 13:07:07,020] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.4624 2842442767.2995 1129.0000
[2019-04-10 13:07:08,036] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1225000, evaluation results [1225000.0, 7884.118495900312, 3164063619.128071, 1775.0, 8254.174695369224, 2927484194.59741, 1338.0, 8659.753086185065, 2779356355.479036, 933.0, 7998.024995752096, 3007848960.6401815, 1766.0, 8498.462364386241, 2842442767.2994905, 1129.0]
[2019-04-10 13:07:08,747] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1225207: loss 0.5976
[2019-04-10 13:07:08,749] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1225207: learning rate 0.0000
[2019-04-10 13:07:12,700] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0693020e-21 1.0000000e+00 5.4688966e-24 1.0905497e-29 2.1736663e-33], sum to 1.0000
[2019-04-10 13:07:12,701] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5698
[2019-04-10 13:07:12,703] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.0, 1.0, 2.0, 0.5248070539752634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733348.285226549, 733348.2852265484, 187574.3951885303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6233400.0000, 
sim time next is 6234000.0000, 
raw observation next is [26.5, 91.0, 1.0, 2.0, 0.525009648198537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733631.4815570363, 733631.4815570363, 187607.6199639468], 
processed observation next is [0.0, 0.13043478260869565, 0.4549763033175356, 0.91, 1.0, 1.0, 0.42772246770908073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2037865226547323, 0.2037865226547323, 0.2800113730805176], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.3298103], dtype=float32), 0.49056515]. 
=============================================
[2019-04-10 13:07:12,748] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.144806]
 [71.09034 ]
 [71.17309 ]
 [71.152534]
 [71.231384]], R is [[71.15198517]
 [71.1605072 ]
 [71.16895294]
 [71.17729187]
 [71.18561554]].
[2019-04-10 13:07:13,139] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1226597: loss 522.0305
[2019-04-10 13:07:13,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1226597: learning rate 0.0000
[2019-04-10 13:07:16,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1258246e-21 1.0000000e+00 7.5890426e-25 3.5168861e-30 1.4209523e-35], sum to 1.0000
[2019-04-10 13:07:16,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5662
[2019-04-10 13:07:16,691] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.46666666666667, 63.0, 1.0, 2.0, 0.5044110230729163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704838.0802898043, 704838.0802898037, 184290.20026298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6280800.0000, 
sim time next is 6281400.0000, 
raw observation next is [30.43333333333333, 63.0, 1.0, 2.0, 0.5033983038012505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703422.489804685, 703422.4898046857, 184130.3470710506], 
processed observation next is [0.0, 0.6956521739130435, 0.6413902053712479, 0.63, 1.0, 1.0, 0.40168470337500056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19539513605685693, 0.19539513605685713, 0.2748214135388815], 
reward next is 0.7252, 
noisyNet noise sample is [array([-0.5140139], dtype=float32), -0.09230288]. 
=============================================
[2019-04-10 13:07:18,841] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1228725: loss 756.8066
[2019-04-10 13:07:18,842] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1228725: learning rate 0.0000
[2019-04-10 13:07:22,191] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1719057e-21 1.0000000e+00 7.5272846e-25 5.1168221e-31 7.0575843e-35], sum to 1.0000
[2019-04-10 13:07:22,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3455
[2019-04-10 13:07:22,203] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 71.0, 1.0, 2.0, 0.5316283261536625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742883.4419142863, 742883.4419142863, 188700.2660037298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6342000.0000, 
sim time next is 6342600.0000, 
raw observation next is [29.96666666666667, 70.5, 1.0, 2.0, 0.5333704921153345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745318.7528925957, 745318.7528925957, 188990.0026327585], 
processed observation next is [0.0, 0.391304347826087, 0.6192733017377569, 0.705, 1.0, 1.0, 0.4377957736329331, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20703298691460992, 0.20703298691460992, 0.28207463079516193], 
reward next is 0.7179, 
noisyNet noise sample is [array([-1.604305], dtype=float32), -1.000025]. 
=============================================
[2019-04-10 13:07:24,483] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1231801: loss 682.8442
[2019-04-10 13:07:24,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1231802: learning rate 0.0000
[2019-04-10 13:07:25,062] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1232169: loss 961.2885
[2019-04-10 13:07:25,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1232169: learning rate 0.0000
[2019-04-10 13:07:25,319] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1232336: loss 795.3531
[2019-04-10 13:07:25,320] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1232336: learning rate 0.0000
[2019-04-10 13:07:25,568] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1232497: loss 590.6280
[2019-04-10 13:07:25,570] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1232497: loss 862.2273
[2019-04-10 13:07:25,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1232497: learning rate 0.0000
[2019-04-10 13:07:25,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1232497: learning rate 0.0000
[2019-04-10 13:07:25,677] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1232565: loss 680.3646
[2019-04-10 13:07:25,679] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1232565: learning rate 0.0000
[2019-04-10 13:07:25,795] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1232640: loss 737.5829
[2019-04-10 13:07:25,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1232640: learning rate 0.0000
[2019-04-10 13:07:25,912] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1232716: loss 741.7146
[2019-04-10 13:07:25,914] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1232716: learning rate 0.0000
[2019-04-10 13:07:25,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3379446e-21 1.0000000e+00 9.8654462e-25 1.4175242e-29 4.7334325e-35], sum to 1.0000
[2019-04-10 13:07:25,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7284
[2019-04-10 13:07:25,986] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 74.0, 1.0, 2.0, 0.5057182101791148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706665.2840431288, 706665.2840431288, 184496.881369619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6463200.0000, 
sim time next is 6463800.0000, 
raw observation next is [28.35, 74.5, 1.0, 2.0, 0.504474665420717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704927.0403484493, 704927.0403484493, 184300.1684257758], 
processed observation next is [1.0, 0.8260869565217391, 0.5426540284360191, 0.745, 1.0, 1.0, 0.4029815246032734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19581306676345814, 0.19581306676345814, 0.27507487824742655], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.6524073], dtype=float32), 0.42283723]. 
=============================================
[2019-04-10 13:07:26,023] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1232787: loss 672.4305
[2019-04-10 13:07:26,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1232788: learning rate 0.0000
[2019-04-10 13:07:26,070] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1232815: loss 654.8568
[2019-04-10 13:07:26,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1232815: learning rate 0.0000
[2019-04-10 13:07:26,148] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1232866: loss 538.2766
[2019-04-10 13:07:26,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1232866: learning rate 0.0000
[2019-04-10 13:07:26,165] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1232875: loss 669.9763
[2019-04-10 13:07:26,167] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1232875: learning rate 0.0000
[2019-04-10 13:07:26,392] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1233024: loss 789.5768
[2019-04-10 13:07:26,393] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1233024: learning rate 0.0000
[2019-04-10 13:07:26,820] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1233302: loss 619.7635
[2019-04-10 13:07:26,821] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1233302: learning rate 0.0000
[2019-04-10 13:07:28,775] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1234579: loss 0.0053
[2019-04-10 13:07:28,778] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1234579: learning rate 0.0000
[2019-04-10 13:07:30,460] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2846203e-20 1.0000000e+00 5.9626929e-23 4.9380717e-27 2.0956731e-32], sum to 1.0000
[2019-04-10 13:07:30,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7120
[2019-04-10 13:07:30,468] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 81.0, 1.0, 2.0, 0.5070527216805558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708530.6854465901, 708530.6854465901, 184708.7648661657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6559200.0000, 
sim time next is 6559800.0000, 
raw observation next is [27.45, 81.66666666666667, 1.0, 2.0, 0.5095254918336827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711987.1715569719, 711987.1715569724, 185102.2386850603], 
processed observation next is [1.0, 0.9565217391304348, 0.5, 0.8166666666666668, 1.0, 1.0, 0.40906685763094297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19777421432138106, 0.19777421432138123, 0.27627199803740343], 
reward next is 0.7237, 
noisyNet noise sample is [array([0.6862647], dtype=float32), -0.77664286]. 
=============================================
[2019-04-10 13:07:32,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8693703e-22 1.0000000e+00 4.7113889e-25 1.0041546e-31 5.9032695e-35], sum to 1.0000
[2019-04-10 13:07:32,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6354
[2019-04-10 13:07:32,349] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 83.66666666666667, 1.0, 2.0, 0.3446965865270571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534664.7639009362, 534664.7639009355, 169486.1890551904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6844200.0000, 
sim time next is 6844800.0000, 
raw observation next is [23.2, 83.33333333333334, 1.0, 2.0, 0.3474344319468126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538128.9982267176, 538128.9982267176, 169746.7032450415], 
processed observation next is [0.0, 0.21739130434782608, 0.29857819905213273, 0.8333333333333335, 1.0, 1.0, 0.21377642403230435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14948027728519933, 0.14948027728519933, 0.2533532884254351], 
reward next is 0.7466, 
noisyNet noise sample is [array([-0.8969815], dtype=float32), -0.6326604]. 
=============================================
[2019-04-10 13:07:32,368] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1236801: loss 0.0067
[2019-04-10 13:07:32,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1236801: learning rate 0.0000
[2019-04-10 13:07:35,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7737619e-21 1.0000000e+00 4.8644153e-24 2.7602342e-31 7.4199666e-35], sum to 1.0000
[2019-04-10 13:07:35,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0721
[2019-04-10 13:07:35,938] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 82.66666666666667, 1.0, 2.0, 0.3399067465857903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 529475.2795447053, 529475.279544706, 169128.0409553437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6841200.0000, 
sim time next is 6841800.0000, 
raw observation next is [23.05, 83.0, 1.0, 2.0, 0.3408964043393971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530618.8737611003, 530618.8737611009, 169209.3866058711], 
processed observation next is [0.0, 0.17391304347826086, 0.2914691943127963, 0.83, 1.0, 1.0, 0.20589928233662302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14739413160030565, 0.14739413160030582, 0.2525513232923449], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.09240104], dtype=float32), 0.4273009]. 
=============================================
[2019-04-10 13:07:37,245] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1239803: loss 0.0041
[2019-04-10 13:07:37,248] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1239803: learning rate 0.0000
[2019-04-10 13:07:37,668] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1240059: loss 0.0027
[2019-04-10 13:07:37,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1240061: learning rate 0.0000
[2019-04-10 13:07:38,161] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1240357: loss 0.0021
[2019-04-10 13:07:38,163] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1240358: learning rate 0.0000
[2019-04-10 13:07:38,276] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1240427: loss 0.0020
[2019-04-10 13:07:38,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1240427: learning rate 0.0000
[2019-04-10 13:07:38,348] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1240472: loss 0.0020
[2019-04-10 13:07:38,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1240476: learning rate 0.0000
[2019-04-10 13:07:38,571] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1240609: loss 0.0020
[2019-04-10 13:07:38,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1240611: learning rate 0.0000
[2019-04-10 13:07:38,727] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1240706: loss 0.0020
[2019-04-10 13:07:38,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1240706: learning rate 0.0000
[2019-04-10 13:07:38,775] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1240736: loss 0.0020
[2019-04-10 13:07:38,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1240736: learning rate 0.0000
[2019-04-10 13:07:38,931] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1240828: loss 0.0020
[2019-04-10 13:07:38,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1240830: learning rate 0.0000
[2019-04-10 13:07:39,043] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1240899: loss 0.0020
[2019-04-10 13:07:39,044] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1240899: learning rate 0.0000
[2019-04-10 13:07:39,051] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1240901: loss 0.0020
[2019-04-10 13:07:39,052] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1240901: learning rate 0.0000
[2019-04-10 13:07:39,065] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1240909: loss 0.0019
[2019-04-10 13:07:39,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1240909: learning rate 0.0000
[2019-04-10 13:07:39,166] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1240975: loss 0.0020
[2019-04-10 13:07:39,168] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1240976: learning rate 0.0000
[2019-04-10 13:07:39,787] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1241358: loss 0.0020
[2019-04-10 13:07:39,793] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1241361: learning rate 0.0000
[2019-04-10 13:07:39,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2031702e-18 1.0000000e+00 8.4685678e-20 3.1160627e-22 1.6078784e-28], sum to 1.0000
[2019-04-10 13:07:39,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-10 13:07:39,977] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.63333333333334, 50.00000000000001, 1.0, 2.0, 0.8191776710525683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1276319.984959819, 1276319.984959818, 265824.3529516124], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6783600.0000, 
sim time next is 6784200.0000, 
raw observation next is [28.7, 49.5, 1.0, 2.0, 0.7831063574919571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1221370.739016451, 1221370.739016451, 256043.5431172958], 
processed observation next is [1.0, 0.5217391304347826, 0.5592417061611374, 0.495, 1.0, 1.0, 0.7386823584240447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3392696497267919, 0.3392696497267919, 0.38215454196611315], 
reward next is 0.6178, 
noisyNet noise sample is [array([0.30637673], dtype=float32), -0.078822926]. 
=============================================
[2019-04-10 13:07:41,285] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1242281: loss -142.7126
[2019-04-10 13:07:41,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1242281: learning rate 0.0000
[2019-04-10 13:07:45,281] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1244747: loss -145.9966
[2019-04-10 13:07:45,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1244748: learning rate 0.0000
[2019-04-10 13:07:47,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8114866e-21 1.0000000e+00 1.5480376e-24 3.8262623e-30 2.0926991e-34], sum to 1.0000
[2019-04-10 13:07:47,503] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2737
[2019-04-10 13:07:47,507] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.85, 59.5, 1.0, 2.0, 0.4562591219867195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646686.5410719265, 646686.5410719265, 178207.5732842541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6951000.0000, 
sim time next is 6951600.0000, 
raw observation next is [30.0, 59.0, 1.0, 2.0, 0.4585956939910301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648770.898782866, 648770.8987828653, 178392.1008403349], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.59, 1.0, 1.0, 0.3477056554108797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1802141385507961, 0.1802141385507959, 0.26625686692587297], 
reward next is 0.7337, 
noisyNet noise sample is [array([-1.4269719], dtype=float32), -0.5511468]. 
=============================================
[2019-04-10 13:07:48,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8184634e-22 1.0000000e+00 2.2896236e-24 8.2550458e-30 8.6604035e-35], sum to 1.0000
[2019-04-10 13:07:48,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8076
[2019-04-10 13:07:48,530] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 57.33333333333334, 1.0, 2.0, 0.3851901583559446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580746.5785393581, 580746.5785393575, 172959.3371488834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6985200.0000, 
sim time next is 6985800.0000, 
raw observation next is [28.35, 58.5, 1.0, 2.0, 0.3891835891993862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584786.8259990885, 584786.8259990878, 173265.0750511373], 
processed observation next is [0.0, 0.8695652173913043, 0.5426540284360191, 0.585, 1.0, 1.0, 0.2640766134932364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1624407849997468, 0.1624407849997466, 0.25860458962856314], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.12631865], dtype=float32), -0.9861988]. 
=============================================
[2019-04-10 13:07:50,234] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1247800: loss -248.5416
[2019-04-10 13:07:50,236] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1247801: learning rate 0.0000
[2019-04-10 13:07:50,877] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1248199: loss -230.3530
[2019-04-10 13:07:50,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1248199: learning rate 0.0000
[2019-04-10 13:07:51,306] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1248460: loss -146.9013
[2019-04-10 13:07:51,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1248460: learning rate 0.0000
[2019-04-10 13:07:51,327] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1248470: loss -54.2297
[2019-04-10 13:07:51,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1248470: learning rate 0.0000
[2019-04-10 13:07:51,396] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1248513: loss -122.0802
[2019-04-10 13:07:51,399] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1248513: learning rate 0.0000
[2019-04-10 13:07:51,565] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1248615: loss -38.7407
[2019-04-10 13:07:51,568] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1248616: learning rate 0.0000
[2019-04-10 13:07:51,614] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1248647: loss 58.4974
[2019-04-10 13:07:51,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1248648: learning rate 0.0000
[2019-04-10 13:07:51,650] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1248666: loss -126.7281
[2019-04-10 13:07:51,651] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1248666: learning rate 0.0000
[2019-04-10 13:07:51,821] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.8178336e-12 1.0000000e+00 3.8417568e-12 4.6195179e-08 9.1301624e-20], sum to 1.0000
[2019-04-10 13:07:51,828] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9065
[2019-04-10 13:07:51,832] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1248779: loss -144.2838
[2019-04-10 13:07:51,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1248779: learning rate 0.0000
[2019-04-10 13:07:51,837] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1970524.835154214 W.
[2019-04-10 13:07:51,842] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 55.0, 1.0, 2.0, 0.7046805559976466, 1.0, 2.0, 0.7046805559976466, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1970524.835154214, 1970524.835154214, 376118.3994372345], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7053600.0000, 
sim time next is 7054200.0000, 
raw observation next is [30.3, 56.5, 1.0, 2.0, 0.7251149241519323, 1.0, 2.0, 0.7251149241519323, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2027720.30692389, 2027720.30692389, 385045.377970401], 
processed observation next is [1.0, 0.6521739130434783, 0.6350710900473934, 0.565, 1.0, 1.0, 0.6688131616288341, 1.0, 1.0, 0.6688131616288341, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5632556408121917, 0.5632556408121917, 0.5746945939856731], 
reward next is 0.4253, 
noisyNet noise sample is [array([-0.41860595], dtype=float32), 0.31754708]. 
=============================================
[2019-04-10 13:07:51,916] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1248829: loss -180.0254
[2019-04-10 13:07:51,918] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1248830: learning rate 0.0000
[2019-04-10 13:07:51,947] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1248848: loss -160.0526
[2019-04-10 13:07:51,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1248848: learning rate 0.0000
[2019-04-10 13:07:52,009] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1248886: loss -173.9553
[2019-04-10 13:07:52,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1248886: learning rate 0.0000
[2019-04-10 13:07:52,069] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1248922: loss -208.3395
[2019-04-10 13:07:52,074] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1248923: learning rate 0.0000
[2019-04-10 13:07:52,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6547161e-22 1.0000000e+00 1.3050006e-25 6.9727978e-30 3.7915827e-36], sum to 1.0000
[2019-04-10 13:07:52,590] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1698
[2019-04-10 13:07:52,594] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 82.0, 1.0, 2.0, 0.4842626667678272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676674.833879201, 676674.8338792004, 181167.0911568115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7068000.0000, 
sim time next is 7068600.0000, 
raw observation next is [26.5, 83.0, 1.0, 2.0, 0.4855467171598969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678469.6492092324, 678469.6492092324, 181362.4945765904], 
processed observation next is [1.0, 0.8260869565217391, 0.4549763033175356, 0.83, 1.0, 1.0, 0.3801767676625264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.188463791447009, 0.188463791447009, 0.2706902904128215], 
reward next is 0.7293, 
noisyNet noise sample is [array([1.0522738], dtype=float32), 0.32682845]. 
=============================================
[2019-04-10 13:07:52,756] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1249349: loss -21.2937
[2019-04-10 13:07:52,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1249350: learning rate 0.0000
[2019-04-10 13:07:53,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6698196e-20 1.0000000e+00 1.3259031e-21 1.5402144e-25 1.2112695e-31], sum to 1.0000
[2019-04-10 13:07:53,087] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5401
[2019-04-10 13:07:53,090] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 91.5, 1.0, 2.0, 0.4737144679098386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663581.705564096, 663581.7055640966, 179793.25310131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7090200.0000, 
sim time next is 7090800.0000, 
raw observation next is [24.86666666666667, 91.66666666666666, 1.0, 2.0, 0.4726774598765522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662464.3345980225, 662464.3345980218, 179681.9748932617], 
processed observation next is [1.0, 0.043478260869565216, 0.3775671406003162, 0.9166666666666665, 1.0, 1.0, 0.36467163840548456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18401787072167292, 0.18401787072167272, 0.2681820520794951], 
reward next is 0.7318, 
noisyNet noise sample is [array([0.17415419], dtype=float32), -1.0197042]. 
=============================================
[2019-04-10 13:07:53,818] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-10 13:07:53,820] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:07:53,821] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:07:53,822] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:07:53,824] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:07:53,823] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:07:53,824] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:07:53,825] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:07:53,827] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:07:53,825] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:07:53,829] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:07:53,845] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run51
[2019-04-10 13:07:53,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run51
[2019-04-10 13:07:53,880] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run51
[2019-04-10 13:07:53,881] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run51
[2019-04-10 13:07:53,920] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run51
[2019-04-10 13:08:06,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:06,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.96666666666667, 91.33333333333333, 1.0, 2.0, 0.3487223212053867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542666.7720986891, 542666.7720986891, 170182.8044519482]
[2019-04-10 13:08:06,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:08:06,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1256165e-21 1.0000000e+00 1.4343692e-23 4.0317475e-29 2.5452704e-33], sampled 0.9573192654851034
[2019-04-10 13:08:12,543] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:12,544] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.33333333333334, 91.66666666666667, 1.0, 2.0, 0.3188246731998339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503774.7514364708, 503774.7514364708, 167301.3087464913]
[2019-04-10 13:08:12,545] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:08:12,548] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2906667e-21 1.0000000e+00 4.6634522e-24 1.3635941e-29 3.2471266e-34], sampled 0.12353019988470282
[2019-04-10 13:08:18,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:18,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.52987165333333, 74.91943655666667, 1.0, 2.0, 0.4813094620719867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 672546.9255685756, 672546.9255685763, 180721.3193207001]
[2019-04-10 13:08:18,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:08:18,983] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9394796e-21 1.0000000e+00 7.7904424e-24 2.3792019e-28 1.0122687e-33], sampled 0.04166142306041809
[2019-04-10 13:08:22,413] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:22,414] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.66666666666666, 90.66666666666667, 1.0, 2.0, 0.411376980288663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608200.7687486969, 608200.7687486969, 175139.2544711489]
[2019-04-10 13:08:22,415] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:08:22,417] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.5690436e-21 1.0000000e+00 7.9811219e-24 3.7209701e-29 7.0006621e-34], sampled 0.7618876574160518
[2019-04-10 13:08:23,381] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:23,383] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.03333333333333, 83.66666666666666, 1.0, 2.0, 0.492803915151457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688613.6462699765, 688613.6462699765, 182475.02472677]
[2019-04-10 13:08:23,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:08:23,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2527270e-21 1.0000000e+00 7.8677342e-24 5.8669634e-29 6.0890130e-34], sampled 0.8517921711017918
[2019-04-10 13:08:38,742] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:38,976] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.5371087, 49.93419927666667, 1.0, 2.0, 0.7985562868836728, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992324906013929, 6.9112, 168.912399541178, 2013052.888543083, 1955500.277886149, 407995.0279183809]
[2019-04-10 13:08:38,976] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:08:38,978] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0059437e-13 1.0000000e+00 2.1583569e-13 6.1193006e-10 7.9479223e-22], sampled 0.904135409458912
[2019-04-10 13:08:38,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2013052.888543083 W.
[2019-04-10 13:08:39,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:39,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.56700760833333, 69.62512658, 1.0, 2.0, 0.5365086299932609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749705.4567628608, 749705.4567628608, 189512.5537386514]
[2019-04-10 13:08:39,251] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:08:39,253] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.7914009e-21 1.0000000e+00 1.8995636e-23 1.1906384e-27 1.7515806e-33], sampled 0.8635306817368226
[2019-04-10 13:08:39,596] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:39,597] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.8, 43.66666666666667, 1.0, 2.0, 0.84603987186991, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.998199878273319, 6.9112, 168.9123700004611, 2079509.950392281, 2017789.45675091, 419828.7719284105]
[2019-04-10 13:08:39,598] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:08:39,599] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8450458e-13 1.0000000e+00 2.3889785e-13 5.3021504e-10 4.6677998e-22], sampled 0.7476842431679585
[2019-04-10 13:08:39,600] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2079509.950392281 W.
[2019-04-10 13:08:57,096] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:08:57,098] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.88092801666667, 83.71565587333333, 1.0, 2.0, 0.7853401679040412, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005980834564051, 6.9112, 168.9119209193774, 1994556.622604878, 1927316.248708145, 404254.4013335482]
[2019-04-10 13:08:57,100] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:08:57,102] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.5766102e-17 1.0000000e+00 2.4742552e-18 2.6285303e-19 2.2567298e-27], sampled 0.11094518066608428
[2019-04-10 13:08:57,103] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1994556.622604878 W.
[2019-04-10 13:09:04,144] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:09:04,144] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.75, 83.0, 1.0, 2.0, 0.4710225647949881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658168.3086919601, 658168.3086919601, 179181.6863837925]
[2019-04-10 13:09:04,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:09:04,146] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2150572e-20 1.0000000e+00 4.6308363e-23 6.1062909e-28 1.0948706e-32], sampled 0.8768003670883591
[2019-04-10 13:09:14,337] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12381238], dtype=float32), 0.09136023]
[2019-04-10 13:09:14,338] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.13333333333333, 72.0, 1.0, 2.0, 0.4132027185178007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 615261.6127524301, 615261.6127524308, 175920.7956607536]
[2019-04-10 13:09:14,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:09:14,342] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.8982353e-21 1.0000000e+00 1.0257115e-23 6.8965890e-29 9.3373456e-34], sampled 0.9914163550933899
[2019-04-10 13:09:15,981] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.2769 3163825844.7317 1771.0000
[2019-04-10 13:09:16,357] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-04-10 13:09:16,408] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-04-10 13:09:16,601] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0250 3007848960.6402 1766.0000
[2019-04-10 13:09:16,678] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5487 2842486287.6268 1129.0000
[2019-04-10 13:09:17,696] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1250000, evaluation results [1250000.0, 7886.276940015828, 3163825844.7316647, 1771.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.024995752096, 3007848960.6401815, 1766.0, 8497.548680688244, 2842486287.6267514, 1129.0]
[2019-04-10 13:09:17,898] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6139200e-20 1.0000000e+00 1.6724699e-22 4.6116997e-27 2.6870816e-32], sum to 1.0000
[2019-04-10 13:09:17,947] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7836
[2019-04-10 13:09:17,956] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4868725111580744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690298.6169674994, 690298.6169674994, 182844.1648852561], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7099200.0000, 
sim time next is 7099800.0000, 
raw observation next is [24.25, 94.16666666666667, 1.0, 2.0, 0.5966713864400905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846837.2712282994, 846837.2712282994, 201808.3221804269], 
processed observation next is [1.0, 0.17391304347826086, 0.3483412322274882, 0.9416666666666668, 1.0, 1.0, 0.514061911373603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23523257534119427, 0.23523257534119427, 0.30120645101556254], 
reward next is 0.6988, 
noisyNet noise sample is [array([-0.37090218], dtype=float32), -0.10608802]. 
=============================================
[2019-04-10 13:09:18,556] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1250292: loss 0.1159
[2019-04-10 13:09:18,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1250297: learning rate 0.0000
[2019-04-10 13:09:24,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3317230e-20 1.0000000e+00 3.5038825e-23 2.4024780e-27 9.0120707e-33], sum to 1.0000
[2019-04-10 13:09:24,741] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1824
[2019-04-10 13:09:24,767] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 90.5, 1.0, 2.0, 0.4939772251370141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690253.6895862136, 690253.6895862136, 182658.1550271027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7187400.0000, 
sim time next is 7188000.0000, 
raw observation next is [25.8, 90.66666666666667, 1.0, 2.0, 0.4922457907611658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687833.5053485857, 687833.5053485857, 182390.561768061], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.9066666666666667, 1.0, 1.0, 0.38824794067610335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19106486259682934, 0.19106486259682934, 0.27222471905680745], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.2868291], dtype=float32), -0.700577]. 
=============================================
[2019-04-10 13:09:24,826] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.07008]
 [68.92012]
 [68.69277]
 [68.47034]
 [68.50471]], R is [[69.21014404]
 [69.24542236]
 [69.27890015]
 [69.30448151]
 [69.31826019]].
[2019-04-10 13:09:25,885] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1252784: loss 0.0807
[2019-04-10 13:09:25,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1252784: learning rate 0.0000
[2019-04-10 13:09:29,248] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1314077e-22 1.0000000e+00 9.9124110e-25 1.0972969e-30 6.6189183e-35], sum to 1.0000
[2019-04-10 13:09:29,249] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7233
[2019-04-10 13:09:29,253] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.78333333333333, 78.5, 1.0, 2.0, 0.4177501772966061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608701.117699015, 608701.117699015, 174927.5002061765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7495800.0000, 
sim time next is 7496400.0000, 
raw observation next is [25.66666666666667, 79.0, 1.0, 2.0, 0.4158893333616966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606903.4914427524, 606903.4914427524, 174784.5962774617], 
processed observation next is [0.0, 0.782608695652174, 0.4154818325434442, 0.79, 1.0, 1.0, 0.296252208869514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16858430317854234, 0.16858430317854234, 0.26087253175740555], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.92845005], dtype=float32), -0.29213148]. 
=============================================
[2019-04-10 13:09:32,029] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8892460e-21 1.0000000e+00 1.2948507e-24 8.8507645e-30 6.9191251e-35], sum to 1.0000
[2019-04-10 13:09:32,052] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6726
[2019-04-10 13:09:32,059] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 87.0, 1.0, 2.0, 0.3220762306654523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507762.1145266998, 507762.1145267004, 167580.1688882732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7282800.0000, 
sim time next is 7283400.0000, 
raw observation next is [22.05, 86.66666666666667, 1.0, 2.0, 0.3440939055081151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542383.9099507106, 542383.9099507106, 170296.9382657273], 
processed observation next is [1.0, 0.30434782608695654, 0.24407582938388633, 0.8666666666666667, 1.0, 1.0, 0.20975169338327118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15066219720853072, 0.15066219720853072, 0.25417453472496615], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.36245075], dtype=float32), -0.96712846]. 
=============================================
[2019-04-10 13:09:33,723] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1255630: loss 0.0352
[2019-04-10 13:09:33,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1255630: learning rate 0.0000
[2019-04-10 13:09:35,459] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1256264: loss 0.0265
[2019-04-10 13:09:35,474] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1256267: learning rate 0.0000
[2019-04-10 13:09:35,485] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1256275: loss 0.0282
[2019-04-10 13:09:35,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1256276: learning rate 0.0000
[2019-04-10 13:09:35,874] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1256439: loss 0.0293
[2019-04-10 13:09:35,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1256439: learning rate 0.0000
[2019-04-10 13:09:35,902] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1256450: loss 0.0306
[2019-04-10 13:09:35,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1256450: learning rate 0.0000
[2019-04-10 13:09:36,008] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1256482: loss 0.0291
[2019-04-10 13:09:36,018] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1256490: learning rate 0.0000
[2019-04-10 13:09:36,291] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1256594: loss 0.0298
[2019-04-10 13:09:36,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1256598: learning rate 0.0000
[2019-04-10 13:09:36,592] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1256722: loss 0.0341
[2019-04-10 13:09:36,599] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1256724: learning rate 0.0000
[2019-04-10 13:09:36,849] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1256802: loss 0.0324
[2019-04-10 13:09:36,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1256802: learning rate 0.0000
[2019-04-10 13:09:37,029] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1256883: loss 0.0330
[2019-04-10 13:09:37,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1256888: learning rate 0.0000
[2019-04-10 13:09:37,101] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1256918: loss 0.0345
[2019-04-10 13:09:37,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1256918: learning rate 0.0000
[2019-04-10 13:09:37,214] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1256963: loss 0.0359
[2019-04-10 13:09:37,230] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1256968: learning rate 0.0000
[2019-04-10 13:09:37,356] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1257027: loss 0.0351
[2019-04-10 13:09:37,359] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1257027: learning rate 0.0000
[2019-04-10 13:09:37,385] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3299539e-20 1.0000000e+00 1.4223879e-22 4.1130118e-27 5.1726929e-32], sum to 1.0000
[2019-04-10 13:09:37,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2794
[2019-04-10 13:09:37,432] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 92.66666666666667, 1.0, 2.0, 0.5826960461003439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 929857.5751164259, 929857.5751164252, 210624.8334735184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7378800.0000, 
sim time next is 7379400.0000, 
raw observation next is [20.83333333333334, 92.83333333333333, 1.0, 2.0, 0.5834743065123338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 929380.7455296658, 929380.7455296658, 210651.7969317116], 
processed observation next is [1.0, 0.391304347826087, 0.1864139020537128, 0.9283333333333332, 1.0, 1.0, 0.49816181507510093, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25816131820268495, 0.25816131820268495, 0.31440566706225614], 
reward next is 0.6856, 
noisyNet noise sample is [array([0.21232288], dtype=float32), -0.8666679]. 
=============================================
[2019-04-10 13:09:38,183] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1134503e-21 1.0000000e+00 3.6003939e-24 6.8109098e-30 1.0407619e-34], sum to 1.0000
[2019-04-10 13:09:38,201] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8248
[2019-04-10 13:09:38,207] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 92.0, 1.0, 2.0, 0.3077772176570869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497613.4900006487, 497613.4900006493, 166909.734932802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7372800.0000, 
sim time next is 7373400.0000, 
raw observation next is [20.16666666666667, 92.0, 1.0, 2.0, 0.3352619262176988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542173.378754787, 542173.3787547864, 170283.4793762723], 
processed observation next is [1.0, 0.34782608695652173, 0.15481832543443946, 0.92, 1.0, 1.0, 0.19911075447915516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15060371632077418, 0.15060371632077402, 0.2541544468302572], 
reward next is 0.7458, 
noisyNet noise sample is [array([0.17712796], dtype=float32), 1.6042141]. 
=============================================
[2019-04-10 13:09:38,556] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1257442: loss 0.0376
[2019-04-10 13:09:38,572] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1257449: learning rate 0.0000
[2019-04-10 13:09:40,275] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1258100: loss -13.7142
[2019-04-10 13:09:40,284] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1258102: learning rate 0.0000
[2019-04-10 13:09:44,260] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1107027e-22 1.0000000e+00 7.4020806e-25 5.7118432e-31 2.6597566e-35], sum to 1.0000
[2019-04-10 13:09:44,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5859
[2019-04-10 13:09:44,281] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 80.33333333333334, 1.0, 2.0, 0.4055258622927768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596066.4046566807, 596066.4046566807, 173904.4246032597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7480200.0000, 
sim time next is 7480800.0000, 
raw observation next is [25.4, 80.0, 1.0, 2.0, 0.4075194854067613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 597900.8704023627, 597900.8704023634, 174040.2020620153], 
processed observation next is [0.0, 0.6086956521739131, 0.4028436018957346, 0.8, 1.0, 1.0, 0.28616805470694134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1660835751117674, 0.1660835751117676, 0.2597614956149482], 
reward next is 0.7402, 
noisyNet noise sample is [array([1.334236], dtype=float32), -3.1572104]. 
=============================================
[2019-04-10 13:09:47,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8961075e-22 1.0000000e+00 2.4001732e-24 5.0541066e-30 4.4534726e-35], sum to 1.0000
[2019-04-10 13:09:47,291] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1260664: loss -95.8587
[2019-04-10 13:09:47,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1260666: learning rate 0.0000
[2019-04-10 13:09:47,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6244
[2019-04-10 13:09:47,336] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 92.16666666666667, 1.0, 2.0, 0.4042794679116462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595490.0639916938, 595490.0639916938, 173890.4112398252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7513800.0000, 
sim time next is 7514400.0000, 
raw observation next is [23.6, 92.33333333333334, 1.0, 2.0, 0.405005078572162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 596095.3343462404, 596095.334346241, 173931.9760749373], 
processed observation next is [0.0, 1.0, 0.3175355450236968, 0.9233333333333335, 1.0, 1.0, 0.28313864888212287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1655820373184001, 0.16558203731840027, 0.25959996429095117], 
reward next is 0.7404, 
noisyNet noise sample is [array([-0.49279886], dtype=float32), 0.057700854]. 
=============================================
[2019-04-10 13:09:52,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7561138e-19 1.0000000e+00 3.7418812e-21 1.3131961e-24 7.8306416e-30], sum to 1.0000
[2019-04-10 13:09:52,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9237
[2019-04-10 13:09:52,135] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 94.5, 1.0, 2.0, 0.4544063925774371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647771.0579292617, 647771.0579292617, 178411.5992356181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7608600.0000, 
sim time next is 7609200.0000, 
raw observation next is [24.03333333333333, 94.66666666666666, 1.0, 2.0, 0.451311847112216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644604.2572524298, 644604.2572524291, 178118.7040082693], 
processed observation next is [1.0, 0.043478260869565216, 0.3380726698262243, 0.9466666666666665, 1.0, 1.0, 0.33892993627977835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17905673812567496, 0.17905673812567477, 0.26584881195264076], 
reward next is 0.7342, 
noisyNet noise sample is [array([-0.21978651], dtype=float32), 0.2117698]. 
=============================================
[2019-04-10 13:09:52,157] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1263593: loss -93.9587
[2019-04-10 13:09:52,159] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1263593: learning rate 0.0000
[2019-04-10 13:09:53,085] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1264193: loss -141.1990
[2019-04-10 13:09:53,087] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1264193: learning rate 0.0000
[2019-04-10 13:09:53,231] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1264285: loss -49.1821
[2019-04-10 13:09:53,232] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1264285: learning rate 0.0000
[2019-04-10 13:09:53,337] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1264354: loss -170.0645
[2019-04-10 13:09:53,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1264357: learning rate 0.0000
[2019-04-10 13:09:53,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1264481: loss 156.6351
[2019-04-10 13:09:53,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1264482: learning rate 0.0000
[2019-04-10 13:09:53,755] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1264624: loss -257.9851
[2019-04-10 13:09:53,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1264624: learning rate 0.0000
[2019-04-10 13:09:53,787] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1264639: loss -65.6339
[2019-04-10 13:09:53,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1264640: learning rate 0.0000
[2019-04-10 13:09:53,867] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1264694: loss -68.9890
[2019-04-10 13:09:53,871] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1264696: learning rate 0.0000
[2019-04-10 13:09:54,071] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1264825: loss -45.8527
[2019-04-10 13:09:54,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1264825: learning rate 0.0000
[2019-04-10 13:09:54,103] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1264848: loss -81.4112
[2019-04-10 13:09:54,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1264848: learning rate 0.0000
[2019-04-10 13:09:54,135] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1264867: loss -257.2285
[2019-04-10 13:09:54,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1264867: learning rate 0.0000
[2019-04-10 13:09:54,275] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1264955: loss 60.7955
[2019-04-10 13:09:54,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1264956: learning rate 0.0000
[2019-04-10 13:09:54,489] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1265096: loss -54.3960
[2019-04-10 13:09:54,492] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1265096: learning rate 0.0000
[2019-04-10 13:09:54,924] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1265390: loss -195.9433
[2019-04-10 13:09:54,924] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1265390: learning rate 0.0000
[2019-04-10 13:09:55,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3697112e-19 1.0000000e+00 4.1522310e-21 2.5350012e-24 2.1647740e-30], sum to 1.0000
[2019-04-10 13:09:55,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7307
[2019-04-10 13:09:55,604] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 95.0, 1.0, 2.0, 0.5683668133518734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794240.0310704067, 794240.0310704067, 194994.3968573486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7699800.0000, 
sim time next is 7700400.0000, 
raw observation next is [24.6, 95.0, 1.0, 2.0, 0.5593320930472424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781610.2002654336, 781610.2002654336, 193408.544843594], 
processed observation next is [1.0, 0.13043478260869565, 0.36492890995260674, 0.95, 1.0, 1.0, 0.469074810900292, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.217113944518176, 0.217113944518176, 0.28866946991581194], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.02162131], dtype=float32), 1.2420326]. 
=============================================
[2019-04-10 13:09:55,695] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6982982e-19 1.0000000e+00 6.6101056e-22 4.9355320e-26 5.3235765e-31], sum to 1.0000
[2019-04-10 13:09:55,702] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3824
[2019-04-10 13:09:55,710] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.5017017368259656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705140.2329091638, 705140.2329091632, 184387.8920285174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7705200.0000, 
sim time next is 7705800.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.495466891035004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 696387.5950469036, 696387.5950469042, 183409.2943589505], 
processed observation next is [1.0, 0.17391304347826086, 0.3601895734597157, 0.94, 1.0, 1.0, 0.392128784379523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19344099862413988, 0.19344099862414005, 0.27374521546112013], 
reward next is 0.7263, 
noisyNet noise sample is [array([-0.8472158], dtype=float32), -0.13919182]. 
=============================================
[2019-04-10 13:09:56,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:09:56,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:09:57,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-04-10 13:09:59,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0565790e-09 9.9870062e-01 9.9609032e-09 1.2994095e-03 3.0374527e-16], sum to 1.0000
[2019-04-10 13:09:59,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6947
[2019-04-10 13:09:59,896] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2024412.024338663 W.
[2019-04-10 13:09:59,903] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.3, 73.5, 1.0, 2.0, 0.4826219970320276, 1.0, 2.0, 0.4826219970320276, 1.0, 2.0, 0.8315902208508394, 6.9112, 6.9112, 170.5573041426782, 2024412.024338663, 2024412.024338663, 402195.3859683078], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7813800.0000, 
sim time next is 7814400.0000, 
raw observation next is [29.4, 73.0, 1.0, 2.0, 0.7363403510567955, 1.0, 2.0, 0.7363403510567955, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2059141.398000091, 2059141.398000092, 390066.7014700694], 
processed observation next is [1.0, 0.43478260869565216, 0.5924170616113744, 0.73, 1.0, 1.0, 0.682337772357585, 1.0, 1.0, 0.682337772357585, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.571983721666692, 0.5719837216666922, 0.5821891066717454], 
reward next is 0.4178, 
noisyNet noise sample is [array([-0.07614482], dtype=float32), 0.2563374]. 
=============================================
[2019-04-10 13:10:00,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1829200e-22 1.0000000e+00 4.4091388e-25 1.6053833e-29 7.7468057e-35], sum to 1.0000
[2019-04-10 13:10:00,183] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5097
[2019-04-10 13:10:00,187] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 86.66666666666666, 1.0, 2.0, 0.5408653677065502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755795.6329067571, 755795.6329067578, 190246.9048509759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7846800.0000, 
sim time next is 7847400.0000, 
raw observation next is [27.45, 87.83333333333334, 1.0, 2.0, 0.5415887712110053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756806.8641954357, 756806.8641954362, 190369.0900165024], 
processed observation next is [1.0, 0.8260869565217391, 0.5, 0.8783333333333334, 1.0, 1.0, 0.4476973147120546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21022412894317657, 0.21022412894317674, 0.28413297017388417], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.5061445], dtype=float32), -0.823893]. 
=============================================
[2019-04-10 13:10:00,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:00,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:00,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-04-10 13:10:03,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2292739e-19 1.0000000e+00 9.9484188e-21 1.3358996e-21 2.5883861e-30], sum to 1.0000
[2019-04-10 13:10:03,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7107
[2019-04-10 13:10:03,592] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 68.0, 1.0, 2.0, 0.4916971828444742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687066.6670211147, 687066.6670211147, 182306.8890577279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7927200.0000, 
sim time next is 7927800.0000, 
raw observation next is [29.58333333333334, 69.5, 1.0, 2.0, 0.4994975400079913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697969.9755551813, 697969.975555182, 183518.550989871], 
processed observation next is [1.0, 0.782608695652174, 0.6011058451816749, 0.695, 1.0, 1.0, 0.3969849879614353, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19388054876532812, 0.19388054876532831, 0.273908285059509], 
reward next is 0.7261, 
noisyNet noise sample is [array([0.7120695], dtype=float32), 0.41245985]. 
=============================================
[2019-04-10 13:10:04,847] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9053505e-11 9.9999917e-01 9.5485647e-11 8.8563576e-07 2.2677585e-18], sum to 1.0000
[2019-04-10 13:10:04,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8161
[2019-04-10 13:10:04,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2253195.379302607 W.
[2019-04-10 13:10:04,862] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 68.0, 1.0, 2.0, 0.9701308140248837, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986120967605503, 6.9112, 168.9124477323636, 2253195.379302607, 2200044.026398772, 454978.8951415661], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7915200.0000, 
sim time next is 7915800.0000, 
raw observation next is [30.36666666666666, 67.5, 1.0, 2.0, 0.5376883950413529, 1.0, 1.0, 0.5376883950413529, 1.0, 2.0, 0.9234096428427578, 6.9112, 6.9112, 170.5573041426782, 2255630.927201175, 2255630.927201175, 440145.6681699686], 
processed observation next is [1.0, 0.6086956521739131, 0.6382306477093204, 0.675, 1.0, 1.0, 0.442998066314883, 1.0, 0.5, 0.442998066314883, 1.0, 1.0, 0.9065971254179974, 0.0, 0.0, 0.8375144448122397, 0.6265641464447709, 0.6265641464447709, 0.6569338330895054], 
reward next is 0.3431, 
noisyNet noise sample is [array([1.0154116], dtype=float32), 0.2587077]. 
=============================================
[2019-04-10 13:10:05,457] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:05,458] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:05,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-04-10 13:10:06,225] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,239] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-04-10 13:10:06,262] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,263] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-04-10 13:10:06,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-04-10 13:10:06,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-04-10 13:10:06,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-04-10 13:10:06,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-04-10 13:10:06,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,826] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-04-10 13:10:06,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-04-10 13:10:06,993] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:06,993] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:06,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-04-10 13:10:07,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:07,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:07,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-04-10 13:10:07,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:07,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:07,064] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run7
[2019-04-10 13:10:07,107] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:07,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:07,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-04-10 13:10:07,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:10:07,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:07,196] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-04-10 13:10:11,834] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 13:10:11,837] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:10:11,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:11,838] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:10:11,839] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:10:11,839] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:11,839] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:10:11,840] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:10:11,842] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:11,843] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:11,842] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:11,862] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run52
[2019-04-10 13:10:11,880] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run52
[2019-04-10 13:10:11,880] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run52
[2019-04-10 13:10:11,922] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run52
[2019-04-10 13:10:11,942] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run52
[2019-04-10 13:10:16,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:16,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.243054735, 74.53289978000001, 1.0, 2.0, 0.2858749776328243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470055.053319014, 470055.0533190133, 164665.6255503657]
[2019-04-10 13:10:16,983] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:10:16,985] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8766481e-21 1.0000000e+00 1.7231843e-23 7.8938045e-30 6.9163576e-34], sampled 0.7078017414412574
[2019-04-10 13:10:30,256] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:30,257] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.89248354833333, 77.81850147833335, 1.0, 2.0, 0.8426157170249254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1189509.161744588, 1189509.161744588, 256340.9693577334]
[2019-04-10 13:10:30,260] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:10:30,265] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.5518738e-19 1.0000000e+00 1.5660078e-20 6.2283550e-24 3.1239289e-30], sampled 0.588420259968544
[2019-04-10 13:10:30,701] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:30,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.06666666666667, 85.66666666666666, 1.0, 2.0, 0.4084868774307545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 609930.7859028808, 609930.7859028814, 175465.9415211303]
[2019-04-10 13:10:30,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:10:30,703] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1116865e-20 1.0000000e+00 7.8425074e-23 6.7686461e-28 2.9779028e-33], sampled 0.13315876530953463
[2019-04-10 13:10:30,873] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:30,873] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.08333333333333, 90.66666666666667, 1.0, 2.0, 0.4214369711522788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 614160.0189941549, 614160.0189941542, 175451.0705419037]
[2019-04-10 13:10:30,874] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:10:30,878] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.1731528e-21 1.0000000e+00 3.2176254e-23 1.9606788e-29 1.6528560e-33], sampled 0.2187923380093405
[2019-04-10 13:10:32,269] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:32,271] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.2, 94.16666666666667, 1.0, 2.0, 0.4775383126244818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678311.9493986702, 678311.9493986702, 181567.5569834771]
[2019-04-10 13:10:32,272] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:10:32,274] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9055033e-20 1.0000000e+00 1.6493571e-22 2.3730756e-27 7.8064411e-33], sampled 0.4398885240521623
[2019-04-10 13:10:33,980] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:33,981] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.3, 90.0, 1.0, 2.0, 0.4857354089669844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678733.3984189575, 678733.3984189581, 181390.7700519404]
[2019-04-10 13:10:33,981] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:10:33,986] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6224752e-20 1.0000000e+00 1.1774975e-22 1.0481485e-27 5.0695269e-33], sampled 0.7150904512749665
[2019-04-10 13:10:54,859] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:54,860] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.05981203, 72.07296011, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.104781825008835, 6.9112, 168.9114763936149, 1591181.387824919, 1453848.981991025, 311348.8689252906]
[2019-04-10 13:10:54,863] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:10:54,865] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7210348e-17 1.0000000e+00 2.1738569e-18 2.8616574e-19 2.7864860e-28], sampled 0.9125128102320051
[2019-04-10 13:10:54,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:54,889] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.68513802166667, 89.46867560166666, 1.0, 2.0, 0.8121617752773068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1135102.997662783, 1135102.997662783, 246922.7448569423]
[2019-04-10 13:10:54,892] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:10:54,894] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9262584e-19 1.0000000e+00 5.0428638e-21 2.1739737e-25 8.8418851e-31], sampled 0.044024321773595054
[2019-04-10 13:10:56,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:10:56,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.26110168666667, 70.60049707666667, 1.0, 2.0, 0.9476051428016213, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005990554962143, 6.9112, 168.9123159494331, 2221668.075554955, 2154420.648446048, 447606.695493033]
[2019-04-10 13:10:56,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:10:56,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.3245733e-15 1.0000000e+00 2.8143326e-15 6.3189078e-14 6.3256858e-25], sampled 0.5303007616204108
[2019-04-10 13:10:56,940] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2221668.075554955 W.
[2019-04-10 13:11:01,027] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:11:01,027] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.16666666666667, 79.66666666666667, 1.0, 2.0, 0.909445646910591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1271151.405519967, 1271151.405519967, 272553.8167131254]
[2019-04-10 13:11:01,028] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:11:01,030] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1397522e-19 1.0000000e+00 5.3556233e-21 3.5451680e-25 7.9416727e-31], sampled 0.7177462929994185
[2019-04-10 13:11:18,239] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:11:18,240] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.16666666666666, 80.33333333333334, 1.0, 2.0, 0.9585219312705775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1339789.64813982, 1339789.64813982, 286542.4416484328]
[2019-04-10 13:11:18,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:11:18,243] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5639369e-19 1.0000000e+00 1.5618071e-21 5.7724669e-26 3.3396286e-31], sampled 0.9634232781174014
[2019-04-10 13:11:26,621] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12222736], dtype=float32), 0.09099771]
[2019-04-10 13:11:26,624] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.742204475, 50.297984615, 1.0, 2.0, 0.4875885333352916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760099.0360742313, 760099.0360742313, 190945.7224539003]
[2019-04-10 13:11:26,624] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:11:26,625] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6316238e-20 1.0000000e+00 8.5218965e-23 1.1708155e-28 6.0194773e-33], sampled 0.6248027996079055
[2019-04-10 13:11:32,307] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.8759 2927432656.9856 1337.0000
[2019-04-10 13:11:32,659] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7887.0635 3163901038.1723 1770.0000
[2019-04-10 13:11:32,914] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-10 13:11:32,934] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.8408 2842321826.1164 1129.0000
[2019-04-10 13:11:32,957] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5927 2779281048.9555 933.0000
[2019-04-10 13:11:33,972] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1275000, evaluation results [1275000.0, 7887.0635327038535, 3163901038.1722503, 1770.0, 8253.875911130981, 2927432656.98563, 1337.0, 8660.592666056795, 2779281048.95547, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8498.840754551371, 2842321826.11639, 1129.0]
[2019-04-10 13:11:38,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7502094e-22 1.0000000e+00 1.2674202e-24 2.2839116e-30 9.5834875e-35], sum to 1.0000
[2019-04-10 13:11:38,402] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0491
[2019-04-10 13:11:38,413] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.06666666666667, 96.0, 1.0, 2.0, 0.2946152574974321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472899.1159788866, 472899.1159788866, 165158.8503494025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 178800.0000, 
sim time next is 179400.0000, 
raw observation next is [20.03333333333333, 96.0, 1.0, 2.0, 0.293820680113893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 471937.8987883507, 471937.8987883501, 165093.0315112215], 
processed observation next is [0.0, 0.043478260869565216, 0.14849921011058448, 0.96, 1.0, 1.0, 0.14918154230589517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13109386077454185, 0.1310938607745417, 0.24640750971824102], 
reward next is 0.7536, 
noisyNet noise sample is [array([-0.49203604], dtype=float32), 0.44790402]. 
=============================================
[2019-04-10 13:11:40,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9288462e-21 1.0000000e+00 5.9118885e-24 1.9720184e-31 1.8518153e-35], sum to 1.0000
[2019-04-10 13:11:40,412] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6133
[2019-04-10 13:11:40,417] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.38333333333333, 88.16666666666667, 1.0, 2.0, 0.3010137077222932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479568.2306969896, 479568.2306969896, 165594.3266130765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 238200.0000, 
sim time next is 238800.0000, 
raw observation next is [21.36666666666667, 88.33333333333334, 1.0, 2.0, 0.3010246977849387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479543.8533998129, 479543.8533998135, 165591.9594199328], 
processed observation next is [0.0, 0.782608695652174, 0.21169036334913136, 0.8833333333333334, 1.0, 1.0, 0.1578610816686008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13320662594439248, 0.13320662594439264, 0.24715217823870567], 
reward next is 0.7528, 
noisyNet noise sample is [array([-1.1361256], dtype=float32), -0.26018068]. 
=============================================
[2019-04-10 13:11:43,131] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4331298e-20 1.0000000e+00 2.3636934e-23 8.5179337e-30 1.3630817e-33], sum to 1.0000
[2019-04-10 13:11:43,138] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3008
[2019-04-10 13:11:43,141] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 93.0, 1.0, 2.0, 0.2839837324739529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 456896.6056773736, 456896.6056773742, 164061.5322567067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 268200.0000, 
sim time next is 268800.0000, 
raw observation next is [20.23333333333333, 93.33333333333334, 1.0, 2.0, 0.2830426559998905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455620.9801583613, 455620.9801583613, 163976.0409970356], 
processed observation next is [0.0, 0.08695652173913043, 0.15797788309636643, 0.9333333333333335, 1.0, 1.0, 0.1361959710842054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1265613833773226, 0.1265613833773226, 0.24474035969706806], 
reward next is 0.7553, 
noisyNet noise sample is [array([0.20720491], dtype=float32), 0.209607]. 
=============================================
[2019-04-10 13:11:50,577] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8928113e-21 1.0000000e+00 3.0700690e-23 1.1248461e-28 1.1742881e-33], sum to 1.0000
[2019-04-10 13:11:50,584] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-10 13:11:50,599] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.25, 79.0, 1.0, 2.0, 0.3317843451468782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541376.6389771572, 541376.6389771566, 170069.3708773235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 376200.0000, 
sim time next is 376800.0000, 
raw observation next is [21.33333333333333, 78.66666666666667, 1.0, 2.0, 0.3587676915180267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585056.6533695918, 585056.6533695924, 173609.863459545], 
processed observation next is [1.0, 0.34782608695652173, 0.21011058451816728, 0.7866666666666667, 1.0, 1.0, 0.2274309536361767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16251573704710884, 0.162515737047109, 0.25911919919335075], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.2794088], dtype=float32), -0.42734402]. 
=============================================
[2019-04-10 13:11:50,858] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.86870663e-21 1.00000000e+00 1.13515996e-23 4.71049844e-29
 6.68308625e-34], sum to 1.0000
[2019-04-10 13:11:50,867] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1303
[2019-04-10 13:11:50,888] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 81.33333333333334, 1.0, 2.0, 0.2356152286388764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388645.8454703358, 388645.8454703358, 159492.8250227705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 456000.0000, 
sim time next is 456600.0000, 
raw observation next is [20.15, 81.16666666666666, 1.0, 2.0, 0.2382334274313591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 392838.4163206305, 392838.4163206298, 159743.9394355263], 
processed observation next is [1.0, 0.2608695652173913, 0.15402843601895733, 0.8116666666666665, 1.0, 1.0, 0.08220894871248084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10912178231128626, 0.10912178231128607, 0.23842379020227805], 
reward next is 0.7616, 
noisyNet noise sample is [array([-1.1112577], dtype=float32), 1.0168241]. 
=============================================
[2019-04-10 13:11:51,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2971635e-21 1.0000000e+00 2.5697431e-22 2.2716288e-27 1.8916224e-32], sum to 1.0000
[2019-04-10 13:11:51,424] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2174
[2019-04-10 13:11:51,454] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 74.0, 1.0, 2.0, 0.4619755834979364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752241.4872019907, 752241.4872019907, 189065.6488971295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 382800.0000, 
sim time next is 383400.0000, 
raw observation next is [22.15, 73.5, 1.0, 2.0, 0.4780898799845202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778347.4763162672, 778347.4763162679, 191802.4131019234], 
processed observation next is [1.0, 0.43478260869565216, 0.24881516587677724, 0.735, 1.0, 1.0, 0.3711926264873738, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2162076323100742, 0.2162076323100744, 0.28627225836107967], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.2662554], dtype=float32), -0.7023235]. 
=============================================
[2019-04-10 13:12:02,538] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1012946e-20 1.0000000e+00 1.4731764e-22 1.4586177e-26 1.5124027e-31], sum to 1.0000
[2019-04-10 13:12:02,543] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7502
[2019-04-10 13:12:02,562] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333333, 63.00000000000001, 1.0, 2.0, 0.4305719741467672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709213.5452492372, 709213.5452492366, 184102.8378267748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [23.05, 61.5, 1.0, 2.0, 0.4410394949101237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726687.4437295337, 726687.4437295337, 185761.3217983033], 
processed observation next is [1.0, 0.43478260869565216, 0.2914691943127963, 0.615, 1.0, 1.0, 0.3265536083254503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2018576232582038, 0.2018576232582038, 0.2772557041765721], 
reward next is 0.7227, 
noisyNet noise sample is [array([0.5495622], dtype=float32), 1.5108529]. 
=============================================
[2019-04-10 13:12:07,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.6275568e-21 1.0000000e+00 1.5226145e-23 2.6327805e-30 4.0624130e-34], sum to 1.0000
[2019-04-10 13:12:07,443] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8383
[2019-04-10 13:12:07,452] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 74.0, 1.0, 2.0, 0.2443662082116722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403511.8456770882, 403511.8456770882, 160304.7886285869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 633600.0000, 
sim time next is 634200.0000, 
raw observation next is [21.16666666666667, 73.16666666666667, 1.0, 2.0, 0.3049239794157823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 166812.8907504143], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.7316666666666667, 1.0, 1.0, 0.16255901134431602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13976901878634496, 0.13976901878634496, 0.24897446380658853], 
reward next is 0.7510, 
noisyNet noise sample is [array([1.0727029], dtype=float32), -0.61963224]. 
=============================================
[2019-04-10 13:12:08,759] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2759648e-19 1.0000000e+00 4.7745237e-21 4.2477882e-25 1.6048374e-30], sum to 1.0000
[2019-04-10 13:12:08,789] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5339
[2019-04-10 13:12:08,796] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 56.33333333333334, 1.0, 2.0, 0.3917393684684735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642896.085983045, 642896.085983045, 178270.7484363686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [24.35, 56.0, 1.0, 2.0, 0.338113927322048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555031.0528448228, 555031.0528448233, 170949.8978471664], 
processed observation next is [1.0, 0.5217391304347826, 0.35308056872037924, 0.56, 1.0, 1.0, 0.2025469003880096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1541752924568952, 0.15417529245689537, 0.25514910126442747], 
reward next is 0.7449, 
noisyNet noise sample is [array([-1.3653667], dtype=float32), 0.51301014]. 
=============================================
[2019-04-10 13:12:09,124] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.240327e-21 1.000000e+00 3.419609e-23 1.266006e-29 3.596369e-33], sum to 1.0000
[2019-04-10 13:12:09,154] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1629
[2019-04-10 13:12:09,158] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 73.16666666666667, 1.0, 2.0, 0.3049239794157823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 166812.8907504143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 634200.0000, 
sim time next is 634800.0000, 
raw observation next is [21.33333333333334, 72.33333333333334, 1.0, 2.0, 0.3499500078984439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 577136.3519913014, 577136.351991302, 172492.5667516778], 
processed observation next is [1.0, 0.34782608695652173, 0.2101105845181678, 0.7233333333333334, 1.0, 1.0, 0.2168072384318601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16031565333091705, 0.1603156533309172, 0.2574515921666833], 
reward next is 0.7425, 
noisyNet noise sample is [array([0.16514276], dtype=float32), -0.39579225]. 
=============================================
[2019-04-10 13:12:12,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6792984e-20 1.0000000e+00 2.8102769e-23 3.6337203e-29 2.8861157e-34], sum to 1.0000
[2019-04-10 13:12:12,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0243
[2019-04-10 13:12:12,159] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.56666666666667, 92.0, 1.0, 2.0, 0.2219362550021812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 370005.9444047123, 370005.9444047123, 157681.2467350347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 706800.0000, 
sim time next is 707400.0000, 
raw observation next is [17.55, 92.0, 1.0, 2.0, 0.2250363663102666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 375212.4245053321, 375212.4245053321, 157944.9050268299], 
processed observation next is [1.0, 0.17391304347826086, 0.03080568720379157, 0.92, 1.0, 1.0, 0.06630887507261035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10422567347370336, 0.10422567347370336, 0.23573866421914907], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.46611825], dtype=float32), 1.746915]. 
=============================================
[2019-04-10 13:12:14,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9471580e-22 1.0000000e+00 1.1300401e-23 3.9592433e-30 6.8547897e-34], sum to 1.0000
[2019-04-10 13:12:14,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1126
[2019-04-10 13:12:14,276] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.51666666666667, 92.0, 1.0, 2.0, 0.2141490871248178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357128.8032411602, 357128.8032411602, 156965.2098925418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 708600.0000, 
sim time next is 709200.0000, 
raw observation next is [17.5, 92.0, 1.0, 2.0, 0.21247095903468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 354364.6273674652, 354364.6273674652, 156808.9730928492], 
processed observation next is [1.0, 0.21739130434782608, 0.028436018957346036, 0.92, 1.0, 1.0, 0.05116983016226504, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09843461871318478, 0.09843461871318478, 0.23404324342216298], 
reward next is 0.7660, 
noisyNet noise sample is [array([-1.6549892], dtype=float32), 2.1833255]. 
=============================================
[2019-04-10 13:12:15,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4929937e-20 1.0000000e+00 2.2631938e-21 1.9224518e-25 1.3619396e-31], sum to 1.0000
[2019-04-10 13:12:15,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0926
[2019-04-10 13:12:15,538] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 49.83333333333334, 1.0, 2.0, 0.6262301618591973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031855.432564975, 1031855.432564975, 221349.0490915797], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 748200.0000, 
sim time next is 748800.0000, 
raw observation next is [25.1, 50.0, 1.0, 2.0, 0.6247459135902054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029735.949621896, 1029735.949621896, 221012.1660798875], 
processed observation next is [1.0, 0.6956521739130435, 0.38862559241706174, 0.5, 1.0, 1.0, 0.5478866428797655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28603776378386003, 0.28603776378386003, 0.32986890459684703], 
reward next is 0.6701, 
noisyNet noise sample is [array([-0.5143198], dtype=float32), -1.1267924]. 
=============================================
[2019-04-10 13:12:17,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2115094e-21 1.0000000e+00 2.2535074e-24 1.6199367e-31 3.3561988e-35], sum to 1.0000
[2019-04-10 13:12:17,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3696
[2019-04-10 13:12:17,647] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666666, 61.66666666666666, 1.0, 2.0, 0.2895305742930316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463387.3086991419, 463387.3086991413, 164486.7115358807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 819600.0000, 
sim time next is 820200.0000, 
raw observation next is [25.03333333333333, 61.83333333333334, 1.0, 2.0, 0.2895031986759039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463378.7550511046, 463378.7550511046, 164486.5213511981], 
processed observation next is [0.0, 0.4782608695652174, 0.38546603475513425, 0.6183333333333334, 1.0, 1.0, 0.14397975744084807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12871632084752904, 0.12871632084752904, 0.24550227067343003], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.37129188], dtype=float32), 0.52836555]. 
=============================================
[2019-04-10 13:12:17,780] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.84427127e-21 1.00000000e+00 1.82398620e-23 1.17813637e-28
 1.26703735e-33], sum to 1.0000
[2019-04-10 13:12:17,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7948
[2019-04-10 13:12:17,791] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 90.0, 1.0, 2.0, 0.4775967003707882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756464.660744584, 756464.6607445847, 190345.0809985363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1068600.0000, 
sim time next is 1069200.0000, 
raw observation next is [21.5, 89.0, 1.0, 2.0, 0.5346183266987886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847247.159568022, 847247.159568022, 200622.6061289288], 
processed observation next is [1.0, 0.391304347826087, 0.21800947867298584, 0.89, 1.0, 1.0, 0.4392991887937212, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23534643321333945, 0.23534643321333945, 0.2994367255655654], 
reward next is 0.7006, 
noisyNet noise sample is [array([1.3210424], dtype=float32), -0.08385753]. 
=============================================
[2019-04-10 13:12:23,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6950571e-20 1.0000000e+00 1.9603427e-21 5.1925408e-25 8.3214054e-31], sum to 1.0000
[2019-04-10 13:12:23,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1268
[2019-04-10 13:12:23,156] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.6386368761396531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 982137.0935775214, 982137.0935775208, 219174.3486969676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1008000.0000, 
sim time next is 1008600.0000, 
raw observation next is [21.7, 97.16666666666667, 1.0, 2.0, 0.670047564466006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029646.155231514, 1029646.155231514, 226111.9375705542], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.9716666666666667, 1.0, 1.0, 0.6024669451397663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2860128208976428, 0.2860128208976428, 0.33748050383664807], 
reward next is 0.6625, 
noisyNet noise sample is [array([-0.47713113], dtype=float32), 1.3830746]. 
=============================================
[2019-04-10 13:12:26,485] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-10 13:12:26,486] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:12:26,487] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:12:26,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:12:26,492] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:12:26,493] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:12:26,492] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:12:26,488] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:12:26,496] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:12:26,497] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:12:26,496] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:12:26,515] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run53
[2019-04-10 13:12:26,534] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run53
[2019-04-10 13:12:26,534] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run53
[2019-04-10 13:12:26,574] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run53
[2019-04-10 13:12:26,574] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run53
[2019-04-10 13:12:45,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12230335], dtype=float32), 0.09117292]
[2019-04-10 13:12:45,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.00683354, 75.26108615, 1.0, 2.0, 0.3739066355982019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580910.1833705379, 580910.1833705385, 173401.129201998]
[2019-04-10 13:12:45,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:12:45,019] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4092062e-21 1.0000000e+00 4.0943446e-23 5.4122862e-29 1.3618655e-33], sampled 0.9097829966509475
[2019-04-10 13:12:45,848] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12230335], dtype=float32), 0.09117292]
[2019-04-10 13:12:45,848] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.75, 86.33333333333334, 1.0, 2.0, 0.7199231183330492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1006126.259310909, 1006126.259310909, 225200.6007986789]
[2019-04-10 13:12:45,849] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:12:45,851] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.15348185e-20 1.00000000e+00 7.85838344e-22 1.13082325e-26
 7.35479661e-32], sampled 0.9141643073655008
[2019-04-10 13:13:00,089] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12230335], dtype=float32), 0.09117292]
[2019-04-10 13:13:00,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.6, 65.33333333333334, 1.0, 2.0, 0.5626279870802422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786217.5843776038, 786217.5843776038, 193989.8558702418]
[2019-04-10 13:13:00,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:13:00,093] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7417999e-20 1.0000000e+00 6.1626359e-22 2.7762607e-26 3.6533818e-32], sampled 0.3232019645703198
[2019-04-10 13:13:41,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12230335], dtype=float32), 0.09117292]
[2019-04-10 13:13:41,003] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.90000000000001, 68.0, 1.0, 2.0, 0.6221006908341978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869358.882751789, 869358.882751789, 204935.7801402227]
[2019-04-10 13:13:41,004] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:13:41,005] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.28856096e-20 1.00000000e+00 1.08131171e-22 6.89099830e-26
 3.71778447e-33], sampled 0.9063038080849807
[2019-04-10 13:13:46,998] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4819 2779432815.4014 934.0000
[2019-04-10 13:13:47,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0250 3007848960.6402 1766.0000
[2019-04-10 13:13:47,299] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.2766 2842238422.0325 1127.0000
[2019-04-10 13:13:47,329] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.3136 3163425802.6093 1761.0000
[2019-04-10 13:13:47,332] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 13:13:48,343] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1300000, evaluation results [1300000.0, 7890.313589083292, 3163425802.609339, 1761.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8659.481908526288, 2779432815.401404, 934.0, 7998.024995752096, 3007848960.6401815, 1766.0, 8498.27655552252, 2842238422.032538, 1127.0]
[2019-04-10 13:13:49,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0169458e-21 1.0000000e+00 8.3334745e-24 2.5910620e-31 2.7015503e-34], sum to 1.0000
[2019-04-10 13:13:49,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0906
[2019-04-10 13:13:49,529] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 98.0, 1.0, 2.0, 0.3146155162227376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 497157.3719752533, 497157.3719752527, 166806.0286991008], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1399200.0000, 
sim time next is 1399800.0000, 
raw observation next is [20.58333333333334, 98.0, 1.0, 2.0, 0.3146256245799867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496975.2427550306, 496975.2427550299, 166788.3153678298], 
processed observation next is [0.0, 0.17391304347826086, 0.17456556082148533, 0.98, 1.0, 1.0, 0.17424774045781533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13804867854306405, 0.13804867854306385, 0.24893778413108927], 
reward next is 0.7511, 
noisyNet noise sample is [array([1.8119063], dtype=float32), 0.42119887]. 
=============================================
[2019-04-10 13:13:50,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.37635634e-21 1.00000000e+00 3.82086452e-23 5.47048905e-29
 1.51707005e-33], sum to 1.0000
[2019-04-10 13:13:50,483] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3627
[2019-04-10 13:13:50,486] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 96.0, 1.0, 2.0, 0.3138780027763934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497627.6762690705, 497627.6762690711, 166872.076501545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1052400.0000, 
sim time next is 1053000.0000, 
raw observation next is [20.45, 96.0, 1.0, 2.0, 0.3037749950522163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483253.8769236712, 483253.8769236712, 165847.8171561614], 
processed observation next is [1.0, 0.17391304347826086, 0.16824644549763035, 0.96, 1.0, 1.0, 0.16117469283399552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1342371880343531, 0.1342371880343531, 0.24753405545695734], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.19647756], dtype=float32), 1.0914173]. 
=============================================
[2019-04-10 13:13:50,498] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.142365]
 [73.06554 ]
 [73.18459 ]
 [73.13056 ]
 [73.1032  ]], R is [[73.219841  ]
 [73.2385788 ]
 [73.25110626]
 [73.26935577]
 [73.28679657]].
[2019-04-10 13:13:58,741] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4953468e-11 9.9999475e-01 1.4812698e-10 5.2292189e-06 1.6064375e-18], sum to 1.0000
[2019-04-10 13:13:58,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-10 13:13:58,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1942980.18846935 W.
[2019-04-10 13:13:58,759] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.33333333333334, 73.83333333333334, 1.0, 2.0, 0.694839222507699, 1.0, 1.0, 0.694839222507699, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1942980.18846935, 1942980.18846935, 371914.778325741], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1263000.0000, 
sim time next is 1263600.0000, 
raw observation next is [28.3, 74.0, 1.0, 2.0, 0.461439811478287, 1.0, 2.0, 0.461439811478287, 1.0, 1.0, 0.7844834722659535, 6.9112, 6.9112, 170.5573041426782, 1935480.714997762, 1935480.714997762, 386681.2410464133], 
processed observation next is [1.0, 0.6521739130434783, 0.5402843601895735, 0.74, 1.0, 1.0, 0.351132302985888, 1.0, 1.0, 0.351132302985888, 1.0, 0.5, 0.737174966177992, 0.0, 0.0, 0.8375144448122397, 0.5376335319438228, 0.5376335319438228, 0.5771361806662886], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.62877756], dtype=float32), -1.6425782]. 
=============================================
[2019-04-10 13:13:58,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5635953e-17 1.0000000e+00 6.7049699e-19 5.7840291e-22 3.1687723e-28], sum to 1.0000
[2019-04-10 13:13:58,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7353
[2019-04-10 13:13:58,939] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 75.0, 1.0, 2.0, 0.956834170579687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1370563.082739704, 1370563.082739704, 291092.4971310168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [27.01666666666667, 74.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.329390880512875, 6.9112, 168.9110677347442, 1780997.064492748, 1484321.378286425, 316201.3668282327], 
processed observation next is [1.0, 0.43478260869565216, 0.4794628751974725, 0.7433333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.04181908805128751, 0.0, 0.8294306703989952, 0.4947214068035411, 0.4123114939684514, 0.47194233854960105], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.45314848], dtype=float32), 1.0803941]. 
=============================================
[2019-04-10 13:13:59,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3131537e-11 9.9999464e-01 3.2109776e-10 5.3189415e-06 1.0202771e-17], sum to 1.0000
[2019-04-10 13:13:59,005] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1876
[2019-04-10 13:13:59,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1977168.078442765 W.
[2019-04-10 13:13:59,019] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.93333333333333, 75.33333333333334, 1.0, 2.0, 0.772915346911683, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.967306491637265, 6.9112, 168.9125758651993, 1977168.078442765, 1937364.291936885, 402527.693942074], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1268400.0000, 
sim time next is 1269000.0000, 
raw observation next is [27.9, 75.5, 1.0, 2.0, 0.7709570448351576, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.967241824378026, 6.9112, 168.9125771212332, 1974427.460684437, 1934669.550958912, 402062.1650027003], 
processed observation next is [1.0, 0.6956521739130435, 0.5213270142180094, 0.755, 1.0, 1.0, 0.7240446323315152, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005604182437802585, 0.0, 0.8294380821776564, 0.5484520724123436, 0.5374082085996977, 0.6000927835861198], 
reward next is 0.1197, 
noisyNet noise sample is [array([0.9503045], dtype=float32), -2.0378797]. 
=============================================
[2019-04-10 13:13:59,028] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[51.43269 ]
 [50.61901 ]
 [49.820385]
 [49.07672 ]
 [49.878395]], R is [[51.83705902]
 [51.43736649]
 [51.05979538]
 [50.98777008]
 [50.8910408 ]].
[2019-04-10 13:14:04,144] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2023321e-19 1.0000000e+00 5.4125974e-21 3.0333496e-25 4.2554142e-31], sum to 1.0000
[2019-04-10 13:14:04,150] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9148
[2019-04-10 13:14:04,155] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333334, 92.33333333333334, 1.0, 2.0, 0.3012415671543864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480381.7677212527, 480381.7677212527, 165658.9165345882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1358400.0000, 
sim time next is 1359000.0000, 
raw observation next is [20.85, 92.5, 1.0, 2.0, 0.298805755712317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476170.2542246285, 476170.2542246285, 165354.0656031102], 
processed observation next is [1.0, 0.7391304347826086, 0.18720379146919444, 0.925, 1.0, 1.0, 0.15518765748471924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13226951506239681, 0.13226951506239681, 0.246797112840463], 
reward next is 0.7532, 
noisyNet noise sample is [array([2.5431976], dtype=float32), -0.45763364]. 
=============================================
[2019-04-10 13:14:04,176] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.2276  ]
 [69.059456]
 [67.68884 ]
 [67.68002 ]
 [67.67422 ]], R is [[71.10506439]
 [71.14675903]
 [71.17617035]
 [71.13980865]
 [71.10385132]].
[2019-04-10 13:14:07,587] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3973507e-20 1.0000000e+00 7.2242777e-23 8.5244067e-29 6.4781117e-34], sum to 1.0000
[2019-04-10 13:14:07,595] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8832
[2019-04-10 13:14:07,601] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 95.33333333333334, 1.0, 2.0, 0.366633294487637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558481.7451203557, 558481.7451203557, 171183.0095961729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1462800.0000, 
sim time next is 1463400.0000, 
raw observation next is [22.15, 95.5, 1.0, 2.0, 0.3652517941314636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556892.7480991762, 556892.7480991769, 171062.7908768603], 
processed observation next is [0.0, 0.9565217391304348, 0.24881516587677724, 0.955, 1.0, 1.0, 0.23524312545959467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15469243002754895, 0.15469243002754915, 0.25531759832367207], 
reward next is 0.7447, 
noisyNet noise sample is [array([1.3790476], dtype=float32), 2.0925832]. 
=============================================
[2019-04-10 13:14:14,651] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5927203e-21 1.0000000e+00 2.5992101e-23 5.6216222e-29 3.7600715e-34], sum to 1.0000
[2019-04-10 13:14:14,652] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2345
[2019-04-10 13:14:14,666] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 89.5, 1.0, 2.0, 0.3446836717122636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534035.2358530081, 534035.2358530075, 169418.3342282879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1549800.0000, 
sim time next is 1550400.0000, 
raw observation next is [22.26666666666667, 90.0, 1.0, 2.0, 0.3444579342784178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 533972.0111294887, 533972.0111294887, 169421.1715931408], 
processed observation next is [0.0, 0.9565217391304348, 0.2543443917851502, 0.9, 1.0, 1.0, 0.21019028226315398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1483255586470802, 0.1483255586470802, 0.25286742028826986], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.72956926], dtype=float32), 0.5227997]. 
=============================================
[2019-04-10 13:14:26,883] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.0385149e-22 1.0000000e+00 2.4642820e-24 1.6540199e-29 2.1571105e-35], sum to 1.0000
[2019-04-10 13:14:26,911] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4157
[2019-04-10 13:14:26,913] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 88.0, 1.0, 2.0, 0.508513914151567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710573.1672812877, 710573.167281287, 184941.0680809185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1716000.0000, 
sim time next is 1716600.0000, 
raw observation next is [26.45, 88.5, 1.0, 2.0, 0.5088501122598655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711043.111764811, 711043.1117648116, 184994.6356829656], 
processed observation next is [1.0, 0.8695652173913043, 0.45260663507109006, 0.885, 1.0, 1.0, 0.4082531473010428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19751197549022528, 0.19751197549022542, 0.2761113965417397], 
reward next is 0.7239, 
noisyNet noise sample is [array([2.1965907], dtype=float32), 0.5946977]. 
=============================================
[2019-04-10 13:14:33,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8651596e-19 1.0000000e+00 1.0088401e-21 2.6536982e-26 4.9406786e-32], sum to 1.0000
[2019-04-10 13:14:33,812] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6130
[2019-04-10 13:14:33,820] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 94.0, 1.0, 2.0, 0.3731417406895722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575732.5297998098, 575732.5297998104, 172868.0394615163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1823400.0000, 
sim time next is 1824000.0000, 
raw observation next is [21.93333333333333, 94.33333333333333, 1.0, 2.0, 0.3691017803113059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569129.1276547808, 569129.1276547813, 172286.3898561079], 
processed observation next is [1.0, 0.08695652173913043, 0.23854660347551332, 0.9433333333333332, 1.0, 1.0, 0.23988166302566977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1580914243485502, 0.15809142434855036, 0.2571438654568775], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.78995746], dtype=float32), 0.73016673]. 
=============================================
[2019-04-10 13:14:33,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.839584]
 [71.61931 ]
 [71.33864 ]
 [71.81722 ]
 [71.6638  ]], R is [[71.92313385]
 [71.94589233]
 [71.96076965]
 [71.95746613]
 [71.98509216]].
[2019-04-10 13:14:35,668] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3001311e-22 1.0000000e+00 1.3353422e-24 1.4771151e-30 1.8682198e-35], sum to 1.0000
[2019-04-10 13:14:35,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3438
[2019-04-10 13:14:35,680] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 87.83333333333334, 1.0, 2.0, 0.4656988414344334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659394.8663762542, 659394.8663762542, 179512.6298097052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1889400.0000, 
sim time next is 1890000.0000, 
raw observation next is [25.1, 88.0, 1.0, 2.0, 0.4644061241375815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658324.6432128618, 658324.6432128613, 179418.0188848631], 
processed observation next is [1.0, 0.9130434782608695, 0.38862559241706174, 0.88, 1.0, 1.0, 0.3547061736597368, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18286795644801718, 0.182867956448017, 0.26778808788785535], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.5551383], dtype=float32), 0.008695814]. 
=============================================
[2019-04-10 13:14:35,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.20006 ]
 [73.15232 ]
 [73.3295  ]
 [73.376976]
 [73.17285 ]], R is [[73.26383972]
 [73.26327515]
 [73.26266479]
 [73.26215363]
 [73.26190948]].
[2019-04-10 13:14:38,423] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.02989576e-14 1.00000000e+00 4.30557774e-15 1.14410380e-13
 7.95552418e-25], sum to 1.0000
[2019-04-10 13:14:38,424] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3323
[2019-04-10 13:14:38,448] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 87.5, 1.0, 2.0, 0.4862461485166368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679447.2991264816, 679447.2991264822, 181470.9392642451], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1877400.0000, 
sim time next is 1878000.0000, 
raw observation next is [26.56666666666666, 87.33333333333334, 1.0, 2.0, 0.4923362882921005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687960.0018907079, 687960.0018907072, 182405.2203738074], 
processed observation next is [1.0, 0.7391304347826086, 0.4581358609794626, 0.8733333333333334, 1.0, 1.0, 0.38835697384590423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19110000052519663, 0.19110000052519643, 0.27224659757284686], 
reward next is 0.7278, 
noisyNet noise sample is [array([-0.8563303], dtype=float32), 0.7294771]. 
=============================================
[2019-04-10 13:14:38,459] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.49287 ]
 [62.991398]
 [58.36885 ]
 [54.923428]
 [54.456284]], R is [[68.74987793]
 [68.79152679]
 [68.10361481]
 [68.08501434]
 [67.90878296]].
[2019-04-10 13:14:39,945] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5172527e-20 1.0000000e+00 3.1116912e-22 5.1368785e-27 5.5481107e-33], sum to 1.0000
[2019-04-10 13:14:39,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6580
[2019-04-10 13:14:39,954] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 95.33333333333334, 1.0, 2.0, 0.4625111874732546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653122.8952995116, 653122.8952995116, 178815.037821896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1990200.0000, 
sim time next is 1990800.0000, 
raw observation next is [24.3, 95.0, 1.0, 2.0, 0.4628879070773849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652952.9311250609, 652952.9311250609, 178780.3507710804], 
processed observation next is [0.0, 0.043478260869565216, 0.3507109004739337, 0.95, 1.0, 1.0, 0.35287699647877696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1813758142014058, 0.1813758142014058, 0.26683634443444837], 
reward next is 0.7332, 
noisyNet noise sample is [array([0.47481218], dtype=float32), -1.0843863]. 
=============================================
[2019-04-10 13:14:41,291] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-10 13:14:41,293] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:14:41,295] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:14:41,295] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:14:41,297] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:14:41,297] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:14:41,302] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:14:41,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run54
[2019-04-10 13:14:41,300] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:14:41,304] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:14:41,301] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:14:41,326] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:14:41,335] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run54
[2019-04-10 13:14:41,356] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run54
[2019-04-10 13:14:41,374] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run54
[2019-04-10 13:14:41,397] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run54
[2019-04-10 13:14:56,399] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1216151], dtype=float32), 0.0908059]
[2019-04-10 13:14:56,400] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.320646105, 98.31292309333334, 1.0, 2.0, 0.4959702971121017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736756.3062346586, 736756.306234658, 188406.7347319907]
[2019-04-10 13:14:56,401] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:14:56,406] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1964944e-20 1.0000000e+00 2.3883480e-22 3.5561338e-28 7.5080683e-33], sampled 0.7338390823405877
[2019-04-10 13:15:14,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1216151], dtype=float32), 0.0908059]
[2019-04-10 13:15:14,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.18064916666667, 98.81646958333334, 1.0, 2.0, 0.3921739973034116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588785.4368374586, 588785.4368374593, 173613.5104969117]
[2019-04-10 13:15:14,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:15:14,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6405299e-20 1.0000000e+00 1.6477785e-22 1.5724904e-28 3.8572811e-33], sampled 0.48526523314595094
[2019-04-10 13:15:38,653] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1216151], dtype=float32), 0.0908059]
[2019-04-10 13:15:38,654] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.69894088166667, 65.719858085, 1.0, 2.0, 0.5818745726654924, 0.0, 2.0, 0.0, 1.0, 2.0, 1.006327582327537, 6.911199999999999, 6.9112, 168.9128821736664, 1626869.157124232, 1626869.157124233, 355250.3607662129]
[2019-04-10 13:15:38,655] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:15:38,656] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.6844063e-14 1.0000000e+00 6.0204080e-14 2.3898523e-14 4.4376436e-23], sampled 0.9221932790160341
[2019-04-10 13:15:39,122] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1216151], dtype=float32), 0.0908059]
[2019-04-10 13:15:39,122] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.49418237, 88.03115558333334, 1.0, 2.0, 0.556626476269195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777827.9893078314, 777827.9893078314, 192943.180356497]
[2019-04-10 13:15:39,123] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:15:39,125] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9903794e-20 1.0000000e+00 3.6260205e-22 6.3513098e-27 6.5719274e-33], sampled 0.6452435848842053
[2019-04-10 13:16:02,197] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.5046 3163896224.2756 1767.0000
[2019-04-10 13:16:02,596] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1218 3007784093.2705 1766.0000
[2019-04-10 13:16:02,612] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4320 2779397896.3857 933.0000
[2019-04-10 13:16:02,623] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2637 2842437621.3210 1130.0000
[2019-04-10 13:16:02,634] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 13:16:03,647] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1325000, evaluation results [1325000.0, 7890.504622118597, 3163896224.275607, 1767.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8660.432005415354, 2779397896.3857465, 933.0, 7998.121812721697, 3007784093.270549, 1766.0, 8497.263695021826, 2842437621.3210006, 1130.0]
[2019-04-10 13:16:06,400] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5514899e-20 1.0000000e+00 2.3495895e-22 2.0036646e-28 9.3141199e-33], sum to 1.0000
[2019-04-10 13:16:06,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5629
[2019-04-10 13:16:06,410] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.0, 1.0, 2.0, 0.5083006152379825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710275.0138739177, 710275.0138739183, 184907.0869315164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2030400.0000, 
sim time next is 2031000.0000, 
raw observation next is [26.31666666666666, 89.33333333333333, 1.0, 2.0, 0.5098641702574394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712460.5837063199, 712460.5837063199, 185156.2316180599], 
processed observation next is [0.0, 0.5217391304347826, 0.4462875197472351, 0.8933333333333333, 1.0, 1.0, 0.4094749039246257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19790571769619997, 0.19790571769619997, 0.276352584504567], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.26809537], dtype=float32), 1.0009164]. 
=============================================
[2019-04-10 13:16:06,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.812225]
 [74.76068 ]
 [74.75714 ]
 [74.74902 ]
 [74.74491 ]], R is [[74.80444336]
 [74.7804184 ]
 [74.75679779]
 [74.73352051]
 [74.7105484 ]].
[2019-04-10 13:16:10,267] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1310214e-20 1.0000000e+00 1.2440369e-21 2.0033217e-27 2.4149222e-32], sum to 1.0000
[2019-04-10 13:16:10,274] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1446
[2019-04-10 13:16:10,277] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 88.16666666666667, 1.0, 2.0, 0.5059188884544717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706945.7951191511, 706945.7951191511, 184529.1445501473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2103000.0000, 
sim time next is 2103600.0000, 
raw observation next is [26.7, 87.33333333333334, 1.0, 2.0, 0.5081910074468077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710121.8019849282, 710121.8019849276, 184889.9440347315], 
processed observation next is [0.0, 0.34782608695652173, 0.46445497630331756, 0.8733333333333334, 1.0, 1.0, 0.4074590451166357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1972560561069245, 0.19725605610692434, 0.2759551403503455], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.7266686], dtype=float32), -0.37876716]. 
=============================================
[2019-04-10 13:16:16,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2295107e-21 1.0000000e+00 6.1876022e-23 6.5833672e-26 1.1048598e-33], sum to 1.0000
[2019-04-10 13:16:16,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8365
[2019-04-10 13:16:16,177] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 73.33333333333334, 1.0, 2.0, 0.5552965038335744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775968.8113924285, 775968.8113924279, 192714.5463767894], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2226000.0000, 
sim time next is 2226600.0000, 
raw observation next is [30.35, 74.0, 1.0, 2.0, 0.5545794842767985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774966.4860106757, 774966.4860106757, 192590.4977421925], 
processed observation next is [1.0, 0.782608695652174, 0.637440758293839, 0.74, 1.0, 1.0, 0.4633487762371066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21526846833629878, 0.21526846833629878, 0.28744850409282463], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.8188759], dtype=float32), 0.8450998]. 
=============================================
[2019-04-10 13:16:18,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8294726e-08 5.5388606e-01 4.9517001e-07 4.4611338e-01 2.3323044e-14], sum to 1.0000
[2019-04-10 13:16:18,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7789
[2019-04-10 13:16:18,376] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.9, 83.33333333333333, 1.0, 2.0, 0.4471698983537586, 1.0, 2.0, 0.4471698983537586, 1.0, 2.0, 0.7720685443441708, 6.911200000000001, 6.9112, 170.5573041426782, 1875574.058587154, 1875574.058587154, 379817.5330419126], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2542200.0000, 
sim time next is 2542800.0000, 
raw observation next is [28.0, 82.66666666666667, 1.0, 2.0, 0.7176713356475061, 1.0, 2.0, 0.7176713356475061, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2006885.467406774, 2006885.467406774, 381779.7625712113], 
processed observation next is [1.0, 0.43478260869565216, 0.5260663507109005, 0.8266666666666667, 1.0, 1.0, 0.6598449827078386, 1.0, 1.0, 0.6598449827078386, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5574681853907706, 0.5574681853907706, 0.5698205411510616], 
reward next is 0.4302, 
noisyNet noise sample is [array([-1.029378], dtype=float32), -0.38913858]. 
=============================================
[2019-04-10 13:16:19,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0844654e-09 7.7998769e-01 5.0260578e-08 2.2001214e-01 1.4235993e-15], sum to 1.0000
[2019-04-10 13:16:19,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-10 13:16:19,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2267261.89057977 W.
[2019-04-10 13:16:19,040] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.26666666666667, 64.0, 1.0, 2.0, 0.5404584255249596, 1.0, 2.0, 0.5404584255249596, 1.0, 1.0, 0.9385975528043436, 6.9112, 6.9112, 170.5573041426782, 2267261.89057977, 2267261.89057977, 444251.5981878709], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2305200.0000, 
sim time next is 2305800.0000, 
raw observation next is [32.3, 64.0, 1.0, 2.0, 0.5258675885412258, 1.0, 2.0, 0.5258675885412258, 1.0, 2.0, 0.913258094227123, 6.9112, 6.9112, 170.5573041426782, 2205997.285171993, 2205997.285171993, 433495.1656915894], 
processed observation next is [1.0, 0.6956521739130435, 0.7298578199052131, 0.64, 1.0, 1.0, 0.42875613077256125, 1.0, 1.0, 0.42875613077256125, 1.0, 1.0, 0.8942171880818572, 0.0, 0.0, 0.8375144448122397, 0.612777023658887, 0.612777023658887, 0.6470077099874468], 
reward next is 0.3530, 
noisyNet noise sample is [array([1.482607], dtype=float32), 0.18540844]. 
=============================================
[2019-04-10 13:16:19,641] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9176734e-10 9.9883956e-01 2.3526960e-09 1.1604772e-03 1.3055376e-16], sum to 1.0000
[2019-04-10 13:16:19,648] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8291
[2019-04-10 13:16:19,654] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2340205.160236395 W.
[2019-04-10 13:16:19,659] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666667, 64.16666666666667, 1.0, 2.0, 0.8367450125319527, 1.0, 2.0, 0.8367450125319527, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2340205.160236395, 2340205.160236396, 438158.5044127593], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2303400.0000, 
sim time next is 2304000.0000, 
raw observation next is [32.2, 64.0, 1.0, 2.0, 0.8549220479954734, 1.0, 2.0, 0.8549220479954734, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2391091.257932258, 2391091.257932258, 447496.5918965309], 
processed observation next is [1.0, 0.6956521739130435, 0.7251184834123224, 0.64, 1.0, 1.0, 0.8252072867415342, 1.0, 1.0, 0.8252072867415342, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6641920160922938, 0.6641920160922938, 0.6679053610395983], 
reward next is 0.3321, 
noisyNet noise sample is [array([-0.8246637], dtype=float32), 0.64280915]. 
=============================================
[2019-04-10 13:16:19,666] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[46.754227]
 [45.570652]
 [44.721176]
 [45.11814 ]
 [46.684414]], R is [[47.44968033]
 [47.32121658]
 [47.19905853]
 [47.06373978]
 [46.5931015 ]].
[2019-04-10 13:16:22,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3739768e-18 1.0000000e+00 4.2465029e-20 1.9947165e-23 1.2323182e-29], sum to 1.0000
[2019-04-10 13:16:22,014] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8755
[2019-04-10 13:16:22,020] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.5, 1.0, 2.0, 0.781209157663075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1091820.399190727, 1091820.399190727, 239349.6336851854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2358600.0000, 
sim time next is 2359200.0000, 
raw observation next is [28.96666666666667, 74.0, 1.0, 2.0, 0.7996931649827262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117667.301985129, 1117667.301985129, 243834.6053594766], 
processed observation next is [1.0, 0.30434782608695654, 0.5718799368088469, 0.74, 1.0, 1.0, 0.7586664638346099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3104631394403136, 0.3104631394403136, 0.363932246805189], 
reward next is 0.6361, 
noisyNet noise sample is [array([0.18069977], dtype=float32), -0.49162552]. 
=============================================
[2019-04-10 13:16:26,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8273912e-20 1.0000000e+00 3.1352968e-21 2.2240161e-26 4.4803853e-32], sum to 1.0000
[2019-04-10 13:16:26,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2043
[2019-04-10 13:16:26,528] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 91.5, 1.0, 2.0, 0.402772868211731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596731.3529397737, 596731.3529397737, 174111.2858917953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.3994368544729122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 593040.6507495865, 593040.6507495872, 173807.9887622953], 
processed observation next is [1.0, 0.8695652173913043, 0.30489731437598716, 0.9233333333333335, 1.0, 1.0, 0.276429945148087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16473351409710738, 0.16473351409710757, 0.25941490860044075], 
reward next is 0.7406, 
noisyNet noise sample is [array([-0.73141575], dtype=float32), -1.4482558]. 
=============================================
[2019-04-10 13:16:27,441] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9940794e-10 9.9947792e-01 1.2480765e-08 5.2207027e-04 2.2587761e-16], sum to 1.0000
[2019-04-10 13:16:27,447] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2838
[2019-04-10 13:16:27,452] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2149641.392037277 W.
[2019-04-10 13:16:27,456] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.81666666666666, 83.66666666666667, 1.0, 2.0, 0.5124468876296249, 1.0, 2.0, 0.5124468876296249, 1.0, 1.0, 0.884432771811789, 6.911199999999999, 6.9112, 170.5573041426782, 2149641.392037277, 2149641.392037277, 422835.1162929137], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2477400.0000, 
sim time next is 2478000.0000, 
raw observation next is [27.93333333333333, 83.33333333333334, 1.0, 2.0, 0.4640589388172332, 1.0, 2.0, 0.4640589388172332, 1.0, 2.0, 0.8017958996789654, 6.9112, 6.9112, 170.5573041426782, 1946476.461927994, 1946476.461927994, 390498.221086636], 
processed observation next is [1.0, 0.6956521739130435, 0.522906793048973, 0.8333333333333335, 1.0, 1.0, 0.35428787809305207, 1.0, 1.0, 0.35428787809305207, 1.0, 1.0, 0.7582876825353236, 0.0, 0.0, 0.8375144448122397, 0.5406879060911094, 0.5406879060911094, 0.5828331658009492], 
reward next is 0.4172, 
noisyNet noise sample is [array([0.5526571], dtype=float32), -1.5015708]. 
=============================================
[2019-04-10 13:16:27,465] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[48.222748]
 [50.76527 ]
 [51.981506]
 [50.78579 ]
 [49.27978 ]], R is [[47.91808319]
 [47.43890381]
 [47.3888588 ]
 [46.99923325]
 [46.59632492]].
[2019-04-10 13:16:32,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2507810e-19 1.0000000e+00 4.7952383e-22 1.4748483e-27 1.0496223e-32], sum to 1.0000
[2019-04-10 13:16:32,486] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5650
[2019-04-10 13:16:32,495] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3496208299396052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538583.736769365, 538583.7367693643, 169700.2904138055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3482396457427301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536454.1334988913, 536454.1334988913, 169525.7583088381], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21474656113581939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14901503708302535, 0.14901503708302535, 0.25302351986393745], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.4664292], dtype=float32), -0.37478164]. 
=============================================
[2019-04-10 13:16:36,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3296301e-22 1.0000000e+00 1.4879682e-22 8.8253820e-29 1.6412952e-33], sum to 1.0000
[2019-04-10 13:16:36,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6334
[2019-04-10 13:16:36,096] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4759374510523071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665038.117422774, 665038.117422774, 179911.7946116094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2701800.0000, 
sim time next is 2702400.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.47593610972697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 665036.2425720012, 665036.2425720006, 179911.5940921991], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.3685977225626144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18473228960333365, 0.1847322896033335, 0.2685247673017897], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.39576322], dtype=float32), 1.9826164]. 
=============================================
[2019-04-10 13:16:41,196] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.7847160e-21 1.0000000e+00 2.1583512e-22 1.5575653e-28 5.5742467e-33], sum to 1.0000
[2019-04-10 13:16:41,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6468
[2019-04-10 13:16:41,204] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3931210197645402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586595.8043169726, 586595.804316972, 173306.165648713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2742000.0000, 
sim time next is 2742600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3927254813660197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586005.8747645393, 586005.87476454, 173252.3685203411], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2683439534530358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16277940965681648, 0.16277940965681667, 0.2585856246572255], 
reward next is 0.7414, 
noisyNet noise sample is [array([1.3785069], dtype=float32), -0.18946326]. 
=============================================
[2019-04-10 13:16:48,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5556476e-21 1.0000000e+00 4.4510900e-22 3.3911032e-26 3.6480342e-32], sum to 1.0000
[2019-04-10 13:16:48,485] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3498
[2019-04-10 13:16:48,550] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4091672451503332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602870.9496955557, 602870.9496955563, 174580.6887437804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4093354669916394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 603119.3580655652, 603119.3580655647, 174603.8807527346], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.2883559843272764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16753315501821256, 0.1675331550182124, 0.26060280709363376], 
reward next is 0.7394, 
noisyNet noise sample is [array([0.32072127], dtype=float32), 1.5947204]. 
=============================================
[2019-04-10 13:16:48,621] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.56627 ]
 [73.449585]
 [73.30015 ]
 [73.13195 ]
 [73.04802 ]], R is [[73.69280243]
 [73.69530487]
 [73.69773102]
 [73.69923401]
 [73.70058441]].
[2019-04-10 13:16:51,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4487900e-20 1.0000000e+00 2.2278843e-22 5.5947148e-27 2.2841782e-33], sum to 1.0000
[2019-04-10 13:16:51,766] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6836
[2019-04-10 13:16:51,815] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4030452091082039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593853.7226833516, 593853.722683351, 173743.3388767832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2829600.0000, 
sim time next is 2830200.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4083411635024524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601657.3465114563, 601657.3465114563, 174467.6989200493], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28715802831620774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16712704069762674, 0.16712704069762674, 0.26039955062693926], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.16348219], dtype=float32), 2.3769069]. 
=============================================
[2019-04-10 13:16:57,074] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 13:16:57,077] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:16:57,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:16:57,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run55
[2019-04-10 13:16:57,193] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:16:57,194] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:16:57,197] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run55
[2019-04-10 13:16:57,250] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:16:57,252] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:16:57,256] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run55
[2019-04-10 13:16:57,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:16:57,326] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:16:57,329] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:16:57,329] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:16:57,332] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run55
[2019-04-10 13:16:57,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run55
[2019-04-10 13:17:37,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12090272], dtype=float32), 0.090598576]
[2019-04-10 13:17:37,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.06666666666666, 91.5, 1.0, 2.0, 0.3797182314824935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571826.2283709187, 571826.2283709194, 172144.4566429699]
[2019-04-10 13:17:37,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:17:37,988] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9837891e-21 1.0000000e+00 2.6544210e-23 4.3363516e-30 5.3852321e-34], sampled 0.2532157151291704
[2019-04-10 13:17:51,859] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12090272], dtype=float32), 0.090598576]
[2019-04-10 13:17:51,860] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.05, 66.5, 1.0, 2.0, 0.934819191905254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1306638.36972699, 1306638.36972699, 279698.539619702]
[2019-04-10 13:17:51,861] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:17:51,864] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1800342e-19 1.0000000e+00 7.1009514e-21 3.3231564e-25 5.1814841e-31], sampled 0.562414405421596
[2019-04-10 13:17:55,927] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12090272], dtype=float32), 0.090598576]
[2019-04-10 13:17:55,928] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.04581808, 84.57391763999999, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 1.0, 2.0, 1.03, 9.19788434170733, 6.9112, 184.5923449428631, 5515003.214386855, 3742164.630311346, 696168.6933916899]
[2019-04-10 13:17:55,929] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:17:55,932] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.3815221e-09 9.9997818e-01 2.7536805e-08 2.1855145e-05 1.0603258e-14], sampled 0.874175305077882
[2019-04-10 13:17:55,934] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 5515003.214386855 W.
[2019-04-10 13:17:58,695] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12090272], dtype=float32), 0.090598576]
[2019-04-10 13:17:58,696] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.26666666666667, 58.33333333333334, 1.0, 2.0, 0.6881674402832869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 961726.1175061441, 961726.1175061434, 218319.1452258741]
[2019-04-10 13:17:58,696] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:17:58,700] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6341341e-20 1.0000000e+00 1.9254124e-22 3.0462280e-28 5.5026676e-33], sampled 0.0841214966139392
[2019-04-10 13:18:26,582] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12090272], dtype=float32), 0.090598576]
[2019-04-10 13:18:26,584] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.22624358, 67.66206256999999, 1.0, 2.0, 0.6781721097693901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031966.568355469, 1031966.568355468, 226837.8320159024]
[2019-04-10 13:18:26,585] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:18:26,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9179894e-19 1.0000000e+00 4.0850040e-21 5.3000643e-26 3.2565218e-31], sampled 0.4727468124570675
[2019-04-10 13:18:33,926] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 13:18:34,044] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.5910 2779435681.1350 934.0000
[2019-04-10 13:18:34,061] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4239 3007808101.3514 1765.0000
[2019-04-10 13:18:34,139] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.8155 3163613242.2676 1773.0000
[2019-04-10 13:18:34,279] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.4542 2842530263.4495 1129.0000
[2019-04-10 13:18:35,294] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1350000, evaluation results [1350000.0, 7884.815457303631, 3163613242.2676315, 1773.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8659.590990649129, 2779435681.1349974, 934.0, 7998.423869034422, 3007808101.3514414, 1765.0, 8498.454249566428, 2842530263.449465, 1129.0]
[2019-04-10 13:18:41,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4117984e-21 1.0000000e+00 1.0223175e-22 1.4015936e-27 6.2074500e-34], sum to 1.0000
[2019-04-10 13:18:41,576] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1239
[2019-04-10 13:18:41,580] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3050693866945573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485804.1972196756, 485804.197219675, 166039.5910107884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3038629939412646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483883.6642718208, 483883.6642718214, 165900.8632374746], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16128071559188503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13441212896439467, 0.13441212896439483, 0.24761322871264868], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.03448802], dtype=float32), -0.51860255]. 
=============================================
[2019-04-10 13:18:42,146] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.85125128e-19 1.00000000e+00 1.60974376e-21 1.03342406e-25
 7.12334824e-32], sum to 1.0000
[2019-04-10 13:18:42,150] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2924
[2019-04-10 13:18:42,154] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3034572994969214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483238.8638803154, 483238.863880316, 165854.415253623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3024600.0000, 
sim time next is 3025200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3039788098850856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484069.1000402491, 484069.1000402485, 165914.2516932636], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16142025287359707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1344636389000692, 0.13446363890006902, 0.24763321148248296], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.15642227], dtype=float32), 0.6729486]. 
=============================================
[2019-04-10 13:18:42,559] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3916716e-19 1.0000000e+00 7.5053078e-21 1.0608002e-25 1.2531946e-31], sum to 1.0000
[2019-04-10 13:18:42,563] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5021
[2019-04-10 13:18:42,569] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666666, 96.0, 1.0, 2.0, 0.6815411511090337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1020290.962484362, 1020290.962484362, 225630.4072453968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [22.83333333333334, 95.0, 1.0, 2.0, 0.6945003252120712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1038103.871645765, 1038103.871645764, 228399.7974447252], 
processed observation next is [1.0, 0.43478260869565216, 0.2812006319115327, 0.95, 1.0, 1.0, 0.631928102665146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28836218656826806, 0.2883621865682678, 0.34089522006675405], 
reward next is 0.6591, 
noisyNet noise sample is [array([-0.5100322], dtype=float32), 0.57757246]. 
=============================================
[2019-04-10 13:18:42,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.47356]
 [67.72306]
 [67.93057]
 [68.13851]
 [68.54047]], R is [[67.23163605]
 [67.22255707]
 [67.22016144]
 [67.22151184]
 [67.21791077]].
[2019-04-10 13:18:45,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0144340e-19 1.0000000e+00 6.0791894e-21 1.7980231e-25 1.2050344e-30], sum to 1.0000
[2019-04-10 13:18:45,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9654
[2019-04-10 13:18:45,936] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 89.0, 1.0, 2.0, 0.663191505572691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 996052.9649350531, 996052.9649350538, 221913.1927502466], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3144600.0000, 
sim time next is 3145200.0000, 
raw observation next is [23.66666666666667, 89.0, 1.0, 2.0, 0.7330855957331389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1094865.145498275, 1094865.145498275, 237409.9708477152], 
processed observation next is [1.0, 0.391304347826087, 0.3206951026856243, 0.89, 1.0, 1.0, 0.6784163804013722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3041292070828542, 0.3041292070828542, 0.3543432400712167], 
reward next is 0.6457, 
noisyNet noise sample is [array([2.1733873], dtype=float32), 0.37704816]. 
=============================================
[2019-04-10 13:18:48,152] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7405413e-17 1.0000000e+00 8.2188083e-17 5.6992599e-17 9.5672403e-26], sum to 1.0000
[2019-04-10 13:18:48,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2497
[2019-04-10 13:18:48,163] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.4777586850166737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667583.7682844135, 667583.7682844135, 180186.5019127022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3173400.0000, 
sim time next is 3174000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.485593023591738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678534.3753033205, 678534.3753033205, 181371.0057863951], 
processed observation next is [1.0, 0.7391304347826086, 0.4786729857819906, 0.84, 1.0, 1.0, 0.3802325585442627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18848177091758903, 0.18848177091758903, 0.27070299371103745], 
reward next is 0.7293, 
noisyNet noise sample is [array([0.94472855], dtype=float32), 0.27121735]. 
=============================================
[2019-04-10 13:18:48,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[65.639084]
 [63.101635]
 [58.712227]
 [54.434902]
 [54.29693 ]], R is [[67.46784973]
 [67.52423859]
 [67.579422  ]
 [67.59713745]
 [67.38919067]].
[2019-04-10 13:19:00,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1370412e-09 9.6977019e-01 1.8002773e-08 3.0229777e-02 5.6126665e-15], sum to 1.0000
[2019-04-10 13:19:00,867] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1885
[2019-04-10 13:19:00,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2529521.161973869 W.
[2019-04-10 13:19:00,883] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.6029112549521173, 1.0, 2.0, 0.6029112549521173, 1.0, 1.0, 1.03, 6.930375265979452, 6.9112, 170.5573041426782, 2529521.161973869, 2515785.138952839, 489035.894652951], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3513600.0000, 
sim time next is 3514200.0000, 
raw observation next is [32.83333333333334, 60.33333333333334, 1.0, 2.0, 0.5809179857468041, 1.0, 2.0, 0.5809179857468041, 1.0, 2.0, 1.008862428728661, 6.911199999999999, 6.9112, 170.5573041426782, 2437158.1587711, 2437158.158771101, 475611.1145544202], 
processed observation next is [1.0, 0.6956521739130435, 0.7551342812006324, 0.6033333333333334, 1.0, 1.0, 0.4950819105383182, 1.0, 1.0, 0.4950819105383182, 1.0, 1.0, 1.0108078399130012, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6769883774364167, 0.6769883774364169, 0.709867335155851], 
reward next is 0.2901, 
noisyNet noise sample is [array([1.5421244], dtype=float32), -0.08448636]. 
=============================================
[2019-04-10 13:19:02,492] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7512197e-19 1.0000000e+00 3.1717223e-21 2.6042109e-24 7.7327677e-31], sum to 1.0000
[2019-04-10 13:19:02,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0909
[2019-04-10 13:19:02,502] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5205249772431118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727362.6020245068, 727362.6020245062, 186874.2498524237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3535800.0000, 
sim time next is 3536400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5192605690757257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725595.1607614532, 725595.1607614526, 186668.7689581645], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.420795866356296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20155421132262588, 0.20155421132262574, 0.27861010292263355], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.8010263], dtype=float32), -0.17041188]. 
=============================================
[2019-04-10 13:19:03,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2636710e-19 1.0000000e+00 9.1217326e-21 8.9366897e-25 2.7064037e-30], sum to 1.0000
[2019-04-10 13:19:03,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3323
[2019-04-10 13:19:03,573] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 81.5, 1.0, 2.0, 0.5865646436976923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 819679.6273570223, 819679.6273570216, 198262.0654952606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3562200.0000, 
sim time next is 3562800.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5959326189443598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832775.7947019509, 832775.7947019509, 199982.5151668325], 
processed observation next is [1.0, 0.21739130434782608, 0.4628751974723541, 0.8066666666666668, 1.0, 1.0, 0.5131718300534455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2313266096394308, 0.2313266096394308, 0.29848136592064556], 
reward next is 0.7015, 
noisyNet noise sample is [array([-0.14708146], dtype=float32), 0.38866392]. 
=============================================
[2019-04-10 13:19:05,511] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8422509e-14 1.0000000e+00 6.8908790e-15 3.4911153e-15 5.0299093e-23], sum to 1.0000
[2019-04-10 13:19:05,517] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3011
[2019-04-10 13:19:05,523] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2216484.236871239 W.
[2019-04-10 13:19:05,526] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.5283650290457884, 1.0, 1.0, 0.5283650290457884, 1.0, 1.0, 0.9089762093329601, 6.9112, 6.9112, 170.5573041426782, 2216484.236871239, 2216484.236871239, 433654.744555379], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3573000.0000, 
sim time next is 3573600.0000, 
raw observation next is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.7200705930933357, 1.0, 2.0, 0.7200705930933357, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2013601.020584433, 2013601.020584433, 382832.1798450688], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590839, 0.7133333333333333, 1.0, 1.0, 0.66273565432932, 1.0, 1.0, 0.66273565432932, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5593336168290092, 0.5593336168290092, 0.5713913132015952], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3361302], dtype=float32), -1.5229621]. 
=============================================
[2019-04-10 13:19:07,350] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8216361e-11 9.9999976e-01 6.2123064e-11 2.1005914e-07 2.6509676e-17], sum to 1.0000
[2019-04-10 13:19:07,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7942
[2019-04-10 13:19:07,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3155425.420551915 W.
[2019-04-10 13:19:07,370] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 9.308516616330783, 6.9112, 168.8990970340019, 3155425.420551915, 1454826.050979828, 308445.1513394431], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3989400.0000, 
sim time next is 3990000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5158223743737959, 1.0, 1.0, 0.5158223743737959, 1.0, 1.0, 0.8958128792213854, 6.911200000000001, 6.9112, 170.5573041426782, 2163815.37787535, 2163815.37787535, 426260.4361071213], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.41665346310095885, 1.0, 0.5, 0.41665346310095885, 1.0, 0.5, 0.8729425356358358, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6010598271875972, 0.6010598271875972, 0.6362096061300317], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.34988886], dtype=float32), -0.3726821]. 
=============================================
[2019-04-10 13:19:07,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[50.18322 ]
 [50.09612 ]
 [51.592945]
 [49.180805]
 [49.73661 ]], R is [[46.2501297 ]
 [45.78762817]
 [45.32975388]
 [44.87645721]
 [44.42769241]].
[2019-04-10 13:19:10,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4682752e-19 1.0000000e+00 4.8493229e-21 5.5697023e-24 1.0006563e-30], sum to 1.0000
[2019-04-10 13:19:10,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6074
[2019-04-10 13:19:10,135] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4953545255053409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692178.8722590733, 692178.872259074, 182871.0488825453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3708600.0000, 
sim time next is 3709200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4934420051251383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689505.5630190618, 689505.5630190624, 182574.681723728], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.74, 1.0, 1.0, 0.3896891628013715, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1915293230608505, 0.19152932306085066, 0.27249952496078805], 
reward next is 0.7275, 
noisyNet noise sample is [array([-0.0328618], dtype=float32), 0.21066202]. 
=============================================
[2019-04-10 13:19:14,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7873053e-20 1.0000000e+00 4.2614604e-22 1.6055632e-26 8.2073098e-32], sum to 1.0000
[2019-04-10 13:19:14,573] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8504
[2019-04-10 13:19:14,576] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5172292827914047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722755.7521721631, 722755.7521721631, 186339.6530535103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3807600.0000, 
sim time next is 3808200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5169078942163573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722306.5037944618, 722306.5037944611, 186287.6925481673], 
processed observation next is [0.0, 0.043478260869565216, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4179613183329606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006406954984616, 0.20064069549846142, 0.2780413321614437], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.35200024], dtype=float32), 1.3462033]. 
=============================================
[2019-04-10 13:19:15,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5103613e-20 1.0000000e+00 2.2758241e-22 2.2902404e-27 8.0609795e-32], sum to 1.0000
[2019-04-10 13:19:15,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9892
[2019-04-10 13:19:15,753] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.563359603511998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787240.3256501538, 787240.3256501532, 194118.7997809594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3834000.0000, 
sim time next is 3834600.0000, 
raw observation next is [31.33333333333334, 69.5, 1.0, 2.0, 0.5714166052600571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798503.4350052854, 798503.4350052854, 195542.0834691583], 
processed observation next is [0.0, 0.391304347826087, 0.6840442338072673, 0.695, 1.0, 1.0, 0.4836344641687435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2218065097236904, 0.2218065097236904, 0.2918538559241169], 
reward next is 0.7081, 
noisyNet noise sample is [array([-1.5312753], dtype=float32), -1.2768136]. 
=============================================
[2019-04-10 13:19:16,005] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-10 13:19:16,007] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:19:16,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:19:16,008] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:19:16,009] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:19:16,009] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:19:16,009] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:19:16,010] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:19:16,010] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:19:16,010] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:19:16,012] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:19:16,035] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run56
[2019-04-10 13:19:16,036] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run56
[2019-04-10 13:19:16,036] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run56
[2019-04-10 13:19:16,087] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run56
[2019-04-10 13:19:16,109] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run56
[2019-04-10 13:19:47,479] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:19:47,480] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.46666666666667, 69.33333333333334, 1.0, 2.0, 0.5372240362921021, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564456753, 750705.5031788577, 750705.5031788572, 189637.0511470276]
[2019-04-10 13:19:47,480] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:19:47,483] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3596410e-14 1.0000000e+00 5.3232772e-14 7.8729953e-11 1.8073643e-22], sampled 0.4057620256819995
[2019-04-10 13:19:47,836] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:19:47,836] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.6, 80.66666666666667, 1.0, 2.0, 0.505221790697618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705971.3818317507, 705971.3818317514, 184416.5583485273]
[2019-04-10 13:19:47,837] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:19:47,839] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.9454227e-20 1.0000000e+00 2.9374827e-22 1.1872730e-27 6.3978120e-32], sampled 0.2740690824685452
[2019-04-10 13:19:53,627] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:19:53,628] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.73333333333333, 80.66666666666667, 1.0, 2.0, 0.5545567646616034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774934.7261574421, 774934.7261574421, 192585.269360329]
[2019-04-10 13:19:53,628] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:19:53,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.860697e-20 1.000000e+00 3.159777e-22 2.086911e-27 5.555463e-32], sampled 0.06264516843056622
[2019-04-10 13:20:06,587] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:20:06,588] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.02978057333333, 70.62481011, 1.0, 2.0, 0.7040680244015775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983957.7580959208, 983957.7580959202, 221736.3366749641]
[2019-04-10 13:20:06,590] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:20:06,592] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1888513e-20 1.0000000e+00 5.6813591e-22 5.0294989e-26 6.5628367e-32], sampled 0.0006051862229257132
[2019-04-10 13:20:07,728] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:20:07,729] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.5, 86.5, 1.0, 2.0, 0.5745347565422327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802862.4184390918, 802862.4184390925, 196097.8567500488]
[2019-04-10 13:20:07,730] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:20:07,731] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3276030e-19 1.0000000e+00 2.9711582e-21 2.9669843e-25 9.9817827e-31], sampled 0.8912379745038461
[2019-04-10 13:20:19,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:20:19,676] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.96962588333333, 68.50653393, 1.0, 2.0, 0.4513271359935815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647929.935568798, 647929.9355687986, 178540.016302756]
[2019-04-10 13:20:19,677] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:20:19,680] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2664703e-22 1.0000000e+00 2.0971690e-24 2.3443169e-29 3.0938575e-35], sampled 0.14895334641359792
[2019-04-10 13:20:19,693] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:20:19,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.29851371666667, 69.427973355, 1.0, 2.0, 0.5038302685724638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704026.2948309104, 704026.2948309104, 184198.5988004312]
[2019-04-10 13:20:19,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:20:19,698] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6518063e-22 1.0000000e+00 4.5527283e-25 7.8004362e-31 6.6933354e-36], sampled 0.6627597918971089
[2019-04-10 13:20:45,900] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7900.4749 3162898556.2546 1743.0000
[2019-04-10 13:20:46,088] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:20:46,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.0, 65.0, 1.0, 2.0, 0.3623803764898217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561138.8702484408, 561138.8702484408, 171658.93867891]
[2019-04-10 13:20:46,091] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:20:46,093] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.09732754e-20 1.00000000e+00 4.16275747e-23 2.58960209e-29
 5.03518230e-33], sampled 0.17929405028727352
[2019-04-10 13:20:46,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12434776], dtype=float32), 0.09451026]
[2019-04-10 13:20:46,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.51666666666667, 58.16666666666666, 1.0, 2.0, 0.5283957188498195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831513.3197592105, 831513.3197592105, 198939.2942795937]
[2019-04-10 13:20:46,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:20:46,176] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.9191493e-20 1.0000000e+00 5.1633746e-22 3.0432212e-27 1.2885144e-31], sampled 0.7222728775743614
[2019-04-10 13:20:46,494] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.2080 2927445837.1651 1338.0000
[2019-04-10 13:20:46,538] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1452 3007590622.7768 1760.0000
[2019-04-10 13:20:46,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.4797 2842139634.0298 1123.0000
[2019-04-10 13:20:46,599] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2550 2779260415.7449 931.0000
[2019-04-10 13:20:47,614] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1375000, evaluation results [1375000.0, 7900.474852502655, 3162898556.2545977, 1743.0, 8255.208025104432, 2927445837.165103, 1338.0, 8660.255039405049, 2779260415.744892, 931.0, 7998.145211605765, 3007590622.776794, 1760.0, 8496.479712049008, 2842139634.029849, 1123.0]
[2019-04-10 13:20:48,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7256665e-21 1.0000000e+00 5.6808796e-23 4.3248109e-27 9.1809488e-34], sum to 1.0000
[2019-04-10 13:20:48,060] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2547
[2019-04-10 13:20:48,067] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5719960276763714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 799313.430501277, 799313.4305012776, 195645.613357322], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3870000.0000, 
sim time next is 3870600.0000, 
raw observation next is [31.66666666666667, 67.5, 1.0, 2.0, 0.5757454453705877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804554.8916714948, 804554.8916714948, 196314.2795485684], 
processed observation next is [0.0, 0.8260869565217391, 0.6998420221169038, 0.675, 1.0, 1.0, 0.4888499341814309, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22348746990874857, 0.22348746990874857, 0.293006387385923], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.2636644], dtype=float32), 1.6512563]. 
=============================================
[2019-04-10 13:20:49,064] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7298983e-20 1.0000000e+00 4.8351872e-23 7.5816524e-28 1.3118634e-33], sum to 1.0000
[2019-04-10 13:20:49,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3148
[2019-04-10 13:20:49,074] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.538117117563818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751953.9177811923, 751953.9177811918, 189783.508443075], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3874800.0000, 
sim time next is 3875400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5369604764834505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750337.0801989657, 750337.0801989657, 189589.4968473333], 
processed observation next is [0.0, 0.8695652173913043, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4421210560041572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20842696672193492, 0.20842696672193492, 0.28296939827960194], 
reward next is 0.7170, 
noisyNet noise sample is [array([-0.02235752], dtype=float32), 1.9599688]. 
=============================================
[2019-04-10 13:20:54,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1917667e-11 9.9999976e-01 3.5474249e-11 2.5827740e-07 8.1159839e-19], sum to 1.0000
[2019-04-10 13:20:54,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2575
[2019-04-10 13:20:54,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1830734.632264389 W.
[2019-04-10 13:20:54,628] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 83.16666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.442226374250771, 6.9112, 168.9099471178965, 1830734.632264389, 1454012.957734184, 311353.9970146725], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3988200.0000, 
sim time next is 3988800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5795295951878836, 1.0, 1.0, 0.5795295951878836, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1620295.880985337, 1620295.880985337, 326822.6294306806], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4934091508287754, 1.0, 0.5, 0.4934091508287754, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4500821891625936, 0.4500821891625936, 0.48779496929952326], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7862942], dtype=float32), -0.46232408]. 
=============================================
[2019-04-10 13:20:56,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9389441e-09 3.9580357e-01 4.1181487e-08 6.0419637e-01 1.6449664e-14], sum to 1.0000
[2019-04-10 13:20:56,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1380
[2019-04-10 13:20:56,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2607817.587098838 W.
[2019-04-10 13:20:56,628] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.5, 71.0, 1.0, 2.0, 0.9323306238956467, 1.0, 2.0, 0.9323306238956467, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2607817.587098838, 2607817.587098838, 489447.3224562979], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4102200.0000, 
sim time next is 4102800.0000, 
raw observation next is [32.66666666666667, 71.0, 1.0, 2.0, 0.8931129127303992, 1.0, 2.0, 0.8931129127303992, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2498012.256786001, 2498012.256786002, 467760.4990942766], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458138, 0.71, 1.0, 1.0, 0.8712203767836134, 1.0, 1.0, 0.8712203767836134, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6938922935516669, 0.6938922935516671, 0.698149998648174], 
reward next is 0.3019, 
noisyNet noise sample is [array([2.061303], dtype=float32), 0.55028737]. 
=============================================
[2019-04-10 13:21:16,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.5206267e-21 1.0000000e+00 2.1786331e-23 4.4521951e-29 2.5537427e-32], sum to 1.0000
[2019-04-10 13:21:16,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0571
[2019-04-10 13:21:16,258] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 77.33333333333333, 1.0, 2.0, 0.5051185135505399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705827.0196237055, 705827.0196237055, 184401.4349118448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4524000.0000, 
sim time next is 4524600.0000, 
raw observation next is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.5013865127069486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700610.3909894257, 700610.3909894257, 183813.1069169299], 
processed observation next is [0.0, 0.34782608695652173, 0.5181674565560824, 0.7566666666666667, 1.0, 1.0, 0.39926085868307054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1946139974970627, 0.1946139974970627, 0.27434792077153713], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.23877592], dtype=float32), -0.83040756]. 
=============================================
[2019-04-10 13:21:17,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1335702e-16 1.0000000e+00 5.3842308e-17 2.0037665e-18 4.5223162e-25], sum to 1.0000
[2019-04-10 13:21:17,455] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8091
[2019-04-10 13:21:17,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1844237.281302219 W.
[2019-04-10 13:21:17,468] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.6779241247416935, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980305339627977, 6.9112, 168.9124896977678, 1844237.281302219, 1795211.710436739, 380853.4195845381], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.6851316596096274, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980737448267842, 6.9112, 168.9124865000416, 1854322.927990901, 1804990.806165386, 382362.4107432378], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.74, 1.0, 1.0, 0.620640553746539, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006953744826784192, 0.0, 0.8294376371861191, 0.5150897022196947, 0.5013863350459405, 0.5706901652884147], 
reward next is 0.0816, 
noisyNet noise sample is [array([-0.05183837], dtype=float32), 0.21343702]. 
=============================================
[2019-04-10 13:21:19,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1580371e-19 1.0000000e+00 5.9735391e-20 1.3148216e-22 7.9759885e-29], sum to 1.0000
[2019-04-10 13:21:19,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6997
[2019-04-10 13:21:19,487] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 94.0, 1.0, 2.0, 0.9976301526291429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9127890931008, 1394489.705403513, 1394489.705403513, 298209.3153583678], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4600800.0000, 
sim time next is 4601400.0000, 
raw observation next is [28.16666666666667, 93.16666666666667, 1.0, 2.0, 0.9919741131948946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564686264, 1386578.516435549, 1386578.516435549, 296494.8538506472], 
processed observation next is [1.0, 0.2608695652173913, 0.5339652448657191, 0.9316666666666668, 1.0, 1.0, 0.9903302568613188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399449470229, 0.3851606990098747, 0.3851606990098747, 0.4425296326129063], 
reward next is 0.5575, 
noisyNet noise sample is [array([-0.06912969], dtype=float32), -0.6174027]. 
=============================================
[2019-04-10 13:21:19,631] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2935063e-19 1.0000000e+00 1.6894763e-21 5.2304695e-26 1.1892974e-30], sum to 1.0000
[2019-04-10 13:21:19,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2696
[2019-04-10 13:21:19,643] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 86.5, 1.0, 2.0, 0.5606127076914825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783400.390023372, 783400.3900233727, 193637.4480344224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4581000.0000, 
sim time next is 4581600.0000, 
raw observation next is [28.0, 87.33333333333333, 1.0, 2.0, 0.5657433917194599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790572.6777913194, 790572.6777913194, 194537.339257731], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.8733333333333333, 1.0, 1.0, 0.4767992671318793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21960352160869984, 0.21960352160869984, 0.290354237698106], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.0460408], dtype=float32), -1.5046664]. 
=============================================
[2019-04-10 13:21:22,323] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4160775e-19 1.0000000e+00 1.4180707e-21 3.9296260e-25 7.8420610e-30], sum to 1.0000
[2019-04-10 13:21:22,330] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7743
[2019-04-10 13:21:22,333] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 89.83333333333333, 1.0, 2.0, 0.9787719358586844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1368112.660358884, 1368112.660358883, 292529.9889960755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4603800.0000, 
sim time next is 4604400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.001329001628105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399663.372522734, 1399663.372522734, 299336.5388062986], 
processed observation next is [1.0, 0.30434782608695654, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0016012067808493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.388795381256315, 0.388795381256315, 0.44677095344223666], 
reward next is 0.5532, 
noisyNet noise sample is [array([-0.19946297], dtype=float32), -0.38263726]. 
=============================================
[2019-04-10 13:21:26,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8081937e-20 1.0000000e+00 3.8717205e-22 4.6951312e-26 6.8167252e-32], sum to 1.0000
[2019-04-10 13:21:26,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8289
[2019-04-10 13:21:26,103] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 77.33333333333333, 1.0, 2.0, 0.5014957585258664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700763.0955392516, 700763.0955392516, 183830.4896508724], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4750800.0000, 
sim time next is 4751400.0000, 
raw observation next is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.4984717210444817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696536.0834554569, 696536.0834554569, 183356.6523347963], 
processed observation next is [1.0, 1.0, 0.5181674565560824, 0.7566666666666667, 1.0, 1.0, 0.3957490614993755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19348224540429357, 0.19348224540429357, 0.27366664527581536], 
reward next is 0.7263, 
noisyNet noise sample is [array([0.99096054], dtype=float32), 0.40404037]. 
=============================================
[2019-04-10 13:21:26,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5264779e-19 1.0000000e+00 3.6095169e-21 4.0586832e-25 4.4966906e-30], sum to 1.0000
[2019-04-10 13:21:26,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9212
[2019-04-10 13:21:26,219] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 83.16666666666667, 1.0, 2.0, 0.8018281900160361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1120652.830650528, 1120652.830650527, 244360.9320847051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4691400.0000, 
sim time next is 4692000.0000, 
raw observation next is [28.33333333333334, 82.33333333333334, 1.0, 2.0, 0.8620881488115991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1204921.269443467, 1204921.269443467, 259722.8437797384], 
processed observation next is [1.0, 0.30434782608695654, 0.5418641390205374, 0.8233333333333335, 1.0, 1.0, 0.8338411431465049, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33470035262318526, 0.33470035262318526, 0.3876460354921469], 
reward next is 0.6124, 
noisyNet noise sample is [array([0.77660227], dtype=float32), 0.45350984]. 
=============================================
[2019-04-10 13:21:26,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[59.528954]
 [59.385082]
 [59.253803]
 [58.993526]
 [58.850266]], R is [[59.24867249]
 [59.29146576]
 [59.33127213]
 [59.37611008]
 [59.40673828]].
[2019-04-10 13:21:28,155] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 13:21:28,157] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:21:28,157] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:21:28,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:21:28,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:21:28,159] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:21:28,158] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:21:28,160] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:21:28,161] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:21:28,160] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:21:28,164] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:21:28,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run57
[2019-04-10 13:21:28,202] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run57
[2019-04-10 13:21:28,203] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run57
[2019-04-10 13:21:28,240] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run57
[2019-04-10 13:21:28,240] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run57
[2019-04-10 13:21:37,173] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12522498], dtype=float32), 0.09539391]
[2019-04-10 13:21:37,175] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.598529705, 47.302581695, 1.0, 2.0, 0.2883767854850461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475206.7898056176, 475206.7898056176, 164928.7149044216]
[2019-04-10 13:21:37,176] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:21:37,179] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.3295368e-21 1.0000000e+00 1.5603345e-23 2.1671536e-30 2.4851335e-33], sampled 0.9625858428695641
[2019-04-10 13:21:40,911] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12522498], dtype=float32), 0.09539391]
[2019-04-10 13:21:40,912] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.0, 88.33333333333334, 1.0, 2.0, 0.3035256989723165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487182.5567858911, 487182.5567858918, 166172.819773463]
[2019-04-10 13:21:40,912] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:21:40,913] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1354761e-21 1.0000000e+00 2.2663190e-23 2.2873044e-29 2.5049303e-33], sampled 0.3157177223443849
[2019-04-10 13:22:10,230] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12522498], dtype=float32), 0.09539391]
[2019-04-10 13:22:10,231] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.111429885531125, 6.9112, 168.9116008852936, 1595901.0433827, 1453852.211240106, 311346.3237199912]
[2019-04-10 13:22:10,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:22:10,233] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.8473121e-17 1.0000000e+00 4.9642153e-18 3.1614200e-20 9.5003174e-27], sampled 0.8888759787229078
[2019-04-10 13:22:11,933] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12522498], dtype=float32), 0.09539391]
[2019-04-10 13:22:11,956] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.3972548, 84.6173173, 1.0, 2.0, 0.5074561554828999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709094.6122773614, 709094.6122773614, 184771.5586755648]
[2019-04-10 13:22:11,957] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:22:11,970] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0974209e-21 1.0000000e+00 2.0760847e-23 6.8279746e-30 2.6084144e-33], sampled 0.17290653487094299
[2019-04-10 13:22:38,333] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12522498], dtype=float32), 0.09539391]
[2019-04-10 13:22:38,335] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.15, 55.66666666666666, 1.0, 2.0, 0.590149447462524, 0.0, 2.0, 0.0, 1.0, 1.0, 1.024894424837826, 6.911199999999999, 6.9112, 168.9123756631533, 1650022.953664004, 1650022.953664005, 361426.2497821121]
[2019-04-10 13:22:38,336] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:22:38,339] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0502050e-18 1.0000000e+00 3.9304753e-20 9.0248756e-24 6.8007620e-29], sampled 0.8623957461512786
[2019-04-10 13:23:00,653] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12522498], dtype=float32), 0.09539391]
[2019-04-10 13:23:00,744] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.45, 86.0, 1.0, 2.0, 0.9160551498881807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1280395.218191525, 1280395.218191525, 274394.9914200369]
[2019-04-10 13:23:00,745] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:23:00,749] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.2648030e-18 1.0000000e+00 8.1256162e-20 1.5935027e-23 7.8799388e-29], sampled 0.011109443466134294
[2019-04-10 13:23:03,186] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-04-10 13:23:03,314] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-04-10 13:23:03,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.4902 3163926701.9357 1768.0000
[2019-04-10 13:23:03,612] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.4227 2842381616.5614 1129.0000
[2019-04-10 13:23:03,651] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779190307.3385 933.0000
[2019-04-10 13:23:04,666] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1400000, evaluation results [1400000.0, 7885.490158605996, 3163926701.9357243, 1768.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8659.987755389953, 2779190307.3384666, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8498.422652700887, 2842381616.5613914, 1129.0]
[2019-04-10 13:23:04,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.8196647e-21 1.0000000e+00 1.8775056e-22 8.8710251e-27 9.3602505e-32], sum to 1.0000
[2019-04-10 13:23:04,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1393
[2019-04-10 13:23:04,769] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5138844485620362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718080.2332563245, 718080.2332563245, 185800.0178881114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4747200.0000, 
sim time next is 4747800.0000, 
raw observation next is [27.16666666666666, 83.16666666666666, 1.0, 2.0, 0.5125261470826995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716181.5608043185, 716181.5608043191, 185581.8764421499], 
processed observation next is [1.0, 0.9565217391304348, 0.4865718799368086, 0.8316666666666666, 1.0, 1.0, 0.41268210491891505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19893932244564402, 0.1989393224456442, 0.27698787528679086], 
reward next is 0.7230, 
noisyNet noise sample is [array([-0.6302663], dtype=float32), 0.47562715]. 
=============================================
[2019-04-10 13:23:08,268] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.7902266e-24 1.0000000e+00 1.1896901e-26 3.6040051e-33 2.6171800e-37], sum to 1.0000
[2019-04-10 13:23:08,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0325
[2019-04-10 13:23:08,282] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.493551619166988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689658.780671726, 689658.7806717253, 182591.6383510742], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4827000.0000, 
sim time next is 4827600.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4942577802557898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690645.8476630813, 690645.8476630813, 182700.9629118017], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 1.0, 1.0, 0.390672024404566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19184606879530033, 0.19184606879530033, 0.2726880043459727], 
reward next is 0.7273, 
noisyNet noise sample is [array([1.3908292], dtype=float32), -0.6015395]. 
=============================================
[2019-04-10 13:23:09,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4450489e-21 1.0000000e+00 1.4413790e-23 2.1750901e-28 2.0216637e-33], sum to 1.0000
[2019-04-10 13:23:09,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6612
[2019-04-10 13:23:09,719] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 65.66666666666667, 1.0, 2.0, 0.5497628870783279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768233.3641580797, 768233.3641580797, 191761.1359522969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5139600.0000, 
sim time next is 5140200.0000, 
raw observation next is [31.83333333333333, 66.33333333333333, 1.0, 2.0, 0.5587821854353346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780841.4773765682, 780841.4773765688, 193319.4404316455], 
processed observation next is [0.0, 0.4782608695652174, 0.7077409162717218, 0.6633333333333333, 1.0, 1.0, 0.4684122716088369, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21690041038238006, 0.21690041038238023, 0.2885364782561873], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.3871784], dtype=float32), 0.61687934]. 
=============================================
[2019-04-10 13:23:11,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9016048e-11 9.9991906e-01 1.1770955e-09 8.0899175e-05 8.9573279e-18], sum to 1.0000
[2019-04-10 13:23:11,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2199
[2019-04-10 13:23:11,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1824680.328640487 W.
[2019-04-10 13:23:11,950] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333334, 65.0, 1.0, 2.0, 0.4350462596603966, 1.0, 2.0, 0.4350462596603966, 1.0, 2.0, 0.7435062238322377, 6.9112, 6.9112, 170.5573041426782, 1824680.328640487, 1824680.328640487, 371266.547798318], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4970400.0000, 
sim time next is 4971000.0000, 
raw observation next is [30.36666666666666, 65.0, 1.0, 2.0, 0.6661989898073372, 1.0, 2.0, 0.6661989898073372, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1862823.831492047, 1862823.831492047, 359984.8405530282], 
processed observation next is [1.0, 0.5217391304347826, 0.6382306477093204, 0.65, 1.0, 1.0, 0.597830108201611, 1.0, 1.0, 0.597830108201611, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5174510643033464, 0.5174510643033464, 0.5372908067955645], 
reward next is 0.4627, 
noisyNet noise sample is [array([0.8674731], dtype=float32), 0.19344123]. 
=============================================
[2019-04-10 13:23:11,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[51.041893]
 [51.023937]
 [50.959175]
 [49.495514]
 [51.721344]], R is [[51.97730637]
 [51.90340424]
 [51.3843689 ]
 [50.95924759]
 [50.7960701 ]].
[2019-04-10 13:23:18,038] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.34082706e-20 1.00000000e+00 5.55137827e-23 3.62560645e-29
 1.17076345e-33], sum to 1.0000
[2019-04-10 13:23:18,044] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9489
[2019-04-10 13:23:18,047] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5159538375002802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720972.8905128329, 720972.8905128335, 186134.0273546086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5050800.0000, 
sim time next is 5051400.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5169274923730585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722333.8987893036, 722333.8987893036, 186291.2622685379], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.63, 1.0, 1.0, 0.41798493056994995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.200648305219251, 0.200648305219251, 0.27804666010229534], 
reward next is 0.7220, 
noisyNet noise sample is [array([1.0649376], dtype=float32), -0.34976164]. 
=============================================
[2019-04-10 13:23:18,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3089293e-19 1.0000000e+00 4.4551886e-20 8.2906581e-19 5.9812839e-30], sum to 1.0000
[2019-04-10 13:23:18,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8307
[2019-04-10 13:23:18,112] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.9, 58.66666666666667, 1.0, 2.0, 0.5801915235344598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810770.2759361384, 810770.2759361389, 197116.7241275434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5334000.0000, 
sim time next is 5334600.0000, 
raw observation next is [34.65, 59.83333333333334, 1.0, 2.0, 0.5850136715667648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817511.4272832045, 817511.4272832045, 197989.9596421068], 
processed observation next is [1.0, 0.7391304347826086, 0.8412322274881516, 0.5983333333333334, 1.0, 1.0, 0.5000164717671864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2270865075786679, 0.2270865075786679, 0.2955074024509057], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.4553191], dtype=float32), -0.5128021]. 
=============================================
[2019-04-10 13:23:18,517] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.1872456e-21 1.0000000e+00 4.2398112e-23 1.1265332e-29 4.2163384e-33], sum to 1.0000
[2019-04-10 13:23:18,517] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6197
[2019-04-10 13:23:18,521] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 63.0, 1.0, 2.0, 0.5270728645859347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736515.5528379027, 736515.5528379027, 187946.740540994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5061000.0000, 
sim time next is 5061600.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5224838780806448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730100.8392509491, 730100.8392509497, 187193.9312395524], 
processed observation next is [0.0, 0.6086956521739131, 0.6682464454976303, 0.63, 1.0, 1.0, 0.42467937118149973, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2028057886808192, 0.20280578868081936, 0.27939392722321255], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.6251416], dtype=float32), -0.28764173]. 
=============================================
[2019-04-10 13:23:24,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.29477495e-09 8.95238042e-01 1.66742815e-08 1.04761936e-01
 2.67357437e-15], sum to 1.0000
[2019-04-10 13:23:24,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4647
[2019-04-10 13:23:24,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2545947.934524485 W.
[2019-04-10 13:23:24,679] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.6068225905639542, 1.0, 2.0, 0.6068225905639542, 1.0, 2.0, 1.03, 6.938011498665479, 6.9112, 170.5573041426782, 2545947.934524485, 2526741.767408367, 490465.6480972185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5239800.0000, 
sim time next is 5240400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9306022139638246, 1.0, 2.0, 0.9306022139638246, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2602978.023796405, 2602978.023796405, 488464.2341784613], 
processed observation next is [1.0, 0.6521739130434783, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9163882095949694, 1.0, 1.0, 0.9163882095949694, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.723049451054557, 0.723049451054557, 0.7290510957887483], 
reward next is 0.2709, 
noisyNet noise sample is [array([1.3920889], dtype=float32), -2.5009158]. 
=============================================
[2019-04-10 13:23:25,688] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0582377e-11 9.9999321e-01 1.2235063e-10 6.8487898e-06 9.2987509e-19], sum to 1.0000
[2019-04-10 13:23:25,694] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3988
[2019-04-10 13:23:25,701] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2187931.675173861 W.
[2019-04-10 13:23:25,706] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.7823482294264833, 1.0, 1.0, 0.7823482294264833, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2187931.675173861, 2187931.675173861, 411375.2194234669], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5230800.0000, 
sim time next is 5231400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.7365688463561714, 1.0, 2.0, 0.7365688463561714, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2059780.988735361, 2059780.988735361, 390179.2031390192], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.6826130678990017, 1.0, 1.0, 0.6826130678990017, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5721613857598226, 0.5721613857598226, 0.5823570196104764], 
reward next is 0.4176, 
noisyNet noise sample is [array([0.44315547], dtype=float32), 1.0869641]. 
=============================================
[2019-04-10 13:23:26,239] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.8182452e-12 9.9998581e-01 4.7622656e-11 1.4204335e-05 3.9798787e-19], sum to 1.0000
[2019-04-10 13:23:26,246] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-10 13:23:26,250] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 70.83333333333334, 1.0, 2.0, 0.4220916984545684, 1.0, 2.0, 0.4220916984545684, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1179875.974753511, 1179875.974753511, 277842.0055216757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5245800.0000, 
sim time next is 5246400.0000, 
raw observation next is [30.66666666666667, 71.66666666666667, 1.0, 2.0, 0.5389898601688142, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564928212, 753173.9029188623, 753173.9029188623, 189932.6102635924], 
processed observation next is [1.0, 0.7391304347826086, 0.6524486571879939, 0.7166666666666667, 1.0, 1.0, 0.44456609658893276, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399450658305, 0.2092149730330173, 0.2092149730330173, 0.28348150785610804], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.5182279], dtype=float32), 1.7104831]. 
=============================================
[2019-04-10 13:23:26,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6819337e-19 1.0000000e+00 7.8515854e-21 3.7568914e-25 2.5173586e-30], sum to 1.0000
[2019-04-10 13:23:26,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0054
[2019-04-10 13:23:26,867] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.78333333333333, 87.33333333333334, 1.0, 2.0, 1.036322603862964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9128935384609, 1448611.003966653, 1448611.003966652, 310192.5487914634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5292600.0000, 
sim time next is 5293200.0000, 
raw observation next is [28.96666666666667, 86.66666666666667, 1.0, 2.0, 0.9336353031729447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1304982.578765579, 1304982.57876558, 279361.8553803675], 
processed observation next is [1.0, 0.2608695652173913, 0.5718799368088469, 0.8666666666666667, 1.0, 1.0, 0.9200425339433069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3624951607682164, 0.36249516076821664, 0.4169579931050261], 
reward next is 0.5830, 
noisyNet noise sample is [array([0.4482962], dtype=float32), 0.24878949]. 
=============================================
[2019-04-10 13:23:39,049] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5096659e-20 1.0000000e+00 2.0827182e-21 4.6004416e-25 1.5423565e-30], sum to 1.0000
[2019-04-10 13:23:39,056] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9560
[2019-04-10 13:23:39,059] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 92.0, 1.0, 2.0, 0.5432124238834983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759076.5392937129, 759076.5392937136, 190643.3372058023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5533200.0000, 
sim time next is 5533800.0000, 
raw observation next is [26.63333333333333, 92.5, 1.0, 2.0, 0.5424073010377316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757951.0718281234, 757951.071828124, 190507.0367575094], 
processed observation next is [1.0, 0.043478260869565216, 0.46129541864139006, 0.925, 1.0, 1.0, 0.4486834952261826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21054196439670095, 0.21054196439670111, 0.2843388608321036], 
reward next is 0.7157, 
noisyNet noise sample is [array([0.3150406], dtype=float32), -0.23362067]. 
=============================================
[2019-04-10 13:23:44,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7130505e-21 1.0000000e+00 1.4404226e-23 3.9317174e-29 9.7264296e-34], sum to 1.0000
[2019-04-10 13:23:44,387] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9179
[2019-04-10 13:23:44,390] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 87.0, 1.0, 2.0, 0.5142109877555255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718536.6795371415, 718536.6795371415, 185852.2360275718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5701800.0000, 
sim time next is 5702400.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5142490701586254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718589.9122754354, 718589.9122754347, 185858.3229558025], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.41475791585376554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19960830896539872, 0.19960830896539852, 0.2774004820235858], 
reward next is 0.7226, 
noisyNet noise sample is [array([0.5201287], dtype=float32), -0.4308298]. 
=============================================
[2019-04-10 13:23:45,123] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 13:23:45,124] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:23:45,125] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:23:45,125] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:23:45,126] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:23:45,126] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:23:45,127] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:23:45,128] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:23:45,128] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:23:45,128] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:23:45,129] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:23:45,142] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run58
[2019-04-10 13:23:45,143] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run58
[2019-04-10 13:23:45,143] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run58
[2019-04-10 13:23:45,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run58
[2019-04-10 13:23:45,199] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run58
[2019-04-10 13:23:53,533] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12283598], dtype=float32), 0.09396195]
[2019-04-10 13:23:53,535] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.23550297333333, 88.18067003666668, 1.0, 2.0, 0.2192941833650476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 365502.1545878722, 365502.1545878722, 157480.7951035607]
[2019-04-10 13:23:53,536] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:23:53,539] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.1323341e-21 1.0000000e+00 2.7748249e-23 9.9845670e-30 3.4134273e-33], sampled 0.5427353906878586
[2019-04-10 13:24:17,180] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12283598], dtype=float32), 0.09396195]
[2019-04-10 13:24:17,181] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.0, 88.0, 1.0, 2.0, 0.5079275817614834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709753.5805608961, 709753.5805608954, 184846.6704589229]
[2019-04-10 13:24:17,181] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:24:17,184] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.03888125e-20 1.00000000e+00 4.36191676e-23 7.84682239e-29
 9.15468903e-33], sampled 0.6066085465864933
[2019-04-10 13:24:36,877] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12283598], dtype=float32), 0.09396195]
[2019-04-10 13:24:36,879] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.32670287, 69.06840204, 1.0, 2.0, 0.6841229209622602, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974292127032, 6.9112, 168.9123160531787, 1852911.375834542, 1785675.486055537, 381250.1463687153]
[2019-04-10 13:24:36,880] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:24:36,887] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.3975717e-12 9.9999988e-01 4.1768519e-11 6.4667404e-08 1.5884865e-19], sampled 0.25134628002270454
[2019-04-10 13:24:36,887] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1852911.375834542 W.
[2019-04-10 13:25:19,100] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7737 2927512725.0370 1338.0000
[2019-04-10 13:25:19,314] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7647 3007775956.8125 1766.0000
[2019-04-10 13:25:19,713] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5195 2779329887.3399 933.0000
[2019-04-10 13:25:19,795] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.0138 3164033044.7224 1775.0000
[2019-04-10 13:25:19,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3754 2842593600.1081 1130.0000
[2019-04-10 13:25:20,859] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1425000, evaluation results [1425000.0, 7884.013815687126, 3164033044.7224007, 1775.0, 8254.773707254759, 2927512725.037018, 1338.0, 8660.519511474935, 2779329887.339914, 933.0, 7998.764734894051, 3007775956.812482, 1766.0, 8496.375370015734, 2842593600.1081443, 1130.0]
[2019-04-10 13:25:22,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8977543e-22 1.0000000e+00 3.6517945e-24 1.5506984e-30 1.4469627e-34], sum to 1.0000
[2019-04-10 13:25:22,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2319
[2019-04-10 13:25:22,630] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.56666666666666, 60.33333333333334, 1.0, 2.0, 0.5521515385808276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771572.4550705901, 771572.4550705901, 192170.8571111246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5761200.0000, 
sim time next is 5761800.0000, 
raw observation next is [32.4, 61.5, 1.0, 2.0, 0.5521724500911104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771601.6872731918, 771601.687273191, 192174.6469133919], 
processed observation next is [0.0, 0.6956521739130435, 0.7345971563981042, 0.615, 1.0, 1.0, 0.46044873504953054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21433380202033103, 0.21433380202033084, 0.2868278312140177], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.40701818], dtype=float32), -0.28144908]. 
=============================================
[2019-04-10 13:25:31,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0776262e-20 1.0000000e+00 1.0075012e-22 1.5950032e-25 2.9605407e-32], sum to 1.0000
[2019-04-10 13:25:31,716] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6797
[2019-04-10 13:25:31,721] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.56666666666667, 80.66666666666667, 1.0, 2.0, 0.5706608332825299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797446.9150249014, 797446.9150249014, 195408.2689505939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5941200.0000, 
sim time next is 5941800.0000, 
raw observation next is [29.45, 81.0, 1.0, 2.0, 0.5688148956551572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794866.4189164727, 794866.4189164727, 195080.8698226971], 
processed observation next is [1.0, 0.782608695652174, 0.5947867298578199, 0.81, 1.0, 1.0, 0.4804998742833219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22079622747679797, 0.22079622747679797, 0.29116547734730913], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.54052573], dtype=float32), 0.38755378]. 
=============================================
[2019-04-10 13:25:38,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4953046e-21 1.0000000e+00 3.9369069e-23 2.4714142e-27 1.6276725e-32], sum to 1.0000
[2019-04-10 13:25:38,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8893
[2019-04-10 13:25:38,848] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 87.33333333333334, 1.0, 2.0, 0.5325411868914179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 744159.4963270944, 744159.4963270938, 188851.6051236656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6477600.0000, 
sim time next is 6478200.0000, 
raw observation next is [27.05, 87.5, 1.0, 2.0, 0.5321549901357916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743619.6459154987, 743619.6459154981, 188787.3500403001], 
processed observation next is [1.0, 1.0, 0.4810426540284361, 0.875, 1.0, 1.0, 0.43633131341661635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2065610127543052, 0.20656101275430505, 0.2817721642392539], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.7427701], dtype=float32), 0.21740611]. 
=============================================
[2019-04-10 13:25:39,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3629725e-20 1.0000000e+00 6.8542859e-21 1.9384095e-25 3.2908782e-30], sum to 1.0000
[2019-04-10 13:25:39,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-10 13:25:39,669] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 92.0, 1.0, 2.0, 0.8195761215711496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1145471.112078917, 1145471.112078916, 248772.8191490403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6149400.0000, 
sim time next is 6150000.0000, 
raw observation next is [26.56666666666667, 92.0, 1.0, 2.0, 0.7094215103613662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 991442.9196514131, 991442.9196514137, 222892.1227399357], 
processed observation next is [1.0, 0.17391304347826086, 0.45813586097946307, 0.92, 1.0, 1.0, 0.6499054341703208, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27540081101428143, 0.2754008110142816, 0.33267481005960553], 
reward next is 0.6673, 
noisyNet noise sample is [array([0.2477661], dtype=float32), -1.1135677]. 
=============================================
[2019-04-10 13:25:39,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[61.86336 ]
 [62.504505]
 [62.52443 ]
 [62.391907]
 [62.2867  ]], R is [[61.99088287]
 [61.99967194]
 [62.05057144]
 [62.09843445]
 [62.14283752]].
[2019-04-10 13:25:42,498] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4000900e-20 1.0000000e+00 4.6616606e-22 1.5503112e-26 7.7283111e-32], sum to 1.0000
[2019-04-10 13:25:42,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4743
[2019-04-10 13:25:42,514] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 85.33333333333334, 1.0, 2.0, 0.5197844609664528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726327.4778389429, 726327.4778389435, 186754.1584573632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6563400.0000, 
sim time next is 6564000.0000, 
raw observation next is [27.1, 85.66666666666667, 1.0, 2.0, 0.5202028244912935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726912.2835005831, 726912.2835005831, 186822.1572256739], 
processed observation next is [1.0, 1.0, 0.4834123222748816, 0.8566666666666667, 1.0, 1.0, 0.4219311138449319, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.201920078750162, 0.201920078750162, 0.27883904063533416], 
reward next is 0.7212, 
noisyNet noise sample is [array([0.12532313], dtype=float32), 0.00068408536]. 
=============================================
[2019-04-10 13:25:42,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.98636]
 [69.00431]
 [69.00004]
 [69.05154]
 [69.10634]], R is [[68.98327637]
 [69.01470184]
 [69.0459671 ]
 [69.07723236]
 [69.1085434 ]].
[2019-04-10 13:25:42,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0299912e-21 1.0000000e+00 9.2546968e-24 3.9485772e-31 3.1585046e-34], sum to 1.0000
[2019-04-10 13:25:42,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0311
[2019-04-10 13:25:42,570] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.52786251708802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737619.3722879834, 737619.3722879834, 188077.1312553247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6241200.0000, 
sim time next is 6241800.0000, 
raw observation next is [26.93333333333333, 89.16666666666667, 1.0, 2.0, 0.5286747282065754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738754.7266024591, 738754.7266024585, 188211.2551154973], 
processed observation next is [0.0, 0.21739130434782608, 0.4755134281200631, 0.8916666666666667, 1.0, 1.0, 0.43213822675491004, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20520964627846086, 0.2052096462784607, 0.2809123210679064], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.6605137], dtype=float32), 0.6009893]. 
=============================================
[2019-04-10 13:25:46,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9503835e-21 1.0000000e+00 1.7249996e-23 5.1731720e-30 2.2361392e-33], sum to 1.0000
[2019-04-10 13:25:46,544] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1514
[2019-04-10 13:25:46,548] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.33333333333334, 1.0, 2.0, 0.5312311069662969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742328.1843169428, 742328.1843169421, 188633.784903191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6319200.0000, 
sim time next is 6319800.0000, 
raw observation next is [26.95, 87.5, 1.0, 2.0, 0.5303874794034517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741148.9103894912, 741148.9103894919, 188493.8891668532], 
processed observation next is [0.0, 0.13043478260869565, 0.476303317535545, 0.875, 1.0, 1.0, 0.43420178241379714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20587469733041425, 0.20587469733041444, 0.28133416293560176], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.36715978], dtype=float32), 0.25599417]. 
=============================================
[2019-04-10 13:25:51,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1021721e-12 9.9999952e-01 1.8833076e-11 5.1554605e-07 2.2862683e-19], sum to 1.0000
[2019-04-10 13:25:51,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0152
[2019-04-10 13:25:51,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2266869.054475242 W.
[2019-04-10 13:25:51,317] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.8105473021359999, 1.0, 1.0, 0.8105473021359999, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2266869.054475242, 2266869.054475242, 425032.3986588956], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6447600.0000, 
sim time next is 6448200.0000, 
raw observation next is [30.0, 67.83333333333334, 1.0, 2.0, 0.5321236198484275, 1.0, 2.0, 0.5321236198484275, 1.0, 1.0, 0.9135243759072044, 6.9112, 6.9112, 170.5573041426782, 2232265.556076157, 2232265.556076157, 436009.3813465786], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.6783333333333335, 1.0, 1.0, 0.43629351788967163, 1.0, 1.0, 0.43629351788967163, 1.0, 0.5, 0.894541921838054, 0.0, 0.0, 0.8375144448122397, 0.6200737655767102, 0.6200737655767102, 0.6507602706665352], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.27735996], dtype=float32), -2.213181]. 
=============================================
[2019-04-10 13:25:55,276] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9893545e-20 1.0000000e+00 5.7586225e-22 1.8226063e-26 5.5407320e-32], sum to 1.0000
[2019-04-10 13:25:55,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3522
[2019-04-10 13:25:55,293] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 87.83333333333334, 1.0, 2.0, 0.5296403702742243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740104.556893132, 740104.5568931315, 188370.3616267271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6479400.0000, 
sim time next is 6480000.0000, 
raw observation next is [26.9, 88.0, 1.0, 2.0, 0.5286159030669341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738672.4974779445, 738672.4974779452, 188201.0023743313], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.88, 1.0, 1.0, 0.4320673530926916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20518680485498458, 0.20518680485498478, 0.28089701846915116], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.9476527], dtype=float32), -0.9056937]. 
=============================================
[2019-04-10 13:25:55,297] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[69.26893]
 [69.28071]
 [69.29697]
 [69.32906]
 [69.36634]], R is [[67.99866486]
 [68.03752899]
 [68.07569885]
 [68.11317444]
 [68.150177  ]].
[2019-04-10 13:26:01,314] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 13:26:01,317] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:26:01,317] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:26:01,318] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:01,319] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:26:01,319] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:01,319] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:26:01,320] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:01,320] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:01,320] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:26:01,322] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:01,343] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run59
[2019-04-10 13:26:01,343] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run59
[2019-04-10 13:26:01,388] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run59
[2019-04-10 13:26:01,405] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run59
[2019-04-10 13:26:01,424] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run59
[2019-04-10 13:26:14,348] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.12212198], dtype=float32), 0.09390386]
[2019-04-10 13:26:14,349] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.1, 65.0, 1.0, 2.0, 0.3730619827756546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601102.6297349466, 601102.6297349466, 175197.137154083]
[2019-04-10 13:26:14,349] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:26:14,354] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.061383e-21 1.000000e+00 1.935670e-23 8.388783e-31 5.953818e-34], sampled 0.5384639598072732
[2019-04-10 13:26:22,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12212198], dtype=float32), 0.09390386]
[2019-04-10 13:26:22,568] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.73333333333333, 92.66666666666667, 1.0, 2.0, 0.5058642672062446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706869.4447191936, 706869.444719193, 184520.1589822065]
[2019-04-10 13:26:22,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:26:22,572] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.8220445e-21 1.0000000e+00 5.8303245e-23 7.5984670e-30 2.1527065e-33], sampled 0.4597791371287633
[2019-04-10 13:26:26,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12212198], dtype=float32), 0.09390386]
[2019-04-10 13:26:26,787] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.58564910833333, 93.50550178166668, 1.0, 2.0, 0.4870096205742243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688233.4712009697, 688233.4712009702, 182577.3703833709]
[2019-04-10 13:26:26,790] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:26:26,792] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.1610141e-21 1.0000000e+00 4.3751150e-23 4.7442004e-30 1.3589904e-33], sampled 0.09368079034225274
[2019-04-10 13:26:31,124] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12212198], dtype=float32), 0.09390386]
[2019-04-10 13:26:31,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.264093215, 96.25749718, 1.0, 2.0, 0.3954670916674812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595241.929707236, 595241.9297072353, 174246.382835837]
[2019-04-10 13:26:31,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:26:31,128] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2985571e-21 1.0000000e+00 2.9934532e-23 1.6286803e-30 1.1801170e-33], sampled 0.5631624481242711
[2019-04-10 13:27:17,676] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12212198], dtype=float32), 0.09390386]
[2019-04-10 13:27:17,680] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.26666666666667, 82.33333333333333, 1.0, 2.0, 0.5533898324559401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773303.4676059919, 773303.4676059919, 192383.3079195256]
[2019-04-10 13:27:17,683] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:27:17,687] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1558123e-20 1.0000000e+00 1.9138205e-22 4.2364541e-28 5.7013012e-33], sampled 0.468981330608914
[2019-04-10 13:27:24,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.12212198], dtype=float32), 0.09390386]
[2019-04-10 13:27:24,251] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.41835225333333, 69.99767238333334, 1.0, 2.0, 0.5148130648687752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719378.2815025328, 719378.2815025328, 185949.5360337226]
[2019-04-10 13:27:24,252] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:27:24,254] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.8941464e-21 1.0000000e+00 1.5567384e-22 3.4343204e-28 4.9822571e-33], sampled 0.2568109813193622
[2019-04-10 13:27:35,313] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.1097 2779577537.4090 934.0000
[2019-04-10 13:27:35,454] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.3626 3163537500.6106 1769.0000
[2019-04-10 13:27:35,879] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-10 13:27:35,938] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7119 2842394090.1963 1129.0000
[2019-04-10 13:27:35,945] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3441 3007761470.1011 1766.0000
[2019-04-10 13:27:36,960] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1450000, evaluation results [1450000.0, 7889.362632279549, 3163537500.6105766, 1769.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8658.10967188952, 2779577537.4089947, 934.0, 7997.344093528681, 3007761470.101147, 1766.0, 8497.711914252905, 2842394090.196327, 1129.0]
[2019-04-10 13:27:40,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.13337905e-20 1.00000000e+00 3.70815971e-22 5.22958411e-29
 5.40389961e-33], sum to 1.0000
[2019-04-10 13:27:40,521] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9403
[2019-04-10 13:27:40,525] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.13333333333333, 78.50000000000001, 1.0, 2.0, 0.3724173087015109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587538.6706923113, 587538.6706923113, 174095.6664532488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6765000.0000, 
sim time next is 6765600.0000, 
raw observation next is [23.26666666666667, 78.0, 1.0, 2.0, 0.3487188422871579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549370.9688078815, 549370.9688078815, 170861.4852200492], 
processed observation next is [1.0, 0.30434782608695654, 0.3017377567140602, 0.78, 1.0, 1.0, 0.21532390637006973, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15260304689107818, 0.15260304689107818, 0.2550171421194764], 
reward next is 0.7450, 
noisyNet noise sample is [array([-0.08766785], dtype=float32), -1.1163967]. 
=============================================
[2019-04-10 13:27:40,678] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8842099e-19 1.0000000e+00 1.7458545e-21 1.0056249e-27 2.8686664e-32], sum to 1.0000
[2019-04-10 13:27:40,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5142
[2019-04-10 13:27:40,689] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 83.16666666666667, 1.0, 2.0, 0.4970624656718193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786147.8112437947, 786147.8112437954, 193619.7682607958], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6747000.0000, 
sim time next is 6747600.0000, 
raw observation next is [22.3, 83.33333333333334, 1.0, 2.0, 0.405761655036166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642075.9602737251, 642075.9602737251, 178947.0463925825], 
processed observation next is [1.0, 0.08695652173913043, 0.25592417061611383, 0.8333333333333335, 1.0, 1.0, 0.2840501867905615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17835443340936807, 0.17835443340936807, 0.26708514386952614], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.4907046], dtype=float32), -2.2146854]. 
=============================================
[2019-04-10 13:27:41,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4064527e-19 1.0000000e+00 1.5979853e-20 4.2802710e-25 3.3135527e-30], sum to 1.0000
[2019-04-10 13:27:41,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5821
[2019-04-10 13:27:41,747] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333333, 66.66666666666667, 1.0, 2.0, 0.8505409820210375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326035.591225285, 1326035.591225284, 274888.0411811259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6772200.0000, 
sim time next is 6772800.0000, 
raw observation next is [25.66666666666667, 65.33333333333334, 1.0, 2.0, 0.8753163340902249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1364327.620822406, 1364327.620822406, 282170.6310863165], 
processed observation next is [1.0, 0.391304347826087, 0.4154818325434442, 0.6533333333333334, 1.0, 1.0, 0.8497787157713553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3789798946728905, 0.3789798946728905, 0.42115019565121864], 
reward next is 0.5788, 
noisyNet noise sample is [array([-0.18709725], dtype=float32), -0.5455921]. 
=============================================
[2019-04-10 13:27:42,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2054223e-20 1.0000000e+00 2.8865312e-21 1.8519299e-26 3.7629039e-31], sum to 1.0000
[2019-04-10 13:27:42,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6021
[2019-04-10 13:27:42,220] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 75.0, 1.0, 2.0, 0.6364402611088918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 910828.0310405577, 910828.031040557, 210518.0316832197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7023600.0000, 
sim time next is 7024200.0000, 
raw observation next is [26.95, 74.0, 1.0, 2.0, 0.7689866624988516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1100967.114886573, 1100967.114886574, 240056.2035162981], 
processed observation next is [1.0, 0.30434782608695654, 0.476303317535545, 0.74, 1.0, 1.0, 0.7216706777094598, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30582419857960363, 0.30582419857960386, 0.35829284106910164], 
reward next is 0.6417, 
noisyNet noise sample is [array([2.422912], dtype=float32), -0.58438843]. 
=============================================
[2019-04-10 13:27:51,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8893983e-20 1.0000000e+00 2.7185507e-23 5.3934431e-31 2.0459626e-33], sum to 1.0000
[2019-04-10 13:27:51,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2595
[2019-04-10 13:27:51,038] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 81.66666666666667, 1.0, 2.0, 0.4206809444509638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 615312.0034052789, 615312.0034052783, 175626.2067373386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6934800.0000, 
sim time next is 6935400.0000, 
raw observation next is [25.45, 80.5, 1.0, 2.0, 0.4227334881001668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617252.3801111848, 617252.3801111855, 175782.2589802505], 
processed observation next is [0.0, 0.2608695652173913, 0.4052132701421801, 0.805, 1.0, 1.0, 0.30449817843393595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1714589944753291, 0.1714589944753293, 0.26236158056753806], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.52692926], dtype=float32), -1.5950555]. 
=============================================
[2019-04-10 13:27:51,149] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2806497e-20 1.0000000e+00 1.1277196e-22 4.0774811e-30 2.7567587e-33], sum to 1.0000
[2019-04-10 13:27:51,157] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0398
[2019-04-10 13:27:51,162] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 60.0, 1.0, 2.0, 0.4541582517543982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644943.6896082959, 644943.6896082959, 178059.7220311403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6950400.0000, 
sim time next is 6951000.0000, 
raw observation next is [29.85, 59.5, 1.0, 2.0, 0.4562591219867195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646686.5410719265, 646686.5410719265, 178207.5732842541], 
processed observation next is [0.0, 0.43478260869565216, 0.613744075829384, 0.595, 1.0, 1.0, 0.3448905084177343, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17963515029775737, 0.17963515029775737, 0.26598145266306583], 
reward next is 0.7340, 
noisyNet noise sample is [array([0.3088559], dtype=float32), -0.7132232]. 
=============================================
[2019-04-10 13:27:51,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.324005]
 [73.27782 ]
 [73.23367 ]
 [73.17823 ]
 [73.09933 ]], R is [[73.36611176]
 [73.36669159]
 [73.36747742]
 [73.36863708]
 [73.3698349 ]].
[2019-04-10 13:27:51,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4075663e-21 1.0000000e+00 4.9774763e-23 2.8647462e-28 1.7864001e-33], sum to 1.0000
[2019-04-10 13:27:51,238] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9137
[2019-04-10 13:27:51,241] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 73.33333333333334, 1.0, 2.0, 0.3889834030130439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584808.1625891158, 584808.1625891165, 173276.4010678578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7330800.0000, 
sim time next is 7331400.0000, 
raw observation next is [25.6, 73.5, 1.0, 2.0, 0.3878582133267932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583848.2853143984, 583848.285314399, 173211.2121714892], 
processed observation next is [1.0, 0.8695652173913043, 0.4123222748815167, 0.735, 1.0, 1.0, 0.26247977509252196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16218007925399955, 0.16218007925399974, 0.2585241972708794], 
reward next is 0.7415, 
noisyNet noise sample is [array([1.0405259], dtype=float32), -0.06759547]. 
=============================================
[2019-04-10 13:28:00,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2805497e-19 1.0000000e+00 1.3295099e-20 9.5891263e-26 3.5914649e-31], sum to 1.0000
[2019-04-10 13:28:00,468] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6378
[2019-04-10 13:28:00,473] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 86.0, 1.0, 2.0, 0.4761557408519673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665343.2341960453, 665343.2341960459, 179944.324408783], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7174800.0000, 
sim time next is 7175400.0000, 
raw observation next is [25.8, 86.16666666666667, 1.0, 2.0, 0.476578657134562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665934.3699529376, 665934.3699529376, 180007.6556964713], 
processed observation next is [1.0, 0.043478260869565216, 0.42180094786729866, 0.8616666666666667, 1.0, 1.0, 0.36937187606573735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18498176943137154, 0.18498176943137154, 0.2686681428305542], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.42293888], dtype=float32), 1.4817406]. 
=============================================
[2019-04-10 13:28:03,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1682596e-21 1.0000000e+00 3.3165252e-22 7.1477754e-29 2.3359921e-33], sum to 1.0000
[2019-04-10 13:28:03,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1578
[2019-04-10 13:28:03,587] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 93.0, 1.0, 2.0, 0.4747879111551414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686057.8155082215, 686057.8155082221, 182617.7688280439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7626600.0000, 
sim time next is 7627200.0000, 
raw observation next is [24.1, 92.66666666666666, 1.0, 2.0, 0.4939262784365839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712060.7058499899, 712060.7058499905, 185430.0023283002], 
processed observation next is [1.0, 0.2608695652173913, 0.3412322274881518, 0.9266666666666665, 1.0, 1.0, 0.3902726246223902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1977946405138861, 0.19779464051388623, 0.27676119750492567], 
reward next is 0.7232, 
noisyNet noise sample is [array([-0.6118326], dtype=float32), 0.045139827]. 
=============================================
[2019-04-10 13:28:10,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5480281e-21 1.0000000e+00 6.8402520e-23 7.2607800e-29 1.3136762e-33], sum to 1.0000
[2019-04-10 13:28:10,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0595
[2019-04-10 13:28:10,460] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.05, 94.5, 1.0, 2.0, 0.3186843401584374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502615.6106929396, 502615.6106929396, 167194.6153053548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7428600.0000, 
sim time next is 7429200.0000, 
raw observation next is [21.06666666666667, 94.33333333333334, 1.0, 2.0, 0.3181200173644202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 501764.1874958618, 501764.1874958611, 167131.322666799], 
processed observation next is [1.0, 1.0, 0.19747235387045833, 0.9433333333333335, 1.0, 1.0, 0.17845785224628938, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13937894097107273, 0.13937894097107253, 0.2494497353235806], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.0184458], dtype=float32), -0.63008505]. 
=============================================
[2019-04-10 13:28:13,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5909428e-11 9.9999976e-01 1.7394754e-10 1.8748231e-07 1.4210393e-19], sum to 1.0000
[2019-04-10 13:28:13,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4928
[2019-04-10 13:28:13,236] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2179611.564084975 W.
[2019-04-10 13:28:13,239] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.26666666666667, 67.66666666666667, 1.0, 2.0, 0.9175568837939484, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.985024520691527, 6.9112, 168.9125174964917, 2179611.564084975, 2127238.044384468, 439733.6451825138], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7836000.0000, 
sim time next is 7836600.0000, 
raw observation next is [30.08333333333333, 68.83333333333333, 1.0, 2.0, 0.9429491579013125, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98410870018291, 6.9112, 168.9124626167621, 2215151.579819291, 2163427.789824145, 447040.121261028], 
processed observation next is [1.0, 0.6956521739130435, 0.6248025276461293, 0.6883333333333332, 1.0, 1.0, 0.9312640456642319, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007290870018290985, 0.0, 0.8294375199082828, 0.6153198832831364, 0.6009521638400402, 0.6672240615836239], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0428767], dtype=float32), 0.049026374]. 
=============================================
[2019-04-10 13:28:13,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7146210e-10 9.9942434e-01 5.2539031e-09 5.7573285e-04 1.4633570e-16], sum to 1.0000
[2019-04-10 13:28:13,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7696
[2019-04-10 13:28:13,536] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2327644.858434092 W.
[2019-04-10 13:28:13,541] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.9, 61.0, 1.0, 2.0, 0.8322582301413545, 1.0, 2.0, 0.8322582301413545, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2327644.858434092, 2327644.858434092, 435871.5166015414], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7747200.0000, 
sim time next is 7747800.0000, 
raw observation next is [30.8, 61.33333333333334, 1.0, 2.0, 0.8426525331267725, 1.0, 2.0, 0.8426525331267725, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2356742.870108654, 2356742.870108654, 441160.0578897686], 
processed observation next is [1.0, 0.6956521739130435, 0.6587677725118484, 0.6133333333333334, 1.0, 1.0, 0.8104247387069548, 1.0, 1.0, 0.8104247387069548, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6546507972524038, 0.6546507972524038, 0.6584478475966696], 
reward next is 0.3416, 
noisyNet noise sample is [array([-0.23499012], dtype=float32), -2.4981413]. 
=============================================
[2019-04-10 13:28:14,669] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7638705e-21 1.0000000e+00 3.7935463e-23 1.7249154e-29 3.1362210e-34], sum to 1.0000
[2019-04-10 13:28:14,677] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8157
[2019-04-10 13:28:14,683] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 86.0, 1.0, 2.0, 0.4029091002163428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594852.926062885, 594852.926062885, 173873.9932508516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7504200.0000, 
sim time next is 7504800.0000, 
raw observation next is [24.3, 86.33333333333334, 1.0, 2.0, 0.4027598588474285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594720.212489671, 594720.2124896715, 173864.3419595282], 
processed observation next is [0.0, 0.8695652173913043, 0.3507109004739337, 0.8633333333333334, 1.0, 1.0, 0.28043356487641985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1652000590249086, 0.16520005902490875, 0.2594990178500421], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.48527664], dtype=float32), 2.484004]. 
=============================================
[2019-04-10 13:28:17,386] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 13:28:17,389] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:28:17,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:28:17,391] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:28:17,391] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:28:17,391] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:28:17,392] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:28:17,392] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:28:17,393] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:28:17,393] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:28:17,393] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:28:17,402] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run60
[2019-04-10 13:28:17,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run60
[2019-04-10 13:28:17,422] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run60
[2019-04-10 13:28:17,455] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run60
[2019-04-10 13:28:17,484] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run60
[2019-04-10 13:28:25,676] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12094404], dtype=float32), 0.08991066]
[2019-04-10 13:28:25,677] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.85, 67.83333333333334, 1.0, 2.0, 0.2685678202909063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 438408.583620106, 438408.5836201054, 162759.5933709742]
[2019-04-10 13:28:25,677] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:28:25,681] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3294136e-20 1.0000000e+00 2.2738195e-22 2.2202999e-29 4.2206514e-33], sampled 0.8569928207703422
[2019-04-10 13:28:27,947] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.12094404], dtype=float32), 0.08991066]
[2019-04-10 13:28:27,947] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.9, 65.0, 1.0, 2.0, 0.3441332969575231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534173.9609378815, 534173.9609378821, 169456.7464312418]
[2019-04-10 13:28:27,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:28:27,949] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4351115e-21 1.0000000e+00 2.9238871e-23 9.6345313e-31 2.0952073e-34], sampled 0.8191215753668121
[2019-04-10 13:28:53,094] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.12094404], dtype=float32), 0.08991066]
[2019-04-10 13:28:53,096] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 94.0, 1.0, 2.0, 0.7715458770249043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078308.110315211, 1078308.110315211, 237045.5242425289]
[2019-04-10 13:28:53,096] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:28:53,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.3916794e-19 1.0000000e+00 4.4447826e-20 2.9238839e-25 4.7146129e-30], sampled 0.6153928943119213
[2019-04-10 13:28:55,303] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12094404], dtype=float32), 0.08991066]
[2019-04-10 13:28:55,304] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.777287385, 61.34810477, 1.0, 2.0, 0.893650928228193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1249061.807142194, 1249061.807142194, 268199.8113955931]
[2019-04-10 13:28:55,304] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:28:55,305] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4788280e-19 1.0000000e+00 3.9218488e-20 5.8289179e-25 2.3635768e-30], sampled 0.9535638990065433
[2019-04-10 13:29:28,281] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.12094404], dtype=float32), 0.08991066]
[2019-04-10 13:29:28,284] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.66688786833333, 80.71170818500002, 1.0, 2.0, 0.4804854461917372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671395.141693862, 671395.1416938626, 180594.8537962307]
[2019-04-10 13:29:28,286] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:29:28,301] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8969653e-22 1.0000000e+00 3.0549386e-24 9.2552242e-32 1.7988576e-35], sampled 0.6633092998169795
[2019-04-10 13:29:48,698] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5128 2927431709.8161 1338.0000
[2019-04-10 13:29:48,924] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4920 2779501990.1428 934.0000
[2019-04-10 13:29:49,477] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.2252 3007742313.7677 1765.0000
[2019-04-10 13:29:49,550] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.4476 2842344215.8775 1128.0000
[2019-04-10 13:29:49,597] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7893.4103 3163399201.5412 1754.0000
[2019-04-10 13:29:50,611] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1475000, evaluation results [1475000.0, 7893.410320974853, 3163399201.541214, 1754.0, 8253.51283618066, 2927431709.816101, 1338.0, 8659.492021980814, 2779501990.142769, 934.0, 7999.2251960471085, 3007742313.7677274, 1765.0, 8498.447563468375, 2842344215.877539, 1128.0]
[2019-04-10 13:29:51,312] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7985894e-21 1.0000000e+00 1.4567078e-22 5.2432645e-30 1.9424950e-33], sum to 1.0000
[2019-04-10 13:29:51,319] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8055
[2019-04-10 13:29:51,323] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 81.5, 1.0, 2.0, 0.4383957447182011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626024.6875124621, 626024.6875124628, 176245.690836109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7551000.0000, 
sim time next is 7551600.0000, 
raw observation next is [26.13333333333333, 80.66666666666667, 1.0, 2.0, 0.4435107975055667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 630477.9918413503, 630477.991841351, 176610.359753923], 
processed observation next is [0.0, 0.391304347826087, 0.43759873617693507, 0.8066666666666668, 1.0, 1.0, 0.32953108133200815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1751327755114862, 0.1751327755114864, 0.26359755187152684], 
reward next is 0.7364, 
noisyNet noise sample is [array([-0.5629566], dtype=float32), 1.5714444]. 
=============================================
[2019-04-10 13:29:51,481] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4706617e-21 1.0000000e+00 1.7425733e-22 8.9578282e-30 7.7845203e-34], sum to 1.0000
[2019-04-10 13:29:51,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1640
[2019-04-10 13:29:51,496] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 73.33333333333334, 1.0, 2.0, 0.4558510037865461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 642940.3347757935, 642940.3347757929, 177741.299175611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7582800.0000, 
sim time next is 7583400.0000, 
raw observation next is [27.35, 74.5, 1.0, 2.0, 0.4594131902218633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645908.7461473537, 645908.7461473542, 177994.9319047655], 
processed observation next is [0.0, 0.782608695652174, 0.4952606635071091, 0.745, 1.0, 1.0, 0.34869059062875096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17941909615204268, 0.17941909615204285, 0.26566407746979925], 
reward next is 0.7343, 
noisyNet noise sample is [array([0.36518878], dtype=float32), 0.9057783]. 
=============================================
[2019-04-10 13:29:52,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:29:52,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:52,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-04-10 13:29:56,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:29:56,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:56,471] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-04-10 13:30:03,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4311848e-20 1.0000000e+00 2.7827248e-21 2.3378637e-26 5.8284748e-32], sum to 1.0000
[2019-04-10 13:30:03,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4796
[2019-04-10 13:30:03,968] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.41666666666667, 96.0, 1.0, 2.0, 0.3845326749506683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580258.5197760034, 580258.5197760027, 172930.3252149941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 154200.0000, 
sim time next is 154800.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3854258006863907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581924.2250599122, 581924.2250599122, 173088.8438988943], 
processed observation next is [1.0, 0.8260869565217391, 0.2606635071090047, 0.96, 1.0, 1.0, 0.2595491574534828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16164561807219782, 0.16164561807219782, 0.2583415580580512], 
reward next is 0.7417, 
noisyNet noise sample is [array([-1.0811347], dtype=float32), -0.73208034]. 
=============================================
[2019-04-10 13:30:04,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:04,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:04,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-04-10 13:30:05,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:05,308] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:05,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-04-10 13:30:05,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:05,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:05,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-04-10 13:30:06,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:06,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:06,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-04-10 13:30:06,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:06,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:06,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-04-10 13:30:06,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:06,592] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:06,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-04-10 13:30:06,731] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:06,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:06,745] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-04-10 13:30:06,881] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:06,882] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:06,904] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-04-10 13:30:06,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0805693e-19 1.0000000e+00 1.2935549e-20 2.1899870e-26 8.6866546e-32], sum to 1.0000
[2019-04-10 13:30:06,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4233
[2019-04-10 13:30:06,990] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 87.66666666666667, 1.0, 2.0, 0.3489643668208988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541355.4460883135, 541355.4460883135, 170033.2861112418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 80400.0000, 
sim time next is 81000.0000, 
raw observation next is [22.45, 88.0, 1.0, 2.0, 0.3473432723090848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539420.6966323056, 539420.6966323056, 169890.0225572612], 
processed observation next is [1.0, 0.9565217391304348, 0.26303317535545023, 0.88, 1.0, 1.0, 0.21366659314347566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14983908239786267, 0.14983908239786267, 0.2535671978466585], 
reward next is 0.7464, 
noisyNet noise sample is [array([-1.4764156], dtype=float32), 0.3152585]. 
=============================================
[2019-04-10 13:30:06,998] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.59925 ]
 [72.62149 ]
 [72.624535]
 [72.5558  ]
 [72.610344]], R is [[72.60070801]
 [72.62091827]
 [72.6407547 ]
 [72.66027832]
 [72.67960358]].
[2019-04-10 13:30:07,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:07,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:07,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-04-10 13:30:07,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:07,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:07,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-04-10 13:30:07,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:07,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:07,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-04-10 13:30:07,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:07,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:07,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run8
[2019-04-10 13:30:07,369] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:07,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:07,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-04-10 13:30:07,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:30:07,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:07,592] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-04-10 13:30:08,833] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8785913e-18 1.0000000e+00 3.2076686e-19 1.2404153e-23 2.8086498e-29], sum to 1.0000
[2019-04-10 13:30:08,841] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4909
[2019-04-10 13:30:08,847] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 75.66666666666667, 1.0, 2.0, 0.3871612907779354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579468.8303373059, 579468.8303373066, 172715.4756314071], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 65400.0000, 
sim time next is 66000.0000, 
raw observation next is [25.26666666666667, 76.33333333333334, 1.0, 2.0, 0.3892099465416368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583728.4871030721, 583728.4871030721, 173136.4213594642], 
processed observation next is [1.0, 0.782608695652174, 0.3965244865718801, 0.7633333333333334, 1.0, 1.0, 0.2641083693272733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1621468019730756, 0.1621468019730756, 0.25841256919323013], 
reward next is 0.7416, 
noisyNet noise sample is [array([1.0523893], dtype=float32), -1.364698]. 
=============================================
[2019-04-10 13:30:08,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.81515 ]
 [68.380646]
 [67.74724 ]
 [66.49918 ]
 [64.518394]], R is [[69.1439743 ]
 [69.19475555]
 [69.24604797]
 [69.29734802]
 [69.34882355]].
[2019-04-10 13:30:11,278] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8000716e-19 1.0000000e+00 5.1379524e-21 5.0640148e-27 1.8545823e-32], sum to 1.0000
[2019-04-10 13:30:11,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4061
[2019-04-10 13:30:11,286] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 83.0, 1.0, 2.0, 0.3655836996509849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560010.8869888592, 560010.8869888592, 171405.9270730347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 72000.0000, 
sim time next is 72600.0000, 
raw observation next is [23.51666666666667, 83.33333333333334, 1.0, 2.0, 0.3641451087769509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558594.5174224947, 558594.5174224947, 171307.190417166], 
processed observation next is [1.0, 0.8695652173913043, 0.31358609794628767, 0.8333333333333335, 1.0, 1.0, 0.2339097696107842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15516514372847076, 0.15516514372847076, 0.2556823737569642], 
reward next is 0.7443, 
noisyNet noise sample is [array([0.5672466], dtype=float32), 0.4123016]. 
=============================================
[2019-04-10 13:30:11,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9529469e-19 1.0000000e+00 7.2984092e-21 6.2554659e-27 2.7426011e-31], sum to 1.0000
[2019-04-10 13:30:11,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3525
[2019-04-10 13:30:11,440] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 73.0, 1.0, 2.0, 0.4791762917907562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773024.1119324766, 773024.1119324766, 191641.6318986931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 394200.0000, 
sim time next is 394800.0000, 
raw observation next is [22.83333333333333, 73.0, 1.0, 2.0, 0.5187384732909831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836339.7480974811, 836339.7480974811, 198703.6201582337], 
processed observation next is [1.0, 0.5652173913043478, 0.2812006319115322, 0.73, 1.0, 1.0, 0.42016683529034105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23231659669374474, 0.23231659669374474, 0.2965725674003488], 
reward next is 0.7034, 
noisyNet noise sample is [array([-0.26401624], dtype=float32), 0.4904162]. 
=============================================
[2019-04-10 13:30:20,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4790044e-21 1.0000000e+00 4.6898243e-22 1.7133210e-29 1.0708633e-32], sum to 1.0000
[2019-04-10 13:30:20,771] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9796
[2019-04-10 13:30:20,775] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333333, 73.0, 1.0, 2.0, 0.2705924929639329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 443960.2050123188, 443960.2050123194, 163011.5151221295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 720600.0000, 
sim time next is 721200.0000, 
raw observation next is [21.86666666666667, 72.0, 1.0, 2.0, 0.4187583075914904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686689.8810980052, 686689.8810980045, 182272.7719313551], 
processed observation next is [1.0, 0.34782608695652173, 0.23538704581358633, 0.72, 1.0, 1.0, 0.2997088043270969, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19074718919389033, 0.19074718919389014, 0.2720489133303807], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.79855776], dtype=float32), 0.748923]. 
=============================================
[2019-04-10 13:30:28,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5006644e-20 1.0000000e+00 6.0251826e-23 3.3444733e-31 1.3141817e-34], sum to 1.0000
[2019-04-10 13:30:28,866] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3748
[2019-04-10 13:30:28,869] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 73.33333333333334, 1.0, 2.0, 0.2975666348156158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473890.9216368221, 473890.9216368227, 165187.7114412063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 907800.0000, 
sim time next is 908400.0000, 
raw observation next is [23.6, 72.66666666666667, 1.0, 2.0, 0.2990771172722378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475818.0563652861, 475818.0563652861, 165316.3083429248], 
processed observation next is [0.0, 0.5217391304347826, 0.3175355450236968, 0.7266666666666667, 1.0, 1.0, 0.15551459912317805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1321716823236906, 0.1321716823236906, 0.2467407587207833], 
reward next is 0.7533, 
noisyNet noise sample is [array([-0.1638969], dtype=float32), 1.4269874]. 
=============================================
[2019-04-10 13:30:31,852] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 13:30:31,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:30:31,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:31,856] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:30:31,858] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:31,858] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:30:31,859] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:31,862] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:30:31,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:30:31,863] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:31,868] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:30:32,560] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run61
[2019-04-10 13:30:32,581] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run61
[2019-04-10 13:30:32,642] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run61
[2019-04-10 13:30:32,751] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run61
[2019-04-10 13:30:32,767] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/5/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run61
[2019-04-10 13:30:46,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:30:46,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.96076785666667, 92.81722507333333, 1.0, 2.0, 0.7481765993501224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1056851.752385399, 1056851.752385399, 233127.9260955442]
[2019-04-10 13:30:46,574] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:30:46,576] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9329143e-19 1.0000000e+00 9.9144737e-21 7.5227987e-27 2.7968222e-31], sampled 0.8005804116922597
[2019-04-10 13:30:59,929] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:30:59,929] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.47233516333333, 93.71984995666668, 1.0, 2.0, 0.3727215825370769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569145.4089643438, 569145.4089643438, 172144.6257268466]
[2019-04-10 13:30:59,930] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:30:59,934] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5684584e-21 1.0000000e+00 1.1802856e-22 5.8485256e-30 4.6068657e-34], sampled 0.4497961159633247
[2019-04-10 13:31:03,433] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:03,433] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.5847745970982471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900618.5821089275, 900618.5821089275, 208019.128533037]
[2019-04-10 13:31:03,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:31:03,437] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3310010e-20 1.0000000e+00 1.5562345e-21 1.7512244e-28 2.3637006e-32], sampled 0.7560885559260557
[2019-04-10 13:31:16,617] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:16,620] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.13333333333334, 66.0, 1.0, 2.0, 0.855941215682043, 1.0, 2.0, 0.855941215682043, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2393913.873762089, 2393913.873762088, 448506.9745523935]
[2019-04-10 13:31:16,621] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:31:16,622] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3590337e-09 9.9997771e-01 4.9357872e-08 2.2291992e-05 1.7974767e-16], sampled 0.7525702978795106
[2019-04-10 13:31:16,622] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2393913.873762089 W.
[2019-04-10 13:31:19,075] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:19,076] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.5, 69.0, 1.0, 2.0, 0.6109733258951872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853802.6200466754, 853802.6200466754, 202808.4581071348]
[2019-04-10 13:31:19,077] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:31:19,079] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4415352e-20 1.0000000e+00 7.3397830e-22 2.3400829e-28 3.5741582e-33], sampled 0.7510236792313983
[2019-04-10 13:31:20,764] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:20,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.3, 46.0, 1.0, 2.0, 0.5135938197198704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717673.983706862, 717673.983706862, 185752.7746663546]
[2019-04-10 13:31:20,766] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:31:20,767] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0522710e-21 1.0000000e+00 2.2791340e-22 9.9423461e-29 4.4493485e-34], sampled 0.26598042451796644
[2019-04-10 13:31:35,598] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:35,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.31687464333334, 56.74913884999999, 1.0, 2.0, 0.7015669432925217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 980460.8031198154, 980460.8031198148, 221180.988335907]
[2019-04-10 13:31:35,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:31:35,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5556663e-19 1.0000000e+00 1.5543131e-20 4.9796527e-26 1.6909572e-31], sampled 0.3201582942360487
[2019-04-10 13:31:39,751] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:39,752] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.16666666666667, 68.0, 1.0, 2.0, 0.5356778982264374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748544.2012886779, 748544.2012886772, 189374.3276812527]
[2019-04-10 13:31:39,753] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:31:39,756] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7591169e-22 1.0000000e+00 6.4116173e-24 1.5381496e-30 1.8708647e-36], sampled 0.28730037632758354
[2019-04-10 13:31:43,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.11926054], dtype=float32), 0.089764304]
[2019-04-10 13:31:43,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.06666666666667, 86.33333333333334, 1.0, 2.0, 0.5100180739280571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712675.7137116652, 712675.7137116652, 185179.2567577323]
[2019-04-10 13:31:43,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:31:43,740] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5253317e-20 1.0000000e+00 9.8531246e-22 8.5976286e-28 3.7324208e-33], sampled 0.17343052965019246
[2019-04-10 13:32:02,791] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1483 3007731824.3662 1765.0000
[2019-04-10 13:32:03,340] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2400 2779246225.0188 930.0000
[2019-04-10 13:32:03,453] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.5070 2927426159.3047 1338.0000
[2019-04-10 13:32:03,670] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.1342 3162983185.5727 1753.0000
[2019-04-10 13:32:03,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.1395 2842153341.4257 1126.0000
[2019-04-10 13:32:04,861] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1500000, evaluation results [1500000.0, 7894.134167308945, 3162983185.5726633, 1753.0, 8254.506955552193, 2927426159.30466, 1338.0, 8661.23997302813, 2779246225.0188375, 930.0, 7999.148308505638, 3007731824.3661647, 1765.0, 8499.139523892181, 2842153341.425681, 1126.0]
