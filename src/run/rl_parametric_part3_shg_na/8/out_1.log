Using TensorFlow backend.
[2019-04-23 09:31:55,720] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_add_time_to_state=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_r_term_zero=True, is_warm_start=True, job_mode='Train', learning_rate=0.0005, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='Part3-NA-Shg-Train-v1-res1/model_data/model.ckpt-2500000', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res2', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-04-23 09:31:55,720] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-23 09:31:55.758602: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-23 09:32:11,784] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-23 09:32:11,784] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-04-23 09:32:11,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,806] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,817] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,820] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,820] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:11,820] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-23 09:32:11,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:11,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-04-23 09:32:12,821] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:12,823] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-23 09:32:12,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:12,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-04-23 09:32:13,709] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 09:32:13,710] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:32:13,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:32:13,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,711] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:32:13,712] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,712] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:32:13,713] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:32:13,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,713] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,714] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,718] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,728] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,748] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,749] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,823] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:13,824] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-23 09:32:13,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-04-23 09:32:14,825] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:14,826] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-23 09:32:14,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:14,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-04-23 09:32:15,827] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:15,828] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-23 09:32:15,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:15,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-04-23 09:32:16,829] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:16,830] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-23 09:32:16,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:16,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-04-23 09:32:17,831] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:17,832] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-23 09:32:17,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:17,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-04-23 09:32:18,833] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:18,834] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-23 09:32:18,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:18,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-04-23 09:32:19,835] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:19,841] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-23 09:32:19,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:19,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-04-23 09:32:20,842] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:20,843] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-23 09:32:20,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:20,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-04-23 09:32:21,844] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:21,845] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-23 09:32:21,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:21,937] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-04-23 09:32:22,845] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:22,848] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-23 09:32:22,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:22,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-04-23 09:32:23,849] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:23,850] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-23 09:32:23,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:23,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-04-23 09:32:24,851] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:24,853] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-23 09:32:24,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:24,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-04-23 09:32:25,853] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:25,854] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-23 09:32:25,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:25,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-04-23 09:32:26,855] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:26,856] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-23 09:32:26,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:26,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run1
[2019-04-23 09:32:28,989] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:32:28,992] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.02409662, 58.63889941666667, 1.0, 2.0, 0.2665326129188206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 444911.7061856221, 444911.7061856221, 161793.5920467583]
[2019-04-23 09:32:28,993] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:32:28,995] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2414902947097438
[2019-04-23 09:32:55,691] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:32:55,692] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.8, 70.66666666666667, 1.0, 2.0, 0.526767664461989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773855.2089769847, 773855.2089769847, 192635.6912338451]
[2019-04-23 09:32:55,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:32:55,696] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.21173565e-32
 0.00000000e+00], sampled 0.9547263456179831
[2019-04-23 09:33:03,064] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:33:03,065] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.16666666666667, 93.16666666666667, 1.0, 2.0, 0.3977676784717663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591824.6310228049, 591824.6310228043, 173734.0849522283]
[2019-04-23 09:33:03,066] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:33:03,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6748005e-31 0.0000000e+00], sampled 0.9014384822302065
[2019-04-23 09:33:41,867] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:33:41,868] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.55, 80.5, 1.0, 2.0, 0.5446991173061282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761154.7657538848, 761154.7657538854, 190895.9316898881]
[2019-04-23 09:33:41,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:33:41,876] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4718773e-26 0.0000000e+00], sampled 0.7871940042631167
[2019-04-23 09:34:23,945] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8346.4557 2972003103.9871 861.0000
[2019-04-23 09:34:24,113] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8389.3026 2914565027.0313 1009.0000
[2019-04-23 09:34:24,141] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8677.6168 2824168262.5245 658.0000
[2019-04-23 09:34:24,172] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8209.1487 3130966285.8621 920.0000
[2019-04-23 09:34:24,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8744.4570 2770794475.5613 713.0000
[2019-04-23 09:34:25,482] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 8209.148728965183, 3130966285.86209, 920.0, 8389.302619220314, 2914565027.0313025, 1009.0, 8744.456997317853, 2770794475.561282, 713.0, 8346.455683757804, 2972003103.987141, 861.0, 8677.61684504349, 2824168262.5245023, 658.0]
[2019-04-23 09:34:30,517] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.3936308e-11 7.6080286e-03 6.5650731e-17 4.2846400e-02 9.4954550e-01], sum to 1.0000
[2019-04-23 09:34:30,530] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3473
[2019-04-23 09:34:30,628] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.83333333333334, 63.0, 1.0, 2.0, 0.4990372728351632, 1.0, 1.0, 0.4990372728351632, 1.0, 1.0, 0.8357239692492493, 6.9112, 6.9112, 170.5573041426782, 2112533.298487618, 2112533.298487618, 410342.9520670532], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [27.75, 63.5, 1.0, 2.0, 0.4407515185565417, 1.0, 2.0, 0.4407515185565417, 1.0, 2.0, 0.7435445528367174, 6.9112, 6.9112, 170.5573041426782, 1885272.804369802, 1885272.804369802, 376167.7621229931], 
processed observation next is [1.0, 0.6086956521739131, 0.514218009478673, 0.635, 1.0, 1.0, 0.32620664886330325, 1.0, 1.0, 0.32620664886330325, 1.0, 1.0, 0.6872494546789235, 0.0, 0.0, 0.8375144448122397, 0.5236868901027227, 0.5236868901027227, 0.5614444210790942], 
reward next is 0.4386, 
noisyNet noise sample is [array([0.2075265], dtype=float32), 1.0416876]. 
=============================================
[2019-04-23 09:34:38,168] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5845407e-19 0.0000000e+00], sum to 1.0000
[2019-04-23 09:34:38,177] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9677
[2019-04-23 09:34:38,294] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 96.0, 1.0, 2.0, 0.3303072886808421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520174.9095118733, 520174.9095118727, 168524.7694924881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 167400.0000, 
sim time next is 168000.0000, 
raw observation next is [20.8, 96.0, 1.0, 2.0, 0.3263997564430149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515268.4718229907, 515268.4718229913, 168168.4146442934], 
processed observation next is [1.0, 0.9565217391304348, 0.1848341232227489, 0.96, 1.0, 1.0, 0.1884334414976083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14313013106194186, 0.14313013106194203, 0.2509976337974528], 
reward next is 0.7490, 
noisyNet noise sample is [array([0.43457466], dtype=float32), -0.9041791]. 
=============================================
[2019-04-23 09:34:38,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[99.55236 ]
 [99.37462 ]
 [99.19079 ]
 [98.99852 ]
 [98.651375]], R is [[99.48912811]
 [99.24271393]
 [98.99836731]
 [98.75645447]
 [98.51647186]].
[2019-04-23 09:34:38,597] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.832436e-22 0.000000e+00], sum to 1.0000
[2019-04-23 09:34:38,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-23 09:34:38,707] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 96.0, 1.0, 2.0, 0.3043500634891076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485130.2792367901, 485130.2792367901, 165997.3618849702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 172800.0000, 
sim time next is 173400.0000, 
raw observation next is [20.36666666666667, 96.0, 1.0, 2.0, 0.3031448632281573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483559.4951141838, 483559.4951141845, 165888.6869817479], 
processed observation next is [0.0, 0.0, 0.1642969984202214, 0.96, 1.0, 1.0, 0.16041549786524978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13432208197616216, 0.13432208197616236, 0.24759505519663863], 
reward next is 0.7524, 
noisyNet noise sample is [array([-0.17055087], dtype=float32), 1.5119392]. 
=============================================
[2019-04-23 09:34:43,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:43,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1883
[2019-04-23 09:34:43,978] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 91.0, 1.0, 2.0, 0.2845031306979766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457763.0814410559, 457763.0814410559, 164120.3348613558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 255000.0000, 
sim time next is 255600.0000, 
raw observation next is [20.5, 91.0, 1.0, 2.0, 0.2833729599606403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456234.4313794903, 456234.4313794903, 164017.6541141958], 
processed observation next is [0.0, 1.0, 0.1706161137440759, 0.91, 1.0, 1.0, 0.13659392766342207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12673178649430286, 0.12673178649430286, 0.2448024688271579], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.37735736], dtype=float32), -0.8226924]. 
=============================================
[2019-04-23 09:34:46,049] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:46,057] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2602
[2019-04-23 09:34:46,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:46,117] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9963
[2019-04-23 09:34:46,165] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 79.0, 1.0, 2.0, 0.3031729866589544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480976.1166820957, 480976.1166820964, 165660.2373612459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 295200.0000, 
sim time next is 295800.0000, 
raw observation next is [22.86666666666667, 78.66666666666667, 1.0, 2.0, 0.303940100671897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482007.6237752045, 482007.6237752045, 165730.9130084243], 
processed observation next is [0.0, 0.43478260869565216, 0.28278041074249627, 0.7866666666666667, 1.0, 1.0, 0.16137361526734575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13389100660422348, 0.13389100660422348, 0.24735957165436465], 
reward next is 0.7526, 
noisyNet noise sample is [array([1.7452663], dtype=float32), -0.06511482]. 
=============================================
[2019-04-23 09:34:46,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 82.66666666666667, 1.0, 2.0, 0.2970708622658521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 472652.8191173339, 472652.8191173346, 165092.4803245918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 290400.0000, 
sim time next is 291000.0000, 
raw observation next is [22.23333333333333, 82.33333333333334, 1.0, 2.0, 0.2981145243526388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474091.621026023, 474091.6210260236, 165190.2972076315], 
processed observation next is [0.0, 0.34782608695652173, 0.25276461295418634, 0.8233333333333335, 1.0, 1.0, 0.1543548486176371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13169211695167304, 0.1316921169516732, 0.24655268239945], 
reward next is 0.7534, 
noisyNet noise sample is [array([-0.6829605], dtype=float32), -0.5423762]. 
=============================================
[2019-04-23 09:34:46,269] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.60206 ]
 [75.580246]
 [75.56973 ]
 [75.55209 ]
 [75.554924]], R is [[75.59590912]
 [75.59354401]
 [75.59124756]
 [75.58899689]
 [75.58678436]].
[2019-04-23 09:34:46,489] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7920: loss 0.0819
[2019-04-23 09:34:46,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7920: learning rate 0.0005
[2019-04-23 09:34:46,552] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7928: loss 0.0200
[2019-04-23 09:34:46,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7930: learning rate 0.0005
[2019-04-23 09:34:46,605] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7950: loss 0.0098
[2019-04-23 09:34:46,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7953: learning rate 0.0005
[2019-04-23 09:34:46,620] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7955: loss 0.0085
[2019-04-23 09:34:46,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7956: learning rate 0.0005
[2019-04-23 09:34:46,663] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 7979: loss 0.0088
[2019-04-23 09:34:46,668] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 7980: learning rate 0.0005
[2019-04-23 09:34:46,676] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7981: loss 0.0060
[2019-04-23 09:34:46,678] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7981: loss 0.0038
[2019-04-23 09:34:46,680] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7982: learning rate 0.0005
[2019-04-23 09:34:46,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7982: learning rate 0.0005
[2019-04-23 09:34:46,698] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7991: loss 0.0041
[2019-04-23 09:34:46,700] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7992: learning rate 0.0005
[2019-04-23 09:34:46,719] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8000: loss 0.0036
[2019-04-23 09:34:46,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8003: learning rate 0.0005
[2019-04-23 09:34:46,728] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8008: loss 0.0235
[2019-04-23 09:34:46,729] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8008: learning rate 0.0005
[2019-04-23 09:34:46,740] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8013: loss 0.0446
[2019-04-23 09:34:46,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8014: learning rate 0.0005
[2019-04-23 09:34:46,745] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8014: loss 0.0081
[2019-04-23 09:34:46,751] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8015: loss 0.0363
[2019-04-23 09:34:46,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8015: learning rate 0.0005
[2019-04-23 09:34:46,754] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8015: learning rate 0.0005
[2019-04-23 09:34:46,757] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8017: loss 0.0346
[2019-04-23 09:34:46,758] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8017: learning rate 0.0005
[2019-04-23 09:34:46,769] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8022: loss 0.0234
[2019-04-23 09:34:46,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8022: learning rate 0.0005
[2019-04-23 09:34:46,828] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8044: loss 0.0386
[2019-04-23 09:34:46,830] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8044: learning rate 0.0005
[2019-04-23 09:34:47,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:47,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6997
[2019-04-23 09:34:47,770] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 78.0, 1.0, 2.0, 0.3066500209044473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485678.6350541453, 485678.6350541453, 165984.1924993817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 316800.0000, 
sim time next is 317400.0000, 
raw observation next is [22.93333333333333, 78.16666666666667, 1.0, 2.0, 0.3057860900630685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484764.9369811529, 484764.9369811529, 165926.998357975], 
processed observation next is [0.0, 0.6956521739130435, 0.28593996840442326, 0.7816666666666667, 1.0, 1.0, 0.16359769887116687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13465692693920914, 0.13465692693920914, 0.24765223635518657], 
reward next is 0.7523, 
noisyNet noise sample is [array([0.3849098], dtype=float32), -1.5702524]. 
=============================================
[2019-04-23 09:34:47,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:47,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2605
[2019-04-23 09:34:47,990] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.3013051075397071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479424.4883262644, 479424.4883262644, 165574.4798935821], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 319800.0000, 
sim time next is 320400.0000, 
raw observation next is [22.6, 79.0, 1.0, 2.0, 0.2993775856819734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476788.9494197393, 476788.9494197399, 165393.446471702], 
processed observation next is [0.0, 0.7391304347826086, 0.27014218009478685, 0.79, 1.0, 1.0, 0.15587660925538965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13244137483881646, 0.13244137483881663, 0.2468558902562716], 
reward next is 0.7531, 
noisyNet noise sample is [array([0.63266504], dtype=float32), 0.36631307]. 
=============================================
[2019-04-23 09:34:48,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.541891e-31 0.000000e+00], sum to 1.0000
[2019-04-23 09:34:48,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5268
[2019-04-23 09:34:48,824] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 85.33333333333333, 1.0, 2.0, 0.2878063677158854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461361.9366500977, 461361.9366500982, 164355.0262740652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 330000.0000, 
sim time next is 330600.0000, 
raw observation next is [21.36666666666667, 85.66666666666667, 1.0, 2.0, 0.2869434534910081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460192.6131244791, 460192.6131244785, 164276.8727312944], 
processed observation next is [0.0, 0.8260869565217391, 0.21169036334913136, 0.8566666666666667, 1.0, 1.0, 0.1408957270976001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12783128142346642, 0.12783128142346625, 0.24518936228551402], 
reward next is 0.7548, 
noisyNet noise sample is [array([-1.0469725], dtype=float32), -1.433855]. 
=============================================
[2019-04-23 09:34:49,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:49,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0724
[2019-04-23 09:34:49,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 86.0, 1.0, 2.0, 0.2764787152356477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 447421.9574306399, 447421.9574306393, 163423.2076608312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [20.8, 86.0, 1.0, 2.0, 0.2733448314107534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442603.5072770095, 442603.5072770101, 163104.8118955987], 
processed observation next is [0.0, 0.9565217391304348, 0.1848341232227489, 0.86, 1.0, 1.0, 0.12451184507319685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1229454186880582, 0.12294541868805837, 0.2434400177546249], 
reward next is 0.7566, 
noisyNet noise sample is [array([-0.03372161], dtype=float32), 1.522575]. 
=============================================
[2019-04-23 09:34:59,711] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9352108e-32 0.0000000e+00], sum to 1.0000
[2019-04-23 09:34:59,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-23 09:34:59,818] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.83333333333333, 87.0, 1.0, 2.0, 0.2321318764183155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385011.7829843339, 385011.7829843339, 159002.7278875467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
processed observation next is [1.0, 1.0, 0.09083728278041096, 0.87, 1.0, 1.0, 0.07366329259156455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10650673415106725, 0.10650673415106725, 0.23717051313799883], 
reward next is 0.7628, 
noisyNet noise sample is [array([1.1752498], dtype=float32), 0.13002636]. 
=============================================
[2019-04-23 09:35:04,553] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15938: loss 0.0476
[2019-04-23 09:35:04,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15938: learning rate 0.0005
[2019-04-23 09:35:04,567] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15943: loss 0.0410
[2019-04-23 09:35:04,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15944: learning rate 0.0005
[2019-04-23 09:35:04,587] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15954: loss 0.0212
[2019-04-23 09:35:04,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15955: learning rate 0.0005
[2019-04-23 09:35:04,604] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15958: loss 0.0128
[2019-04-23 09:35:04,606] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15960: learning rate 0.0005
[2019-04-23 09:35:04,621] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15966: loss 0.0213
[2019-04-23 09:35:04,626] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15967: learning rate 0.0005
[2019-04-23 09:35:04,635] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15972: loss 0.0054
[2019-04-23 09:35:04,638] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15972: learning rate 0.0005
[2019-04-23 09:35:04,644] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15974: loss 0.0096
[2019-04-23 09:35:04,647] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15975: learning rate 0.0005
[2019-04-23 09:35:04,655] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15978: loss 0.0058
[2019-04-23 09:35:04,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15978: learning rate 0.0005
[2019-04-23 09:35:04,692] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15992: loss 0.0039
[2019-04-23 09:35:04,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15995: learning rate 0.0005
[2019-04-23 09:35:04,705] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15996: loss 0.0033
[2019-04-23 09:35:04,708] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15996: learning rate 0.0005
[2019-04-23 09:35:04,733] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16012: loss 0.0035
[2019-04-23 09:35:04,736] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16014: learning rate 0.0005
[2019-04-23 09:35:04,742] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16015: loss 0.0034
[2019-04-23 09:35:04,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16016: learning rate 0.0005
[2019-04-23 09:35:04,753] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16020: loss 0.0045
[2019-04-23 09:35:04,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16021: learning rate 0.0005
[2019-04-23 09:35:04,775] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16032: loss 0.0081
[2019-04-23 09:35:04,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16032: learning rate 0.0005
[2019-04-23 09:35:04,783] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16034: loss 0.0157
[2019-04-23 09:35:04,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16035: learning rate 0.0005
[2019-04-23 09:35:04,819] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16053: loss 0.0348
[2019-04-23 09:35:04,822] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16053: learning rate 0.0005
[2019-04-23 09:35:06,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.551411e-34 0.000000e+00], sum to 1.0000
[2019-04-23 09:35:06,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4360
[2019-04-23 09:35:06,307] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.06666666666667, 87.0, 1.0, 2.0, 0.211391117839434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 352652.3475898298, 352652.3475898298, 156682.8825518083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [18.3, 86.0, 1.0, 2.0, 0.2135148031529875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 355961.443750149, 355961.4437501484, 156949.4386614617], 
processed observation next is [1.0, 0.2608695652173913, 0.06635071090047404, 0.86, 1.0, 1.0, 0.05242747367829816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09887817881948582, 0.09887817881948567, 0.23425289352456968], 
reward next is 0.7657, 
noisyNet noise sample is [array([-0.3664112], dtype=float32), 1.0957714]. 
=============================================
[2019-04-23 09:35:08,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2590164e-31 0.0000000e+00], sum to 1.0000
[2019-04-23 09:35:08,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2128
[2019-04-23 09:35:08,818] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 60.0, 1.0, 2.0, 0.246842079243286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 406347.7311477184, 406347.7311477184, 160589.7111319035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 669600.0000, 
sim time next is 670200.0000, 
raw observation next is [23.18333333333333, 61.33333333333333, 1.0, 2.0, 0.2504187298447559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412273.4695163005, 412273.4695163011, 160939.509386387], 
processed observation next is [1.0, 0.782608695652174, 0.29778830963665076, 0.6133333333333333, 1.0, 1.0, 0.0968900359575372, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11452040819897236, 0.11452040819897254, 0.24020822296475675], 
reward next is 0.7598, 
noisyNet noise sample is [array([-0.8678966], dtype=float32), 1.0657357]. 
=============================================
[2019-04-23 09:35:22,204] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23909: loss 0.1117
[2019-04-23 09:35:22,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23909: learning rate 0.0005
[2019-04-23 09:35:22,219] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23912: loss 0.0676
[2019-04-23 09:35:22,222] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23912: learning rate 0.0005
[2019-04-23 09:35:22,250] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23925: loss 0.0857
[2019-04-23 09:35:22,253] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23925: learning rate 0.0005
[2019-04-23 09:35:22,284] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23940: loss 0.0571
[2019-04-23 09:35:22,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23941: learning rate 0.0005
[2019-04-23 09:35:22,323] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23960: loss 0.0559
[2019-04-23 09:35:22,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23961: learning rate 0.0005
[2019-04-23 09:35:22,337] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23966: loss 0.0443
[2019-04-23 09:35:22,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23966: learning rate 0.0005
[2019-04-23 09:35:22,354] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23971: loss 0.0433
[2019-04-23 09:35:22,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23972: learning rate 0.0005
[2019-04-23 09:35:22,373] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23980: loss 0.0442
[2019-04-23 09:35:22,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23982: learning rate 0.0005
[2019-04-23 09:35:22,386] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23986: loss 0.0257
[2019-04-23 09:35:22,389] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23986: learning rate 0.0005
[2019-04-23 09:35:22,392] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23989: loss 0.0191
[2019-04-23 09:35:22,396] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23989: learning rate 0.0005
[2019-04-23 09:35:22,402] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 23992: loss 0.0085
[2019-04-23 09:35:22,410] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 23993: learning rate 0.0005
[2019-04-23 09:35:22,466] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24022: loss 0.0059
[2019-04-23 09:35:22,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24023: learning rate 0.0005
[2019-04-23 09:35:22,518] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24045: loss 0.0261
[2019-04-23 09:35:22,519] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24046: loss 0.0267
[2019-04-23 09:35:22,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24046: learning rate 0.0005
[2019-04-23 09:35:22,521] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24047: learning rate 0.0005
[2019-04-23 09:35:22,589] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24075: loss 0.0542
[2019-04-23 09:35:22,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24079: learning rate 0.0005
[2019-04-23 09:35:22,611] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24088: loss 0.0424
[2019-04-23 09:35:22,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24088: learning rate 0.0005
[2019-04-23 09:35:24,430] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:35:24,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-23 09:35:24,556] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 87.66666666666667, 1.0, 2.0, 0.3384132657816531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524183.5858627975, 524183.585862798, 168622.8294463988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [22.55, 88.0, 1.0, 2.0, 0.3387289176552802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524766.1754814181, 524766.1754814174, 168672.0485317231], 
processed observation next is [0.0, 0.8695652173913043, 0.26777251184834133, 0.88, 1.0, 1.0, 0.2032878525967231, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14576838207817172, 0.14576838207817153, 0.2517493261667509], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.28329897], dtype=float32), -0.599047]. 
=============================================
[2019-04-23 09:35:24,732] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 09:35:24,738] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:35:24,740] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:35:24,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,743] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:35:24,744] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:35:24,746] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,745] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:35:24,746] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,748] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,749] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,766] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,767] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,782] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,821] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,822] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run2
[2019-04-23 09:35:28,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:35:28,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.93333333333333, 89.83333333333333, 1.0, 2.0, 0.2929882873432826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469089.0690366681, 469089.0690366681, 164883.7680691813]
[2019-04-23 09:35:28,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:35:28,804] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1474123916898269
[2019-04-23 09:35:57,913] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:35:57,914] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.77149693, 70.66947462, 1.0, 2.0, 0.4303315642890675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104055, 606943.6261604563, 606943.6261604563, 174137.3055505385]
[2019-04-23 09:35:57,916] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:35:57,919] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.254142e-38 0.000000e+00], sampled 0.16709336821516285
[2019-04-23 09:35:59,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:35:59,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.7, 85.5, 1.0, 2.0, 0.7473744893047615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044509.702720558, 1044509.702720558, 231403.9013687684]
[2019-04-23 09:35:59,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:35:59,059] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6120957e-38 0.0000000e+00], sampled 0.9713376384883396
[2019-04-23 09:36:10,864] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:10,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.6, 57.66666666666667, 1.0, 2.0, 0.5130342743368855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716891.8351393198, 716891.8351393198, 185663.1108818094]
[2019-04-23 09:36:10,866] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:36:10,869] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44918777299403645
[2019-04-23 09:36:12,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:12,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 81.16666666666666, 1.0, 2.0, 0.5106250491987964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713524.1577941555, 713524.1577941562, 185276.5599778855]
[2019-04-23 09:36:12,151] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:36:12,152] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6337055e-37 0.0000000e+00], sampled 0.8838392040616314
[2019-04-23 09:36:47,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:47,775] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.01666666666667, 94.16666666666667, 1.0, 2.0, 0.6116502410487251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 854748.953485946, 854748.9534859468, 202930.3302393857]
[2019-04-23 09:36:47,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:36:47,783] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5372494125553657
[2019-04-23 09:36:48,299] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:48,302] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066]
[2019-04-23 09:36:48,303] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:36:48,306] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43320294242967716
[2019-04-23 09:37:06,110] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:37:06,114] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.25796125833333, 90.86245502, 1.0, 2.0, 0.4017387458499152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660750.6507338983, 660750.6507338983, 179727.6665769322]
[2019-04-23 09:37:06,115] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:37:06,117] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22021562064898048
[2019-04-23 09:37:11,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:37:11,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.36666666666667, 61.0, 1.0, 2.0, 0.403386701501198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 629678.8521892619, 629678.8521892613, 177793.5041055562]
[2019-04-23 09:37:11,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:37:11,941] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7109363684468294
[2019-04-23 09:37:13,596] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6985 2927265673.6025 1340.0000
[2019-04-23 09:37:13,744] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.5478 3163641139.4299 1775.0000
[2019-04-23 09:37:13,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2097 2779160217.1565 933.0000
[2019-04-23 09:37:13,852] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7839 3007412927.2304 1766.0000
[2019-04-23 09:37:13,921] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3815 2842403427.4721 1132.0000
[2019-04-23 09:37:14,932] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 25000, evaluation results [25000.0, 7885.547773540918, 3163641139.4298544, 1775.0, 8254.698514502707, 2927265673.6025105, 1340.0, 8661.209722185526, 2779160217.1564946, 933.0, 7997.78389971888, 3007412927.2304306, 1766.0, 8497.381504661924, 2842403427.4721303, 1132.0]
[2019-04-23 09:37:18,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.7866345e-25 0.0000000e+00], sum to 1.0000
[2019-04-23 09:37:18,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8851
[2019-04-23 09:37:18,191] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 95.0, 1.0, 2.0, 0.4035655307059746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622761.6777443201, 622761.6777443201, 177079.374243855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 995400.0000, 
sim time next is 996000.0000, 
raw observation next is [21.76666666666667, 95.0, 1.0, 2.0, 0.4490403868515986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693652.4545397687, 693652.4545397687, 183952.0066599745], 
processed observation next is [1.0, 0.5217391304347826, 0.23064770932069528, 0.95, 1.0, 1.0, 0.33619323717060073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19268123737215798, 0.19268123737215798, 0.27455523382085745], 
reward next is 0.7254, 
noisyNet noise sample is [array([-0.35741326], dtype=float32), -0.53296876]. 
=============================================
[2019-04-23 09:37:18,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.41277 ]
 [64.984505]
 [63.840626]
 [64.24222 ]
 [64.07577 ]], R is [[65.21655273]
 [65.30008698]
 [65.36699677]
 [65.36919403]
 [65.40205383]].
[2019-04-23 09:37:18,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1001671e-27 0.0000000e+00], sum to 1.0000
[2019-04-23 09:37:18,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9729
[2019-04-23 09:37:18,814] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.5330327924032255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824628.4409084235, 824628.4409084235, 198432.8941335611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1003200.0000, 
sim time next is 1003800.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.5202789958299762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804925.5168352509, 804925.5168352509, 196103.687232592], 
processed observation next is [1.0, 0.6086956521739131, 0.22274881516587688, 0.96, 1.0, 1.0, 0.42202288654213993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22359042134312523, 0.22359042134312523, 0.29269207049640594], 
reward next is 0.7073, 
noisyNet noise sample is [array([0.7404093], dtype=float32), -0.8224834]. 
=============================================
[2019-04-23 09:37:19,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7637566e-36 1.0000000e+00 0.0000000e+00 3.4303653e-21 0.0000000e+00], sum to 1.0000
[2019-04-23 09:37:19,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8900
[2019-04-23 09:37:19,147] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333333, 96.83333333333334, 1.0, 2.0, 0.5879670029979287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 905193.3262040943, 905193.3262040943, 208626.7930460681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [21.7, 97.0, 1.0, 2.0, 0.6386368761396531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 982137.0935775214, 982137.0935775208, 219174.3486969676], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.97, 1.0, 1.0, 0.5646227423369314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2728158593270893, 0.2728158593270891, 0.32712589357756355], 
reward next is 0.6729, 
noisyNet noise sample is [array([0.5377404], dtype=float32), -0.5845203]. 
=============================================
[2019-04-23 09:37:19,160] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.63264]
 [70.60004]
 [70.69384]
 [70.59424]
 [70.08832]], R is [[70.49804688]
 [70.48168182]
 [70.46871948]
 [70.46642303]
 [70.45937347]].
[2019-04-23 09:37:30,141] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31860: loss 0.3683
[2019-04-23 09:37:30,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31860: learning rate 0.0005
[2019-04-23 09:37:30,244] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31902: loss 0.5176
[2019-04-23 09:37:30,248] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31903: learning rate 0.0005
[2019-04-23 09:37:30,287] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 31920: loss 0.4676
[2019-04-23 09:37:30,288] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 31920: learning rate 0.0005
[2019-04-23 09:37:30,317] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31929: loss 0.4099
[2019-04-23 09:37:30,323] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31931: learning rate 0.0005
[2019-04-23 09:37:30,344] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31943: loss 0.3643
[2019-04-23 09:37:30,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31944: learning rate 0.0005
[2019-04-23 09:37:30,355] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31946: loss 0.3508
[2019-04-23 09:37:30,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31946: learning rate 0.0005
[2019-04-23 09:37:30,406] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31969: loss 0.3205
[2019-04-23 09:37:30,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31971: learning rate 0.0005
[2019-04-23 09:37:30,437] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31985: loss 0.1858
[2019-04-23 09:37:30,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31985: learning rate 0.0005
[2019-04-23 09:37:30,442] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31986: loss 0.1610
[2019-04-23 09:37:30,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31986: learning rate 0.0005
[2019-04-23 09:37:30,496] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32004: loss 0.1227
[2019-04-23 09:37:30,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32005: learning rate 0.0005
[2019-04-23 09:37:30,591] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32045: loss 0.0604
[2019-04-23 09:37:30,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32046: learning rate 0.0005
[2019-04-23 09:37:30,610] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32056: loss 0.0222
[2019-04-23 09:37:30,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32057: learning rate 0.0005
[2019-04-23 09:37:30,622] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32059: loss 0.0362
[2019-04-23 09:37:30,624] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32059: loss 0.0198
[2019-04-23 09:37:30,625] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32059: learning rate 0.0005
[2019-04-23 09:37:30,629] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32061: loss 0.0069
[2019-04-23 09:37:30,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32061: learning rate 0.0005
[2019-04-23 09:37:30,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32062: learning rate 0.0005
[2019-04-23 09:37:30,666] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32075: loss 0.0072
[2019-04-23 09:37:30,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32075: learning rate 0.0005
[2019-04-23 09:37:30,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:30,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3187
[2019-04-23 09:37:30,872] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 85.33333333333334, 1.0, 2.0, 0.3554413071659422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547751.2515474142, 547751.2515474149, 170464.3484644801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [23.0, 86.16666666666666, 1.0, 2.0, 0.3560965178943104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548579.436239281, 548579.436239281, 170528.3612299317], 
processed observation next is [1.0, 0.9565217391304348, 0.28909952606635075, 0.8616666666666666, 1.0, 1.0, 0.22421267216181975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1523831767331336, 0.1523831767331336, 0.2545199421342264], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.8303618], dtype=float32), -0.46187267]. 
=============================================
[2019-04-23 09:37:41,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:42,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6370
[2019-04-23 09:37:42,110] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 98.0, 1.0, 2.0, 0.3099197336711298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491720.4763980514, 491720.4763980507, 166442.265502585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1395000.0000, 
sim time next is 1395600.0000, 
raw observation next is [20.43333333333333, 98.0, 1.0, 2.0, 0.3124547762544969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495360.7909442527, 495360.7909442527, 166703.9298310911], 
processed observation next is [0.0, 0.13043478260869565, 0.1674565560821484, 0.98, 1.0, 1.0, 0.17163226054758662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13760021970673686, 0.13760021970673686, 0.24881183556879272], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.6505396], dtype=float32), 0.969189]. 
=============================================
[2019-04-23 09:37:48,341] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39860: loss 0.0812
[2019-04-23 09:37:48,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39860: learning rate 0.0005
[2019-04-23 09:37:48,426] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39891: loss 0.1391
[2019-04-23 09:37:48,430] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39891: learning rate 0.0005
[2019-04-23 09:37:48,465] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39907: loss 0.1145
[2019-04-23 09:37:48,469] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39908: learning rate 0.0005
[2019-04-23 09:37:48,492] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39919: loss 0.1187
[2019-04-23 09:37:48,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39919: learning rate 0.0005
[2019-04-23 09:37:48,549] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39935: loss 0.0877
[2019-04-23 09:37:48,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39937: learning rate 0.0005
[2019-04-23 09:37:48,575] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39944: loss 0.0571
[2019-04-23 09:37:48,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39944: learning rate 0.0005
[2019-04-23 09:37:48,660] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39975: loss 0.0401
[2019-04-23 09:37:48,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39976: learning rate 0.0005
[2019-04-23 09:37:48,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39985: loss 0.0094
[2019-04-23 09:37:48,689] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39988: learning rate 0.0005
[2019-04-23 09:37:48,717] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39998: loss 0.0052
[2019-04-23 09:37:48,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39998: learning rate 0.0005
[2019-04-23 09:37:48,750] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40004: loss 0.0028
[2019-04-23 09:37:48,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40004: learning rate 0.0005
[2019-04-23 09:37:48,763] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40005: loss 0.0018
[2019-04-23 09:37:48,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40007: learning rate 0.0005
[2019-04-23 09:37:48,821] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40031: loss 0.0012
[2019-04-23 09:37:48,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40034: learning rate 0.0005
[2019-04-23 09:37:48,843] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40041: loss 0.0010
[2019-04-23 09:37:48,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40041: learning rate 0.0005
[2019-04-23 09:37:48,855] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40045: loss 0.0018
[2019-04-23 09:37:48,859] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40045: learning rate 0.0005
[2019-04-23 09:37:48,936] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40071: loss 0.0027
[2019-04-23 09:37:48,938] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40071: learning rate 0.0005
[2019-04-23 09:37:49,273] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40195: loss 0.0387
[2019-04-23 09:37:49,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40195: learning rate 0.0005
[2019-04-23 09:37:51,042] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:51,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-23 09:37:51,153] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 74.0, 1.0, 2.0, 0.3532127284353498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543324.9260934021, 543324.9260934027, 170067.8775380704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1537200.0000, 
sim time next is 1537800.0000, 
raw observation next is [24.6, 75.5, 1.0, 2.0, 0.3542998769845044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544581.0258525242, 544581.0258525242, 170159.7827732003], 
processed observation next is [0.0, 0.8260869565217391, 0.36492890995260674, 0.755, 1.0, 1.0, 0.22204804455964383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15127250718125673, 0.15127250718125673, 0.25396982503462734], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.7240056], dtype=float32), 1.1018342]. 
=============================================
[2019-04-23 09:37:54,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:54,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8885
[2019-04-23 09:37:54,532] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 85.16666666666667, 1.0, 2.0, 0.3847513131915307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590258.5987439593, 590258.5987439593, 174078.2045986252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1583400.0000, 
sim time next is 1584000.0000, 
raw observation next is [23.4, 85.0, 1.0, 2.0, 0.3990236293460754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610780.2515295085, 610780.2515295091, 175897.4022419333], 
processed observation next is [1.0, 0.34782608695652173, 0.30805687203791465, 0.85, 1.0, 1.0, 0.2759320835494884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16966118098041902, 0.16966118098041919, 0.26253343618199], 
reward next is 0.7375, 
noisyNet noise sample is [array([0.9413071], dtype=float32), 2.3995712]. 
=============================================
[2019-04-23 09:37:54,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.18559 ]
 [71.096115]
 [70.8802  ]
 [70.98984 ]
 [71.29495 ]], R is [[71.21181488]
 [71.23987579]
 [71.2660141 ]
 [71.28653717]
 [71.30900574]].
[2019-04-23 09:37:56,434] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:56,443] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3525
[2019-04-23 09:37:56,454] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 86.33333333333334, 1.0, 2.0, 0.7236314791354455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080989.417543761, 1080989.417543761, 235162.7322399884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1606800.0000, 
sim time next is 1607400.0000, 
raw observation next is [23.9, 87.0, 1.0, 2.0, 0.7590619797247687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1133001.807641249, 1133001.807641248, 243730.3944333877], 
processed observation next is [1.0, 0.6086956521739131, 0.33175355450236965, 0.87, 1.0, 1.0, 0.7097132285840587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31472272434479137, 0.3147227243447911, 0.3637767081095339], 
reward next is 0.6362, 
noisyNet noise sample is [array([-1.1490645], dtype=float32), -0.038624994]. 
=============================================
[2019-04-23 09:38:02,841] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5564364e-25 1.0000000e+00 8.5805126e-31 7.8984128e-15 2.6762476e-25], sum to 1.0000
[2019-04-23 09:38:02,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0769
[2019-04-23 09:38:02,859] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1928698.098685594 W.
[2019-04-23 09:38:02,968] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.56666666666667, 76.5, 1.0, 2.0, 0.459824215619476, 1.0, 2.0, 0.459824215619476, 1.0, 1.0, 0.7888773306295616, 6.9112, 6.9112, 170.5573041426782, 1928698.098685594, 1928698.098685594, 386876.1187746875], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1699800.0000, 
sim time next is 1700400.0000, 
raw observation next is [28.63333333333334, 76.0, 1.0, 2.0, 0.7449230836696593, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.983022695440308, 6.9112, 168.9125286380091, 1937993.950945889, 1887040.587924125, 395520.2954888534], 
processed observation next is [1.0, 0.6956521739130435, 0.5560821484992104, 0.76, 1.0, 1.0, 0.6926784140598304, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007182269544030806, 0.0, 0.8294378441028303, 0.5383316530405248, 0.5241779410900347, 0.5903287992370947], 
reward next is 0.0506, 
noisyNet noise sample is [array([-1.004074], dtype=float32), -1.3016319]. 
=============================================
[2019-04-23 09:38:09,637] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47814: loss 0.0977
[2019-04-23 09:38:09,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47816: learning rate 0.0005
[2019-04-23 09:38:09,725] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47847: loss 0.1200
[2019-04-23 09:38:09,732] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47847: learning rate 0.0005
[2019-04-23 09:38:09,893] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47912: loss 0.1215
[2019-04-23 09:38:09,895] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47912: learning rate 0.0005
[2019-04-23 09:38:09,937] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47929: loss 0.1373
[2019-04-23 09:38:09,941] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47931: learning rate 0.0005
[2019-04-23 09:38:10,030] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47966: loss 0.1394
[2019-04-23 09:38:10,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47966: learning rate 0.0005
[2019-04-23 09:38:10,049] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47971: loss 0.1554
[2019-04-23 09:38:10,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47971: learning rate 0.0005
[2019-04-23 09:38:10,078] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47982: loss 0.1532
[2019-04-23 09:38:10,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47982: learning rate 0.0005
[2019-04-23 09:38:10,085] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47983: loss 0.2120
[2019-04-23 09:38:10,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47985: learning rate 0.0005
[2019-04-23 09:38:10,123] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47996: loss 0.1747
[2019-04-23 09:38:10,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47998: learning rate 0.0005
[2019-04-23 09:38:10,153] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48007: loss 0.1301
[2019-04-23 09:38:10,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48007: learning rate 0.0005
[2019-04-23 09:38:10,192] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48022: loss 0.2027
[2019-04-23 09:38:10,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48022: learning rate 0.0005
[2019-04-23 09:38:10,219] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48030: loss 0.1877
[2019-04-23 09:38:10,222] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48030: learning rate 0.0005
[2019-04-23 09:38:10,240] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48039: loss 0.2124
[2019-04-23 09:38:10,244] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48040: learning rate 0.0005
[2019-04-23 09:38:10,253] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48043: loss 0.1684
[2019-04-23 09:38:10,261] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48044: learning rate 0.0005
[2019-04-23 09:38:10,492] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48129: loss 0.0415
[2019-04-23 09:38:10,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48129: learning rate 0.0005
[2019-04-23 09:38:10,516] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48137: loss 0.0298
[2019-04-23 09:38:10,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48138: learning rate 0.0005
[2019-04-23 09:38:10,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:38:10,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5380
[2019-04-23 09:38:10,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 94.66666666666667, 1.0, 2.0, 0.3361926734803173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525136.4967936483, 525136.4967936483, 168819.4305102034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1808400.0000, 
sim time next is 1809000.0000, 
raw observation next is [21.45, 95.0, 1.0, 2.0, 0.3386862032060498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528242.5919976203, 528242.5919976203, 169046.6448126051], 
processed observation next is [1.0, 0.9565217391304348, 0.2156398104265403, 0.95, 1.0, 1.0, 0.2032363894048793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1467340533326723, 0.1467340533326723, 0.2523084250934405], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.8225947], dtype=float32), -0.59887433]. 
=============================================
[2019-04-23 09:38:10,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.96179 ]
 [85.91781 ]
 [85.873215]
 [85.69507 ]
 [85.68802 ]], R is [[85.89198303]
 [85.78109741]
 [85.67167664]
 [85.56351471]
 [85.45627594]].
[2019-04-23 09:38:13,703] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2858607e-30 0.0000000e+00], sum to 1.0000
[2019-04-23 09:38:13,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-23 09:38:13,721] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 90.0, 1.0, 2.0, 0.8642812292572144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238442.543499014, 1238442.543499014, 264706.07610416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1850400.0000, 
sim time next is 1851000.0000, 
raw observation next is [24.71666666666667, 89.66666666666667, 1.0, 2.0, 0.8951863576699861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1278924.57930032, 1278924.57930032, 272699.238558078], 
processed observation next is [1.0, 0.43478260869565216, 0.3704581358609796, 0.8966666666666667, 1.0, 1.0, 0.8737185032168507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35525682758342225, 0.35525682758342225, 0.40701378889265377], 
reward next is 0.5930, 
noisyNet noise sample is [array([-0.6357273], dtype=float32), 0.5777712]. 
=============================================
[2019-04-23 09:38:13,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.30991 ]
 [70.9194  ]
 [70.822975]
 [70.733116]
 [70.786285]], R is [[71.23170471]
 [71.12430573]
 [71.00756073]
 [70.89545441]
 [70.78931427]].
[2019-04-23 09:38:15,517] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 09:38:15,518] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:38:15,519] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:38:15,519] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,520] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:38:15,520] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,520] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:38:15,521] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,522] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:38:15,523] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,526] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,558] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,580] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,580] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,619] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run3
[2019-04-23 09:38:37,558] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:38:37,561] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.46666666666667, 52.66666666666667, 1.0, 2.0, 0.2038880258608933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 340797.1709729611, 340797.1709729611, 155688.7188106712]
[2019-04-23 09:38:37,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:38:37,564] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5686517220505963
[2019-04-23 09:38:49,376] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:38:49,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.9, 93.5, 1.0, 2.0, 0.3901065997402537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585834.8545793705, 585834.8545793712, 173349.9853749929]
[2019-04-23 09:38:49,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:38:49,380] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4225549312590958
[2019-04-23 09:39:03,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:39:03,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.58839391, 88.78940779999999, 1.0, 2.0, 0.3302651360059471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521193.7786214587, 521193.778621458, 168624.0941074877]
[2019-04-23 09:39:03,652] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:39:03,655] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8813291109176173
[2019-04-23 09:39:59,218] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:39:59,219] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.88845082333334, 91.69522763, 1.0, 2.0, 0.532954605354385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744737.399278292, 744737.3992782925, 188918.8279295965]
[2019-04-23 09:39:59,220] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:39:59,222] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24528811499198677
[2019-04-23 09:40:23,965] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:40:25,294] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:40:25,458] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:40:25,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:40:25,709] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:40:26,723] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 50000, evaluation results [50000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:40:27,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:27,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6663
[2019-04-23 09:40:27,914] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333333, 88.33333333333334, 1.0, 2.0, 0.4619212663082412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655440.5529497884, 655440.5529497884, 179131.7906215914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1890600.0000, 
sim time next is 1891200.0000, 
raw observation next is [24.96666666666667, 88.66666666666667, 1.0, 2.0, 0.4590128919104424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651900.7031326913, 651900.703132692, 178778.1598062251], 
processed observation next is [1.0, 0.9130434782608695, 0.3823064770932071, 0.8866666666666667, 1.0, 1.0, 0.3482083035065572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1810835286479698, 0.18108352864797, 0.2668330743376494], 
reward next is 0.7332, 
noisyNet noise sample is [array([-1.6581613], dtype=float32), 1.5497767]. 
=============================================
[2019-04-23 09:40:33,671] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:33,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2358
[2019-04-23 09:40:33,693] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 91.5, 1.0, 2.0, 0.4119402021999823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607025.5068812162, 607025.5068812162, 174971.4271783108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1967400.0000, 
sim time next is 1968000.0000, 
raw observation next is [23.53333333333333, 92.0, 1.0, 2.0, 0.4073195822012597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601361.8269960075, 601361.8269960075, 174476.4283479802], 
processed observation next is [1.0, 0.782608695652174, 0.3143759873617693, 0.92, 1.0, 1.0, 0.2859272074713972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1670449519433354, 0.1670449519433354, 0.26041257962385106], 
reward next is 0.7396, 
noisyNet noise sample is [array([-2.1825202], dtype=float32), -0.68494695]. 
=============================================
[2019-04-23 09:40:33,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.10615 ]
 [72.605225]
 [72.71682 ]
 [72.96151 ]
 [72.82404 ]], R is [[72.71128082]
 [72.72301483]
 [72.73377228]
 [72.74394989]
 [72.75439453]].
[2019-04-23 09:40:38,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:38,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8394
[2019-04-23 09:40:38,189] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.0, 1.0, 2.0, 0.5168110363721877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722171.1124931996, 722171.1124931996, 186272.0921017974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2035800.0000, 
sim time next is 2036400.0000, 
raw observation next is [27.36666666666667, 83.33333333333334, 1.0, 2.0, 0.5168558437300147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722233.745791788, 722233.7457917886, 186279.3772949345], 
processed observation next is [0.0, 0.5652173913043478, 0.49605055292259104, 0.8333333333333335, 1.0, 1.0, 0.4178986069036321, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20062048494216334, 0.2006204849421635, 0.27802892133572316], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.5673664], dtype=float32), 0.31211057]. 
=============================================
[2019-04-23 09:40:39,563] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:39,570] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6679
[2019-04-23 09:40:39,574] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 88.0, 1.0, 2.0, 0.4859981027887919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679100.5861074259, 679100.5861074264, 181431.3260307892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2055600.0000, 
sim time next is 2056200.0000, 
raw observation next is [25.73333333333333, 88.16666666666667, 1.0, 2.0, 0.484744198171802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677347.9067545087, 677347.9067545087, 181240.2720779809], 
processed observation next is [0.0, 0.8260869565217391, 0.41864139020537117, 0.8816666666666667, 1.0, 1.0, 0.3792098773154241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18815219632069688, 0.18815219632069688, 0.2705078687731058], 
reward next is 0.7295, 
noisyNet noise sample is [array([-1.2554067], dtype=float32), -0.39216426]. 
=============================================
[2019-04-23 09:40:42,189] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55823: loss 0.0331
[2019-04-23 09:40:42,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55824: learning rate 0.0005
[2019-04-23 09:40:42,339] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55877: loss 0.0053
[2019-04-23 09:40:42,345] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55877: learning rate 0.0005
[2019-04-23 09:40:42,414] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55904: loss 0.0014
[2019-04-23 09:40:42,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55904: learning rate 0.0005
[2019-04-23 09:40:42,564] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55959: loss 0.0131
[2019-04-23 09:40:42,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55960: learning rate 0.0005
[2019-04-23 09:40:42,586] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55968: loss 0.0035
[2019-04-23 09:40:42,589] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55968: learning rate 0.0005
[2019-04-23 09:40:42,626] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55981: loss 0.0019
[2019-04-23 09:40:42,629] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55983: learning rate 0.0005
[2019-04-23 09:40:42,642] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 55988: loss 0.0022
[2019-04-23 09:40:42,644] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 55988: learning rate 0.0005
[2019-04-23 09:40:42,647] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55989: loss 0.0031
[2019-04-23 09:40:42,649] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55990: learning rate 0.0005
[2019-04-23 09:40:42,677] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56000: loss 0.0018
[2019-04-23 09:40:42,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56000: learning rate 0.0005
[2019-04-23 09:40:42,688] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56000: loss 0.0014
[2019-04-23 09:40:42,691] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56002: learning rate 0.0005
[2019-04-23 09:40:42,714] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56010: loss 0.0014
[2019-04-23 09:40:42,719] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56011: learning rate 0.0005
[2019-04-23 09:40:42,721] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56011: loss 0.0015
[2019-04-23 09:40:42,724] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56011: learning rate 0.0005
[2019-04-23 09:40:42,737] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56019: loss 0.0018
[2019-04-23 09:40:42,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56019: learning rate 0.0005
[2019-04-23 09:40:42,771] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56032: loss 0.0046
[2019-04-23 09:40:42,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56033: learning rate 0.0005
[2019-04-23 09:40:42,974] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56107: loss 0.0039
[2019-04-23 09:40:42,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56107: learning rate 0.0005
[2019-04-23 09:40:43,061] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56143: loss 0.0015
[2019-04-23 09:40:43,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56143: learning rate 0.0005
[2019-04-23 09:40:47,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:47,234] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2554
[2019-04-23 09:40:47,240] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5079720506626362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709815.7400262054, 709815.7400262054, 184854.4702663285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2164800.0000, 
sim time next is 2165400.0000, 
raw observation next is [25.45, 94.0, 1.0, 2.0, 0.5063128050034075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707496.4177415373, 707496.4177415373, 184590.9725924404], 
processed observation next is [1.0, 0.043478260869565216, 0.4052132701421801, 0.94, 1.0, 1.0, 0.405196150606515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1965267827059826, 0.1965267827059826, 0.27550891431707525], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.06705478], dtype=float32), -0.051217925]. 
=============================================
[2019-04-23 09:40:52,065] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9353507e-32 0.0000000e+00], sum to 1.0000
[2019-04-23 09:40:52,074] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1089
[2019-04-23 09:40:52,083] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.06666666666666, 80.16666666666666, 1.0, 2.0, 0.5576038116014002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779194.2153856759, 779194.2153856759, 193113.7596043959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2231400.0000, 
sim time next is 2232000.0000, 
raw observation next is [28.9, 81.0, 1.0, 2.0, 0.5560766237647707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777059.3461913638, 777059.3461913643, 192848.7764723962], 
processed observation next is [1.0, 0.8695652173913043, 0.5687203791469194, 0.81, 1.0, 1.0, 0.4651525587527358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21584981838648992, 0.2158498183864901, 0.28783399473491966], 
reward next is 0.7122, 
noisyNet noise sample is [array([-1.3722591], dtype=float32), 0.09608543]. 
=============================================
[2019-04-23 09:40:52,102] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[46.876053]
 [47.031845]
 [47.418293]
 [47.549435]
 [47.847973]], R is [[47.18795395]
 [47.427845  ]
 [47.66516495]
 [47.90035629]
 [48.13343048]].
[2019-04-23 09:40:52,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.475846e-30 0.000000e+00], sum to 1.0000
[2019-04-23 09:40:52,755] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7856
[2019-04-23 09:40:52,761] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 82.33333333333334, 1.0, 2.0, 0.5445081688488133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760887.8415388272, 760887.8415388279, 190863.5972684024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2236200.0000, 
sim time next is 2236800.0000, 
raw observation next is [28.2, 82.66666666666667, 1.0, 2.0, 0.5433030048505412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759203.1609540338, 759203.1609540345, 190659.0467544961], 
processed observation next is [1.0, 0.9130434782608695, 0.5355450236966824, 0.8266666666666667, 1.0, 1.0, 0.4497626564464351, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21088976693167608, 0.21088976693167627, 0.28456574142462104], 
reward next is 0.7154, 
noisyNet noise sample is [array([-0.27895296], dtype=float32), -0.047800645]. 
=============================================
[2019-04-23 09:40:55,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9239902e-36 1.0000000e+00 0.0000000e+00 1.3428681e-22 0.0000000e+00], sum to 1.0000
[2019-04-23 09:40:55,926] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2686
[2019-04-23 09:40:55,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2017103.533751438 W.
[2019-04-23 09:40:55,941] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.8014505591823877, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999001516798011, 6.9112, 168.9124350512162, 2017103.533751438, 1954814.30813656, 408461.6664497059], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2292000.0000, 
sim time next is 2292600.0000, 
raw observation next is [31.91666666666666, 63.83333333333334, 1.0, 2.0, 0.4914297843066927, 1.0, 1.0, 0.4914297843066927, 1.0, 2.0, 0.8534510170276295, 6.9112, 6.9112, 170.5573041426782, 2061392.820521601, 2061392.820521601, 409269.7810848841], 
processed observation next is [1.0, 0.5217391304347826, 0.7116903633491308, 0.6383333333333334, 1.0, 1.0, 0.3872648003695093, 1.0, 0.5, 0.3872648003695093, 1.0, 1.0, 0.821281728082475, 0.0, 0.0, 0.8375144448122397, 0.5726091168115558, 0.5726091168115558, 0.6108504195296777], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49060172], dtype=float32), 0.5159018]. 
=============================================
[2019-04-23 09:40:57,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.22775895e-35 1.00000000e+00 0.00000000e+00 1.27055085e-26
 0.00000000e+00], sum to 1.0000
[2019-04-23 09:40:57,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6050
[2019-04-23 09:40:57,589] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.28333333333333, 64.66666666666667, 1.0, 2.0, 0.2601591305303317, 1.0, 2.0, 0.2601591305303317, 1.0, 2.0, 0.4518103737920136, 6.911199999999999, 6.9112, 170.5573041426782, 1090791.753957912, 1090791.753957913, 288997.9938681801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2308200.0000, 
sim time next is 2308800.0000, 
raw observation next is [32.16666666666667, 65.33333333333334, 1.0, 2.0, 0.5320381759676455, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104249, 743456.355688527, 743456.3556885277, 188771.4594889885], 
processed observation next is [1.0, 0.7391304347826086, 0.7235387045813588, 0.6533333333333334, 1.0, 1.0, 0.4361905734549945, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522727, 0.20651565435792416, 0.20651565435792435, 0.28174844699849033], 
reward next is 0.7183, 
noisyNet noise sample is [array([1.6826578], dtype=float32), 0.6405528]. 
=============================================
[2019-04-23 09:40:58,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:58,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6488
[2019-04-23 09:40:58,223] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 77.66666666666667, 1.0, 2.0, 0.5745063385012651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802822.6916874194, 802822.6916874194, 196092.6855707108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2320800.0000, 
sim time next is 2321400.0000, 
raw observation next is [29.78333333333333, 77.83333333333333, 1.0, 2.0, 0.572678841892785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800267.9619169089, 800267.9619169089, 195766.490518366], 
processed observation next is [1.0, 0.8695652173913043, 0.6105845181674565, 0.7783333333333333, 1.0, 1.0, 0.4851552311961264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22229665608803023, 0.22229665608803023, 0.2921887918184567], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.51159537], dtype=float32), -0.4811942]. 
=============================================
[2019-04-23 09:41:03,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1026521e-19 1.0000000e+00 6.2999669e-25 2.3977091e-11 7.0697098e-20], sum to 1.0000
[2019-04-23 09:41:03,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-23 09:41:03,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2729489.594059058 W.
[2019-04-23 09:41:03,255] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.06666666666667, 61.16666666666666, 1.0, 2.0, 0.6598633515895871, 1.0, 2.0, 0.6505217153090561, 1.0, 2.0, 1.03, 7.005094567632042, 6.9112, 170.5573041426782, 2729489.594059058, 2662229.095877411, 508950.4596790414], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [33.1, 61.0, 1.0, 2.0, 0.9550466305432843, 1.0, 2.0, 0.9550466305432843, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2671424.351617706, 2671424.351617706, 502413.6521837124], 
processed observation next is [1.0, 0.6956521739130435, 0.7677725118483413, 0.61, 1.0, 1.0, 0.9458393139075715, 1.0, 1.0, 0.9458393139075715, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7420623198938072, 0.7420623198938072, 0.7498711226622573], 
reward next is 0.2501, 
noisyNet noise sample is [array([0.8275662], dtype=float32), 0.6242577]. 
=============================================
[2019-04-23 09:41:03,378] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63775: loss 0.0706
[2019-04-23 09:41:03,380] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63776: learning rate 0.0005
[2019-04-23 09:41:03,606] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63861: loss 0.0439
[2019-04-23 09:41:03,611] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63861: learning rate 0.0005
[2019-04-23 09:41:03,644] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63871: loss 0.3303
[2019-04-23 09:41:03,647] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63873: learning rate 0.0005
[2019-04-23 09:41:03,802] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63933: loss 0.1098
[2019-04-23 09:41:03,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63933: learning rate 0.0005
[2019-04-23 09:41:03,900] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63969: loss 0.3040
[2019-04-23 09:41:03,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63969: learning rate 0.0005
[2019-04-23 09:41:03,932] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63979: loss 0.2609
[2019-04-23 09:41:03,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63979: learning rate 0.0005
[2019-04-23 09:41:03,949] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63985: loss 0.0279
[2019-04-23 09:41:03,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63990: learning rate 0.0005
[2019-04-23 09:41:03,983] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64000: loss 0.0456
[2019-04-23 09:41:03,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64000: learning rate 0.0005
[2019-04-23 09:41:03,999] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64005: loss 0.5877
[2019-04-23 09:41:04,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64006: learning rate 0.0005
[2019-04-23 09:41:04,018] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64011: loss 0.4100
[2019-04-23 09:41:04,022] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64011: learning rate 0.0005
[2019-04-23 09:41:04,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64025: loss 0.1572
[2019-04-23 09:41:04,070] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64026: learning rate 0.0005
[2019-04-23 09:41:04,083] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64031: loss 0.2660
[2019-04-23 09:41:04,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64031: learning rate 0.0005
[2019-04-23 09:41:04,195] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64072: loss 0.2738
[2019-04-23 09:41:04,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64074: learning rate 0.0005
[2019-04-23 09:41:04,229] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64089: loss 0.1642
[2019-04-23 09:41:04,235] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64089: learning rate 0.0005
[2019-04-23 09:41:04,260] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64099: loss 0.0398
[2019-04-23 09:41:04,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64099: learning rate 0.0005
[2019-04-23 09:41:04,352] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64130: loss 0.0168
[2019-04-23 09:41:04,358] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64132: learning rate 0.0005
[2019-04-23 09:41:08,466] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:41:08,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8700
[2019-04-23 09:41:08,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1698504.129978935 W.
[2019-04-23 09:41:08,485] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.404986740620096, 1.0, 1.0, 0.404986740620096, 1.0, 2.0, 0.6852729263455949, 6.9112, 6.9112, 170.5573041426782, 1698504.129978935, 1698504.129978935, 353055.3729815353], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [26.05, 89.0, 1.0, 2.0, 0.5770410844151131, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9741224474753358, 6.911200000000001, 6.9112, 168.9129564997117, 1613344.885351168, 1613344.885351167, 347033.1204681488], 
processed observation next is [1.0, 0.5217391304347826, 0.43364928909952616, 0.89, 1.0, 1.0, 0.49041094507844957, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.9684420091162632, 8.881784197001253e-17, 0.0, 0.829439945099666, 0.4481513570419911, 0.4481513570419908, 0.5179598812957444], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5155593], dtype=float32), 0.8650728]. 
=============================================
[2019-04-23 09:41:12,101] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:41:12,111] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6081
[2019-04-23 09:41:12,115] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 95.0, 1.0, 2.0, 0.5464744912966144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763636.5397815894, 763636.53978159, 191197.9855881855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [26.43333333333333, 95.0, 1.0, 2.0, 0.5453855525236866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762114.3246316728, 762114.3246316728, 191012.578711956], 
processed observation next is [1.0, 0.043478260869565216, 0.4518167456556081, 0.95, 1.0, 1.0, 0.45227175002853803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.211698423508798, 0.211698423508798, 0.28509340106262093], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.89268357], dtype=float32), 1.3439715]. 
=============================================
[2019-04-23 09:41:24,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:41:24,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2399
[2019-04-23 09:41:24,429] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.442320514599603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635328.231742431, 635328.231742431, 177274.088662323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2686800.0000, 
sim time next is 2687400.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4420703068049817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634967.3739069798, 634967.3739069805, 177237.9213649489], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.32779555036744784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17637982608527217, 0.17637982608527236, 0.26453421099246105], 
reward next is 0.7355, 
noisyNet noise sample is [array([-1.5812435], dtype=float32), -1.274261]. 
=============================================
[2019-04-23 09:41:24,504] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71704: loss 0.0081
[2019-04-23 09:41:24,505] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71704: learning rate 0.0005
[2019-04-23 09:41:24,829] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71831: loss 0.0106
[2019-04-23 09:41:24,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71832: learning rate 0.0005
[2019-04-23 09:41:24,958] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71880: loss 0.0344
[2019-04-23 09:41:24,960] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71880: learning rate 0.0005
[2019-04-23 09:41:25,026] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71906: loss 0.0391
[2019-04-23 09:41:25,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71906: learning rate 0.0005
[2019-04-23 09:41:25,044] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71911: loss 0.0362
[2019-04-23 09:41:25,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71914: learning rate 0.0005
[2019-04-23 09:41:25,237] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71980: loss 0.0892
[2019-04-23 09:41:25,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71982: learning rate 0.0005
[2019-04-23 09:41:25,303] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72009: loss 0.1123
[2019-04-23 09:41:25,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72011: learning rate 0.0005
[2019-04-23 09:41:25,315] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72014: loss 0.0367
[2019-04-23 09:41:25,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72014: learning rate 0.0005
[2019-04-23 09:41:25,328] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72017: loss 0.0789
[2019-04-23 09:41:25,329] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72018: loss 0.0540
[2019-04-23 09:41:25,331] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72018: learning rate 0.0005
[2019-04-23 09:41:25,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72018: learning rate 0.0005
[2019-04-23 09:41:25,386] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72037: loss 0.1086
[2019-04-23 09:41:25,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72037: learning rate 0.0005
[2019-04-23 09:41:25,435] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72055: loss 0.0352
[2019-04-23 09:41:25,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72056: learning rate 0.0005
[2019-04-23 09:41:25,511] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72087: loss 0.0092
[2019-04-23 09:41:25,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72087: learning rate 0.0005
[2019-04-23 09:41:25,563] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 72104: loss 0.0035
[2019-04-23 09:41:25,565] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 72104: learning rate 0.0005
[2019-04-23 09:41:25,580] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72109: loss 0.0043
[2019-04-23 09:41:25,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72109: learning rate 0.0005
[2019-04-23 09:41:25,607] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72116: loss 0.0047
[2019-04-23 09:41:25,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72117: learning rate 0.0005
[2019-04-23 09:41:33,264] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-23 09:41:33,266] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:41:33,266] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:41:33,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:41:33,269] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,270] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,270] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:41:33,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:41:33,275] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,276] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,293] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,293] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,311] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,374] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run4
[2019-04-23 09:41:42,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:41:42,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 74.33333333333333, 1.0, 2.0, 0.3197170662374055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506777.1611194508, 506777.1611194514, 167556.1229432241]
[2019-04-23 09:41:42,195] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:41:42,199] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1587251912027856
[2019-04-23 09:41:50,325] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:41:50,326] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.68614035666667, 98.58740946333333, 1.0, 2.0, 0.3452096814513745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551808.4233626828, 551808.4233626828, 171115.3912764342]
[2019-04-23 09:41:50,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:41:50,331] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.738325315836886
[2019-04-23 09:41:50,458] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:41:50,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.65, 96.5, 1.0, 2.0, 0.5395014637421485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832332.804350214, 832332.804350214, 199398.5118316869]
[2019-04-23 09:41:50,462] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:41:50,467] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18373290179087765
[2019-04-23 09:42:24,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:42:24,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.65449890833333, 98.47447721666666, 1.0, 2.0, 0.4484980531466788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700949.915453486, 700949.9154534866, 184682.2181351544]
[2019-04-23 09:42:24,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:42:24,300] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1429759035276238
[2019-04-23 09:43:41,401] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:43:41,660] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:43:41,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:43:41,803] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:43:41,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:43:42,864] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 75000, evaluation results [75000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:43:55,010] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79706: loss 2.4005
[2019-04-23 09:43:55,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79706: learning rate 0.0005
[2019-04-23 09:43:55,407] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79829: loss 3.1662
[2019-04-23 09:43:55,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79830: learning rate 0.0005
[2019-04-23 09:43:55,533] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79852: loss 2.7859
[2019-04-23 09:43:55,536] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79852: learning rate 0.0005
[2019-04-23 09:43:55,704] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79890: loss 3.2891
[2019-04-23 09:43:55,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79892: learning rate 0.0005
[2019-04-23 09:43:55,875] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79939: loss 3.4709
[2019-04-23 09:43:55,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79939: learning rate 0.0005
[2019-04-23 09:43:55,984] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79950: loss 3.2159
[2019-04-23 09:43:55,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79950: learning rate 0.0005
[2019-04-23 09:43:56,194] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80011: loss 3.0790
[2019-04-23 09:43:56,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80012: learning rate 0.0005
[2019-04-23 09:43:56,294] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80022: loss 2.8649
[2019-04-23 09:43:56,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80023: learning rate 0.0005
[2019-04-23 09:43:56,386] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80033: loss 2.5416
[2019-04-23 09:43:56,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80033: learning rate 0.0005
[2019-04-23 09:43:56,473] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80040: loss 2.6604
[2019-04-23 09:43:56,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80040: learning rate 0.0005
[2019-04-23 09:43:56,581] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80058: loss 2.4846
[2019-04-23 09:43:56,586] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80059: loss 2.2331
[2019-04-23 09:43:56,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80058: learning rate 0.0005
[2019-04-23 09:43:56,667] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80059: learning rate 0.0005
[2019-04-23 09:43:56,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80077: loss 2.2046
[2019-04-23 09:43:56,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80079: learning rate 0.0005
[2019-04-23 09:43:56,841] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80084: loss 2.2280
[2019-04-23 09:43:56,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80085: learning rate 0.0005
[2019-04-23 09:43:56,937] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80094: loss 2.1308
[2019-04-23 09:43:56,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80094: learning rate 0.0005
[2019-04-23 09:43:57,030] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80107: loss 2.4267
[2019-04-23 09:43:57,035] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80107: learning rate 0.0005
[2019-04-23 09:44:00,477] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:00,488] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-23 09:44:00,496] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.5764305647285458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868067.4489496684, 868067.4489496684, 204174.1835280488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.6057840286348392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912274.0170086146, 912274.0170086146, 210012.8193050214], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.5250409983552279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2534094491690596, 0.2534094491690596, 0.31345196911197226], 
reward next is 0.6865, 
noisyNet noise sample is [array([-0.8129751], dtype=float32), -1.8449723]. 
=============================================
[2019-04-23 09:44:00,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:00,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9972
[2019-04-23 09:44:00,859] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.5290479725213855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810072.181546793, 810072.1815467936, 196816.9398456459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.5855272744419193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893171.9750684722, 893171.9750684722, 207240.9124076013], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.97, 1.0, 1.0, 0.500635270411951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24810332640790894, 0.24810332640790894, 0.30931479463821093], 
reward next is 0.6907, 
noisyNet noise sample is [array([-0.7251426], dtype=float32), -0.36925295]. 
=============================================
[2019-04-23 09:44:14,812] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:14,822] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1264
[2019-04-23 09:44:14,829] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.578124993567868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807881.3708210465, 807881.3708210465, 196741.2898296856], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.77, 1.0, 1.0, 0.4917168597203228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22441149189473514, 0.22441149189473514, 0.2936437161637098], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.31103012], dtype=float32), -1.3185787]. 
=============================================
[2019-04-23 09:44:17,331] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87698: loss 0.0229
[2019-04-23 09:44:17,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87698: learning rate 0.0005
[2019-04-23 09:44:17,600] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87795: loss 0.0149
[2019-04-23 09:44:17,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87796: learning rate 0.0005
[2019-04-23 09:44:17,769] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87861: loss 0.0359
[2019-04-23 09:44:17,772] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87862: learning rate 0.0005
[2019-04-23 09:44:17,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87926: loss 0.0287
[2019-04-23 09:44:17,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87928: learning rate 0.0005
[2019-04-23 09:44:18,034] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87958: loss 0.0212
[2019-04-23 09:44:18,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87959: learning rate 0.0005
[2019-04-23 09:44:18,048] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87962: loss 0.0224
[2019-04-23 09:44:18,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87962: learning rate 0.0005
[2019-04-23 09:44:18,088] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87976: loss 0.0145
[2019-04-23 09:44:18,096] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87979: learning rate 0.0005
[2019-04-23 09:44:18,181] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88008: loss 0.0171
[2019-04-23 09:44:18,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88008: learning rate 0.0005
[2019-04-23 09:44:18,218] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88025: loss 0.0213
[2019-04-23 09:44:18,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88025: learning rate 0.0005
[2019-04-23 09:44:18,260] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88039: loss 0.0366
[2019-04-23 09:44:18,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88040: learning rate 0.0005
[2019-04-23 09:44:18,306] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88056: loss 0.0315
[2019-04-23 09:44:18,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88057: learning rate 0.0005
[2019-04-23 09:44:18,334] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88067: loss 0.0574
[2019-04-23 09:44:18,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88068: learning rate 0.0005
[2019-04-23 09:44:18,345] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88070: loss 0.0648
[2019-04-23 09:44:18,347] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88070: learning rate 0.0005
[2019-04-23 09:44:18,421] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88099: loss 0.0701
[2019-04-23 09:44:18,423] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88099: learning rate 0.0005
[2019-04-23 09:44:18,501] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88129: loss 0.0499
[2019-04-23 09:44:18,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88130: learning rate 0.0005
[2019-04-23 09:44:18,549] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88145: loss 0.0327
[2019-04-23 09:44:18,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88146: learning rate 0.0005
[2019-04-23 09:44:20,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:20,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-23 09:44:20,391] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 77.0, 1.0, 2.0, 0.5900030291869829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824486.3753322603, 824486.3753322603, 198898.6271320656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [30.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5886233678286997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822557.652259809, 822557.652259809, 198645.9138852008], 
processed observation next is [0.0, 0.6956521739130435, 0.6366508688783573, 0.7766666666666667, 1.0, 1.0, 0.504365503408072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22848823673883584, 0.22848823673883584, 0.2964864386346281], 
reward next is 0.7035, 
noisyNet noise sample is [array([0.15837975], dtype=float32), -0.369176]. 
=============================================
[2019-04-23 09:44:22,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:22,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5701
[2019-04-23 09:44:22,166] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5375003237314502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751091.7178992851, 751091.7178992851, 189680.1092237539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3367800.0000, 
sim time next is 3368400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5387947595640994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752901.1765240583, 752901.176524059, 189897.467371552], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44433103561939685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091392157011273, 0.2091392157011275, 0.2834290557784358], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.6094721], dtype=float32), -0.8732687]. 
=============================================
[2019-04-23 09:44:31,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:31,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8146
[2019-04-23 09:44:31,053] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 79.0, 1.0, 2.0, 0.7490208551332743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046811.75168703, 1046811.75168703, 231779.4291621183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.7951577808035653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1111325.240513955, 1111325.240513955, 242723.3068325832], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.79, 1.0, 1.0, 0.7532021455464641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30870145569832086, 0.30870145569832086, 0.3622735922874376], 
reward next is 0.6377, 
noisyNet noise sample is [array([0.4042055], dtype=float32), 0.13292319]. 
=============================================
[2019-04-23 09:44:36,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:36,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3711
[2019-04-23 09:44:36,394] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7268589847660603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015824.093397069, 1015824.093397069, 226746.1007951709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3553200.0000, 
sim time next is 3553800.0000, 
raw observation next is [26.91666666666666, 79.33333333333334, 1.0, 2.0, 0.7438852858019134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039630.902383026, 1039630.902383027, 230599.472486983], 
processed observation next is [1.0, 0.13043478260869565, 0.4747235387045811, 0.7933333333333334, 1.0, 1.0, 0.6914280551830282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28878636177306277, 0.28878636177306305, 0.3441783171447507], 
reward next is 0.6558, 
noisyNet noise sample is [array([-0.14580765], dtype=float32), 0.079631045]. 
=============================================
[2019-04-23 09:44:38,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:38,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1340
[2019-04-23 09:44:38,353] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2252596.248552679 W.
[2019-04-23 09:44:38,363] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 0.5369656516698038, 1.0, 1.0, 0.5369656516698038, 1.0, 2.0, 0.9325317596959024, 6.911199999999999, 6.9112, 170.5573041426782, 2252596.248552679, 2252596.248552679, 441649.8140312906], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.7854771491153139, 1.0, 2.0, 0.7854771491153139, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2196691.051231064, 2196691.051231063, 412865.8897629573], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.7415387338738721, 1.0, 1.0, 0.7415387338738721, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6101919586752955, 0.6101919586752953, 0.6162177459148617], 
reward next is 0.3838, 
noisyNet noise sample is [array([0.20610292], dtype=float32), -1.2093532]. 
=============================================
[2019-04-23 09:44:38,376] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[45.928867]
 [47.219994]
 [47.6591  ]
 [47.28787 ]
 [48.542656]], R is [[46.90467072]
 [46.43562317]
 [45.9712677 ]
 [45.8938446 ]
 [45.83606339]].
[2019-04-23 09:44:38,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:38,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6481
[2019-04-23 09:44:38,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2299437.998989636 W.
[2019-04-23 09:44:38,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 1.003169263889139, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000558170183028, 6.9112, 168.9123545011351, 2299437.998989636, 2236044.463527616, 464487.5258107574], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5274655429183829, 1.0, 1.0, 0.5274655429183829, 1.0, 2.0, 0.9160332125286569, 6.911200000000001, 6.9112, 170.5573041426782, 2212707.564343199, 2212707.564343198, 434658.7969856866], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.43068137701009984, 1.0, 0.5, 0.43068137701009984, 1.0, 1.0, 0.8976014786934838, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6146409900953331, 0.6146409900953328, 0.648744473112965], 
reward next is 0.3513, 
noisyNet noise sample is [array([-0.08388752], dtype=float32), -0.8226827]. 
=============================================
[2019-04-23 09:44:38,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[48.41023 ]
 [48.12054 ]
 [47.940205]
 [47.54949 ]
 [47.458   ]], R is [[47.31481552]
 [46.84166718]
 [46.37324905]
 [45.90951538]
 [45.45042038]].
[2019-04-23 09:44:38,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95743: loss -49.1243
[2019-04-23 09:44:38,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95743: learning rate 0.0005
[2019-04-23 09:44:39,061] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95814: loss 53.6367
[2019-04-23 09:44:39,062] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95814: learning rate 0.0005
[2019-04-23 09:44:39,113] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95832: loss -248.7453
[2019-04-23 09:44:39,114] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95832: learning rate 0.0005
[2019-04-23 09:44:39,338] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95915: loss -172.8149
[2019-04-23 09:44:39,343] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95916: learning rate 0.0005
[2019-04-23 09:44:39,386] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95933: loss -271.7799
[2019-04-23 09:44:39,390] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95933: learning rate 0.0005
[2019-04-23 09:44:39,432] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95951: loss -208.0046
[2019-04-23 09:44:39,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95951: learning rate 0.0005
[2019-04-23 09:44:39,523] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95982: loss -179.4750
[2019-04-23 09:44:39,526] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95982: learning rate 0.0005
[2019-04-23 09:44:39,630] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96027: loss -58.7132
[2019-04-23 09:44:39,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96027: learning rate 0.0005
[2019-04-23 09:44:39,650] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96036: loss 16.8237
[2019-04-23 09:44:39,652] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96036: learning rate 0.0005
[2019-04-23 09:44:39,661] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96037: loss -28.1817
[2019-04-23 09:44:39,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96038: learning rate 0.0005
[2019-04-23 09:44:39,714] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96059: loss 27.8904
[2019-04-23 09:44:39,715] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96059: learning rate 0.0005
[2019-04-23 09:44:39,729] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96066: loss -3.1959
[2019-04-23 09:44:39,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96066: learning rate 0.0005
[2019-04-23 09:44:39,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96071: loss -31.8060
[2019-04-23 09:44:39,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96072: learning rate 0.0005
[2019-04-23 09:44:39,822] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96102: loss -49.0251
[2019-04-23 09:44:39,825] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96103: learning rate 0.0005
[2019-04-23 09:44:39,910] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96130: loss -61.7807
[2019-04-23 09:44:39,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96131: learning rate 0.0005
[2019-04-23 09:44:40,052] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96184: loss -56.1033
[2019-04-23 09:44:40,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96185: learning rate 0.0005
[2019-04-23 09:44:43,480] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:43,493] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-23 09:44:43,503] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1986822.479053878 W.
[2019-04-23 09:44:43,508] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.4736689112166798, 1.0, 1.0, 0.4736689112166798, 1.0, 2.0, 0.8150314036204213, 6.9112, 6.9112, 170.5573041426782, 1986822.479053878, 1986822.479053878, 396124.4567329639], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3661800.0000, 
sim time next is 3662400.0000, 
raw observation next is [29.66666666666667, 72.66666666666667, 1.0, 2.0, 0.8249122871374834, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990288177907447, 6.9112, 168.9124860303226, 2049939.696320287, 1993831.977187275, 414671.0099807826], 
processed observation next is [1.0, 0.391304347826087, 0.6050552922590839, 0.7266666666666667, 1.0, 1.0, 0.7890509483584137, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00790881779074466, 0.0, 0.8294376348795838, 0.5694276934223019, 0.5538422158853542, 0.6189119551951979], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.50519514], dtype=float32), -0.7972966]. 
=============================================
[2019-04-23 09:44:44,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:44,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5647
[2019-04-23 09:44:44,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2178067.943854704 W.
[2019-04-23 09:44:44,168] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.7788247945120497, 1.0, 2.0, 0.7788247945120497, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2178067.943854704, 2178067.943854704, 409688.6080548358], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3661200.0000, 
sim time next is 3661800.0000, 
raw observation next is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.7057778701679969, 1.0, 2.0, 0.7057778701679969, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1973596.122498524, 1973596.122498524, 376605.3471016664], 
processed observation next is [1.0, 0.391304347826087, 0.5892575039494474, 0.7333333333333334, 1.0, 1.0, 0.6455155062265022, 1.0, 1.0, 0.6455155062265022, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5482211451384789, 0.5482211451384789, 0.5620975329875617], 
reward next is 0.4379, 
noisyNet noise sample is [array([-1.4440963], dtype=float32), -0.27285486]. 
=============================================
[2019-04-23 09:44:50,200] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 09:44:50,202] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:44:50,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,203] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:44:50,206] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:44:50,207] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,204] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:44:50,207] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:44:50,208] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,209] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,212] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,230] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,231] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,271] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,288] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,311] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run5
[2019-04-23 09:44:52,536] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:44:52,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 89.5, 1.0, 2.0, 0.4135951609476725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626516.2791282908, 626516.2791282908, 177227.6711084742]
[2019-04-23 09:44:52,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:44:52,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3641793054958158
[2019-04-23 09:45:05,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:45:05,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 63.0, 1.0, 2.0, 0.339616266163696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529191.2975675158, 529191.2975675158, 169109.6313013383]
[2019-04-23 09:45:05,807] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:45:05,814] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22239858031408244
[2019-04-23 09:45:08,802] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:45:08,802] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.60527901666667, 89.04442147833333, 1.0, 2.0, 0.2772225403852945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454207.8395548508, 454207.8395548508, 163713.7701505831]
[2019-04-23 09:45:08,804] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:45:08,808] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6908207535117291
[2019-04-23 09:45:32,647] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:45:32,647] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.68222579333333, 93.26226953666668, 1.0, 2.0, 0.8205785663779307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1146872.924139572, 1146872.924139572, 249031.0281789241]
[2019-04-23 09:45:32,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:45:32,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7200378210256436
[2019-04-23 09:46:57,380] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:46:57,381] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.8069539, 72.064530515, 1.0, 2.0, 0.7473084481801283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044417.360102242, 1044417.360102242, 231388.5616718912]
[2019-04-23 09:46:57,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:46:57,383] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6720242013393194
[2019-04-23 09:46:59,128] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:47:00,295] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:47:00,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:47:00,455] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:47:00,749] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:47:01,767] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 100000, evaluation results [100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:47:01,912] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7918226e-37 0.0000000e+00], sum to 1.0000
[2019-04-23 09:47:01,921] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9914
[2019-04-23 09:47:01,927] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1776223.590963016 W.
[2019-04-23 09:47:01,937] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 68.83333333333334, 1.0, 2.0, 0.6352487977747312, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.96240978740479, 6.9112, 168.9126303600648, 1776223.590963016, 1739893.67542198, 372353.1063976838], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3748200.0000, 
sim time next is 3748800.0000, 
raw observation next is [29.66666666666667, 67.66666666666667, 1.0, 2.0, 0.4630493226714034, 1.0, 1.0, 0.4630493226714034, 1.0, 2.0, 0.7865663872897917, 6.911199999999999, 6.9112, 170.5573041426782, 1942237.829281321, 1942237.829281322, 387577.6294630373], 
processed observation next is [1.0, 0.391304347826087, 0.6050552922590839, 0.6766666666666667, 1.0, 1.0, 0.35307147309807635, 1.0, 0.5, 0.35307147309807635, 1.0, 1.0, 0.7397151064509655, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5395105081337003, 0.5395105081337005, 0.5784740738254288], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48943597], dtype=float32), 1.7666098]. 
=============================================
[2019-04-23 09:47:05,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:05,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9708
[2019-04-23 09:47:05,092] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5199642814043567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726578.8381689326, 726578.8381689332, 186783.0717770881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5197819404245921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726323.954523323, 726323.9545233224, 186753.4403347651], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4214240246079423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20175665403425638, 0.20175665403425622, 0.2787364781115897], 
reward next is 0.7213, 
noisyNet noise sample is [array([0.02207906], dtype=float32), 0.25232762]. 
=============================================
[2019-04-23 09:47:05,928] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:05,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2104
[2019-04-23 09:47:05,948] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5184873768171205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724514.3625625372, 724514.3625625379, 186543.3496807762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3800400.0000, 
sim time next is 3801000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.518497269532667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724528.1909779884, 724528.1909779878, 186544.9532383817], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41987622835261074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20125783082721901, 0.20125783082721885, 0.27842530334086824], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.80104715], dtype=float32), 0.005227282]. 
=============================================
[2019-04-23 09:47:05,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.65469 ]
 [75.6099  ]
 [75.58675 ]
 [75.58776 ]
 [75.602905]], R is [[75.63366699]
 [75.5989151 ]
 [75.56423187]
 [75.52966309]
 [75.49568176]].
[2019-04-23 09:47:07,568] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:07,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-23 09:47:07,585] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5358341797314213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748762.6625055039, 748762.6625055044, 189401.0652399772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3822600.0000, 
sim time next is 3823200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5357262742093543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748611.8246158515, 748611.8246158515, 189383.0216842303], 
processed observation next is [0.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.440634065312475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20794772905995876, 0.20794772905995876, 0.2826612263943736], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.52428716], dtype=float32), -0.40398607]. 
=============================================
[2019-04-23 09:47:08,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:08,877] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3215
[2019-04-23 09:47:08,885] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.5, 61.0, 1.0, 2.0, 0.616858181621116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862029.7291866334, 862029.7291866334, 203929.5574139173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3848400.0000, 
sim time next is 3849000.0000, 
raw observation next is [34.58333333333334, 60.83333333333334, 1.0, 2.0, 0.6432020043697867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898859.5364505397, 898859.5364505397, 209070.0666631784], 
processed observation next is [0.0, 0.5652173913043478, 0.8380726698262247, 0.6083333333333334, 1.0, 1.0, 0.5701228968310683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24968320456959436, 0.24968320456959436, 0.3120448756166842], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.79209435], dtype=float32), -0.8459704]. 
=============================================
[2019-04-23 09:47:08,895] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.916695]
 [72.81635 ]
 [72.81455 ]
 [72.79055 ]
 [72.76064 ]], R is [[72.64749146]
 [72.61664581]
 [72.58614349]
 [72.55599213]
 [72.52616119]].
[2019-04-23 09:47:11,559] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103671: loss 0.6940
[2019-04-23 09:47:11,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103671: learning rate 0.0005
[2019-04-23 09:47:11,842] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103777: loss 0.3664
[2019-04-23 09:47:11,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103777: learning rate 0.0005
[2019-04-23 09:47:11,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103792: loss 0.4071
[2019-04-23 09:47:11,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103792: learning rate 0.0005
[2019-04-23 09:47:12,278] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103945: loss 0.6708
[2019-04-23 09:47:12,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103947: learning rate 0.0005
[2019-04-23 09:47:12,294] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103951: loss 0.6271
[2019-04-23 09:47:12,296] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103951: learning rate 0.0005
[2019-04-23 09:47:12,309] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103954: loss 0.6396
[2019-04-23 09:47:12,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103954: learning rate 0.0005
[2019-04-23 09:47:12,342] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103964: loss 0.7573
[2019-04-23 09:47:12,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103964: learning rate 0.0005
[2019-04-23 09:47:12,473] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104016: loss 0.9235
[2019-04-23 09:47:12,477] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104016: learning rate 0.0005
[2019-04-23 09:47:12,519] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104031: loss 0.9624
[2019-04-23 09:47:12,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104033: learning rate 0.0005
[2019-04-23 09:47:12,603] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104059: loss 0.8820
[2019-04-23 09:47:12,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104060: learning rate 0.0005
[2019-04-23 09:47:12,630] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104071: loss 1.0815
[2019-04-23 09:47:12,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104071: learning rate 0.0005
[2019-04-23 09:47:12,666] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104086: loss 1.1332
[2019-04-23 09:47:12,666] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104086: learning rate 0.0005
[2019-04-23 09:47:12,684] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104092: loss 1.0705
[2019-04-23 09:47:12,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104093: learning rate 0.0005
[2019-04-23 09:47:12,700] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104096: loss 1.1379
[2019-04-23 09:47:12,703] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104097: learning rate 0.0005
[2019-04-23 09:47:12,719] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104106: loss 1.0261
[2019-04-23 09:47:12,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104106: learning rate 0.0005
[2019-04-23 09:47:13,007] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104208: loss 0.5020
[2019-04-23 09:47:13,008] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104208: learning rate 0.0005
[2019-04-23 09:47:17,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:17,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-23 09:47:17,259] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.610584911719432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853259.6139355367, 853259.6139355374, 202733.9930727872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3964200.0000, 
sim time next is 3964800.0000, 
raw observation next is [31.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6046563939403528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844971.5333202941, 844971.5333202948, 201616.6447879491], 
processed observation next is [0.0, 0.9130434782608695, 0.6840442338072673, 0.7366666666666667, 1.0, 1.0, 0.5236824023377744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23471431481119281, 0.234714314811193, 0.30092036535514793], 
reward next is 0.6991, 
noisyNet noise sample is [array([-1.6957014], dtype=float32), -0.118513666]. 
=============================================
[2019-04-23 09:47:24,319] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:24,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7392
[2019-04-23 09:47:24,448] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 85.5, 1.0, 2.0, 0.5400614900624227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754671.9095572466, 754671.909557246, 190111.1631032673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [27.66666666666666, 85.66666666666666, 1.0, 2.0, 0.5398106426584461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754321.2555846643, 754321.255584665, 190068.885851162], 
processed observation next is [1.0, 0.043478260869565216, 0.5102685624012636, 0.8566666666666666, 1.0, 1.0, 0.44555499115475433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2095336821068512, 0.2095336821068514, 0.28368490425546566], 
reward next is 0.7163, 
noisyNet noise sample is [array([1.6167109], dtype=float32), 0.9208033]. 
=============================================
[2019-04-23 09:47:26,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:26,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4715
[2019-04-23 09:47:26,341] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.735177323856685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027455.038609861, 1027455.038609861, 228622.2823998151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4089600.0000, 
sim time next is 4090200.0000, 
raw observation next is [28.33333333333334, 83.16666666666667, 1.0, 2.0, 0.7867123272419863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1099515.62842214, 1099515.62842214, 240676.6542889805], 
processed observation next is [1.0, 0.34782608695652173, 0.5418641390205374, 0.8316666666666667, 1.0, 1.0, 0.7430269002915497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30542100789503884, 0.30542100789503884, 0.35921888699847837], 
reward next is 0.6408, 
noisyNet noise sample is [array([1.0439641], dtype=float32), -1.1602951]. 
=============================================
[2019-04-23 09:47:30,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:30,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4057
[2019-04-23 09:47:30,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 85.66666666666667, 1.0, 2.0, 0.9084117563382776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1269705.450704207, 1269705.450704207, 272265.2237851233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [28.5, 86.5, 1.0, 2.0, 0.9235018350140526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1290809.980066686, 1290809.980066686, 276487.330545371], 
processed observation next is [1.0, 0.13043478260869565, 0.5497630331753555, 0.865, 1.0, 1.0, 0.9078335361615091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35855832779630165, 0.35855832779630165, 0.41266765753040446], 
reward next is 0.5873, 
noisyNet noise sample is [array([-0.40806472], dtype=float32), -1.3773901]. 
=============================================
[2019-04-23 09:47:33,095] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111705: loss -145.7855
[2019-04-23 09:47:33,096] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111705: learning rate 0.0005
[2019-04-23 09:47:33,139] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111722: loss -108.6601
[2019-04-23 09:47:33,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111722: learning rate 0.0005
[2019-04-23 09:47:33,403] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111817: loss -93.5915
[2019-04-23 09:47:33,405] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111817: learning rate 0.0005
[2019-04-23 09:47:33,712] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111935: loss -39.5687
[2019-04-23 09:47:33,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111935: learning rate 0.0005
[2019-04-23 09:47:33,743] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111947: loss -62.8649
[2019-04-23 09:47:33,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111947: learning rate 0.0005
[2019-04-23 09:47:33,765] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111955: loss -75.0018
[2019-04-23 09:47:33,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111955: learning rate 0.0005
[2019-04-23 09:47:33,793] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111965: loss -101.8634
[2019-04-23 09:47:33,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111965: learning rate 0.0005
[2019-04-23 09:47:33,811] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111971: loss -11.7853
[2019-04-23 09:47:33,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111972: learning rate 0.0005
[2019-04-23 09:47:33,835] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111978: loss -65.0363
[2019-04-23 09:47:33,837] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111978: learning rate 0.0005
[2019-04-23 09:47:34,142] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112092: loss -6.3219
[2019-04-23 09:47:34,143] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112092: learning rate 0.0005
[2019-04-23 09:47:34,148] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112092: loss -44.9352
[2019-04-23 09:47:34,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112092: learning rate 0.0005
[2019-04-23 09:47:34,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112112: loss -14.6313
[2019-04-23 09:47:34,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112112: learning rate 0.0005
[2019-04-23 09:47:34,219] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112119: loss -74.7644
[2019-04-23 09:47:34,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112119: learning rate 0.0005
[2019-04-23 09:47:34,270] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112135: loss -56.2893
[2019-04-23 09:47:34,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112136: learning rate 0.0005
[2019-04-23 09:47:34,307] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112149: loss -37.4421
[2019-04-23 09:47:34,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112149: learning rate 0.0005
[2019-04-23 09:47:34,323] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112155: loss -18.5634
[2019-04-23 09:47:34,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112155: learning rate 0.0005
[2019-04-23 09:47:40,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.46369662e-32 1.00000000e+00 0.00000000e+00 1.25472666e-29
 0.00000000e+00], sum to 1.0000
[2019-04-23 09:47:40,741] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3600
[2019-04-23 09:47:40,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3437362.376763312 W.
[2019-04-23 09:47:40,754] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 47.0, 1.0, 2.0, 0.9968155816987803, 1.0, 2.0, 0.8189978303636527, 1.0, 1.0, 1.03, 7.005121147468668, 6.9112, 170.5573041426782, 3437362.376763312, 3370082.838363491, 631491.6616453897], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4290000.0000, 
sim time next is 4290600.0000, 
raw observation next is [38.0, 46.0, 1.0, 2.0, 0.8571112443067193, 1.0, 2.0, 0.7491456616676221, 1.0, 2.0, 1.03, 7.005110123503014, 6.9112, 170.5573041426782, 3143821.34708386, 3076549.705599064, 575546.2923005044], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.46, 1.0, 1.0, 0.8278448726586979, 1.0, 1.0, 0.6977658574308699, 1.0, 1.0, 1.0365853658536586, 0.00939101235030142, 0.0, 0.8375144448122397, 0.8732837075232944, 0.8545971404441844, 0.8590243168664244], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2125844], dtype=float32), 0.69218796]. 
=============================================
[2019-04-23 09:47:42,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:42,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1443
[2019-04-23 09:47:42,826] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6182377847733076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863958.4425649181, 863958.4425649188, 204193.0585556088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4326000.0000, 
sim time next is 4326600.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6179569735819913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863565.8625013917, 863565.8625013924, 204139.2569754155], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5397071970867365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2398794062503866, 0.23987940625038678, 0.30468545817226195], 
reward next is 0.6953, 
noisyNet noise sample is [array([-0.2984685], dtype=float32), 2.4166045]. 
=============================================
[2019-04-23 09:47:43,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:43,026] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8978
[2019-04-23 09:47:43,032] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.6211749877591769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868064.7239830779, 868064.7239830779, 204757.4684565242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6198830505090315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866258.5622388503, 866258.5622388503, 204508.9283890536], 
processed observation next is [1.0, 0.0, 0.6366508688783573, 0.8233333333333335, 1.0, 1.0, 0.5420277716976283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24062737839968062, 0.24062737839968062, 0.30523720655082626], 
reward next is 0.6948, 
noisyNet noise sample is [array([-0.825131], dtype=float32), 1.8142822]. 
=============================================
[2019-04-23 09:47:46,163] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:46,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2192
[2019-04-23 09:47:46,182] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5578264390727593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779505.4286276018, 779505.4286276018, 193153.9909183258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [32.0, 66.33333333333334, 1.0, 2.0, 0.5586634548170861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780675.5023987315, 780675.5023987315, 193299.1968412533], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6633333333333334, 1.0, 1.0, 0.4682692226711881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21685430622186988, 0.21685430622186988, 0.2885062639421691], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.2459883], dtype=float32), 2.6337128]. 
=============================================
[2019-04-23 09:47:54,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:54,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1215
[2019-04-23 09:47:54,084] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5577636451529746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779417.6483492494, 779417.64834925, 193140.5940172905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5571239812961392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778523.4559554188, 778523.4559554193, 193029.4314875403], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.4664144352965532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21625651554317188, 0.21625651554317205, 0.28810362908588105], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.04236376], dtype=float32), 0.20512027]. 
=============================================
[2019-04-23 09:47:54,113] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119579: loss 0.0468
[2019-04-23 09:47:54,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119579: learning rate 0.0005
[2019-04-23 09:47:54,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:54,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9340
[2019-04-23 09:47:54,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5577636451529746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779417.6483492494, 779417.64834925, 193140.5940172905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5571239812961392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778523.4559554188, 778523.4559554193, 193029.4314875403], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.4664144352965532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21625651554317188, 0.21625651554317205, 0.28810362908588105], 
reward next is 0.7119, 
noisyNet noise sample is [array([-1.1175146], dtype=float32), 0.8102009]. 
=============================================
[2019-04-23 09:47:54,449] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119703: loss 0.0939
[2019-04-23 09:47:54,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119703: learning rate 0.0005
[2019-04-23 09:47:54,500] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119724: loss 0.0481
[2019-04-23 09:47:54,501] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119724: learning rate 0.0005
[2019-04-23 09:47:54,911] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119888: loss 0.1079
[2019-04-23 09:47:54,916] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119889: learning rate 0.0005
[2019-04-23 09:47:54,990] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119916: loss 0.0859
[2019-04-23 09:47:54,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119917: learning rate 0.0005
[2019-04-23 09:47:55,051] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119938: loss 0.0427
[2019-04-23 09:47:55,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119938: learning rate 0.0005
[2019-04-23 09:47:55,096] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119953: loss 0.0373
[2019-04-23 09:47:55,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119955: learning rate 0.0005
[2019-04-23 09:47:55,117] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119959: loss 0.0393
[2019-04-23 09:47:55,124] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119961: learning rate 0.0005
[2019-04-23 09:47:55,189] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119989: loss 0.0324
[2019-04-23 09:47:55,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119989: learning rate 0.0005
[2019-04-23 09:47:55,451] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120089: loss 0.0337
[2019-04-23 09:47:55,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120089: learning rate 0.0005
[2019-04-23 09:47:55,604] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120143: loss 0.0617
[2019-04-23 09:47:55,606] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120144: learning rate 0.0005
[2019-04-23 09:47:55,622] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120154: loss 0.0617
[2019-04-23 09:47:55,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120154: learning rate 0.0005
[2019-04-23 09:47:55,651] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120164: loss 0.0504
[2019-04-23 09:47:55,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120164: learning rate 0.0005
[2019-04-23 09:47:55,696] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120180: loss 0.0502
[2019-04-23 09:47:55,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120181: learning rate 0.0005
[2019-04-23 09:47:55,707] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120183: loss 0.0526
[2019-04-23 09:47:55,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120184: learning rate 0.0005
[2019-04-23 09:47:55,859] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120240: loss 0.0608
[2019-04-23 09:47:55,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120241: learning rate 0.0005
[2019-04-23 09:47:57,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:57,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1692
[2019-04-23 09:47:57,289] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.5013865127069486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700610.3909894257, 700610.3909894257, 183813.1069169299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.497565174219769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695268.9118359083, 695268.9118359083, 183214.9644337025], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.74, 1.0, 1.0, 0.39465683640936017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1931302532877523, 0.1931302532877523, 0.2734551707965709], 
reward next is 0.7265, 
noisyNet noise sample is [array([-2.1518946], dtype=float32), 0.5277869]. 
=============================================
[2019-04-23 09:47:57,639] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:57,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5687
[2019-04-23 09:47:57,658] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.83333333333334, 1.0, 2.0, 0.498187785142159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696139.1975420427, 696139.1975420427, 183312.3870787389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4525800.0000, 
sim time next is 4526400.0000, 
raw observation next is [28.0, 75.66666666666667, 1.0, 2.0, 0.5004519591862049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699304.0666094151, 699304.0666094158, 183666.8470487834], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7566666666666667, 1.0, 1.0, 0.398134890585789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19425112961372643, 0.19425112961372662, 0.27412962246087075], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.22683795], dtype=float32), -0.6749651]. 
=============================================
[2019-04-23 09:48:08,616] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-23 09:48:08,617] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:48:08,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:48:08,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:48:08,620] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:48:08,619] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:48:08,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,623] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,623] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,625] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,647] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,647] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,649] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,693] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,694] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-04-23 09:48:11,740] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:11,742] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.840409835, 93.94808416000001, 1.0, 2.0, 0.3000175735058938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486221.2797052862, 486221.2797052862, 166072.9488457477]
[2019-04-23 09:48:11,744] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:48:11,750] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8890620599138706
[2019-04-23 09:48:54,344] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:54,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.40598191, 92.25725217, 1.0, 2.0, 0.4857461252780294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678748.3774420738, 678748.3774420738, 181393.3493274871]
[2019-04-23 09:48:54,346] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:48:54,349] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14799916367403576
[2019-04-23 09:48:55,225] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:55,227] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3395761230537678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523113.9895834465, 523113.9895834465, 168447.6071190568]
[2019-04-23 09:48:55,228] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:48:55,230] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3332244327286761
[2019-04-23 09:48:55,453] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:55,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.440947, 80.47096381, 1.0, 2.0, 0.6378728900277771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891409.1003228223, 891409.1003228223, 208004.8176528347]
[2019-04-23 09:48:55,456] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:48:55,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.02748286723757054
[2019-04-23 09:48:57,476] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:57,478] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.73780723333333, 99.80042975, 1.0, 2.0, 0.2395584700995803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396004.5478011331, 396004.5478011325, 159818.6823886287]
[2019-04-23 09:48:57,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:48:57,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6695715393943125
[2019-04-23 09:49:23,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:49:23,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.3, 51.16666666666667, 1.0, 2.0, 0.6493881457780336, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129565103979, 907508.2157245894, 907508.2157245894, 210303.2241905436]
[2019-04-23 09:49:23,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:49:23,321] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10082124073241483
[2019-04-23 09:49:53,160] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:49:53,161] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.4, 87.0, 1.0, 2.0, 0.6770387758926479, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988562930211798, 6.9112, 168.9124328949123, 1842998.402226777, 1788114.647043369, 380400.3056830283]
[2019-04-23 09:49:53,161] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:49:53,164] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3409199713974237
[2019-04-23 09:49:53,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1842998.402226777 W.
[2019-04-23 09:50:15,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:50:17,096] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:50:17,131] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:50:17,217] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:50:17,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:50:18,342] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 125000, evaluation results [125000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:50:23,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:23,828] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9500
[2019-04-23 09:50:23,835] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4857725200252103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678785.2714381652, 678785.2714381646, 181396.6560870234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4756800.0000, 
sim time next is 4757400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.485108267694878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677856.794554996, 677856.794554996, 181295.4846495466], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.37964851529503374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18829355404305442, 0.18829355404305442, 0.2705902755963382], 
reward next is 0.7294, 
noisyNet noise sample is [array([1.1118612], dtype=float32), -0.8333189]. 
=============================================
[2019-04-23 09:50:25,426] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127659: loss -396.1098
[2019-04-23 09:50:25,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127661: learning rate 0.0005
[2019-04-23 09:50:25,575] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127713: loss -497.2686
[2019-04-23 09:50:25,578] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127713: learning rate 0.0005
[2019-04-23 09:50:25,895] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127836: loss -627.3439
[2019-04-23 09:50:25,899] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127836: learning rate 0.0005
[2019-04-23 09:50:25,907] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127837: loss -517.9501
[2019-04-23 09:50:25,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127840: learning rate 0.0005
[2019-04-23 09:50:26,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:26,165] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127936: loss -498.7962
[2019-04-23 09:50:26,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127936: learning rate 0.0005
[2019-04-23 09:50:26,169] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-23 09:50:26,179] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127941: loss -346.2711
[2019-04-23 09:50:26,180] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2914717.182409708 W.
[2019-04-23 09:50:26,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127941: learning rate 0.0005
[2019-04-23 09:50:26,185] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.793961163515005, 6.9112, 168.9081247747239, 2914717.182409708, 2288473.944845961, 474290.7591136899], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4809600.0000, 
sim time next is 4810200.0000, 
raw observation next is [31.83333333333334, 63.5, 1.0, 2.0, 0.9594760898215843, 1.0, 1.0, 0.9594760898215843, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2683827.594331028, 2683827.594331028, 504974.6078630974], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.635, 1.0, 1.0, 0.9511760118332341, 1.0, 0.5, 0.9511760118332341, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7455076650919522, 0.7455076650919522, 0.7536934445717871], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.52137387], dtype=float32), 0.17942068]. 
=============================================
[2019-04-23 09:50:26,214] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127954: loss -411.7746
[2019-04-23 09:50:26,215] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127954: learning rate 0.0005
[2019-04-23 09:50:26,255] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127969: loss -450.5604
[2019-04-23 09:50:26,260] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127969: learning rate 0.0005
[2019-04-23 09:50:26,303] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127985: loss -482.9890
[2019-04-23 09:50:26,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127985: learning rate 0.0005
[2019-04-23 09:50:26,499] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128058: loss -568.6868
[2019-04-23 09:50:26,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128058: learning rate 0.0005
[2019-04-23 09:50:26,632] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128103: loss -442.2797
[2019-04-23 09:50:26,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128103: learning rate 0.0005
[2019-04-23 09:50:26,687] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128127: loss -472.6254
[2019-04-23 09:50:26,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128127: learning rate 0.0005
[2019-04-23 09:50:26,740] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128146: loss -448.4095
[2019-04-23 09:50:26,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128146: learning rate 0.0005
[2019-04-23 09:50:26,796] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128167: loss -479.4893
[2019-04-23 09:50:26,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128168: learning rate 0.0005
[2019-04-23 09:50:26,800] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128168: loss -497.6752
[2019-04-23 09:50:26,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128169: learning rate 0.0005
[2019-04-23 09:50:26,869] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128191: loss -338.1742
[2019-04-23 09:50:26,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128191: learning rate 0.0005
[2019-04-23 09:50:30,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:30,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-23 09:50:30,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1833375.87399357 W.
[2019-04-23 09:50:30,880] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.6701621294473366, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982901788839088, 6.9112, 168.912529332979, 1833375.87399357, 1782508.285849429, 379148.2280836794], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.4334938045408064, 1.0, 1.0, 0.4334938045408064, 1.0, 2.0, 0.7431128817906862, 6.9112, 6.9112, 170.5573041426782, 1818163.463784503, 1818163.463784503, 370702.6137315314], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.74, 1.0, 1.0, 0.3174624151094053, 1.0, 0.5, 0.3174624151094053, 1.0, 1.0, 0.6867230265740076, 0.0, 0.0, 0.8375144448122397, 0.5050454066068064, 0.5050454066068064, 0.5532874831813902], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.73219526], dtype=float32), 1.7546736]. 
=============================================
[2019-04-23 09:50:34,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:34,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6211
[2019-04-23 09:50:34,177] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5100935444767921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712781.2081546375, 712781.2081546369, 185192.638322905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4922400.0000, 
sim time next is 4923000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5097186140793517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712257.1220534906, 712257.1220534906, 185132.8167083495], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4092995350353635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19784920057041405, 0.19784920057041405, 0.2763176368781336], 
reward next is 0.7237, 
noisyNet noise sample is [array([1.5545969], dtype=float32), 0.010216483]. 
=============================================
[2019-04-23 09:50:34,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.40461]
 [74.37405]
 [74.3565 ]
 [74.26869]
 [74.24147]], R is [[74.42498779]
 [74.40433502]
 [74.3838501 ]
 [74.36361694]
 [74.34370422]].
[2019-04-23 09:50:46,577] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135575: loss 0.8814
[2019-04-23 09:50:46,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135575: learning rate 0.0005
[2019-04-23 09:50:46,785] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 135651: loss 0.8791
[2019-04-23 09:50:46,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 135653: learning rate 0.0005
[2019-04-23 09:50:47,216] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135816: loss 0.8989
[2019-04-23 09:50:47,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135816: learning rate 0.0005
[2019-04-23 09:50:47,228] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 135820: loss 0.8978
[2019-04-23 09:50:47,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 135820: learning rate 0.0005
[2019-04-23 09:50:47,525] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135934: loss 0.8638
[2019-04-23 09:50:47,525] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 135934: loss 0.8633
[2019-04-23 09:50:47,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135935: learning rate 0.0005
[2019-04-23 09:50:47,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 135935: learning rate 0.0005
[2019-04-23 09:50:47,533] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 135935: loss 0.8613
[2019-04-23 09:50:47,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135936: loss 0.8634
[2019-04-23 09:50:47,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 135936: learning rate 0.0005
[2019-04-23 09:50:47,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135936: learning rate 0.0005
[2019-04-23 09:50:47,552] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135940: loss 0.8599
[2019-04-23 09:50:47,557] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135941: learning rate 0.0005
[2019-04-23 09:50:48,095] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136145: loss 0.8344
[2019-04-23 09:50:48,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136145: learning rate 0.0005
[2019-04-23 09:50:48,123] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136157: loss 0.8432
[2019-04-23 09:50:48,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136160: learning rate 0.0005
[2019-04-23 09:50:48,137] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136164: loss 0.8373
[2019-04-23 09:50:48,139] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136164: loss 0.8313
[2019-04-23 09:50:48,142] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136164: learning rate 0.0005
[2019-04-23 09:50:48,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136164: learning rate 0.0005
[2019-04-23 09:50:48,219] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136190: loss 0.8380
[2019-04-23 09:50:48,223] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136190: learning rate 0.0005
[2019-04-23 09:50:48,240] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136196: loss 0.8509
[2019-04-23 09:50:48,248] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136197: learning rate 0.0005
[2019-04-23 09:50:48,408] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136260: loss 0.8301
[2019-04-23 09:50:48,412] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136261: learning rate 0.0005
[2019-04-23 09:50:51,654] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:51,664] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9202
[2019-04-23 09:50:51,670] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 64.0, 1.0, 2.0, 0.5497218410204334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768175.986051796, 768175.9860517967, 191753.2879752476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5156400.0000, 
sim time next is 5157000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.5469941449358731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764362.9584741134, 764362.9584741141, 191286.8474243076], 
processed observation next is [0.0, 0.6956521739130435, 0.6919431279620853, 0.645, 1.0, 1.0, 0.45420981317575065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21232304402058705, 0.21232304402058724, 0.28550275734971287], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.6607234], dtype=float32), -0.16415522]. 
=============================================
[2019-04-23 09:50:51,688] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.4238 ]
 [73.37617]
 [73.3928 ]
 [73.33786]
 [73.33259]], R is [[73.47109985]
 [73.45018768]
 [73.42795563]
 [73.40699768]
 [73.38644409]].
[2019-04-23 09:50:56,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:56,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5375
[2019-04-23 09:50:56,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2755920.275750069 W.
[2019-04-23 09:50:56,427] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.574118423542581, 6.9112, 168.9092531528229, 2755920.275750069, 2285633.441298408, 474692.8131645269], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5223000.0000, 
sim time next is 5223600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.9123623699653448, 1.0, 1.0, 0.9123623699653448, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2551907.439784277, 2551907.439784277, 478280.8072696804], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.8944124939341503, 1.0, 0.5, 0.8944124939341503, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7088631777178548, 0.7088631777178548, 0.7138519511487766], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5878052], dtype=float32), -0.07863935]. 
=============================================
[2019-04-23 09:51:03,139] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:03,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8205
[2019-04-23 09:51:03,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2048326.953944854 W.
[2019-04-23 09:51:03,159] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [36.21666666666667, 52.0, 1.0, 2.0, 0.7324768447454107, 1.0, 1.0, 0.7324768447454107, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2048326.953944854, 2048326.953944854, 388350.3427033167], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5316600.0000, 
sim time next is 5317200.0000, 
raw observation next is [36.2, 52.0, 1.0, 2.0, 0.9864511959997594, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00599259176818, 6.9112, 168.9123930989401, 2276038.160631194, 2208789.257832014, 459168.7210722109], 
processed observation next is [1.0, 0.5652173913043478, 0.9146919431279622, 0.52, 1.0, 1.0, 0.9836761397587462, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009479259176818022, 0.0, 0.8294371785439515, 0.6322328223975539, 0.6135525716200039, 0.6853264493615088], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8792398], dtype=float32), 1.0501719]. 
=============================================
[2019-04-23 09:51:07,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:07,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8371
[2019-04-23 09:51:07,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3154081.112698884 W.
[2019-04-23 09:51:07,783] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.01666666666667, 60.66666666666666, 1.0, 2.0, 0.8619947058759556, 1.0, 2.0, 0.7515873924522406, 1.0, 2.0, 1.03, 7.005110508765996, 6.9112, 170.5573041426782, 3154081.112698884, 3086809.195234542, 577379.5853603571], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5395800.0000, 
sim time next is 5396400.0000, 
raw observation next is [35.2, 60.0, 1.0, 2.0, 0.8731737009514918, 1.0, 2.0, 0.7571768899900087, 1.0, 2.0, 1.03, 7.005111390716066, 6.9112, 170.5573041426782, 3177567.612445308, 3110295.063204255, 581608.2601424089], 
processed observation next is [1.0, 0.4782608695652174, 0.8672985781990523, 0.6, 1.0, 1.0, 0.8471972300620383, 1.0, 1.0, 0.7074420361325405, 1.0, 1.0, 1.0365853658536586, 0.009391139071606602, 0.0, 0.8375144448122397, 0.8826576701236967, 0.8639708508900709, 0.8680720300632968], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29023135], dtype=float32), 0.4321464]. 
=============================================
[2019-04-23 09:51:08,072] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143628: loss -54.6212
[2019-04-23 09:51:08,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143629: learning rate 0.0005
[2019-04-23 09:51:08,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:08,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5586
[2019-04-23 09:51:08,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2345593.846060887 W.
[2019-04-23 09:51:08,104] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.3, 66.66666666666667, 1.0, 2.0, 0.5591132942403283, 1.0, 2.0, 0.5591132942403283, 1.0, 1.0, 0.970994890503583, 6.9112, 6.9112, 170.5573041426782, 2345593.846060887, 2345593.846060887, 458432.5332861157], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5390400.0000, 
sim time next is 5391000.0000, 
raw observation next is [33.5, 66.0, 1.0, 2.0, 0.829842654425213, 1.0, 2.0, 0.829842654425213, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2320882.746340751, 2320882.746340751, 434674.4882491853], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.66, 1.0, 1.0, 0.7949911499098952, 1.0, 1.0, 0.7949911499098952, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6446896517613198, 0.6446896517613198, 0.6487678929092319], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6670949], dtype=float32), 0.9026759]. 
=============================================
[2019-04-23 09:51:08,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[32.0251  ]
 [31.30671 ]
 [28.123363]
 [26.003176]
 [27.44597 ]], R is [[32.26314163]
 [32.25628281]
 [32.27173615]
 [31.94901848]
 [31.62952805]].
[2019-04-23 09:51:08,218] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143681: loss -12.4814
[2019-04-23 09:51:08,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143681: learning rate 0.0005
[2019-04-23 09:51:08,652] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143842: loss -26.5266
[2019-04-23 09:51:08,654] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143842: learning rate 0.0005
[2019-04-23 09:51:08,788] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143892: loss -20.5130
[2019-04-23 09:51:08,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143892: learning rate 0.0005
[2019-04-23 09:51:08,842] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143911: loss -87.6924
[2019-04-23 09:51:08,843] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143911: learning rate 0.0005
[2019-04-23 09:51:08,897] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143931: loss -31.2438
[2019-04-23 09:51:08,901] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143932: learning rate 0.0005
[2019-04-23 09:51:08,917] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 143937: loss -30.2159
[2019-04-23 09:51:08,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 143938: learning rate 0.0005
[2019-04-23 09:51:08,987] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143962: loss -49.0695
[2019-04-23 09:51:08,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143963: learning rate 0.0005
[2019-04-23 09:51:09,062] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 143988: loss -33.5299
[2019-04-23 09:51:09,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 143989: learning rate 0.0005
[2019-04-23 09:51:09,349] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144093: loss -39.4555
[2019-04-23 09:51:09,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144093: learning rate 0.0005
[2019-04-23 09:51:09,398] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144110: loss -45.0792
[2019-04-23 09:51:09,402] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144111: learning rate 0.0005
[2019-04-23 09:51:09,420] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144119: loss -64.6569
[2019-04-23 09:51:09,422] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144119: learning rate 0.0005
[2019-04-23 09:51:09,473] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144138: loss -50.5932
[2019-04-23 09:51:09,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144140: learning rate 0.0005
[2019-04-23 09:51:09,484] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144142: loss -41.0866
[2019-04-23 09:51:09,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144142: learning rate 0.0005
[2019-04-23 09:51:09,568] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144173: loss -62.4672
[2019-04-23 09:51:09,573] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144174: learning rate 0.0005
[2019-04-23 09:51:09,809] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144259: loss -37.7257
[2019-04-23 09:51:09,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144259: learning rate 0.0005
[2019-04-23 09:51:16,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9790066e-32 1.0000000e+00 8.5946384e-37 5.1830958e-27 0.0000000e+00], sum to 1.0000
[2019-04-23 09:51:16,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5067
[2019-04-23 09:51:16,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2837836.848022943 W.
[2019-04-23 09:51:16,208] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.96666666666667, 48.33333333333333, 1.0, 2.0, 1.014472375895918, 1.0, 2.0, 1.014472375895918, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2837836.848022943, 2837836.848022943, 537790.568746211], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5498400.0000, 
sim time next is 5499000.0000, 
raw observation next is [35.85, 48.5, 1.0, 2.0, 0.7228090117642364, 1.0, 2.0, 0.6819945453963807, 1.0, 1.0, 1.03, 7.005099530694256, 6.9112, 170.5573041426782, 2861696.02909468, 2794431.975669685, 528629.5078243153], 
processed observation next is [1.0, 0.6521739130434783, 0.8981042654028437, 0.485, 1.0, 1.0, 0.6660349539328149, 1.0, 1.0, 0.6168608980679285, 1.0, 0.5, 1.0365853658536586, 0.009389953069425605, 0.0, 0.8375144448122397, 0.7949155636374111, 0.7762311043526903, 0.7889992654094259], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1236567], dtype=float32), -0.54269516]. 
=============================================
[2019-04-23 09:51:16,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[23.8885  ]
 [23.16751 ]
 [23.201496]
 [23.543066]
 [23.082035]], R is [[23.19743919]
 [23.16279221]
 [22.93116379]
 [22.7018528 ]
 [22.47483444]].
[2019-04-23 09:51:19,662] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:19,673] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6513
[2019-04-23 09:51:19,681] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 92.5, 1.0, 2.0, 0.701682999488077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 980623.0700098822, 980623.0700098815, 221208.1142939565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5549400.0000, 
sim time next is 5550000.0000, 
raw observation next is [26.33333333333333, 91.66666666666667, 1.0, 2.0, 0.6955827965759316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 972093.9514108831, 972093.9514108831, 219894.0566086252], 
processed observation next is [1.0, 0.21739130434782608, 0.44707740916271704, 0.9166666666666667, 1.0, 1.0, 0.6332322850312428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2700260976141342, 0.2700260976141342, 0.3282000844904854], 
reward next is 0.6718, 
noisyNet noise sample is [array([-1.1355404], dtype=float32), 0.6231233]. 
=============================================
[2019-04-23 09:51:19,698] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[53.471413]
 [53.438675]
 [53.479248]
 [53.611897]
 [53.680355]], R is [[53.81160355]
 [53.94332504]
 [54.07694244]
 [54.19968414]
 [54.34266281]].
[2019-04-23 09:51:25,218] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 09:51:25,219] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:51:25,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,223] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:51:25,225] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:51:25,225] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,226] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,228] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:51:25,227] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:51:25,233] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,234] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,276] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,298] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,299] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,317] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run7
[2019-04-23 09:51:38,505] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:51:38,506] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.973782195, 77.02405769333333, 1.0, 2.0, 0.2760897921279168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 448471.9042329647, 448471.9042329641, 163470.7127273229]
[2019-04-23 09:51:38,506] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:51:38,508] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7973601657671212
[2019-04-23 09:52:04,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:04,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.26666666666667, 96.33333333333334, 1.0, 2.0, 0.7307054591673569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021202.330666356, 1021202.330666356, 227613.4451615379]
[2019-04-23 09:52:04,942] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:04,944] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.45377882558486504
[2019-04-23 09:52:06,432] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:06,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.56666666666666, 90.33333333333333, 1.0, 2.0, 0.444090748929569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636703.5922903854, 636703.5922903861, 177381.4598196742]
[2019-04-23 09:52:06,435] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:06,438] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6291313579806148
[2019-04-23 09:52:21,020] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:21,021] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.46064661166667, 64.69382938333332, 1.0, 2.0, 0.5779327012082538, 0.0, 2.0, 0.0, 1.0, 1.0, 1.003677976733882, 6.911200000000001, 6.9112, 168.9127216761939, 1615839.649447756, 1615839.649447755, 353693.210234365]
[2019-04-23 09:52:21,022] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:52:21,023] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6198113105662647
[2019-04-23 09:52:22,277] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:22,279] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.2, 69.0, 1.0, 2.0, 0.5765971281113492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805745.4966921896, 805745.4966921896, 196466.4421906566]
[2019-04-23 09:52:22,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:52:22,282] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09287748269267637
[2019-04-23 09:52:44,755] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:44,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.56666666666666, 53.0, 1.0, 2.0, 0.5373747842663013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750916.2296672772, 750916.2296672772, 189659.0769790997]
[2019-04-23 09:52:44,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:44,760] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11682841138233069
[2019-04-23 09:52:49,476] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:49,477] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.08333333333333, 77.5, 1.0, 2.0, 0.5459009997419011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762834.8621751659, 762834.8621751665, 191100.422018578]
[2019-04-23 09:52:49,478] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:49,479] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7654681722152197
[2019-04-23 09:53:14,606] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:53:14,607] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.26871077666667, 49.76786628333334, 1.0, 2.0, 0.4215926951721732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609687.5767481984, 609687.5767481984, 174882.2940384457]
[2019-04-23 09:53:14,608] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:53:14,614] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3619530385632047
[2019-04-23 09:53:32,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:53:32,943] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:53:33,034] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:53:33,367] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:53:33,415] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:53:34,430] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 150000, evaluation results [150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:53:38,552] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151550: loss 0.0222
[2019-04-23 09:53:38,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151552: learning rate 0.0005
[2019-04-23 09:53:38,707] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151610: loss 0.0921
[2019-04-23 09:53:38,711] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151611: learning rate 0.0005
[2019-04-23 09:53:39,175] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151780: loss 0.0250
[2019-04-23 09:53:39,177] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151780: learning rate 0.0005
[2019-04-23 09:53:39,250] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 151806: loss 0.0493
[2019-04-23 09:53:39,256] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 151807: learning rate 0.0005
[2019-04-23 09:53:39,433] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151878: loss 0.0013
[2019-04-23 09:53:39,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151878: learning rate 0.0005
[2019-04-23 09:53:39,572] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151926: loss 0.0155
[2019-04-23 09:53:39,578] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151928: learning rate 0.0005
[2019-04-23 09:53:39,590] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 151935: loss 0.0180
[2019-04-23 09:53:39,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 151935: learning rate 0.0005
[2019-04-23 09:53:39,723] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151982: loss 0.0022
[2019-04-23 09:53:39,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151982: learning rate 0.0005
[2019-04-23 09:53:39,902] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152050: loss 0.0146
[2019-04-23 09:53:39,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152051: learning rate 0.0005
[2019-04-23 09:53:40,072] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152112: loss 0.0450
[2019-04-23 09:53:40,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152112: learning rate 0.0005
[2019-04-23 09:53:40,118] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152129: loss 0.0537
[2019-04-23 09:53:40,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152130: learning rate 0.0005
[2019-04-23 09:53:40,163] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152144: loss 0.0245
[2019-04-23 09:53:40,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152144: learning rate 0.0005
[2019-04-23 09:53:40,177] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152147: loss 0.0322
[2019-04-23 09:53:40,179] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152147: learning rate 0.0005
[2019-04-23 09:53:40,281] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152189: loss 0.0152
[2019-04-23 09:53:40,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152189: learning rate 0.0005
[2019-04-23 09:53:40,362] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152221: loss 0.0182
[2019-04-23 09:53:40,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152221: learning rate 0.0005
[2019-04-23 09:53:40,793] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152380: loss 0.0561
[2019-04-23 09:53:40,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152380: learning rate 0.0005
[2019-04-23 09:53:47,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:53:47,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-23 09:53:47,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1670926.500087064 W.
[2019-04-23 09:53:47,503] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.65, 90.0, 1.0, 2.0, 0.5976199588551261, 0.0, 2.0, 0.0, 1.0, 2.0, 1.017765769310614, 6.911200000000001, 6.9112, 168.9128980057304, 1670926.500087064, 1670926.500087064, 361728.8488257495], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5797800.0000, 
sim time next is 5798400.0000, 
raw observation next is [26.6, 90.33333333333334, 1.0, 2.0, 0.5636158608221066, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9596911912790405, 6.9112, 6.9112, 168.9129564958222, 1575781.538551021, 1575781.538551021, 340797.9052790949], 
processed observation next is [1.0, 0.08695652173913043, 0.4597156398104266, 0.9033333333333334, 1.0, 1.0, 0.4742359768941043, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9508429161939518, 0.0, 0.0, 0.8294399450805667, 0.43771709404195025, 0.43771709404195025, 0.5086535899687983], 
reward next is 0.4913, 
noisyNet noise sample is [array([0.71629417], dtype=float32), -1.2518607]. 
=============================================
[2019-04-23 09:54:00,163] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159601: loss 53.3639
[2019-04-23 09:54:00,165] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159602: learning rate 0.0005
[2019-04-23 09:54:00,383] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159682: loss 131.9834
[2019-04-23 09:54:00,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159682: learning rate 0.0005
[2019-04-23 09:54:00,689] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159796: loss 78.5443
[2019-04-23 09:54:00,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159796: learning rate 0.0005
[2019-04-23 09:54:00,774] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159827: loss -82.1083
[2019-04-23 09:54:00,775] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159827: learning rate 0.0005
[2019-04-23 09:54:00,881] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159866: loss 108.5683
[2019-04-23 09:54:00,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159866: learning rate 0.0005
[2019-04-23 09:54:00,998] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159908: loss -17.3382
[2019-04-23 09:54:01,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159908: learning rate 0.0005
[2019-04-23 09:54:01,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159929: loss 167.7493
[2019-04-23 09:54:01,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159929: learning rate 0.0005
[2019-04-23 09:54:01,234] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159995: loss 4.6929
[2019-04-23 09:54:01,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159995: learning rate 0.0005
[2019-04-23 09:54:01,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160000: loss 81.7639
[2019-04-23 09:54:01,248] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160000: learning rate 0.0005
[2019-04-23 09:54:01,417] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160061: loss 83.5877
[2019-04-23 09:54:01,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160062: learning rate 0.0005
[2019-04-23 09:54:01,492] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160089: loss 15.6302
[2019-04-23 09:54:01,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160089: learning rate 0.0005
[2019-04-23 09:54:01,496] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 160090: loss -39.4087
[2019-04-23 09:54:01,499] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 160090: learning rate 0.0005
[2019-04-23 09:54:01,517] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160096: loss -25.4194
[2019-04-23 09:54:01,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160099: learning rate 0.0005
[2019-04-23 09:54:01,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160202: loss -53.9671
[2019-04-23 09:54:01,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160202: learning rate 0.0005
[2019-04-23 09:54:02,003] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160273: loss 7.5500
[2019-04-23 09:54:02,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160273: learning rate 0.0005
[2019-04-23 09:54:02,048] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160289: loss 26.4820
[2019-04-23 09:54:02,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160292: learning rate 0.0005
[2019-04-23 09:54:03,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:03,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8567
[2019-04-23 09:54:03,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333334, 74.0, 1.0, 2.0, 0.5351201013424776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747764.4741624974, 747764.4741624967, 189282.4901317209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6027600.0000, 
sim time next is 6028200.0000, 
raw observation next is [29.45, 75.0, 1.0, 2.0, 0.533764301461443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745869.2455511234, 745869.2455511227, 189056.2838746074], 
processed observation next is [1.0, 0.782608695652174, 0.5947867298578199, 0.75, 1.0, 1.0, 0.43827024272463017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20718590154197872, 0.20718590154197852, 0.2821735580218021], 
reward next is 0.7178, 
noisyNet noise sample is [array([-2.4476666], dtype=float32), 1.6665088]. 
=============================================
[2019-04-23 09:54:18,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:18,442] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0048
[2019-04-23 09:54:18,449] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 89.5, 1.0, 2.0, 0.5272986350211235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736831.1469815219, 736831.1469815212, 187984.0942567497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6240600.0000, 
sim time next is 6241200.0000, 
raw observation next is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.52786251708802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737619.3722879834, 737619.3722879834, 188077.1312553247], 
processed observation next is [0.0, 0.21739130434782608, 0.4723538704581361, 0.8933333333333333, 1.0, 1.0, 0.43115965914219273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2048942700799954, 0.2048942700799954, 0.28071213620197716], 
reward next is 0.7193, 
noisyNet noise sample is [array([-2.0602086], dtype=float32), -0.83169895]. 
=============================================
[2019-04-23 09:54:21,412] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167452: loss 0.0105
[2019-04-23 09:54:21,417] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167452: learning rate 0.0005
[2019-04-23 09:54:22,007] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167675: loss 0.0278
[2019-04-23 09:54:22,012] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167676: learning rate 0.0005
[2019-04-23 09:54:22,171] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167739: loss 0.0027
[2019-04-23 09:54:22,175] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167739: learning rate 0.0005
[2019-04-23 09:54:22,220] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 167758: loss 0.0016
[2019-04-23 09:54:22,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 167759: learning rate 0.0005
[2019-04-23 09:54:22,451] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167844: loss 0.0209
[2019-04-23 09:54:22,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167844: learning rate 0.0005
[2019-04-23 09:54:22,632] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167913: loss 0.0156
[2019-04-23 09:54:22,634] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167914: learning rate 0.0005
[2019-04-23 09:54:22,653] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167924: loss 0.0100
[2019-04-23 09:54:22,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167924: learning rate 0.0005
[2019-04-23 09:54:22,931] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168022: loss 0.0019
[2019-04-23 09:54:22,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168022: learning rate 0.0005
[2019-04-23 09:54:22,954] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168031: loss 0.0078
[2019-04-23 09:54:22,955] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168031: learning rate 0.0005
[2019-04-23 09:54:23,059] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168071: loss 0.0049
[2019-04-23 09:54:23,061] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168072: learning rate 0.0005
[2019-04-23 09:54:23,130] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168100: loss 0.0253
[2019-04-23 09:54:23,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168100: learning rate 0.0005
[2019-04-23 09:54:23,190] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 168118: loss 0.0455
[2019-04-23 09:54:23,193] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 168118: learning rate 0.0005
[2019-04-23 09:54:23,248] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168139: loss 0.0401
[2019-04-23 09:54:23,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168139: learning rate 0.0005
[2019-04-23 09:54:23,538] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168250: loss 0.0194
[2019-04-23 09:54:23,541] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168250: learning rate 0.0005
[2019-04-23 09:54:23,829] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 168357: loss 0.0041
[2019-04-23 09:54:23,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 168357: learning rate 0.0005
[2019-04-23 09:54:23,897] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168382: loss 0.0081
[2019-04-23 09:54:23,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168382: learning rate 0.0005
[2019-04-23 09:54:27,884] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:27,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3912
[2019-04-23 09:54:27,906] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 70.5, 1.0, 2.0, 0.5158433814178596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720818.4913110913, 720818.4913110913, 186115.4075364385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6367800.0000, 
sim time next is 6368400.0000, 
raw observation next is [28.9, 72.0, 1.0, 2.0, 0.5143807941019103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718774.0400198154, 718774.0400198147, 185879.6017012107], 
processed observation next is [0.0, 0.7391304347826086, 0.5687203791469194, 0.72, 1.0, 1.0, 0.4149166193998919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19965945556105985, 0.19965945556105966, 0.2774322413450906], 
reward next is 0.7226, 
noisyNet noise sample is [array([-0.00224469], dtype=float32), -0.96323943]. 
=============================================
[2019-04-23 09:54:32,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:32,444] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4565
[2019-04-23 09:54:32,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2123125.736693108 W.
[2019-04-23 09:54:32,462] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 67.16666666666667, 1.0, 2.0, 0.8772017456978204, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979495320398028, 6.9112, 168.91255064797, 2123125.736693108, 2074674.802276952, 428746.9819633072], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6450600.0000, 
sim time next is 6451200.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.9147550710907213, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977333334459289, 6.9112, 168.9125074257809, 2175689.718422257, 2128772.579560882, 439181.1518022362], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.897295266374363, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006613333445928937, 0.0, 0.8294377399410788, 0.6043582551172936, 0.5913257165446895, 0.655494256421248], 
reward next is 0.0138, 
noisyNet noise sample is [array([0.8398278], dtype=float32), -0.27890012]. 
=============================================
[2019-04-23 09:54:41,596] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 09:54:41,598] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:54:41,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:54:41,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,602] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:54:41,604] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,605] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:54:41,606] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,607] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,607] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:54:41,612] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,624] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,665] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,666] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,704] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run8
[2019-04-23 09:54:49,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:54:49,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.56666666666667, 85.66666666666667, 1.0, 2.0, 0.3063295940111303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505308.0982090575, 505308.0982090575, 166984.6376327895]
[2019-04-23 09:54:49,246] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:54:49,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2614907172212252
[2019-04-23 09:55:17,415] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:55:17,416] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.50985306666666, 91.24886720666667, 1.0, 2.0, 0.4897413125986473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684332.7768400114, 684332.7768400114, 182004.5005769795]
[2019-04-23 09:55:17,418] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:55:17,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3718510350623593
[2019-04-23 09:55:34,436] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:55:34,437] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.95374612666667, 91.75319116, 1.0, 2.0, 0.3040831278845786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487455.1501065402, 487455.1501065395, 166190.6948996339]
[2019-04-23 09:55:34,438] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:55:34,440] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0896850357655139
[2019-04-23 09:56:18,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:56:18,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.18654572, 68.188914475, 1.0, 2.0, 0.464722003493342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661300.4903722984, 661300.4903722984, 179787.6388867173]
[2019-04-23 09:56:18,104] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:56:18,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5740378231212584
[2019-04-23 09:56:22,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:56:22,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.944618385, 88.15753422, 1.0, 2.0, 0.5999678798302436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838417.0263012078, 838417.0263012078, 200740.2178655962]
[2019-04-23 09:56:22,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:56:22,482] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17381507174897437
[2019-04-23 09:56:42,436] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:56:42,439] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.21848618, 83.94704945, 1.0, 2.0, 0.3340499116203408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 527452.1393759404, 527452.1393759411, 169119.0018721139]
[2019-04-23 09:56:42,441] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:56:42,444] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15717445715836953
[2019-04-23 09:56:53,598] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:56:53,822] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:56:53,861] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:56:53,947] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:56:53,961] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:56:54,974] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 175000, evaluation results [175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:56:55,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:56:55,210] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7334
[2019-04-23 09:56:55,216] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 77.0, 1.0, 2.0, 0.4946960288855848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691258.4289693049, 691258.4289693049, 182769.3002074526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6555600.0000, 
sim time next is 6556200.0000, 
raw observation next is [27.66666666666666, 77.66666666666667, 1.0, 2.0, 0.4962764284963369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693467.5050002947, 693467.5050002947, 183014.8726285548], 
processed observation next is [1.0, 0.9130434782608695, 0.5102685624012636, 0.7766666666666667, 1.0, 1.0, 0.39310413071847816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19262986250008185, 0.19262986250008185, 0.2731565263112758], 
reward next is 0.7268, 
noisyNet noise sample is [array([-1.0499811], dtype=float32), 1.1432102]. 
=============================================
[2019-04-23 09:56:56,139] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175438: loss -3.5714
[2019-04-23 09:56:56,141] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175438: learning rate 0.0005
[2019-04-23 09:56:56,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175719: loss -164.8328
[2019-04-23 09:56:56,894] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175721: learning rate 0.0005
[2019-04-23 09:56:56,918] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175730: loss 19.9320
[2019-04-23 09:56:56,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175730: learning rate 0.0005
[2019-04-23 09:56:57,032] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 175773: loss -146.9077
[2019-04-23 09:56:57,038] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 175773: learning rate 0.0005
[2019-04-23 09:56:57,259] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175859: loss -197.3134
[2019-04-23 09:56:57,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175861: learning rate 0.0005
[2019-04-23 09:56:57,286] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175867: loss -173.8707
[2019-04-23 09:56:57,287] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175867: learning rate 0.0005
[2019-04-23 09:56:57,361] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175891: loss -98.9881
[2019-04-23 09:56:57,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175891: learning rate 0.0005
[2019-04-23 09:56:57,591] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175981: loss -97.6831
[2019-04-23 09:56:57,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175981: learning rate 0.0005
[2019-04-23 09:56:57,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176015: loss -30.8638
[2019-04-23 09:56:57,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176015: learning rate 0.0005
[2019-04-23 09:56:57,694] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176023: loss -136.7369
[2019-04-23 09:56:57,703] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176028: learning rate 0.0005
[2019-04-23 09:56:57,838] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176080: loss -105.4004
[2019-04-23 09:56:57,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176080: learning rate 0.0005
[2019-04-23 09:56:57,912] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 176109: loss -181.8378
[2019-04-23 09:56:57,914] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 176109: learning rate 0.0005
[2019-04-23 09:56:58,087] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176170: loss -159.5296
[2019-04-23 09:56:58,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176171: learning rate 0.0005
[2019-04-23 09:56:58,199] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176211: loss -109.5986
[2019-04-23 09:56:58,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176212: learning rate 0.0005
[2019-04-23 09:56:58,405] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176290: loss -64.4436
[2019-04-23 09:56:58,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176291: learning rate 0.0005
[2019-04-23 09:56:58,644] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176378: loss -7.5602
[2019-04-23 09:56:58,646] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176378: learning rate 0.0005
[2019-04-23 09:57:06,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:06,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5865
[2019-04-23 09:57:06,918] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 69.83333333333333, 1.0, 2.0, 0.3731243859180476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 567287.1956160339, 567287.1956160332, 171911.6274366681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6731400.0000, 
sim time next is 6732000.0000, 
raw observation next is [25.7, 70.0, 1.0, 2.0, 0.3701595994005154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564494.007154932, 564494.007154932, 171719.5684334686], 
processed observation next is [1.0, 0.9565217391304348, 0.4170616113744076, 0.7, 1.0, 1.0, 0.24115614385604264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15680389087637, 0.15680389087637, 0.2562978633335352], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.0564003], dtype=float32), 1.9291044]. 
=============================================
[2019-04-23 09:57:06,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.8418  ]
 [67.79381 ]
 [67.799644]
 [67.60704 ]
 [67.528786]], R is [[67.86051941]
 [67.92533112]
 [67.98913574]
 [68.05178833]
 [68.11329651]].
[2019-04-23 09:57:09,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:09,638] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6682
[2019-04-23 09:57:09,645] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 79.0, 1.0, 2.0, 0.3478090090862956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549331.0909467237, 549331.0909467237, 170876.5057150823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6764400.0000, 
sim time next is 6765000.0000, 
raw observation next is [23.13333333333333, 78.50000000000001, 1.0, 2.0, 0.3724173087015109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587538.6706923113, 587538.6706923113, 174095.6664532488], 
processed observation next is [1.0, 0.30434782608695654, 0.29541864139020524, 0.7850000000000001, 1.0, 1.0, 0.24387627554398905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1632051863034198, 0.1632051863034198, 0.25984427828843104], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.7163119], dtype=float32), 1.1638778]. 
=============================================
[2019-04-23 09:57:09,661] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.532616]
 [65.18942 ]
 [65.27956 ]
 [65.33555 ]
 [65.13953 ]], R is [[65.55167389]
 [65.64111328]
 [65.72491455]
 [65.81033325]
 [65.89789581]].
[2019-04-23 09:57:17,839] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183412: loss 0.1456
[2019-04-23 09:57:17,844] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183414: learning rate 0.0005
[2019-04-23 09:57:18,555] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183679: loss 0.0408
[2019-04-23 09:57:18,560] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183680: learning rate 0.0005
[2019-04-23 09:57:18,595] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 183693: loss 0.0432
[2019-04-23 09:57:18,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 183695: learning rate 0.0005
[2019-04-23 09:57:18,706] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183736: loss 0.0358
[2019-04-23 09:57:18,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183736: learning rate 0.0005
[2019-04-23 09:57:19,048] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183864: loss 0.0022
[2019-04-23 09:57:19,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183865: learning rate 0.0005
[2019-04-23 09:57:19,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183870: loss 0.0036
[2019-04-23 09:57:19,069] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183871: learning rate 0.0005
[2019-04-23 09:57:19,242] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183935: loss 0.0142
[2019-04-23 09:57:19,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183935: learning rate 0.0005
[2019-04-23 09:57:19,381] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 183989: loss 0.0026
[2019-04-23 09:57:19,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 183989: learning rate 0.0005
[2019-04-23 09:57:19,429] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184005: loss 0.0115
[2019-04-23 09:57:19,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184005: learning rate 0.0005
[2019-04-23 09:57:19,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:19,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4036
[2019-04-23 09:57:19,542] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.93333333333333, 50.66666666666667, 1.0, 2.0, 0.3428301808668725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529361.4641478624, 529361.4641478618, 168987.8760007616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6888000.0000, 
sim time next is 6888600.0000, 
raw observation next is [28.8, 51.5, 1.0, 2.0, 0.3473424583430342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535688.6631004182, 535688.6631004182, 169481.8156276213], 
processed observation next is [0.0, 0.7391304347826086, 0.5639810426540285, 0.515, 1.0, 1.0, 0.21366561246148696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14880240641678283, 0.14880240641678283, 0.25295793377256914], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.478378], dtype=float32), 0.4995742]. 
=============================================
[2019-04-23 09:57:19,553] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184049: loss 0.0021
[2019-04-23 09:57:19,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184050: learning rate 0.0005
[2019-04-23 09:57:19,814] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 184144: loss 0.0043
[2019-04-23 09:57:19,817] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184145: learning rate 0.0005
[2019-04-23 09:57:19,861] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184160: loss 0.0027
[2019-04-23 09:57:19,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184161: learning rate 0.0005
[2019-04-23 09:57:19,972] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184200: loss 0.0033
[2019-04-23 09:57:19,975] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184200: learning rate 0.0005
[2019-04-23 09:57:20,050] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184234: loss 0.0025
[2019-04-23 09:57:20,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184234: learning rate 0.0005
[2019-04-23 09:57:20,324] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184335: loss 0.0021
[2019-04-23 09:57:20,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184335: learning rate 0.0005
[2019-04-23 09:57:20,693] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184473: loss 0.0038
[2019-04-23 09:57:20,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184473: learning rate 0.0005
[2019-04-23 09:57:23,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:23,103] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8404
[2019-04-23 09:57:23,108] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.26666666666667, 52.0, 1.0, 2.0, 0.4516574988904564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 643087.5587748671, 643087.5587748665, 177912.960581691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6961200.0000, 
sim time next is 6961800.0000, 
raw observation next is [31.08333333333333, 52.0, 1.0, 2.0, 0.4441880433620905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636648.3770958986, 636648.3770958979, 177370.571620863], 
processed observation next is [0.0, 0.5652173913043478, 0.6721958925750393, 0.52, 1.0, 1.0, 0.3303470401952897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1768467714155274, 0.1768467714155272, 0.2647321964490492], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.4067079], dtype=float32), 0.5355394]. 
=============================================
[2019-04-23 09:57:27,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:27,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8992
[2019-04-23 09:57:27,241] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.65, 83.5, 1.0, 2.0, 0.8336761607015539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1185683.662403752, 1185683.662403752, 255271.9060994001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7007400.0000, 
sim time next is 7008000.0000, 
raw observation next is [25.6, 83.66666666666667, 1.0, 2.0, 0.7935991486742273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1129934.362737885, 1129934.362737885, 245252.8569832499], 
processed observation next is [1.0, 0.08695652173913043, 0.4123222748815167, 0.8366666666666667, 1.0, 1.0, 0.7513242755111172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31387065631607913, 0.31387065631607913, 0.3660490402735073], 
reward next is 0.6340, 
noisyNet noise sample is [array([-1.2389833], dtype=float32), 1.9955999]. 
=============================================
[2019-04-23 09:57:27,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.583664]
 [66.74798 ]
 [66.47123 ]
 [67.05182 ]
 [66.74173 ]], R is [[66.21154785]
 [66.16842651]
 [66.08798981]
 [66.00204468]
 [66.07502747]].
[2019-04-23 09:57:39,175] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191385: loss 0.0812
[2019-04-23 09:57:39,179] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191386: learning rate 0.0005
[2019-04-23 09:57:39,910] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 191661: loss 0.0760
[2019-04-23 09:57:39,914] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 191662: learning rate 0.0005
[2019-04-23 09:57:39,987] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191687: loss 0.0790
[2019-04-23 09:57:39,991] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191689: learning rate 0.0005
[2019-04-23 09:57:40,038] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191711: loss 0.0771
[2019-04-23 09:57:40,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191711: learning rate 0.0005
[2019-04-23 09:57:40,452] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191868: loss 0.0760
[2019-04-23 09:57:40,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191869: learning rate 0.0005
[2019-04-23 09:57:40,506] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191884: loss 0.0773
[2019-04-23 09:57:40,509] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191884: learning rate 0.0005
[2019-04-23 09:57:40,553] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191897: loss 0.0772
[2019-04-23 09:57:40,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191897: learning rate 0.0005
[2019-04-23 09:57:40,688] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191953: loss 0.0887
[2019-04-23 09:57:40,691] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191954: learning rate 0.0005
[2019-04-23 09:57:40,716] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191961: loss 0.0978
[2019-04-23 09:57:40,719] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191961: learning rate 0.0005
[2019-04-23 09:57:40,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192009: loss 0.0831
[2019-04-23 09:57:40,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192009: learning rate 0.0005
[2019-04-23 09:57:41,094] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 192093: loss 0.0825
[2019-04-23 09:57:41,097] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 192094: learning rate 0.0005
[2019-04-23 09:57:41,212] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192139: loss 0.1106
[2019-04-23 09:57:41,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192140: learning rate 0.0005
[2019-04-23 09:57:41,333] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192180: loss 0.0888
[2019-04-23 09:57:41,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192180: learning rate 0.0005
[2019-04-23 09:57:41,448] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 192222: loss 0.0955
[2019-04-23 09:57:41,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 192224: learning rate 0.0005
[2019-04-23 09:57:41,668] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 192303: loss 0.1013
[2019-04-23 09:57:41,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 192303: learning rate 0.0005
[2019-04-23 09:57:41,972] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192408: loss 0.1358
[2019-04-23 09:57:41,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192409: learning rate 0.0005
[2019-04-23 09:57:56,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:56,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7044
[2019-04-23 09:57:56,587] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666666, 82.66666666666667, 1.0, 2.0, 0.2898212026336421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465893.8763467673, 465893.8763467673, 164675.0138125497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7416600.0000, 
sim time next is 7417200.0000, 
raw observation next is [21.63333333333333, 83.33333333333334, 1.0, 2.0, 0.2928300893170977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470243.2167977881, 470243.2167977875, 164974.4481674853], 
processed observation next is [1.0, 0.8695652173913043, 0.2243285939968403, 0.8333333333333335, 1.0, 1.0, 0.14798805941819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13062311577716337, 0.1306231157771632, 0.24623051965296314], 
reward next is 0.7538, 
noisyNet noise sample is [array([0.423527], dtype=float32), 0.06962994]. 
=============================================
[2019-04-23 09:58:00,954] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199385: loss 0.0430
[2019-04-23 09:58:00,957] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199385: learning rate 0.0005
[2019-04-23 09:58:01,665] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199651: loss 0.0247
[2019-04-23 09:58:01,670] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199651: learning rate 0.0005
[2019-04-23 09:58:01,777] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 199692: loss 0.0155
[2019-04-23 09:58:01,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 199692: learning rate 0.0005
[2019-04-23 09:58:01,860] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199722: loss 0.0129
[2019-04-23 09:58:01,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199722: learning rate 0.0005
[2019-04-23 09:58:02,251] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199869: loss 0.0207
[2019-04-23 09:58:02,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199869: learning rate 0.0005
[2019-04-23 09:58:02,356] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199908: loss 0.0038
[2019-04-23 09:58:02,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199908: learning rate 0.0005
[2019-04-23 09:58:02,418] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 199931: loss 0.0092
[2019-04-23 09:58:02,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 199931: learning rate 0.0005
[2019-04-23 09:58:02,472] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199951: loss 0.0123
[2019-04-23 09:58:02,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199951: learning rate 0.0005
[2019-04-23 09:58:02,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199974: loss 0.0486
[2019-04-23 09:58:02,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199974: learning rate 0.0005
[2019-04-23 09:58:02,604] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-23 09:58:02,605] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:58:02,606] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:58:02,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,606] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:58:02,607] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:58:02,608] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,610] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,609] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,612] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:58:02,616] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,656] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,657] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,705] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,723] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run9
[2019-04-23 09:58:20,961] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:58:20,962] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.05, 78.5, 1.0, 2.0, 0.3171040195629773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500848.2221666477, 500848.2221666477, 167076.9413814815]
[2019-04-23 09:58:20,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:58:20,968] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08410307981498089
[2019-04-23 09:58:24,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:58:24,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.0, 56.0, 1.0, 2.0, 0.2066179980198703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 345241.1681552971, 345241.1681552971, 155998.3848966024]
[2019-04-23 09:58:24,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:58:24,781] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2353325573211137
[2019-04-23 09:58:39,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:58:39,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.09368976333334, 81.42426167333333, 1.0, 2.0, 0.62560111552353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874252.5885463987, 874252.5885463987, 205613.9544992502]
[2019-04-23 09:58:39,693] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:58:39,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.27379451976561553
[2019-04-23 09:59:05,635] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:05,636] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 79.33333333333333, 1.0, 2.0, 0.5703613088713821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797028.1997051478, 797028.1997051478, 195353.5753770476]
[2019-04-23 09:59:05,639] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:05,643] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.814967736313254
[2019-04-23 09:59:15,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:15,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.76666666666667, 63.66666666666667, 1.0, 2.0, 0.5584023508663726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780310.502096027, 780310.502096027, 193251.9119116482]
[2019-04-23 09:59:15,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:59:15,479] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4173267115302882
[2019-04-23 09:59:18,148] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:18,149] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.5, 50.0, 1.0, 2.0, 0.6446017235272711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 900816.4402990958, 900816.4402990965, 209344.9879821384]
[2019-04-23 09:59:18,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:18,153] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15029234255396562
[2019-04-23 09:59:19,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:19,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 70.5, 1.0, 2.0, 0.9660308241915481, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.99566853991017, 6.9112, 168.9123871082448, 2247456.906069078, 2187532.218679623, 453421.1834107374]
[2019-04-23 09:59:19,806] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:19,808] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15293557783858658
[2019-04-23 09:59:19,809] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2247456.906069078 W.
[2019-04-23 09:59:29,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:29,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.29851371666667, 69.427973355, 1.0, 2.0, 0.5038302685724638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704026.2948309104, 704026.2948309104, 184198.5988004312]
[2019-04-23 09:59:29,504] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:59:29,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46352673131625965
[2019-04-23 09:59:59,905] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:59,906] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.93333333333334, 76.33333333333334, 1.0, 2.0, 1.002648680476791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129317068804, 1401509.244939603, 1401509.244939602, 299741.7657488313]
[2019-04-23 09:59:59,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:59,909] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7917811788243619
[2019-04-23 10:00:12,580] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:00:12,981] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:00:13,106] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:00:13,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:00:13,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:00:14,174] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 200000, evaluation results [200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:00:14,263] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200039: loss 0.0174
[2019-04-23 10:00:14,268] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200040: learning rate 0.0005
[2019-04-23 10:00:14,501] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200123: loss 0.0116
[2019-04-23 10:00:14,504] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200123: learning rate 0.0005
[2019-04-23 10:00:14,588] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200152: loss 0.0348
[2019-04-23 10:00:14,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200154: learning rate 0.0005
[2019-04-23 10:00:14,845] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200242: loss 0.0155
[2019-04-23 10:00:14,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200243: learning rate 0.0005
[2019-04-23 10:00:15,035] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200311: loss 0.0507
[2019-04-23 10:00:15,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200311: learning rate 0.0005
[2019-04-23 10:00:15,222] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 200379: loss 0.0270
[2019-04-23 10:00:15,225] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 200379: learning rate 0.0005
[2019-04-23 10:00:15,537] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200488: loss 0.0028
[2019-04-23 10:00:15,539] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200489: learning rate 0.0005
[2019-04-23 10:00:17,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:17,432] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4541
[2019-04-23 10:00:17,439] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.0, 1.0, 2.0, 0.3996088689809195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590598.5511635176, 590598.5511635176, 173498.3430802972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7545600.0000, 
sim time next is 7546200.0000, 
raw observation next is [24.03333333333333, 89.0, 1.0, 2.0, 0.4032313478690614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594119.3487265224, 594119.3487265224, 173767.9098471192], 
processed observation next is [0.0, 0.34782608695652173, 0.3380726698262243, 0.89, 1.0, 1.0, 0.2810016239386282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.165033152424034, 0.165033152424034, 0.25935508932405854], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.0224499], dtype=float32), 0.69054884]. 
=============================================
[2019-04-23 10:00:18,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:18,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-23 10:00:18,796] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 62.0, 1.0, 2.0, 0.4445139563660606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633156.6034245412, 633156.6034245412, 176913.5538732694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7573800.0000, 
sim time next is 7574400.0000, 
raw observation next is [29.0, 62.0, 1.0, 2.0, 0.43750673349833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 627111.2829354965, 627111.2829354971, 176419.7443471122], 
processed observation next is [0.0, 0.6956521739130435, 0.5734597156398105, 0.62, 1.0, 1.0, 0.3222972692750964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1741975785931935, 0.17419757859319365, 0.26331305126434656], 
reward next is 0.7367, 
noisyNet noise sample is [array([-1.0767745], dtype=float32), -0.2965151]. 
=============================================
[2019-04-23 10:00:25,129] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:25,140] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2556
[2019-04-23 10:00:25,146] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.08333333333334, 64.83333333333334, 1.0, 2.0, 0.5775474897580325, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9754391719738575, 6.911199999999999, 6.9112, 168.9129565104285, 1614761.817344894, 1614761.817344895, 347443.1441871512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7657800.0000, 
sim time next is 7658400.0000, 
raw observation next is [29.96666666666667, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.89190821441428, 6.9112, 168.9015837761941, 2859740.31682906, 1454652.348294085, 309549.8994474969], 
processed observation next is [1.0, 0.6521739130434783, 0.6192733017377569, 0.6566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.1980708214414279, 0.0, 0.8293840998207391, 0.7943723102302945, 0.4040700967483569, 0.46201477529477153], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12257934], dtype=float32), 0.36658284]. 
=============================================
[2019-04-23 10:00:34,783] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207376: loss 0.5074
[2019-04-23 10:00:34,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207378: learning rate 0.0005
[2019-04-23 10:00:35,492] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207624: loss 0.4816
[2019-04-23 10:00:35,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207625: learning rate 0.0005
[2019-04-23 10:00:35,724] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207705: loss 0.3602
[2019-04-23 10:00:35,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207706: learning rate 0.0005
[2019-04-23 10:00:35,783] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 207728: loss 0.3549
[2019-04-23 10:00:35,793] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 207729: learning rate 0.0005
[2019-04-23 10:00:36,111] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207841: loss 0.2566
[2019-04-23 10:00:36,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207841: learning rate 0.0005
[2019-04-23 10:00:36,263] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 207896: loss 0.2596
[2019-04-23 10:00:36,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 207896: learning rate 0.0005
[2019-04-23 10:00:36,279] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207901: loss 0.2406
[2019-04-23 10:00:36,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207903: learning rate 0.0005
[2019-04-23 10:00:36,335] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207918: loss 0.2517
[2019-04-23 10:00:36,339] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207919: learning rate 0.0005
[2019-04-23 10:00:36,565] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207998: loss 0.2416
[2019-04-23 10:00:36,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207998: learning rate 0.0005
[2019-04-23 10:00:36,621] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208017: loss 0.2274
[2019-04-23 10:00:36,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208018: learning rate 0.0005
[2019-04-23 10:00:36,862] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208102: loss 0.2197
[2019-04-23 10:00:36,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208105: learning rate 0.0005
[2019-04-23 10:00:36,922] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 208125: loss 0.2215
[2019-04-23 10:00:36,924] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 208125: learning rate 0.0005
[2019-04-23 10:00:37,337] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 208268: loss 0.3431
[2019-04-23 10:00:37,341] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 208269: learning rate 0.0005
[2019-04-23 10:00:37,389] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208288: loss 0.3300
[2019-04-23 10:00:37,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208288: learning rate 0.0005
[2019-04-23 10:00:37,499] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 208330: loss 0.3566
[2019-04-23 10:00:37,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 208330: learning rate 0.0005
[2019-04-23 10:00:37,755] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208423: loss 0.3809
[2019-04-23 10:00:37,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208423: learning rate 0.0005
[2019-04-23 10:00:39,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:39,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0134
[2019-04-23 10:00:39,223] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.75, 71.0, 1.0, 2.0, 0.3406889169242498, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5817093134495274, 6.911200000000001, 6.9112, 168.912956510431, 952233.2690440153, 952233.2690440146, 234748.4142841789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7837800.0000, 
sim time next is 7838400.0000, 
raw observation next is [29.6, 72.0, 1.0, 2.0, 0.4860680376062967, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679198.3394897704, 679198.3394897704, 181445.0872116965], 
processed observation next is [1.0, 0.7391304347826086, 0.6018957345971565, 0.72, 1.0, 1.0, 0.38080486458589957, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1886662054138251, 0.1886662054138251, 0.27081356300253207], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2906259], dtype=float32), -0.8065065]. 
=============================================
[2019-04-23 10:00:46,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:46,720] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:46,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-04-23 10:00:47,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run2
[2019-04-23 10:00:47,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-04-23 10:00:47,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,628] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-04-23 10:00:47,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-04-23 10:00:47,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-04-23 10:00:47,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-04-23 10:00:47,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-04-23 10:00:48,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,060] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,061] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-04-23 10:00:48,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-04-23 10:00:48,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-04-23 10:00:48,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-04-23 10:00:48,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,183] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-04-23 10:00:48,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,216] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-04-23 10:00:48,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-04-23 10:00:48,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,289] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-04-23 10:00:56,262] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:56,272] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1232
[2019-04-23 10:00:56,280] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 89.0, 1.0, 2.0, 0.3437129169526595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534126.1433099344, 534126.1433099338, 169468.9904006323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 90600.0000, 
sim time next is 91200.0000, 
raw observation next is [22.33333333333334, 89.0, 1.0, 2.0, 0.3438353877293951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534082.3449685606, 534082.3449685606, 169459.2737652238], 
processed observation next is [1.0, 0.043478260869565216, 0.2575039494470777, 0.89, 1.0, 1.0, 0.20944022617999408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14835620693571128, 0.14835620693571128, 0.2529242892018266], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.42834514], dtype=float32), -2.8660831]. 
=============================================
[2019-04-23 10:01:02,569] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:02,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7082
[2019-04-23 10:01:02,594] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333333, 96.0, 1.0, 2.0, 0.2844410003306096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458693.0151236086, 458693.0151236086, 164184.2491094271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 189600.0000, 
sim time next is 190200.0000, 
raw observation next is [19.81666666666667, 96.0, 1.0, 2.0, 0.2839575869409868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458056.6161098718, 458056.6161098718, 164140.8983375772], 
processed observation next is [0.0, 0.17391304347826086, 0.13823064770932092, 0.96, 1.0, 1.0, 0.13729829751926118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12723794891940882, 0.12723794891940882, 0.2449864154292197], 
reward next is 0.7550, 
noisyNet noise sample is [array([1.6156354], dtype=float32), -0.50289804]. 
=============================================
[2019-04-23 10:01:12,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:12,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7300
[2019-04-23 10:01:12,179] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([-0.73738986], dtype=float32), 0.8929609]. 
=============================================
[2019-04-23 10:01:14,037] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:14,045] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3184
[2019-04-23 10:01:14,053] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.31666666666667, 87.0, 1.0, 2.0, 0.2605029842532295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424335.7549007521, 424335.7549007521, 161896.8102515732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [20.3, 87.0, 1.0, 2.0, 0.2602382537486013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424014.5985252329, 424014.5985252323, 161874.0045438348], 
processed observation next is [1.0, 0.08695652173913043, 0.16113744075829392, 0.87, 1.0, 1.0, 0.10872078764891725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11778183292367582, 0.11778183292367564, 0.24160299185646986], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.46957198], dtype=float32), -0.1327862]. 
=============================================
[2019-04-23 10:01:15,928] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:15,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7266
[2019-04-23 10:01:15,945] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.41666666666667, 78.33333333333333, 1.0, 2.0, 0.3726745707419122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607374.3786738176, 607374.3786738182, 175496.8364599389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 377400.0000, 
sim time next is 378000.0000, 
raw observation next is [21.5, 78.0, 1.0, 2.0, 0.3832612156722808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 624262.9154209254, 624262.915420926, 176952.8392955865], 
processed observation next is [1.0, 0.391304347826087, 0.21800947867298584, 0.78, 1.0, 1.0, 0.2569412237015431, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1734063653947015, 0.17340636539470164, 0.264108715366547], 
reward next is 0.7359, 
noisyNet noise sample is [array([2.7778876], dtype=float32), -2.1855567]. 
=============================================
[2019-04-23 10:01:15,959] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[72.96454 ]
 [73.15325 ]
 [73.44691 ]
 [73.784424]
 [74.18585 ]], R is [[72.90325165]
 [72.91228485]
 [72.92404938]
 [72.94097137]
 [72.9631424 ]].
[2019-04-23 10:01:16,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:16,125] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0878
[2019-04-23 10:01:16,134] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.1, 89.5, 1.0, 2.0, 0.2208344335891245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367540.3790585317, 367540.3790585317, 157758.9833277278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 531000.0000, 
sim time next is 531600.0000, 
raw observation next is [18.03333333333333, 89.66666666666667, 1.0, 2.0, 0.2186143000393094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 363966.4045390229, 363966.4045390223, 157534.2777165281], 
processed observation next is [1.0, 0.13043478260869565, 0.05371248025276459, 0.8966666666666667, 1.0, 1.0, 0.05857144583049324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10110177903861747, 0.1011017790386173, 0.2351257876366091], 
reward next is 0.7649, 
noisyNet noise sample is [array([-1.8034661], dtype=float32), -1.5833055]. 
=============================================
[2019-04-23 10:01:16,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:16,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-23 10:01:16,895] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 86.0, 1.0, 2.0, 0.244680711747906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 402682.2676706034, 402682.2676706034, 160382.3105867617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.2444180700154164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 402250.722132477, 402250.7221324776, 160356.9301463709], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.08966032531977879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11173631170346583, 0.111736311703466, 0.23933870171100133], 
reward next is 0.7607, 
noisyNet noise sample is [array([1.2159958], dtype=float32), 0.7595636]. 
=============================================
[2019-04-23 10:01:17,697] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:17,705] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3912
[2019-04-23 10:01:17,711] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
processed observation next is [1.0, 0.30434782608695654, 0.15797788309636643, 0.8083333333333332, 1.0, 1.0, 0.11282927181585911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12070006551133781, 0.12070006551133781, 0.24218520355860731], 
reward next is 0.7578, 
noisyNet noise sample is [array([1.082157], dtype=float32), -2.91153]. 
=============================================
[2019-04-23 10:01:21,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:21,307] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5798
[2019-04-23 10:01:21,313] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 84.0, 1.0, 2.0, 0.2326657747229007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 384388.3960436397, 384388.3960436397, 159187.0100435298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 442800.0000, 
sim time next is 443400.0000, 
raw observation next is [19.61666666666667, 83.83333333333333, 1.0, 2.0, 0.2367096035335818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 391107.8924866772, 391107.8924866778, 159561.6484643982], 
processed observation next is [1.0, 0.13043478260869565, 0.12875197472353894, 0.8383333333333333, 1.0, 1.0, 0.08037301630552024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10864108124629923, 0.10864108124629938, 0.23815171412596745], 
reward next is 0.7618, 
noisyNet noise sample is [array([-0.8046277], dtype=float32), -1.247049]. 
=============================================
[2019-04-23 10:01:22,643] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-23 10:01:22,644] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:01:22,644] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:01:22,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,646] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:01:22,648] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,647] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:01:22,648] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:01:22,650] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,650] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,669] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,688] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,690] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run10
[2019-04-23 10:01:37,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:01:37,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.0, 92.5, 1.0, 2.0, 0.3544759504115104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548808.8898096017, 548808.8898096017, 170620.6795310446]
[2019-04-23 10:01:37,514] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:01:37,519] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6497306637666496
[2019-04-23 10:01:53,803] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:01:53,803] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 84.5, 1.0, 2.0, 0.5529395100161054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844919.2874635188, 844919.2874635188, 201067.8893506665]
[2019-04-23 10:01:53,804] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:01:53,808] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.29691998121766117
[2019-04-23 10:02:13,646] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:13,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.4581022426458296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683473.5253452257, 683473.5253452263, 182696.7856311444]
[2019-04-23 10:02:13,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:02:13,651] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8653014828869345
[2019-04-23 10:02:28,330] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:28,331] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.23483681166667, 65.16833487666668, 1.0, 2.0, 0.8325328909906673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163589.913726398, 1163589.913726398, 252069.3145354791]
[2019-04-23 10:02:28,332] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:02:28,337] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8369137254553556
[2019-04-23 10:02:40,743] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:40,744] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.09999999999999, 49.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.109584983538773, 6.9112, 168.9117187630503, 1594591.418086402, 1453851.314191885, 311356.3024867051]
[2019-04-23 10:02:40,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:02:40,749] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31131253143728144
[2019-04-23 10:02:52,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:52,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.18333333333333, 78.16666666666667, 1.0, 2.0, 0.5619285007923895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785239.759397992, 785239.759397992, 193867.0914209615]
[2019-04-23 10:02:52,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:02:52,880] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33004301485963694
[2019-04-23 10:03:32,978] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:03:33,046] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:03:33,225] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:03:33,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:03:33,335] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:03:34,355] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 225000, evaluation results [225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:03:43,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:43,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8709
[2019-04-23 10:03:43,817] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 60.5, 1.0, 2.0, 0.2436747180262885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402194.8189099152, 402194.8189099146, 160246.7239985138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 756600.0000, 
sim time next is 757200.0000, 
raw observation next is [22.9, 62.0, 1.0, 2.0, 0.2448529244925271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403930.1500833399, 403930.1500833393, 160369.7330999935], 
processed observation next is [1.0, 0.782608695652174, 0.2843601895734597, 0.62, 1.0, 1.0, 0.09018424637653867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11220281946759442, 0.11220281946759425, 0.23935781059700523], 
reward next is 0.7606, 
noisyNet noise sample is [array([1.4326662], dtype=float32), 0.5234606]. 
=============================================
[2019-04-23 10:03:44,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:44,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-23 10:03:44,788] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.28333333333333, 90.66666666666667, 1.0, 2.0, 0.2055258604055006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 155935.3899002669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 611400.0000, 
sim time next is 612000.0000, 
raw observation next is [17.2, 91.0, 1.0, 2.0, 0.2048236585511596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342264.3359998221, 342264.3359998221, 155835.6189262077], 
processed observation next is [1.0, 0.08695652173913043, 0.014218009478673018, 0.91, 1.0, 1.0, 0.04195621512187903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09507342666661726, 0.09507342666661726, 0.23259047600926525], 
reward next is 0.7674, 
noisyNet noise sample is [array([0.95099324], dtype=float32), -0.07311777]. 
=============================================
[2019-04-23 10:03:44,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.550415]
 [71.54889 ]
 [71.59788 ]
 [71.61695 ]
 [71.66792 ]], R is [[71.94636536]
 [71.99416351]
 [72.04131317]
 [72.08778381]
 [72.13359833]].
[2019-04-23 10:03:53,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:53,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1695
[2019-04-23 10:03:53,131] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 89.33333333333334, 1.0, 2.0, 0.2540526982891675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417111.0264477726, 417111.0264477726, 161313.052710072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 778200.0000, 
sim time next is 778800.0000, 
raw observation next is [19.5, 89.66666666666667, 1.0, 2.0, 0.2547393510533554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418024.3709610438, 418024.3709610438, 161381.6543271054], 
processed observation next is [0.0, 0.0, 0.12322274881516594, 0.8966666666666667, 1.0, 1.0, 0.1020956036787414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11611788082251216, 0.11611788082251216, 0.2408681407867245], 
reward next is 0.7591, 
noisyNet noise sample is [array([1.2591063], dtype=float32), -0.6010082]. 
=============================================
[2019-04-23 10:03:56,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:56,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3198
[2019-04-23 10:03:56,832] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 65.0, 1.0, 2.0, 0.2900126695283051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464387.5928239186, 464387.5928239193, 164558.2732376559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 829800.0000, 
sim time next is 830400.0000, 
raw observation next is [24.46666666666667, 65.66666666666667, 1.0, 2.0, 0.2917979729636215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 466567.3884336081, 466567.3884336088, 164701.5387627984], 
processed observation next is [0.0, 0.6086956521739131, 0.3586097946287521, 0.6566666666666667, 1.0, 1.0, 0.14674454573930298, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12960205234266892, 0.1296020523426691, 0.24582319218328116], 
reward next is 0.7542, 
noisyNet noise sample is [array([0.1917116], dtype=float32), 0.61985284]. 
=============================================
[2019-04-23 10:04:11,192] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:11,204] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4708
[2019-04-23 10:04:11,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.16666666666667, 1.0, 2.0, 0.670047564466006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029646.155231514, 1029646.155231514, 226111.9375705542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1008600.0000, 
sim time next is 1009200.0000, 
raw observation next is [21.7, 97.33333333333334, 1.0, 2.0, 0.6354124623894929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 975815.7821863971, 975815.7821863965, 218323.9132498642], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.9733333333333334, 1.0, 1.0, 0.5607379064933649, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2710599394962214, 0.27105993949622126, 0.32585658694009584], 
reward next is 0.6741, 
noisyNet noise sample is [array([-1.0917808], dtype=float32), -0.34124917]. 
=============================================
[2019-04-23 10:04:12,823] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:12,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1010
[2019-04-23 10:04:12,843] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.66666666666666, 1.0, 2.0, 0.6585724213295752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010208.83483966, 1010208.83483966, 223312.2328489435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1010400.0000, 
sim time next is 1011000.0000, 
raw observation next is [21.7, 97.83333333333334, 1.0, 2.0, 0.6618211589878472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014583.8273416, 1014583.8273416, 223976.1209066036], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.9783333333333334, 1.0, 1.0, 0.5925556132383701, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2818288409282222, 0.2818288409282222, 0.3342927177710502], 
reward next is 0.6657, 
noisyNet noise sample is [array([-0.7240373], dtype=float32), -0.458278]. 
=============================================
[2019-04-23 10:04:12,868] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[63.37812 ]
 [63.395203]
 [63.35344 ]
 [63.12236 ]
 [63.402718]], R is [[63.33546066]
 [63.36880493]
 [63.40576553]
 [63.44585419]
 [63.47391891]].
[2019-04-23 10:04:21,514] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:21,528] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2021
[2019-04-23 10:04:21,541] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 94.0, 1.0, 2.0, 0.2792906045621624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 451695.6538115449, 451695.6538115443, 163708.5847561406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1137600.0000, 
sim time next is 1138200.0000, 
raw observation next is [19.85, 94.33333333333334, 1.0, 2.0, 0.2743350859392549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 443821.2545537705, 443821.2545537712, 163187.4146740149], 
processed observation next is [1.0, 0.17391304347826086, 0.1398104265402845, 0.9433333333333335, 1.0, 1.0, 0.12570492281837936, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12328368182049179, 0.12328368182049201, 0.24356330548360433], 
reward next is 0.7564, 
noisyNet noise sample is [array([0.43962803], dtype=float32), 1.2256786]. 
=============================================
[2019-04-23 10:04:28,479] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:28,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2672
[2019-04-23 10:04:28,509] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1689902.738290467 W.
[2019-04-23 10:04:28,514] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.16666666666666, 71.66666666666667, 1.0, 2.0, 0.6044015601179256, 0.0, 1.0, 0.0, 1.0, 2.0, 1.013076794567798, 6.911200000000001, 6.9112, 168.9129563112766, 1689902.738290467, 1689902.738290466, 362245.4865772667], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1251600.0000, 
sim time next is 1252200.0000, 
raw observation next is [28.18333333333333, 71.83333333333333, 1.0, 2.0, 0.6176865012990023, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.915483503226586, 6.9112, 168.9129001136094, 1727077.570169854, 1724038.70656873, 369278.97148001], 
processed observation next is [1.0, 0.4782608695652174, 0.5347551342812005, 0.7183333333333333, 1.0, 1.0, 0.5393813268662677, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0004283503226585772, 0.0, 0.82943966821809, 0.4797437694916261, 0.47889964071353613, 0.5511626440000149], 
reward next is 0.4274, 
noisyNet noise sample is [array([1.1136059], dtype=float32), 0.21372125]. 
=============================================
[2019-04-23 10:04:29,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:29,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-23 10:04:29,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1981707.509335753 W.
[2019-04-23 10:04:29,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 75.0, 1.0, 2.0, 0.7086759035549475, 1.0, 1.0, 0.7086759035549475, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1981707.509335753, 1981707.509335753, 377851.397848755], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.6584990297678837, 1.0, 2.0, 0.6584990297678837, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1841274.724014334, 1841274.724014335, 356855.0679642363], 
processed observation next is [1.0, 0.6956521739130435, 0.524486571879937, 0.7516666666666667, 1.0, 1.0, 0.5885530479131128, 1.0, 1.0, 0.5885530479131128, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5114652011150927, 0.5114652011150931, 0.5326195044242333], 
reward next is 0.4674, 
noisyNet noise sample is [array([0.84896684], dtype=float32), -0.5786036]. 
=============================================
[2019-04-23 10:04:36,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:36,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8722
[2019-04-23 10:04:36,969] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3025808591077417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481811.8164296245, 481811.8164296245, 165751.3338185707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [20.88333333333333, 92.83333333333333, 1.0, 2.0, 0.3061218539966665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487057.8767615511, 487057.8767615511, 166124.2753755978], 
processed observation next is [1.0, 0.7391304347826086, 0.18878357030015785, 0.9283333333333332, 1.0, 1.0, 0.1640022337309235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13529385465598642, 0.13529385465598642, 0.24794667966507133], 
reward next is 0.7521, 
noisyNet noise sample is [array([1.1500489], dtype=float32), -0.60920155]. 
=============================================
[2019-04-23 10:04:37,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:37,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5499
[2019-04-23 10:04:37,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3025808591077417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481811.8164296245, 481811.8164296245, 165751.3338185707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [20.88333333333333, 92.83333333333333, 1.0, 2.0, 0.3061218539966665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487057.8767615511, 487057.8767615511, 166124.2753755978], 
processed observation next is [1.0, 0.7391304347826086, 0.18878357030015785, 0.9283333333333332, 1.0, 1.0, 0.1640022337309235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13529385465598642, 0.13529385465598642, 0.24794667966507133], 
reward next is 0.7521, 
noisyNet noise sample is [array([-1.5062777], dtype=float32), 1.1836458]. 
=============================================
[2019-04-23 10:04:40,847] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-23 10:04:40,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:04:40,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,849] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:04:40,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:04:40,851] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:04:40,851] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,851] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:04:40,855] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,857] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,855] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,880] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,880] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,916] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,936] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,955] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run11
[2019-04-23 10:05:05,050] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:05,051] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.15137839333333, 96.11075226833333, 1.0, 2.0, 0.4438844172172695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633313.8529845127, 633313.8529845127, 176958.7522847053]
[2019-04-23 10:05:05,052] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:05:05,056] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8585482374944623
[2019-04-23 10:05:09,215] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:09,219] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.43333333333333, 88.33333333333334, 1.0, 2.0, 0.438490591079734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 638124.4035881577, 638124.4035881584, 177767.3400177261]
[2019-04-23 10:05:09,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:05:09,225] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.520006417082703
[2019-04-23 10:05:18,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:18,217] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.73758738, 93.29024662333333, 1.0, 2.0, 0.575155878261168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803730.7104815161, 803730.7104815161, 196209.4930188654]
[2019-04-23 10:05:18,217] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:05:18,220] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6106063740242511
[2019-04-23 10:05:19,380] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:19,382] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.83333333333334, 81.33333333333334, 1.0, 2.0, 0.7222089761594981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009322.370755592, 1009322.370755592, 225709.0557350525]
[2019-04-23 10:05:19,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:05:19,388] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6843136173478381
[2019-04-23 10:05:44,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:44,798] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.46639253333333, 60.59586582666667, 1.0, 2.0, 0.635650895989715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888302.628718366, 888302.628718366, 207575.2151924756]
[2019-04-23 10:05:44,798] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:05:44,803] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8485308278506953
[2019-04-23 10:05:50,394] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:50,395] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.96666666666667, 60.0, 1.0, 2.0, 0.7080693390763718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104262, 989552.32962193, 989552.32962193, 222601.5872647566]
[2019-04-23 10:05:50,395] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:05:50,397] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2869290060617846
[2019-04-23 10:06:02,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:02,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 70.0, 1.0, 2.0, 0.4822587121336495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673873.7594510508, 673873.7594510501, 180864.0671062706]
[2019-04-23 10:06:02,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:06:02,127] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6381020450136674
[2019-04-23 10:06:13,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:13,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 83.0, 1.0, 2.0, 1.009399198420276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128868874903, 1410951.436535815, 1410951.436535816, 301806.3331834676]
[2019-04-23 10:06:13,045] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:06:13,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2877440172040816
[2019-04-23 10:06:27,784] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:27,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.87871423, 85.96236060999999, 1.0, 2.0, 0.5764077105633096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805480.701397891, 805480.701397891, 196433.8561203916]
[2019-04-23 10:06:27,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:06:27,788] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08628963344871599
[2019-04-23 10:06:31,701] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:31,702] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.75861793666667, 64.34988939166666, 1.0, 2.0, 0.6745729741398708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 998806.5287862775, 998806.5287862769, 222710.0358102998]
[2019-04-23 10:06:31,702] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:06:31,708] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03694831903573914
[2019-04-23 10:06:50,870] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:06:51,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:06:51,527] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:06:51,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:06:51,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:06:52,816] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:06:55,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:06:55,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8148
[2019-04-23 10:06:55,206] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1448400.0000, 
sim time next is 1449000.0000, 
raw observation next is [25.3, 80.0, 1.0, 2.0, 0.4123818573780608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606327.0677499198, 606327.0677499193, 174866.3882194718], 
processed observation next is [0.0, 0.782608695652174, 0.39810426540284366, 0.8, 1.0, 1.0, 0.2920263341904347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16842418548608884, 0.16842418548608867, 0.2609946092827937], 
reward next is 0.7390, 
noisyNet noise sample is [array([-0.85932314], dtype=float32), -1.1816754]. 
=============================================
[2019-04-23 10:06:55,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.15084]
 [78.16515]
 [78.15668]
 [78.06439]
 [78.04112]], R is [[78.11395264]
 [78.07159424]
 [78.02990723]
 [77.98831177]
 [77.94687653]].
[2019-04-23 10:06:56,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:06:56,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7382
[2019-04-23 10:06:56,547] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 82.66666666666666, 1.0, 2.0, 0.4058292445935688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599728.062371842, 599728.062371842, 174341.8726408554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1450200.0000, 
sim time next is 1450800.0000, 
raw observation next is [24.5, 84.0, 1.0, 2.0, 0.4026097145536569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596571.4829347419, 596571.4829347419, 174099.046317623], 
processed observation next is [0.0, 0.8260869565217391, 0.3601895734597157, 0.84, 1.0, 1.0, 0.28025266813693606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16571430081520608, 0.16571430081520608, 0.25984932286212387], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.398358], dtype=float32), -0.7387242]. 
=============================================
[2019-04-23 10:07:04,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:04,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7926
[2019-04-23 10:07:04,263] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.00000000000001, 1.0, 2.0, 0.3264188989787366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511970.4120501378, 511970.4120501378, 167843.0780645584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1559400.0000, 
sim time next is 1560000.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3260876342298332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511458.9051673762, 511458.9051673768, 167803.87279087], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.18805739063835328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14207191810204894, 0.1420719181020491, 0.2504535414789104], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.22030732], dtype=float32), 0.35776243]. 
=============================================
[2019-04-23 10:07:04,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[69.40311 ]
 [69.49996 ]
 [69.312904]
 [69.21328 ]
 [69.14645 ]], R is [[69.54258728]
 [69.59664917]
 [69.65006256]
 [69.70274353]
 [69.75473785]].
[2019-04-23 10:07:07,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:07,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7302
[2019-04-23 10:07:07,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.83333333333334, 1.0, 2.0, 0.4194867575474115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612962.5116811729, 612962.5116811722, 175384.3472161093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1637400.0000, 
sim time next is 1638000.0000, 
raw observation next is [23.1, 98.0, 1.0, 2.0, 0.4200944851691179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613363.8565777503, 613363.8565777509, 175408.5805615911], 
processed observation next is [1.0, 1.0, 0.2938388625592418, 0.98, 1.0, 1.0, 0.30131865683026254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1703788490493751, 0.17037884904937525, 0.26180385158446434], 
reward next is 0.7382, 
noisyNet noise sample is [array([0.7507412], dtype=float32), -0.8389249]. 
=============================================
[2019-04-23 10:07:07,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.4363  ]
 [71.4301  ]
 [71.41347 ]
 [71.40527 ]
 [71.401764]], R is [[71.53502655]
 [71.55791473]
 [71.58065033]
 [71.60327911]
 [71.62571716]].
[2019-04-23 10:07:07,758] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:07,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5330
[2019-04-23 10:07:07,779] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 98.83333333333333, 1.0, 2.0, 0.4276244205896276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619928.9886196837, 619928.9886196837, 175914.8479870278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1644600.0000, 
sim time next is 1645200.0000, 
raw observation next is [23.2, 99.0, 1.0, 2.0, 0.4291391812672065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 621195.0623200836, 621195.0623200836, 176011.604917373], 
processed observation next is [1.0, 0.043478260869565216, 0.29857819905213273, 0.99, 1.0, 1.0, 0.3122158810448271, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.172554183977801, 0.172554183977801, 0.2627038879363776], 
reward next is 0.7373, 
noisyNet noise sample is [array([0.38487834], dtype=float32), -0.28917244]. 
=============================================
[2019-04-23 10:07:14,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:14,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2124
[2019-04-23 10:07:14,606] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.31666666666667, 77.33333333333333, 1.0, 2.0, 0.4976794542122041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695428.6523850602, 695428.6523850609, 183234.7198775038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1705800.0000, 
sim time next is 1706400.0000, 
raw observation next is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.1357678387], 
processed observation next is [1.0, 0.782608695652174, 0.5355450236966824, 0.78, 1.0, 1.0, 0.4030883318132741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19584748788950676, 0.19584748788950693, 0.2750972175639384], 
reward next is 0.7249, 
noisyNet noise sample is [array([0.7349468], dtype=float32), -0.38958633]. 
=============================================
[2019-04-23 10:07:16,639] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:16,651] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6628
[2019-04-23 10:07:16,657] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 93.16666666666666, 1.0, 2.0, 0.523230978863991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 188570.2929640439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [24.5, 93.0, 1.0, 2.0, 0.4871256155342776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 182666.2265049748], 
processed observation next is [1.0, 0.2608695652173913, 0.3601895734597157, 0.93, 1.0, 1.0, 0.38207905486057536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1913762893031219, 0.19137628930312206, 0.2726361589626489], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.7341562], dtype=float32), 0.07274519]. 
=============================================
[2019-04-23 10:07:20,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:20,047] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4077
[2019-04-23 10:07:20,060] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 97.0, 1.0, 2.0, 0.3606247084474044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549235.8589680403, 549235.8589680396, 170393.7417863672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1833600.0000, 
sim time next is 1834200.0000, 
raw observation next is [22.15, 96.5, 1.0, 2.0, 0.3621832438266025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 550631.5804476335, 550631.5804476335, 170480.1388647433], 
processed observation next is [1.0, 0.21739130434782608, 0.24881516587677724, 0.965, 1.0, 1.0, 0.23154607689952109, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15295321679100932, 0.15295321679100932, 0.25444796845484074], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.14773272], dtype=float32), -0.502697]. 
=============================================
[2019-04-23 10:07:31,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:32,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-23 10:07:32,017] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 95.5, 1.0, 2.0, 0.3992332777631029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784818, 174110.2442602914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
processed observation next is [1.0, 0.8695652173913043, 0.2843601895734597, 0.9566666666666667, 1.0, 1.0, 0.2791246345450137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16599520968387887, 0.16599520968387868, 0.2600946575224543], 
reward next is 0.7399, 
noisyNet noise sample is [array([-0.90151834], dtype=float32), -0.4676022]. 
=============================================
[2019-04-23 10:07:39,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:39,523] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5596
[2019-04-23 10:07:39,532] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 91.5, 1.0, 2.0, 0.4756561449921946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664644.9191639184, 664644.9191639189, 179869.5470382831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2064600.0000, 
sim time next is 2065200.0000, 
raw observation next is [24.96666666666667, 91.66666666666666, 1.0, 2.0, 0.4765954530357104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665957.8466101834, 665957.8466101841, 180009.9954166866], 
processed observation next is [0.0, 0.9130434782608695, 0.3823064770932071, 0.9166666666666665, 1.0, 1.0, 0.3693921120912173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18498829072505094, 0.18498829072505113, 0.2686716349502785], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.4851866], dtype=float32), 0.18996802]. 
=============================================
[2019-04-23 10:07:43,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:43,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2089
[2019-04-23 10:07:43,202] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 82.33333333333334, 1.0, 2.0, 0.5247232997634513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733231.209425492, 733231.209425492, 187561.110487983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2107200.0000, 
sim time next is 2107800.0000, 
raw observation next is [28.15, 81.5, 1.0, 2.0, 0.5281077366108463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737962.1537803897, 737962.1537803903, 188117.8881391904], 
processed observation next is [0.0, 0.391304347826087, 0.533175355450237, 0.815, 1.0, 1.0, 0.43145510435041723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20498948716121937, 0.2049894871612195, 0.280772967371926], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.16529123], dtype=float32), 0.64468545]. 
=============================================
[2019-04-23 10:07:45,043] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:45,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5606
[2019-04-23 10:07:45,063] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.68333333333333, 90.66666666666667, 1.0, 2.0, 0.5344547091604308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746834.3436235257, 746834.3436235257, 189170.4358686113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2152200.0000, 
sim time next is 2152800.0000, 
raw observation next is [26.6, 91.0, 1.0, 2.0, 0.5327190006761872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744408.0558479098, 744408.0558479098, 188881.2250719339], 
processed observation next is [0.0, 0.9565217391304348, 0.4597156398104266, 0.91, 1.0, 1.0, 0.43701084418817726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20678001551330827, 0.20678001551330827, 0.281912276226767], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.2864439], dtype=float32), -0.42539585]. 
=============================================
[2019-04-23 10:07:52,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:52,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6016
[2019-04-23 10:07:52,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 85.0, 1.0, 2.0, 0.5202554624139636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726985.8629665411, 726985.8629665418, 186830.5868944517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2244600.0000, 
sim time next is 2245200.0000, 
raw observation next is [27.1, 85.0, 1.0, 2.0, 0.5183004901568588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724253.125288148, 724253.1252881474, 186513.1048810808], 
processed observation next is [1.0, 1.0, 0.4834123222748816, 0.85, 1.0, 1.0, 0.4196391447672997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20118142369115222, 0.20118142369115205, 0.2783777684792251], 
reward next is 0.7216, 
noisyNet noise sample is [array([-1.6560458], dtype=float32), -0.82045245]. 
=============================================
[2019-04-23 10:07:57,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:57,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9718
[2019-04-23 10:07:57,778] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.43333333333334, 69.33333333333334, 1.0, 2.0, 0.5688671780349456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794939.5060654316, 794939.5060654316, 195089.9930337572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2312400.0000, 
sim time next is 2313000.0000, 
raw observation next is [31.3, 70.0, 1.0, 2.0, 0.5691510590202485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795336.3521258673, 795336.352125868, 195140.2342272678], 
processed observation next is [1.0, 0.782608695652174, 0.6824644549763034, 0.7, 1.0, 1.0, 0.4809048903858416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22092676447940757, 0.22092676447940776, 0.29125408093622057], 
reward next is 0.7087, 
noisyNet noise sample is [array([-0.67537564], dtype=float32), -0.1822483]. 
=============================================
[2019-04-23 10:07:57,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.18768 ]
 [64.60584 ]
 [64.51973 ]
 [64.49762 ]
 [64.452385]], R is [[63.93965912]
 [64.00908661]
 [64.07962036]
 [64.15209961]
 [64.22558594]].
[2019-04-23 10:07:59,683] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 10:07:59,685] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:07:59,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,688] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:07:59,689] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,689] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:07:59,691] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,691] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:07:59,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:07:59,694] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,697] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,737] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,738] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,757] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,800] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run12
[2019-04-23 10:08:11,445] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:11,446] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 50.33333333333334, 1.0, 2.0, 0.5445268599407027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872902.984441503, 872902.9844415024, 203283.7117461775]
[2019-04-23 10:08:11,447] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:08:11,453] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6093949340112617
[2019-04-23 10:08:15,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:15,505] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.0, 60.0, 1.0, 2.0, 0.3479882098658624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537665.9709052722, 537665.9709052715, 169671.9400954718]
[2019-04-23 10:08:15,505] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:08:15,509] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8566934088955152
[2019-04-23 10:08:23,697] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:23,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.95341154333333, 91.05955045166667, 1.0, 2.0, 0.6168149212529731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861969.2503652268, 861969.2503652268, 203920.7520631097]
[2019-04-23 10:08:23,700] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:08:23,704] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9460381904428234
[2019-04-23 10:08:31,195] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:31,196] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.9, 90.5, 1.0, 2.0, 0.645481610843356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 980669.2851604467, 980669.285160446, 219343.8134598392]
[2019-04-23 10:08:31,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:08:31,199] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03601436036800432
[2019-04-23 10:08:45,306] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:45,306] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.2, 53.0, 1.0, 2.0, 0.557469596149129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779006.594262574, 779006.594262574, 193090.0051797389]
[2019-04-23 10:08:45,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:08:45,310] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2960345722198069
[2019-04-23 10:09:12,884] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:09:12,885] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.48333333333333, 75.33333333333334, 1.0, 2.0, 0.5478595057094756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765572.6388595215, 765572.6388595221, 191434.4672676616]
[2019-04-23 10:09:12,885] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:09:12,887] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5513436801389294
[2019-04-23 10:09:42,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:09:42,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.96123649, 89.91220284666667, 1.0, 2.0, 0.5813982477806315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812457.2194660412, 812457.2194660412, 197331.1953135226]
[2019-04-23 10:09:42,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:09:42,768] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05164548975690941
[2019-04-23 10:10:02,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:10:02,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.73333333333333, 75.33333333333334, 1.0, 2.0, 0.4659669663162758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665474.4254368199, 665474.4254368205, 180278.3304316765]
[2019-04-23 10:10:02,448] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:10:02,450] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6900982730155261
[2019-04-23 10:10:10,031] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:10:10,066] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:10:10,258] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:10:10,302] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:10:10,425] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:10:11,441] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 275000, evaluation results [275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:10:21,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:21,221] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-23 10:10:21,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1796217.904786802 W.
[2019-04-23 10:10:21,236] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 84.66666666666667, 1.0, 2.0, 0.6423987578056709, 1.0, 2.0, 0.6423987578056709, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1796217.904786802, 1796217.904786802, 350438.2880722096], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [27.6, 84.33333333333333, 1.0, 2.0, 0.6221227319954996, 1.0, 2.0, 0.6221227319954996, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1739477.879708518, 1739477.879708518, 342565.448613683], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.8433333333333333, 1.0, 1.0, 0.544726183127108, 1.0, 1.0, 0.544726183127108, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4831882999190328, 0.4831882999190328, 0.5112917143487806], 
reward next is 0.4887, 
noisyNet noise sample is [array([0.24830467], dtype=float32), -0.2677218]. 
=============================================
[2019-04-23 10:10:27,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:27,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-23 10:10:27,728] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 89.66666666666667, 1.0, 2.0, 0.5173097320172007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722868.206990218, 722868.206990218, 186352.1543742471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2586000.0000, 
sim time next is 2586600.0000, 
raw observation next is [26.1, 90.0, 1.0, 2.0, 0.5143747832153845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718765.6378186536, 718765.6378186536, 185878.5886857517], 
processed observation next is [1.0, 0.9565217391304348, 0.4360189573459717, 0.9, 1.0, 1.0, 0.41490937736793315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19965712161629268, 0.19965712161629268, 0.2774307293817189], 
reward next is 0.7226, 
noisyNet noise sample is [array([1.9811676], dtype=float32), -0.38159505]. 
=============================================
[2019-04-23 10:10:33,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:33,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8334
[2019-04-23 10:10:33,091] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3949073917860469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 589260.5812168231, 589260.5812168224, 173549.8204274147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2676600.0000, 
sim time next is 2677200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3950110177672582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589415.3091307866, 589415.3091307866, 173564.0042775566], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27109761176778097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16372647475855182, 0.16372647475855182, 0.2590507526530696], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.43474853], dtype=float32), -0.7452561]. 
=============================================
[2019-04-23 10:10:38,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:38,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2811
[2019-04-23 10:10:38,049] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3841903961835303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578664.1240266134, 578664.1240266134, 172755.6668044197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2725200.0000, 
sim time next is 2725800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3840836943958654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578503.4916854348, 578503.4916854348, 172741.3072146917], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25793216192272944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069541435706522, 0.16069541435706522, 0.2578228465890921], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.14040506], dtype=float32), -0.9038085]. 
=============================================
[2019-04-23 10:10:42,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:42,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1613
[2019-04-23 10:10:42,783] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.3513653200284982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542214.1043638098, 542214.1043638104, 170026.38428374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3488649461903132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537651.3392306102, 537651.3392306102, 169630.7982640818], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21549993516905205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14934759423072505, 0.14934759423072505, 0.25318029591654], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.8481115], dtype=float32), 0.9150287]. 
=============================================
[2019-04-23 10:10:46,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:46,353] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2268
[2019-04-23 10:10:46,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 90.66666666666667, 1.0, 2.0, 0.3643887805136739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556863.0691090797, 556863.0691090804, 171098.9585283331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2848800.0000, 
sim time next is 2849400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.3613194830044288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 553177.8879240341, 553177.8879240347, 170815.9815989881], 
processed observation next is [1.0, 1.0, 0.2654028436018958, 0.915, 1.0, 1.0, 0.23050540121015517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15366052442334283, 0.153660524423343, 0.2549492262671464], 
reward next is 0.7451, 
noisyNet noise sample is [array([1.2474526], dtype=float32), -0.18079893]. 
=============================================
[2019-04-23 10:10:46,373] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:46,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1829
[2019-04-23 10:10:46,396] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3625740804952334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558502.2154738517, 558502.2154738524, 171362.3075981955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2860200.0000, 
sim time next is 2860800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3608919693442823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555931.4741309017, 555931.4741309017, 171145.0564798836], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2299903245111835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15442540948080605, 0.15442540948080605, 0.25544038280579645], 
reward next is 0.7446, 
noisyNet noise sample is [array([1.1588892], dtype=float32), 1.2013886]. 
=============================================
[2019-04-23 10:10:48,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:48,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-23 10:10:48,954] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.6692399577254918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027449.271806899, 1027449.271806899, 225822.4052598489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2902800.0000, 
sim time next is 2903400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.6907842199617018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1058590.6258932, 1058590.625893201, 230597.951246472], 
processed observation next is [1.0, 0.6086956521739131, 0.2654028436018958, 0.915, 1.0, 1.0, 0.6274508674237371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.294052951637, 0.2940529516370003, 0.34417604663652535], 
reward next is 0.6558, 
noisyNet noise sample is [array([0.18706751], dtype=float32), 0.3892721]. 
=============================================
[2019-04-23 10:10:50,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:50,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1888
[2019-04-23 10:10:50,046] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.08333333333334, 99.5, 1.0, 2.0, 0.3084631333891708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490992.1554524192, 490992.1554524192, 166413.9369656206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2923800.0000, 
sim time next is 2924400.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.308800462644727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491248.2889387395, 491248.2889387395, 166428.6719962212], 
processed observation next is [1.0, 0.8695652173913043, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16722947306593614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13645785803853874, 0.13645785803853874, 0.24840100297943463], 
reward next is 0.7516, 
noisyNet noise sample is [array([1.5095273], dtype=float32), -0.17214331]. 
=============================================
[2019-04-23 10:10:54,193] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:54,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5172
[2019-04-23 10:10:54,212] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5497304460693864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869077.117597409, 869077.117597409, 203339.2766194669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2991600.0000, 
sim time next is 2992200.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.6323485296520729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 999692.4680557448, 999692.4680557442, 220503.448472093], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.5570464212675577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2776923522377069, 0.2776923522377067, 0.3291096245852134], 
reward next is 0.6709, 
noisyNet noise sample is [array([-0.32147932], dtype=float32), -1.3864592]. 
=============================================
[2019-04-23 10:10:59,681] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:59,693] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9084
[2019-04-23 10:10:59,701] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4293043876365769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623128.9560877865, 623128.9560877858, 176247.5925242753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3091800.0000, 
sim time next is 3092400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4290735432712089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622794.3850525762, 622794.3850525768, 176215.0042449138], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3121367991219385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1729984402923823, 0.17299844029238243, 0.2630074690222594], 
reward next is 0.7370, 
noisyNet noise sample is [array([-0.49690524], dtype=float32), -0.5324249]. 
=============================================
[2019-04-23 10:11:03,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:11:03,216] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3296
[2019-04-23 10:11:03,219] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 69.66666666666666, 1.0, 2.0, 0.5884164292016372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822268.358886864, 822268.358886864, 198608.6892002957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3332400.0000, 
sim time next is 3333000.0000, 
raw observation next is [32.0, 70.33333333333334, 1.0, 2.0, 0.5922542599998402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 827633.5325379958, 827633.5325379951, 199312.7587013185], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.7033333333333335, 1.0, 1.0, 0.508740072288964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22989820348277662, 0.22989820348277643, 0.297481729404953], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.9178076], dtype=float32), 0.9819553]. 
=============================================
[2019-04-23 10:11:03,226] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.041176]
 [73.00806 ]
 [72.98877 ]
 [72.971275]
 [73.002655]], R is [[73.02402496]
 [72.9973526 ]
 [72.97180176]
 [72.94740295]
 [72.92346191]].
[2019-04-23 10:11:04,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:11:04,361] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5177
[2019-04-23 10:11:04,363] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.4920327516193084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687535.7214298253, 687535.7214298247, 182357.0076422961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3181800.0000, 
sim time next is 3182400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4913679868666576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686606.5206404879, 686606.5206404879, 182254.3936055178], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.387190345622479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19072403351124664, 0.19072403351124664, 0.27202148299331014], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.7041428], dtype=float32), 0.19914801]. 
=============================================
[2019-04-23 10:11:08,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:11:08,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9133
[2019-04-23 10:11:08,584] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.5817994424674762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813018.0712923688, 813018.0712923688, 197404.7590774237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3252000.0000, 
sim time next is 3252600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5804348998496559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811110.5043199442, 811110.5043199449, 197158.2650330917], 
processed observation next is [0.0, 0.6521739130434783, 0.7630331753554502, 0.63, 1.0, 1.0, 0.4944998793369348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22530847342220672, 0.22530847342220692, 0.29426606721356974], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.50958186], dtype=float32), -0.31186077]. 
=============================================
[2019-04-23 10:11:09,372] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-23 10:11:09,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:11:09,375] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:11:09,376] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,376] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:11:09,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,377] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,378] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:11:09,379] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:11:09,379] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,380] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,399] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,400] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,448] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,448] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run13
[2019-04-23 10:11:14,745] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:11:14,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.06666666666667, 89.33333333333334, 1.0, 2.0, 0.2549050236881701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415034.4809460548, 415034.4809460548, 161326.1703894624]
[2019-04-23 10:11:14,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:11:14,749] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21328225567161552
[2019-04-23 10:12:29,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:12:29,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.76666666666667, 77.5, 1.0, 2.0, 0.5792351173049398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809433.2670798957, 809433.2670798957, 196940.6422608315]
[2019-04-23 10:12:29,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:12:29,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.36646481896220506
[2019-04-23 10:12:31,766] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:12:31,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.13333333333334, 77.33333333333334, 1.0, 2.0, 0.5849246423122585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817386.9679044632, 817386.9679044632, 197970.9101162657]
[2019-04-23 10:12:31,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:12:31,770] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1013578059788921
[2019-04-23 10:12:51,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:12:51,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.25, 88.5, 1.0, 2.0, 0.5898681718824761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824297.8489187291, 824297.8489187291, 198872.7697888904]
[2019-04-23 10:12:51,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:12:51,648] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4864260195757609
[2019-04-23 10:13:16,483] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:13:17,439] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:13:17,571] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:13:17,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:13:17,837] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:13:18,854] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 300000, evaluation results [300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:13:19,183] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:19,193] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0279
[2019-04-23 10:13:19,201] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5197891354897839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726334.0120772389, 726334.0120772389, 186754.518086806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3445800.0000, 
sim time next is 3446400.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5198032636949518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726353.76105566, 726353.76105566, 186756.7224078644], 
processed observation next is [1.0, 0.9130434782608695, 0.5102685624012641, 0.8066666666666668, 1.0, 1.0, 0.4214497152951226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20176493362657222, 0.20176493362657222, 0.27874137672815585], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.58424723], dtype=float32), -0.797]. 
=============================================
[2019-04-23 10:13:23,665] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:23,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9652
[2019-04-23 10:13:23,684] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5968682171159964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834083.7436432667, 834083.7436432667, 200164.2127618514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3337800.0000, 
sim time next is 3338400.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5964550567010019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833506.1528253664, 833506.1528253664, 200087.6766423961], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5138012731337371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2315294868959351, 0.2315294868959351, 0.29863832334685986], 
reward next is 0.7014, 
noisyNet noise sample is [array([-1.4053011], dtype=float32), -1.5955559]. 
=============================================
[2019-04-23 10:13:23,753] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:23,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-23 10:13:23,773] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5350653751139779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747687.9740799333, 747687.9740799333, 189272.5834599001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3362400.0000, 
sim time next is 3363000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5368056235281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750120.615587785, 750120.6155877844, 189563.6615732112], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4419344861785306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20836683766327363, 0.20836683766327346, 0.2829308381689719], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.31653222], dtype=float32), -0.16907473]. 
=============================================
[2019-04-23 10:13:23,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.773155]
 [71.72623 ]
 [71.739845]
 [71.73015 ]
 [71.73069 ]], R is [[71.72959137]
 [71.72980499]
 [71.73075867]
 [71.73235321]
 [71.73438263]].
[2019-04-23 10:13:25,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:25,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3580
[2019-04-23 10:13:25,598] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.00000000000001, 1.0, 2.0, 0.5529439566125797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772680.1768895868, 772680.1768895861, 192306.9920530774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3528600.0000, 
sim time next is 3529200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.550857715978353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769763.821872252, 769763.821872252, 191948.1791811568], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4588647180462084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21382328385340335, 0.21382328385340335, 0.28648981967336834], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.9914334], dtype=float32), 0.110124424]. 
=============================================
[2019-04-23 10:13:38,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:38,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0143
[2019-04-23 10:13:38,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2028186.858156472 W.
[2019-04-23 10:13:38,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.8093697736766838, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991563694953897, 6.9112, 168.912477560935, 2028186.858156472, 1971174.248709893, 410705.5414852551], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3577200.0000, 
sim time next is 3577800.0000, 
raw observation next is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.79237666067048, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989364403948253, 6.9112, 168.9124259383805, 2004404.314993207, 1948951.970802914, 406583.9403263928], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.6666666666666665, 1.0, 1.0, 0.7498513983981686, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007816440394825274, 0.0, 0.8294373398006377, 0.556778976387002, 0.5413755474452538, 0.6068417019796908], 
reward next is 0.0023, 
noisyNet noise sample is [array([2.303847], dtype=float32), -0.16948862]. 
=============================================
[2019-04-23 10:13:50,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:50,113] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-23 10:13:50,121] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 76.5, 1.0, 2.0, 0.7581051865048776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1068852.72550017, 1068852.72550017, 235174.7294630643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3738600.0000, 
sim time next is 3739200.0000, 
raw observation next is [27.33333333333333, 75.66666666666666, 1.0, 2.0, 0.7779722481120304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087510.913913973, 1087510.913913973, 238602.1432485454], 
processed observation next is [1.0, 0.2608695652173913, 0.494470774091627, 0.7566666666666666, 1.0, 1.0, 0.7324966844723257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3020863649761036, 0.3020863649761036, 0.35612260186350064], 
reward next is 0.6439, 
noisyNet noise sample is [array([2.454161], dtype=float32), 0.42475387]. 
=============================================
[2019-04-23 10:13:50,919] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:50,921] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5648
[2019-04-23 10:13:50,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2528869.450373106 W.
[2019-04-23 10:13:50,932] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666666, 63.5, 1.0, 2.0, 0.6027560765562527, 1.0, 2.0, 0.6027560765562527, 1.0, 1.0, 1.03, 6.930072308021879, 6.9112, 170.5573041426782, 2528869.450373106, 2515350.448460625, 488979.9525018409], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3757800.0000, 
sim time next is 3758400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.6794876499962594, 1.0, 2.0, 0.6603338645123923, 1.0, 2.0, 1.03, 7.005096114832019, 6.9112, 170.5573041426782, 2770705.526044153, 2703443.919540226, 514928.2387295628], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.63, 1.0, 1.0, 0.613840542164168, 1.0, 1.0, 0.5907636921836051, 1.0, 1.0, 1.0365853658536586, 0.009389611483201943, 0.0, 0.8375144448122397, 0.7696404239011536, 0.7509566443167295, 0.7685496100441236], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.16749541], dtype=float32), -0.37918893]. 
=============================================
[2019-04-23 10:13:51,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:51,203] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1022
[2019-04-23 10:13:51,208] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 79.0, 1.0, 2.0, 0.4825924020485993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695678.6407714129, 695678.6407714129, 183628.2395879161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3736200.0000, 
sim time next is 3736800.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.486988359873998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702015.3451218415, 702015.3451218408, 184320.2300252992], 
processed observation next is [1.0, 0.2608695652173913, 0.4312796208530806, 0.79, 1.0, 1.0, 0.3819136865951783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19500426253384487, 0.19500426253384467, 0.2751048209332824], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.8937628], dtype=float32), -0.2416629]. 
=============================================
[2019-04-23 10:13:52,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:52,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-23 10:13:52,738] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.66666666666667, 1.0, 2.0, 0.5874176855503325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820872.1493224866, 820872.1493224866, 198425.2997707888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5831125585352475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814853.7495796885, 814853.7495796885, 197641.9999554863], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.67, 1.0, 1.0, 0.49772597413885233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2263482637721357, 0.2263482637721357, 0.2949880596350542], 
reward next is 0.7050, 
noisyNet noise sample is [array([-0.06134652], dtype=float32), 1.5017084]. 
=============================================
[2019-04-23 10:13:52,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.688644]
 [72.607666]
 [72.50015 ]
 [72.35144 ]
 [72.12762 ]], R is [[72.80142212]
 [72.7772522 ]
 [72.75221252]
 [72.72635651]
 [72.69854736]].
[2019-04-23 10:13:58,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:58,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4594
[2019-04-23 10:13:58,509] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.66666666666667, 1.0, 2.0, 0.6045622374870508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844839.9029176588, 844839.9029176594, 201599.4660853402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5995248817101846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837797.7203946159, 837797.7203946159, 200657.9150921838], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5174998574821501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23272158899850443, 0.23272158899850443, 0.2994894255107221], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.2527415], dtype=float32), -0.6980992]. 
=============================================
[2019-04-23 10:14:03,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:14:03,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8997
[2019-04-23 10:14:03,316] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.66666666666667, 1.0, 2.0, 0.5874176855503325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820872.1493224866, 820872.1493224866, 198425.2997707888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5831125585352475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814853.7495796885, 814853.7495796885, 197641.9999554863], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.67, 1.0, 1.0, 0.49772597413885233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2263482637721357, 0.2263482637721357, 0.2949880596350542], 
reward next is 0.7050, 
noisyNet noise sample is [array([1.4173863], dtype=float32), -0.80398715]. 
=============================================
[2019-04-23 10:14:03,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.535416]
 [68.459404]
 [68.35998 ]
 [68.21987 ]
 [68.00691 ]], R is [[68.68315125]
 [68.70016479]
 [68.71589661]
 [68.73040771]
 [68.74256134]].
[2019-04-23 10:14:10,212] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:14:10,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4424
[2019-04-23 10:14:10,222] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 85.0, 1.0, 2.0, 0.5417378231236277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757015.2210022269, 757015.2210022275, 190394.0819614282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4064400.0000, 
sim time next is 4065000.0000, 
raw observation next is [27.76666666666667, 85.16666666666667, 1.0, 2.0, 0.5412760465771025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756369.7125669847, 756369.7125669853, 190316.0600203178], 
processed observation next is [1.0, 0.043478260869565216, 0.515007898894155, 0.8516666666666667, 1.0, 1.0, 0.4473205380447018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21010269793527353, 0.2101026979352737, 0.28405382092584747], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.32365757], dtype=float32), 0.3579674]. 
=============================================
[2019-04-23 10:14:10,245] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[56.704704]
 [56.651108]
 [56.670403]
 [56.730286]
 [56.743942]], R is [[56.74113846]
 [56.88955688]
 [57.03650284]
 [57.18194199]
 [57.32579803]].
[2019-04-23 10:14:18,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:14:18,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0252
[2019-04-23 10:14:18,497] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3468371.453389085 W.
[2019-04-23 10:14:18,505] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.66666666666667, 65.0, 1.0, 2.0, 1.011571745565208, 1.0, 2.0, 0.8263759122968666, 1.0, 2.0, 1.03, 7.0051223121691, 6.9112, 170.5573041426782, 3468371.453389085, 3401091.080666969, 637829.0843416115], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4116000.0000, 
sim time next is 4116600.0000, 
raw observation next is [35.5, 65.5, 1.0, 2.0, 1.004732078630981, 1.0, 2.0, 0.8229560788297532, 1.0, 2.0, 1.03, 7.005121772308661, 6.9112, 170.5573041426782, 3453998.261891277, 3386718.275893153, 634882.3626573604], 
processed observation next is [1.0, 0.6521739130434783, 0.8815165876777251, 0.655, 1.0, 1.0, 1.0057012995553989, 1.0, 1.0, 0.7866940708792207, 1.0, 1.0, 1.0365853658536586, 0.009392177230866139, 0.0, 0.8375144448122397, 0.9594439616364658, 0.940755076636987, 0.947585615906508], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6057465], dtype=float32), 2.6423419]. 
=============================================
[2019-04-23 10:14:25,377] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 10:14:25,379] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:14:25,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:14:25,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:14:25,385] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,386] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:14:25,389] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,388] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:14:25,390] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,392] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,408] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,444] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,446] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,489] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run14
[2019-04-23 10:14:33,155] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:33,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.23484933333333, 42.11325310333334, 1.0, 2.0, 0.3264115394861993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 529246.960260036, 529246.9602600366, 169247.453800797]
[2019-04-23 10:14:33,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:14:33,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.36922485829218843
[2019-04-23 10:14:45,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:45,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.35, 77.5, 1.0, 2.0, 0.3263948956647002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513931.2196525275, 513931.2196525269, 168039.3934773798]
[2019-04-23 10:14:45,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:14:45,622] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26469325736990246
[2019-04-23 10:14:46,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:46,530] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.26824027666667, 93.71724167333333, 1.0, 2.0, 0.2707529227719555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445327.9625740654, 445327.9625740654, 163025.3367552402]
[2019-04-23 10:14:46,530] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:14:46,532] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5350512042021871
[2019-04-23 10:14:54,717] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:54,720] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.13333333333334, 89.66666666666667, 1.0, 2.0, 0.4761794893569412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667543.9311283854, 667543.9311283854, 180227.6367925566]
[2019-04-23 10:14:54,739] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:14:54,742] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5374738831070744
[2019-04-23 10:15:10,372] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:15:10,373] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.63333333333333, 95.0, 1.0, 2.0, 0.4917218843777851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687101.1945504928, 687101.1945504935, 182308.3853727541]
[2019-04-23 10:15:10,374] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:15:10,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.603433884258899
[2019-04-23 10:15:24,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:15:24,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333333, 72.33333333333334, 1.0, 2.0, 0.9964681212858892, 1.0, 2.0, 0.9964681212858892, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2787416.410930124, 2787416.410930123, 526860.1557280621]
[2019-04-23 10:15:24,444] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:15:24,446] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.594743829227054
[2019-04-23 10:15:24,447] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2787416.410930124 W.
[2019-04-23 10:15:42,959] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:15:42,960] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.45, 92.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.388211114558344, 6.9112, 168.9099225703471, 1792388.732774632, 1453986.708988603, 311352.9593223593]
[2019-04-23 10:15:42,962] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:15:42,963] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5557077283372885
[2019-04-23 10:15:42,964] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1792388.732774632 W.
[2019-04-23 10:15:55,824] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:15:55,825] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.925284375, 80.05565027, 1.0, 2.0, 0.9768324609161791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1365399.948129619, 1365399.948129618, 291943.1686049504]
[2019-04-23 10:15:55,826] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:15:55,829] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32726153460345864
[2019-04-23 10:16:02,415] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:16:02,418] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.97479703, 93.708651355, 1.0, 2.0, 0.5167457894279242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722079.9080293713, 722079.9080293719, 186259.4176857271]
[2019-04-23 10:16:02,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:16:02,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7831854200092154
[2019-04-23 10:16:03,337] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:16:03,544] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:16:03,580] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:16:03,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:16:03,686] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:16:04,700] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 325000, evaluation results [325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:16:06,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:06,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-23 10:16:06,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2967240.031496293 W.
[2019-04-23 10:16:06,952] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 48.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 6.991947749970717, 6.9112, 170.5573041426782, 2967240.031496293, 2909397.133583805, 553223.7355734549], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4294800.0000, 
sim time next is 4295400.0000, 
raw observation next is [36.83333333333334, 48.33333333333333, 1.0, 2.0, 0.4448340137783953, 1.0, 2.0, 0.4448340137783953, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1243484.600139249, 1243484.600139249, 284024.0362755536], 
processed observation next is [1.0, 0.7391304347826086, 0.9447077409162722, 0.4833333333333333, 1.0, 1.0, 0.3311253178052956, 1.0, 1.0, 0.3311253178052956, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3454123889275692, 0.3454123889275692, 0.42391647205306504], 
reward next is 0.5761, 
noisyNet noise sample is [array([-0.75807685], dtype=float32), -1.8932053]. 
=============================================
[2019-04-23 10:16:08,688] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:08,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6405
[2019-04-23 10:16:08,697] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6239946049151847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872006.63247808, 872006.6324780794, 205301.6594285692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4316400.0000, 
sim time next is 4317000.0000, 
raw observation next is [31.0, 79.00000000000001, 1.0, 2.0, 0.6264043020017939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875375.4721404426, 875375.4721404426, 205768.4096034746], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.7900000000000001, 1.0, 1.0, 0.5498847012069805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24315985337234516, 0.24315985337234516, 0.3071170292589173], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.13007514], dtype=float32), -0.020046063]. 
=============================================
[2019-04-23 10:16:08,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.08011 ]
 [63.057156]
 [63.095016]
 [63.113785]
 [63.12652 ]], R is [[63.11087799]
 [63.17335129]
 [63.23549652]
 [63.29731369]
 [63.35871124]].
[2019-04-23 10:16:11,295] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:11,302] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3190
[2019-04-23 10:16:11,306] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 79.00000000000001, 1.0, 2.0, 0.520589465356836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727452.7462166852, 727452.7462166859, 186885.1744092829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4529400.0000, 
sim time next is 4530000.0000, 
raw observation next is [28.33333333333334, 79.0, 1.0, 2.0, 0.5251308930394936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733800.9636913787, 733800.9636913781, 187627.6321228274], 
processed observation next is [0.0, 0.43478260869565216, 0.5418641390205374, 0.79, 1.0, 1.0, 0.4278685458307151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20383360102538298, 0.2038336010253828, 0.28004124197436925], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.52107435], dtype=float32), -0.9768468]. 
=============================================
[2019-04-23 10:16:11,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.949   ]
 [64.97456 ]
 [64.912895]
 [64.90102 ]
 [64.86649 ]], R is [[65.00250244]
 [65.07354736]
 [65.14522552]
 [65.21718597]
 [65.28936005]].
[2019-04-23 10:16:15,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:15,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1139
[2019-04-23 10:16:15,206] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 86.5, 1.0, 2.0, 0.615433754593709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860038.3547904453, 860038.3547904453, 203656.5748547878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4415400.0000, 
sim time next is 4416000.0000, 
raw observation next is [29.33333333333334, 87.33333333333333, 1.0, 2.0, 0.61397848311866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858003.8631620125, 858003.8631620125, 203379.1088855984], 
processed observation next is [0.0, 0.08695652173913043, 0.5892575039494474, 0.8733333333333333, 1.0, 1.0, 0.5349138350827228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23833440643389237, 0.23833440643389237, 0.3035509087844752], 
reward next is 0.6964, 
noisyNet noise sample is [array([-0.32616684], dtype=float32), -1.4140708]. 
=============================================
[2019-04-23 10:16:15,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.46773]
 [71.54212]
 [71.55203]
 [71.33948]
 [71.36418]], R is [[71.42842102]
 [71.41017151]
 [71.39162445]
 [71.37307739]
 [71.35514832]].
[2019-04-23 10:16:16,917] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:16,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2450
[2019-04-23 10:16:16,930] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 79.00000000000001, 1.0, 2.0, 0.5937601008905806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829738.660852878, 829738.6608528786, 199588.4261845828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [29.66666666666667, 79.0, 1.0, 2.0, 0.5847947394891898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 817205.3689186447, 817205.368918644, 197946.8030978707], 
processed observation next is [0.0, 0.782608695652174, 0.6050552922590839, 0.79, 1.0, 1.0, 0.4997526981797467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2270014913662902, 0.22700149136629, 0.29544298969831445], 
reward next is 0.7046, 
noisyNet noise sample is [array([0.734477], dtype=float32), 0.6036905]. 
=============================================
[2019-04-23 10:16:18,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:18,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8785
[2019-04-23 10:16:18,576] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5264601328612228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735659.0437157999, 735659.0437158006, 187844.5504560716], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4460400.0000, 
sim time next is 4461000.0000, 
raw observation next is [30.33333333333333, 66.83333333333333, 1.0, 2.0, 0.5300821452160126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740722.0960106722, 740722.0960106716, 188443.2998642867], 
processed observation next is [0.0, 0.6521739130434783, 0.6366508688783569, 0.6683333333333333, 1.0, 1.0, 0.4338339098988103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20575613778074225, 0.20575613778074212, 0.2812586565138607], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.8856838], dtype=float32), 1.0693555]. 
=============================================
[2019-04-23 10:16:18,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.521645]
 [78.32829 ]
 [78.19136 ]
 [78.02597 ]
 [77.843735]], R is [[78.46305084]
 [78.39805603]
 [78.32991028]
 [78.25854492]
 [78.18405914]].
[2019-04-23 10:16:39,565] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:39,574] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0757
[2019-04-23 10:16:39,601] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5578854881691763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779587.9739847527, 779587.9739847521, 193163.1672444348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4646400.0000, 
sim time next is 4647000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5586445498791025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780649.0749579842, 780649.0749579842, 193295.1489402235], 
processed observation next is [1.0, 0.782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.46824644563747286, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2168469652661067, 0.2168469652661067, 0.28850022229884104], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.52354825], dtype=float32), 0.5327855]. 
=============================================
[2019-04-23 10:16:39,613] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[54.70618 ]
 [54.588814]
 [54.819393]
 [54.75614 ]
 [54.090576]], R is [[54.87101364]
 [55.0340004 ]
 [55.19492722]
 [55.35440063]
 [55.5137558 ]].
[2019-04-23 10:16:46,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:16:46,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3253
[2019-04-23 10:16:46,239] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5437312217850644, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9314094933357456, 6.9112, 6.9112, 168.9129460415061, 1520147.396299583, 1520147.396299583, 330295.3872784373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4710600.0000, 
sim time next is 4711200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.50012507314503, 6.9112, 168.9039652772707, 2581667.164626897, 1454489.03401967, 310472.528830925], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.15889250731450302, 0.0, 0.8293957940812812, 0.7171297679519157, 0.40402473167213054, 0.4633918340760075], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11803323], dtype=float32), -1.1679953]. 
=============================================
[2019-04-23 10:17:04,684] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:17:04,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5880
[2019-04-23 10:17:04,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2305795.633236536 W.
[2019-04-23 10:17:04,768] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 1.007711436767631, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.992086779465314, 6.9112, 168.9124757841552, 2305795.633236536, 2248411.931290225, 466208.9997123772], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.6700666154738013, 1.0, 1.0, 0.6700666154738013, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1873647.931856131, 1873647.931856131, 361573.7525374222], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.6024898981612063, 1.0, 0.5, 0.6024898981612063, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5204577588489253, 0.5204577588489253, 0.5396623172200331], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.865628], dtype=float32), 0.82682896]. 
=============================================
[2019-04-23 10:17:11,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:17:11,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3168
[2019-04-23 10:17:11,807] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5100482194782313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712717.851843456, 712717.8518434553, 185185.4043191342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4921200.0000, 
sim time next is 4921800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5102488134825769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712998.2467218299, 712998.2467218299, 185217.4242867615], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4099383294970806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19805506853384164, 0.19805506853384164, 0.2764439168459127], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.84778696], dtype=float32), -0.9389954]. 
=============================================
[2019-04-23 10:17:20,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:17:20,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7196
[2019-04-23 10:17:20,821] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 79.0, 1.0, 2.0, 0.5097309338202364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712274.3428589283, 712274.3428589288, 185134.5623917884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4998000.0000, 
sim time next is 4998600.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5055472138692355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706426.262918072, 706426.2629180714, 184469.471636405], 
processed observation next is [1.0, 0.8695652173913043, 0.5023696682464456, 0.79, 1.0, 1.0, 0.4042737516496813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19622951747724224, 0.19622951747724204, 0.2753275696065746], 
reward next is 0.7247, 
noisyNet noise sample is [array([-1.0562091], dtype=float32), -1.5835494]. 
=============================================
[2019-04-23 10:17:22,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:17:22,482] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1266
[2019-04-23 10:17:22,517] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 66.5, 1.0, 2.0, 0.4924588773555876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688131.3556795565, 688131.3556795565, 182425.4165266161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4987800.0000, 
sim time next is 4988400.0000, 
raw observation next is [30.33333333333334, 67.66666666666667, 1.0, 2.0, 0.5005539020913174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 699446.56292066, 699446.5629206594, 183685.0184413859], 
processed observation next is [1.0, 0.7391304347826086, 0.6366508688783573, 0.6766666666666667, 1.0, 1.0, 0.39825771336303306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19429071192240555, 0.1942907119224054, 0.27415674394236705], 
reward next is 0.7258, 
noisyNet noise sample is [array([0.4256929], dtype=float32), -0.48184967]. 
=============================================
[2019-04-23 10:17:32,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:17:32,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2138
[2019-04-23 10:17:32,581] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4817170333074691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673240.9676341783, 673240.9676341789, 180796.5302469205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5110200.0000, 
sim time next is 5110800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4808737448191366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672062.9188923375, 672062.9188923368, 180669.213913349], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3745466805049838, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1866841441367604, 0.1866841441367602, 0.2696555431542522], 
reward next is 0.7303, 
noisyNet noise sample is [array([-1.1150341], dtype=float32), -1.3270739]. 
=============================================
[2019-04-23 10:17:48,946] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-23 10:17:48,949] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:17:48,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:17:48,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run15
[2019-04-23 10:17:48,950] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:17:48,981] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:17:48,983] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run15
[2019-04-23 10:17:49,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:17:49,058] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:17:49,059] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run15
[2019-04-23 10:17:49,132] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:17:49,160] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:17:49,163] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run15
[2019-04-23 10:17:49,146] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:17:49,212] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:17:49,214] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run15
[2019-04-23 10:18:20,520] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3075883]
[2019-04-23 10:18:20,522] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.61518741666666, 62.86750667666666, 1.0, 2.0, 0.9551070594486595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1335013.454229786, 1335013.454229786, 285544.1320276433]
[2019-04-23 10:18:20,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:18:20,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5571572609574444
[2019-04-23 10:19:46,700] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:19:47,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:19:47,082] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:19:47,119] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:19:47,162] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:19:48,175] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 350000, evaluation results [350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:19:50,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:19:50,020] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1447
[2019-04-23 10:19:50,024] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.8135634201034204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1137063.029716143, 1137063.029716142, 247265.5454943389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5205600.0000, 
sim time next is 5206200.0000, 
raw observation next is [27.16666666666666, 83.16666666666667, 1.0, 2.0, 0.7675019187544418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1072653.442791051, 1072653.442791051, 236088.4467856895], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.8316666666666667, 1.0, 1.0, 0.7198818298246287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29795928966418084, 0.29795928966418084, 0.35237081609804405], 
reward next is 0.6476, 
noisyNet noise sample is [array([0.59756047], dtype=float32), -1.5114931]. 
=============================================
[2019-04-23 10:19:54,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:19:54,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3090
[2019-04-23 10:19:54,047] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2512544.687680716 W.
[2019-04-23 10:19:54,088] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8983034659081581, 1.0, 1.0, 0.8983034659081581, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2512544.687680716, 2512544.687680716, 470569.8159030639], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5236800.0000, 
sim time next is 5237400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9289138052291641, 1.0, 2.0, 0.9289138052291641, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2598250.482304678, 2598250.482304678, 487513.6481349727], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9143539822038121, 1.0, 1.0, 0.9143539822038121, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7217362450846327, 0.7217362450846327, 0.7276323106492131], 
reward next is 0.2724, 
noisyNet noise sample is [array([2.4854307], dtype=float32), -0.618191]. 
=============================================
[2019-04-23 10:19:56,527] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:19:56,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1654
[2019-04-23 10:19:56,570] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666667, 80.33333333333334, 1.0, 2.0, 0.5446155168730131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761037.9018531352, 761037.9018531352, 190881.7070866035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5260800.0000, 
sim time next is 5261400.0000, 
raw observation next is [28.55, 80.5, 1.0, 2.0, 0.5446991173061282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761154.7657538848, 761154.7657538854, 190895.9316898881], 
processed observation next is [1.0, 0.9130434782608695, 0.552132701421801, 0.805, 1.0, 1.0, 0.4514447196459376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2114318793760791, 0.21143187937607927, 0.2849193010296837], 
reward next is 0.7151, 
noisyNet noise sample is [array([3.242655], dtype=float32), 0.93078494]. 
=============================================
[2019-04-23 10:19:58,557] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:19:58,578] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0703
[2019-04-23 10:19:58,581] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 81.0, 1.0, 2.0, 0.6137134874838422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857633.3956585486, 857633.3956585493, 203328.8708518688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5349600.0000, 
sim time next is 5350200.0000, 
raw observation next is [30.31666666666666, 81.5, 1.0, 2.0, 0.6153620116768689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859938.0570115658, 859938.0570115658, 203642.9743526781], 
processed observation next is [1.0, 0.9565217391304348, 0.6358609794628749, 0.815, 1.0, 1.0, 0.536580736960083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23887168250321272, 0.23887168250321272, 0.30394473783981807], 
reward next is 0.6961, 
noisyNet noise sample is [array([-0.47156549], dtype=float32), -1.6947547]. 
=============================================
[2019-04-23 10:20:13,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9118688e-23 1.0000000e+00 2.3768626e-27 2.2883743e-17 1.3614838e-29], sum to 1.0000
[2019-04-23 10:20:13,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2007
[2019-04-23 10:20:13,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3390713.594238613 W.
[2019-04-23 10:20:13,337] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.86666666666667, 63.33333333333334, 1.0, 2.0, 0.9746163238959848, 1.0, 2.0, 0.807898201462255, 1.0, 2.0, 1.03, 7.005119395395543, 6.9112, 170.5573041426782, 3390713.594238613, 3323435.310920042, 622116.1234840852], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5399400.0000, 
sim time next is 5400000.0000, 
raw observation next is [36.0, 64.0, 1.0, 2.0, 1.025054985921446, 1.0, 2.0, 0.8331175324749852, 1.0, 2.0, 1.03, 7.005123376448536, 6.9112, 170.5573041426782, 3496706.241044334, 3429425.105935565, 643689.3501859115], 
processed observation next is [1.0, 0.5217391304347826, 0.9052132701421801, 0.64, 1.0, 1.0, 1.0301867300258385, 1.0, 1.0, 0.79893678611444, 1.0, 1.0, 1.0365853658536586, 0.009392337644853565, 0.0, 0.8375144448122397, 0.9713072891789817, 0.9526180849821014, 0.9607303734118082], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28151482], dtype=float32), 0.8969637]. 
=============================================
[2019-04-23 10:20:13,467] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[25.348942]
 [25.631075]
 [24.626122]
 [24.929703]
 [24.509312]], R is [[25.49877357]
 [25.24378586]
 [24.99134827]
 [24.7414341 ]
 [24.49402046]].
[2019-04-23 10:20:28,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4511969e-34 0.0000000e+00], sum to 1.0000
[2019-04-23 10:20:28,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0016
[2019-04-23 10:20:28,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2690243.109250204 W.
[2019-04-23 10:20:28,487] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.09999999999999, 54.0, 1.0, 2.0, 0.9617671881026024, 1.0, 2.0, 0.9617671881026024, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2690243.109250204, 2690243.109250204, 506319.4366852856], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5488800.0000, 
sim time next is 5489400.0000, 
raw observation next is [36.2, 53.0, 1.0, 2.0, 0.9508229904470415, 1.0, 2.0, 0.9508229904470415, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2659597.552462756, 2659597.552462756, 499986.149540877], 
processed observation next is [1.0, 0.5217391304347826, 0.9146919431279622, 0.53, 1.0, 1.0, 0.9407505909000501, 1.0, 1.0, 0.9407505909000501, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7387770979063212, 0.7387770979063212, 0.7462479843893687], 
reward next is 0.2538, 
noisyNet noise sample is [array([0.7397376], dtype=float32), 0.5445016]. 
=============================================
[2019-04-23 10:20:35,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:20:35,230] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8516
[2019-04-23 10:20:35,231] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2317898.68133174 W.
[2019-04-23 10:20:35,239] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.1, 48.66666666666666, 1.0, 2.0, 0.5525177842485611, 1.0, 1.0, 0.5525177842485611, 1.0, 2.0, 0.9481828030076447, 6.911200000000001, 6.9112, 170.5573041426782, 2317898.68133174, 2317898.681331739, 451059.0026721738], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5579400.0000, 
sim time next is 5580000.0000, 
raw observation next is [34.2, 48.0, 1.0, 2.0, 0.5657132144232574, 1.0, 2.0, 0.5657132144232574, 1.0, 2.0, 0.9714029325616649, 6.911199999999999, 6.9112, 170.5573041426782, 2373308.144004972, 2373308.144004972, 461261.7603160247], 
processed observation next is [1.0, 0.6086956521739131, 0.8199052132701423, 0.48, 1.0, 1.0, 0.47676290894368356, 1.0, 1.0, 0.47676290894368356, 1.0, 1.0, 0.9651255275142254, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6592522622236033, 0.6592522622236033, 0.6884503885313802], 
reward next is 0.3115, 
noisyNet noise sample is [array([-2.666025], dtype=float32), 0.06842214]. 
=============================================
[2019-04-23 10:20:35,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[37.721416]
 [38.548615]
 [37.443222]
 [37.88988 ]
 [39.249844]], R is [[36.22688293]
 [36.19139099]
 [35.82947922]
 [35.47118378]
 [35.4726181 ]].
[2019-04-23 10:20:54,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:20:54,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4656
[2019-04-23 10:20:54,996] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.06666666666667, 63.0, 1.0, 2.0, 0.5542723323873684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774537.1170702635, 774537.1170702629, 192536.5733243508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5661600.0000, 
sim time next is 5662200.0000, 
raw observation next is [32.13333333333333, 62.5, 1.0, 2.0, 0.5529391610602431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772673.473179553, 772673.473179553, 192306.5209606165], 
processed observation next is [0.0, 0.5217391304347826, 0.7219589257503949, 0.625, 1.0, 1.0, 0.46137248320511215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21463152032765362, 0.21463152032765362, 0.28702465815017386], 
reward next is 0.7130, 
noisyNet noise sample is [array([0.07714252], dtype=float32), 1.9600586]. 
=============================================
[2019-04-23 10:20:59,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:20:59,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1336
[2019-04-23 10:20:59,202] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.1, 60.0, 1.0, 2.0, 0.5354387577611922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748209.9141096384, 748209.9141096377, 189335.2411248458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5673600.0000, 
sim time next is 5674200.0000, 
raw observation next is [32.06666666666667, 59.83333333333333, 1.0, 2.0, 0.5413215084498287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756433.2628154951, 756433.2628154951, 190323.2964620353], 
processed observation next is [0.0, 0.6956521739130435, 0.7187993680884678, 0.5983333333333333, 1.0, 1.0, 0.4473753113853358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.210120350782082, 0.210120350782082, 0.2840646215851273], 
reward next is 0.7159, 
noisyNet noise sample is [array([1.2760693], dtype=float32), -0.091487914]. 
=============================================
[2019-04-23 10:21:06,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:21:06,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1573
[2019-04-23 10:21:06,326] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 87.0, 1.0, 2.0, 0.5118142065993034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715186.3914972004, 715186.3914972009, 185467.6518458023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5723400.0000, 
sim time next is 5724000.0000, 
raw observation next is [26.7, 86.0, 1.0, 2.0, 0.5123498811948195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715935.171580182, 715935.1715801826, 185553.5177551277], 
processed observation next is [0.0, 0.2608695652173913, 0.46445497630331756, 0.86, 1.0, 1.0, 0.41246973637930057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19887088099449501, 0.19887088099449518, 0.2769455488882503], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.04910326], dtype=float32), -0.8745276]. 
=============================================
[2019-04-23 10:21:06,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.22294]
 [68.20708]
 [68.16942]
 [68.1415 ]
 [68.11757]], R is [[68.36936188]
 [68.40885162]
 [68.44808197]
 [68.48707581]
 [68.52583313]].
[2019-04-23 10:21:28,605] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:21:28,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2899
[2019-04-23 10:21:28,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2055688.430529372 W.
[2019-04-23 10:21:28,701] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.46666666666667, 63.66666666666667, 1.0, 2.0, 0.8290197322682801, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983340750047, 6.9112, 168.912316012488, 2055688.430529372, 1988446.121386309, 415120.5997352469], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5834400.0000, 
sim time next is 5835000.0000, 
raw observation next is [32.53333333333333, 63.33333333333333, 1.0, 2.0, 0.4818749869978494, 1.0, 1.0, 0.4818749869978494, 1.0, 2.0, 0.8368574939219237, 6.9112, 6.9112, 170.5573041426782, 2021275.651068567, 2021275.651068567, 402838.6856058252], 
processed observation next is [1.0, 0.5217391304347826, 0.7409162717219588, 0.6333333333333333, 1.0, 1.0, 0.3757529963829511, 1.0, 0.5, 0.3757529963829511, 1.0, 1.0, 0.8010457242950287, 0.0, 0.0, 0.8375144448122397, 0.5614654586301575, 0.5614654586301575, 0.6012517695609332], 
reward next is 0.3987, 
noisyNet noise sample is [array([1.7776779], dtype=float32), 0.50661725]. 
=============================================
[2019-04-23 10:21:28,736] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[44.59473 ]
 [43.44934 ]
 [40.917747]
 [38.114086]
 [37.191982]], R is [[43.56660843]
 [43.1309433 ]
 [42.69963455]
 [42.27264023]
 [41.84991455]].
[2019-04-23 10:22:02,262] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:22:02,262] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9012
[2019-04-23 10:22:02,390] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6074400.0000, 
sim time next is 6075000.0000, 
raw observation next is [28.2, 83.0, 1.0, 2.0, 0.7095382681428593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 991606.1691697689, 991606.1691697689, 222918.5250526618], 
processed observation next is [1.0, 0.30434782608695654, 0.5355450236966824, 0.83, 1.0, 1.0, 0.650046106196216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2754461581027136, 0.2754461581027136, 0.3327142164965102], 
reward next is 0.6673, 
noisyNet noise sample is [array([0.47998434], dtype=float32), -1.615322]. 
=============================================
[2019-04-23 10:22:02,687] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[61.159912]
 [61.309654]
 [60.826828]
 [60.079544]
 [60.285244]], R is [[61.36340714]
 [61.41611481]
 [61.47405243]
 [61.5246315 ]
 [61.55469894]].
[2019-04-23 10:22:05,487] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:22:05,488] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4288
[2019-04-23 10:22:05,489] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2260004.722613769 W.
[2019-04-23 10:22:05,573] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.5387300612734868, 1.0, 2.0, 0.5387300612734868, 1.0, 2.0, 0.9314620763988319, 6.9112, 6.9112, 170.5573041426782, 2260004.722613769, 2260004.722613769, 442151.1276937019], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6090600.0000, 
sim time next is 6091200.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.5614451344448984, 1.0, 2.0, 0.5614451344448984, 1.0, 2.0, 0.9709176046616464, 6.9112, 6.9112, 170.5573041426782, 2355385.608532633, 2355385.608532633, 459389.6713198411], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.65, 1.0, 1.0, 0.47162064390951614, 1.0, 1.0, 0.47162064390951614, 1.0, 1.0, 0.96453366422152, 0.0, 0.0, 0.8375144448122397, 0.6542737801479537, 0.6542737801479537, 0.6856562258505091], 
reward next is 0.3143, 
noisyNet noise sample is [array([-0.7674116], dtype=float32), 0.4893922]. 
=============================================
[2019-04-23 10:22:07,591] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-23 10:22:07,593] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:22:07,594] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:22:07,609] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:22:07,609] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:22:07,610] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run16
[2019-04-23 10:22:07,882] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run16
[2019-04-23 10:22:07,985] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:22:07,985] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:22:07,987] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run16
[2019-04-23 10:22:08,196] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:22:08,214] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:22:08,216] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run16
[2019-04-23 10:22:08,301] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:22:08,301] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:22:08,303] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run16
[2019-04-23 10:22:46,057] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3412795]
[2019-04-23 10:22:46,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.13333333333333, 89.0, 1.0, 2.0, 0.2973304285406252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 475196.2511347563, 475196.251134757, 165303.487228895]
[2019-04-23 10:22:46,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:22:46,085] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8847379987328792
[2019-04-23 10:23:42,889] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3412795]
[2019-04-23 10:23:42,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.3, 66.33333333333333, 1.0, 2.0, 0.5588240866706361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780900.0516301028, 780900.0516301021, 193325.4256175249]
[2019-04-23 10:23:42,891] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:23:42,893] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5947022166532432
[2019-04-23 10:24:00,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3412795]
[2019-04-23 10:24:00,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.53333333333333, 71.33333333333333, 1.0, 2.0, 0.9794115849447333, 1.0, 2.0, 0.9794115849447333, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2739651.910788787, 2739651.910788787, 516677.5786353247]
[2019-04-23 10:24:00,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:24:00,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44159013159867555
[2019-04-23 10:24:00,126] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2739651.910788787 W.
[2019-04-23 10:24:19,940] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3412795]
[2019-04-23 10:24:19,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.98039134166667, 93.84962847833333, 1.0, 2.0, 0.4172034038496066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691219.2267678737, 691219.2267678737, 181884.7396573791]
[2019-04-23 10:24:19,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:24:19,944] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4004863317638304
[2019-04-23 10:24:21,793] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3412795]
[2019-04-23 10:24:21,794] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.13333333333333, 88.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.193416639156082, 6.9112, 168.9108230394727, 2484074.000057637, 2283862.331917429, 475464.9762050391]
[2019-04-23 10:24:21,795] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:24:21,798] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.648606017003237
[2019-04-23 10:24:21,800] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2484074.000057637 W.
[2019-04-23 10:24:23,675] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3412795]
[2019-04-23 10:24:23,676] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.2, 74.0, 1.0, 2.0, 0.7828491315243161, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984441967675316, 6.9112, 168.9124613658662, 1991070.388365133, 1939110.168016802, 404431.1019729472]
[2019-04-23 10:24:23,676] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:24:23,678] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16611153106268206
[2019-04-23 10:24:23,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1991070.388365133 W.
[2019-04-23 10:24:24,212] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:24:24,890] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:24:24,975] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:24:25,119] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:24:25,144] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:24:26,160] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 375000, evaluation results [375000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:24:26,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:26,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5279
[2019-04-23 10:24:26,363] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.13333333333333, 69.83333333333333, 1.0, 2.0, 0.3562021917402006, 0.0, 1.0, 0.0, 1.0, 2.0, 0.610336200231557, 6.9112, 6.9112, 168.912956510431, 995613.5497587607, 995613.5497587607, 240704.9951897306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6109800.0000, 
sim time next is 6110400.0000, 
raw observation next is [29.96666666666667, 70.66666666666667, 1.0, 2.0, 0.4934384427462676, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689500.5835540358, 689500.5835540358, 182577.3944528445], 
processed observation next is [1.0, 0.7391304347826086, 0.6192733017377569, 0.7066666666666667, 1.0, 1.0, 0.3896848707786356, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19152793987612104, 0.19152793987612104, 0.27250357381021567], 
reward next is 0.7275, 
noisyNet noise sample is [array([-0.50676703], dtype=float32), -0.6583311]. 
=============================================
[2019-04-23 10:24:28,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:28,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6568
[2019-04-23 10:24:28,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2103519.509587951 W.
[2019-04-23 10:24:28,058] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.21666666666667, 77.5, 1.0, 2.0, 0.7521942124807139, 1.0, 2.0, 0.7521942124807139, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2103519.509587951, 2103519.509587951, 397269.4274198665], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6173400.0000, 
sim time next is 6174000.0000, 
raw observation next is [29.3, 77.0, 1.0, 2.0, 0.8337413166488459, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.996340802897883, 6.9112, 168.9124496218919, 2062296.741763607, 2001895.107126532, 416710.9299965855], 
processed observation next is [1.0, 0.4782608695652174, 0.5876777251184835, 0.77, 1.0, 1.0, 0.7996883333118625, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008514080289788329, 0.0, 0.8294374560975212, 0.5728602060454464, 0.5560819742018144, 0.6219566119352022], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.73038983], dtype=float32), 2.3047404]. 
=============================================
[2019-04-23 10:24:28,076] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[47.764523]
 [48.076153]
 [49.213596]
 [49.021786]
 [48.07426 ]], R is [[47.9381752 ]
 [47.45879364]
 [46.98420715]
 [46.94724655]
 [46.47777557]].
[2019-04-23 10:24:28,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:28,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6577
[2019-04-23 10:24:28,305] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 91.66666666666667, 1.0, 2.0, 0.846577179794729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1183229.856326668, 1183229.856326669, 255664.8578708396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6153000.0000, 
sim time next is 6153600.0000, 
raw observation next is [26.63333333333333, 91.33333333333334, 1.0, 2.0, 0.738013600574525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031420.832863585, 1031420.832863585, 229264.418308013], 
processed observation next is [1.0, 0.21739130434782608, 0.46129541864139006, 0.9133333333333334, 1.0, 1.0, 0.6843537356319579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2865057869065514, 0.2865057869065514, 0.3421856989671836], 
reward next is 0.6578, 
noisyNet noise sample is [array([-0.05555799], dtype=float32), 0.96863997]. 
=============================================
[2019-04-23 10:24:29,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:29,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5513
[2019-04-23 10:24:29,810] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2180087.031010252 W.
[2019-04-23 10:24:29,812] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.46666666666667, 82.66666666666667, 1.0, 2.0, 0.7795460376197326, 1.0, 1.0, 0.7795460376197326, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2180087.031010252, 2180087.031010252, 410036.9805771842], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6168000.0000, 
sim time next is 6168600.0000, 
raw observation next is [28.55, 82.0, 1.0, 2.0, 0.9284421142497604, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.9975665715977, 6.9112, 168.9124425206784, 2194847.811724694, 2133576.579769736, 442383.8507061291], 
processed observation next is [1.0, 0.391304347826087, 0.552132701421801, 0.82, 1.0, 1.0, 0.9137856798189885, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008636657159769977, 0.0, 0.8294374212273119, 0.6096799477013038, 0.5926601610471489, 0.6602744040389986], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1810826], dtype=float32), 0.29801965]. 
=============================================
[2019-04-23 10:24:33,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:33,688] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7763
[2019-04-23 10:24:33,691] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.71666666666667, 88.83333333333334, 1.0, 2.0, 0.5217213575510007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729034.9538810509, 729034.9538810509, 187069.6210340689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6220200.0000, 
sim time next is 6220800.0000, 
raw observation next is [26.7, 89.0, 1.0, 2.0, 0.5218301179650611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729186.9839966983, 729186.9839966976, 187087.3749641027], 
processed observation next is [0.0, 0.0, 0.46445497630331756, 0.89, 1.0, 1.0, 0.4238917083916398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20255193999908286, 0.20255193999908266, 0.27923488800612345], 
reward next is 0.7208, 
noisyNet noise sample is [array([-0.5221646], dtype=float32), -0.41169825]. 
=============================================
[2019-04-23 10:24:35,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:35,920] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9764
[2019-04-23 10:24:35,925] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 88.5, 1.0, 2.0, 0.5249457330007851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733542.1376911312, 733542.1376911306, 187596.7300470663], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6323400.0000, 
sim time next is 6324000.0000, 
raw observation next is [26.66666666666667, 88.66666666666666, 1.0, 2.0, 0.5242056698408817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732507.6408724061, 732507.6408724061, 187475.409232148], 
processed observation next is [0.0, 0.17391304347826086, 0.4628751974723541, 0.8866666666666666, 1.0, 1.0, 0.4267538190853996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20347434468677947, 0.20347434468677947, 0.2798140436300716], 
reward next is 0.7202, 
noisyNet noise sample is [array([-2.3857427], dtype=float32), 1.5795166]. 
=============================================
[2019-04-23 10:24:35,936] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.93517 ]
 [69.03161 ]
 [69.2172  ]
 [69.32106 ]
 [69.329445]], R is [[68.82119751]
 [68.8529892 ]
 [68.88420105]
 [68.91478729]
 [68.94483185]].
[2019-04-23 10:24:40,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:40,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-23 10:24:40,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2287413.254902935 W.
[2019-04-23 10:24:40,926] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.95, 68.5, 1.0, 2.0, 0.8178864207791535, 1.0, 1.0, 0.8178864207791535, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2287413.254902935, 2287413.254902935, 428666.6782589763], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6442200.0000, 
sim time next is 6442800.0000, 
raw observation next is [29.96666666666667, 68.33333333333333, 1.0, 2.0, 0.5451264585863256, 1.0, 2.0, 0.5451264585863256, 1.0, 1.0, 0.9367228864171988, 6.9112, 6.9112, 170.5573041426782, 2286862.541302542, 2286862.541302542, 445761.8686527267], 
processed observation next is [1.0, 0.5652173913043478, 0.6192733017377569, 0.6833333333333332, 1.0, 1.0, 0.45195958865822355, 1.0, 1.0, 0.45195958865822355, 1.0, 0.5, 0.9228327883136571, 0.0, 0.0, 0.8375144448122397, 0.6352395948062617, 0.6352395948062617, 0.6653162218697414], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02824182], dtype=float32), 0.33954957]. 
=============================================
[2019-04-23 10:24:52,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:52,284] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9702
[2019-04-23 10:24:52,287] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 57.0, 1.0, 2.0, 0.7322619065415515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138701.865627081, 1138701.865627082, 242436.8192250228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6777000.0000, 
sim time next is 6777600.0000, 
raw observation next is [27.53333333333333, 56.0, 1.0, 2.0, 0.8572734533926873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1331977.314721376, 1331977.314721376, 276348.2852844648], 
processed observation next is [1.0, 0.43478260869565216, 0.5039494470774091, 0.56, 1.0, 1.0, 0.8280403052923944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36999369853371555, 0.36999369853371555, 0.41246012729024595], 
reward next is 0.5875, 
noisyNet noise sample is [array([0.08921674], dtype=float32), -1.3005059]. 
=============================================
[2019-04-23 10:24:55,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:55,049] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1477
[2019-04-23 10:24:55,056] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333334, 94.33333333333334, 1.0, 2.0, 0.7472043357951452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044271.784005457, 1044271.784005457, 231361.2113274533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6661200.0000, 
sim time next is 6661800.0000, 
raw observation next is [25.0, 94.5, 1.0, 2.0, 0.6793456878011365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 949392.0524513375, 949392.0524513375, 216447.2680255564], 
processed observation next is [1.0, 0.08695652173913043, 0.38388625592417064, 0.945, 1.0, 1.0, 0.6136695033748631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.263720014569816, 0.263720014569816, 0.3230556239187409], 
reward next is 0.6769, 
noisyNet noise sample is [array([1.0349921], dtype=float32), 0.03842199]. 
=============================================
[2019-04-23 10:24:56,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:56,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9713
[2019-04-23 10:24:56,139] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.01666666666667, 94.16666666666667, 1.0, 2.0, 0.5159751547977751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721002.6885459501, 721002.6885459494, 186135.2218646462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6673800.0000, 
sim time next is 6674400.0000, 
raw observation next is [25.1, 94.0, 1.0, 2.0, 0.5193463287648424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725715.0390567803, 725715.0390567797, 186680.9908676649], 
processed observation next is [1.0, 0.2608695652173913, 0.38862559241706174, 0.94, 1.0, 1.0, 0.4208991912829427, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20158751084910564, 0.20158751084910548, 0.2786283445786043], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.6771588], dtype=float32), -0.06824257]. 
=============================================
[2019-04-23 10:24:57,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:24:57,119] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7197
[2019-04-23 10:24:57,133] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1996867.435940485 W.
[2019-04-23 10:24:57,147] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 68.0, 1.0, 2.0, 0.7140921778819578, 1.0, 2.0, 0.7140921778819578, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1996867.435940485, 1996867.435940485, 380207.9549098995], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6696000.0000, 
sim time next is 6696600.0000, 
raw observation next is [29.56666666666667, 67.33333333333334, 1.0, 2.0, 0.7974214535900547, 1.0, 2.0, 0.7974214535900547, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2230127.056402405, 2230127.056402405, 418608.0923089404], 
processed observation next is [1.0, 0.5217391304347826, 0.6003159557661929, 0.6733333333333335, 1.0, 1.0, 0.7559294621566923, 1.0, 1.0, 0.7559294621566923, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6194797378895569, 0.6194797378895569, 0.6247881974760305], 
reward next is 0.3752, 
noisyNet noise sample is [array([0.23394376], dtype=float32), 1.0421833]. 
=============================================
[2019-04-23 10:25:00,097] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:25:00,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5117
[2019-04-23 10:25:00,108] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 67.0, 1.0, 2.0, 0.4170665138822869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611146.1835130262, 611146.1835130256, 175261.3542101542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6723600.0000, 
sim time next is 6724200.0000, 
raw observation next is [27.33333333333334, 67.0, 1.0, 2.0, 0.4122982921770527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606925.3413061538, 606925.3413061538, 174943.6983084941], 
processed observation next is [1.0, 0.8260869565217391, 0.4944707740916275, 0.67, 1.0, 1.0, 0.29192565322536473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16859037258504272, 0.16859037258504272, 0.2611099974753643], 
reward next is 0.7389, 
noisyNet noise sample is [array([0.4577912], dtype=float32), -1.5439905]. 
=============================================
[2019-04-23 10:25:02,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:25:02,471] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6671
[2019-04-23 10:25:02,481] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 77.0, 1.0, 2.0, 0.3400555338465463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529808.52405428, 529808.52405428, 169157.2424415597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6740400.0000, 
sim time next is 6741000.0000, 
raw observation next is [23.7, 77.5, 1.0, 2.0, 0.3380968728914415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527551.4511512365, 527551.4511512365, 168997.2161876444], 
processed observation next is [1.0, 0.0, 0.3222748815165877, 0.775, 1.0, 1.0, 0.20252635288125484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14654206976423237, 0.14654206976423237, 0.2522346510263349], 
reward next is 0.7478, 
noisyNet noise sample is [array([0.6143118], dtype=float32), 0.30822027]. 
=============================================
[2019-04-23 10:25:02,550] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.80466 ]
 [69.26228 ]
 [70.6539  ]
 [73.44202 ]
 [73.430595]], R is [[68.49498749]
 [68.55756378]
 [68.61936188]
 [68.68034363]
 [68.74045563]].
[2019-04-23 10:25:16,947] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:25:16,967] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4697
[2019-04-23 10:25:16,974] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.56666666666667, 34.66666666666667, 1.0, 2.0, 0.2648799904228961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 432452.1975484979, 432452.1975484979, 162377.8519226228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6873600.0000, 
sim time next is 6874200.0000, 
raw observation next is [29.65, 33.5, 1.0, 2.0, 0.2608519516318573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427038.6891723659, 427038.6891723666, 161990.3035745148], 
processed observation next is [0.0, 0.5652173913043478, 0.6042654028436019, 0.335, 1.0, 1.0, 0.1094601826889847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11862185810343497, 0.11862185810343516, 0.24177657249927584], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.80909455], dtype=float32), -1.2161043]. 
=============================================
[2019-04-23 10:25:23,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:25:23,025] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0349
[2019-04-23 10:25:23,044] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 87.33333333333334, 1.0, 2.0, 0.4820045854936138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673518.5485279667, 673518.548527966, 180823.9668520683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7078800.0000, 
sim time next is 7079400.0000, 
raw observation next is [25.5, 87.5, 1.0, 2.0, 0.4808562141444735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671913.3894996588, 671913.3894996588, 180650.4006258951], 
processed observation next is [1.0, 0.9565217391304348, 0.40758293838862564, 0.875, 1.0, 1.0, 0.3745255592102091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18664260819434966, 0.18664260819434966, 0.26962746362073897], 
reward next is 0.7304, 
noisyNet noise sample is [array([-0.36261958], dtype=float32), -0.6216106]. 
=============================================
[2019-04-23 10:25:37,192] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-23 10:25:37,197] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:25:37,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:25:37,223] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run17
[2019-04-23 10:25:37,251] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:25:37,252] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:25:37,253] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run17
[2019-04-23 10:25:37,284] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:25:37,297] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:25:37,298] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run17
[2019-04-23 10:25:37,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:25:37,339] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:25:37,340] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run17
[2019-04-23 10:25:37,434] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:25:37,434] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:25:37,436] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run17
[2019-04-23 10:26:53,280] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33347428]
[2019-04-23 10:26:53,286] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.21383439666667, 55.63211262333333, 1.0, 2.0, 0.8232477099778911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1183300.849432181, 1183300.849432181, 254317.4965343701]
[2019-04-23 10:26:53,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:26:53,295] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.776336989232535
[2019-04-23 10:28:10,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33347428]
[2019-04-23 10:28:10,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.75, 56.0, 1.0, 2.0, 0.8865839618069077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1239178.497244386, 1239178.497244386, 266280.8228141968]
[2019-04-23 10:28:10,851] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:28:10,852] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15017644703914546
[2019-04-23 10:28:31,893] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33347428]
[2019-04-23 10:28:31,894] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.05, 65.5, 1.0, 2.0, 0.5467230823538769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763984.0427045726, 763984.0427045719, 191240.0161263811]
[2019-04-23 10:28:31,895] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:28:31,896] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6804676059432295
[2019-04-23 10:28:40,535] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33347428]
[2019-04-23 10:28:40,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.25, 86.0, 1.0, 2.0, 0.5774167580075429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806891.2948821672, 806891.2948821672, 196613.2177071353]
[2019-04-23 10:28:40,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:28:40,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7722587110830552
[2019-04-23 10:28:43,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33347428]
[2019-04-23 10:28:43,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.87379915333333, 89.60483756, 1.0, 2.0, 0.2460706193482206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 408505.8440220045, 408505.8440220045, 160276.9386375831]
[2019-04-23 10:28:43,982] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:28:43,986] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9862461974910139
[2019-04-23 10:28:49,165] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:28:49,216] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:28:49,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:28:49,471] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:28:49,473] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:28:50,489] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 400000, evaluation results [400000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:28:52,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:28:52,421] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2286
[2019-04-23 10:28:52,428] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 93.83333333333334, 1.0, 2.0, 0.490802998810062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695510.1720054905, 695510.1720054899, 183410.4509354638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7098600.0000, 
sim time next is 7099200.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4868725111580744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690298.6169674994, 690298.6169674994, 182844.1648852561], 
processed observation next is [1.0, 0.17391304347826086, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3817741098290053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19174961582430539, 0.19174961582430539, 0.27290173863471057], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.16197234], dtype=float32), 0.19720776]. 
=============================================
[2019-04-23 10:28:54,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:28:54,612] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6490
[2019-04-23 10:28:54,616] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 81.5, 1.0, 2.0, 0.6088619389701049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 954108.9138077963, 954108.9138077969, 214617.374334406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7288200.0000, 
sim time next is 7288800.0000, 
raw observation next is [23.23333333333333, 80.33333333333334, 1.0, 2.0, 0.6296461902907524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 984885.2195044209, 984885.2195044215, 218927.0914203375], 
processed observation next is [1.0, 0.34782608695652173, 0.3001579778830963, 0.8033333333333335, 1.0, 1.0, 0.5537905907117499, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2735792276401169, 0.27357922764011705, 0.3267568528661754], 
reward next is 0.6732, 
noisyNet noise sample is [array([1.7366107], dtype=float32), -0.93215096]. 
=============================================
[2019-04-23 10:28:57,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:28:57,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4348
[2019-04-23 10:28:57,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1822943.810494138 W.
[2019-04-23 10:28:57,033] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.25, 76.0, 1.0, 2.0, 0.6519488778183834, 1.0, 2.0, 0.6519488778183834, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1822943.810494138, 1822943.810494138, 354232.7693450259], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7209000.0000, 
sim time next is 7209600.0000, 
raw observation next is [29.33333333333333, 75.0, 1.0, 2.0, 0.428768860756767, 1.0, 2.0, 0.428768860756767, 1.0, 1.0, 0.7406331466467753, 6.911200000000001, 6.9112, 170.5573041426782, 1798329.433749043, 1798329.433749042, 368780.1351180889], 
processed observation next is [1.0, 0.43478260869565216, 0.5892575039494469, 0.75, 1.0, 1.0, 0.31176971175514095, 1.0, 1.0, 0.31176971175514095, 1.0, 0.5, 0.6836989593253358, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4995359538191786, 0.4995359538191783, 0.5504181121165507], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.30573016], dtype=float32), -1.5271848]. 
=============================================
[2019-04-23 10:29:01,758] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:01,765] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4357
[2019-04-23 10:29:01,770] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 88.33333333333334, 1.0, 2.0, 0.3245502997702647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512005.7534928402, 512005.7534928408, 167911.230375331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7280400.0000, 
sim time next is 7281000.0000, 
raw observation next is [21.85, 88.0, 1.0, 2.0, 0.3243149078412004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511550.5761409131, 511550.5761409138, 167874.7034761465], 
processed observation next is [1.0, 0.2608695652173913, 0.23459715639810438, 0.88, 1.0, 1.0, 0.1859215757122896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14209738226136476, 0.14209738226136495, 0.25055925891962166], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.32394475], dtype=float32), 1.3174639]. 
=============================================
[2019-04-23 10:29:01,785] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.570076]
 [75.42372 ]
 [75.63244 ]
 [75.534195]
 [75.50221 ]], R is [[75.67784882]
 [75.67046356]
 [75.65808868]
 [75.65159607]
 [75.64485931]].
[2019-04-23 10:29:02,597] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:02,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8247
[2019-04-23 10:29:02,609] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 65.5, 1.0, 2.0, 0.3666095040643219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548563.7826171903, 548563.7826171903, 169998.3087747669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7320600.0000, 
sim time next is 7321200.0000, 
raw observation next is [27.06666666666666, 66.0, 1.0, 2.0, 0.3735589569850205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558884.8204358664, 558884.8204358664, 170885.9874909668], 
processed observation next is [1.0, 0.7391304347826086, 0.4818325434439175, 0.66, 1.0, 1.0, 0.24525175540363917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15524578345440732, 0.15524578345440732, 0.2550537126730848], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.18326253], dtype=float32), 0.043030877]. 
=============================================
[2019-04-23 10:29:02,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:02,910] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9306
[2019-04-23 10:29:02,914] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 71.5, 1.0, 2.0, 0.391417254083236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587086.5493565543, 587086.549356555, 173442.4740356298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7327800.0000, 
sim time next is 7328400.0000, 
raw observation next is [25.96666666666667, 72.0, 1.0, 2.0, 0.3904936106291216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585732.4335577345, 585732.4335577345, 173320.3874318259], 
processed observation next is [1.0, 0.8260869565217391, 0.42969984202211703, 0.72, 1.0, 1.0, 0.2656549525652067, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16270345376603734, 0.16270345376603734, 0.25868714542063564], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.5763325], dtype=float32), -0.8224749]. 
=============================================
[2019-04-23 10:29:05,223] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:05,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6256
[2019-04-23 10:29:05,229] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 72.83333333333333, 1.0, 2.0, 0.5686544204305177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876473.0143512196, 876473.0143512203, 204890.7498148936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7351800.0000, 
sim time next is 7352400.0000, 
raw observation next is [24.83333333333334, 73.66666666666667, 1.0, 2.0, 0.4606569003450017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709042.0398696957, 709042.0398696951, 185532.1371341849], 
processed observation next is [1.0, 0.08695652173913043, 0.3759873617693526, 0.7366666666666667, 1.0, 1.0, 0.350189036560243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19695612218602657, 0.1969561221860264, 0.2769136375137088], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.6303334], dtype=float32), 1.2984679]. 
=============================================
[2019-04-23 10:29:06,871] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:06,879] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0532
[2019-04-23 10:29:06,884] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 87.0, 1.0, 2.0, 0.2817976931909142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456471.5194258331, 456471.5194258331, 164021.1520503433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7407600.0000, 
sim time next is 7408200.0000, 
raw observation next is [20.73333333333333, 86.5, 1.0, 2.0, 0.2814994678713811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455956.5888777226, 455956.5888777226, 163986.9808121778], 
processed observation next is [1.0, 0.7391304347826086, 0.18167456556082143, 0.865, 1.0, 1.0, 0.13433670827877237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12665460802158962, 0.12665460802158962, 0.24475668777936985], 
reward next is 0.7552, 
noisyNet noise sample is [array([1.0766308], dtype=float32), -1.3528383]. 
=============================================
[2019-04-23 10:29:07,410] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:07,417] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6764
[2019-04-23 10:29:07,422] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 82.66666666666667, 1.0, 2.0, 0.2874157617010465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462797.0075401866, 462797.0075401873, 164463.9532688753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7414800.0000, 
sim time next is 7415400.0000, 
raw observation next is [21.63333333333333, 82.33333333333334, 1.0, 2.0, 0.2874782655347496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462706.1998459483, 462706.1998459483, 164457.4963176636], 
processed observation next is [1.0, 0.8260869565217391, 0.2243285939968403, 0.8233333333333335, 1.0, 1.0, 0.14154007895752962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12852949995720786, 0.12852949995720786, 0.24545894972785612], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.47768977], dtype=float32), -0.8761387]. 
=============================================
[2019-04-23 10:29:34,014] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:34,014] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:34,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-04-23 10:29:44,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:44,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:44,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run3
[2019-04-23 10:29:48,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:48,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:48,947] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-04-23 10:29:49,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:49,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:49,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-04-23 10:29:50,055] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:50,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:50,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-04-23 10:29:50,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:50,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:50,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:50,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:50,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-04-23 10:29:50,240] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-04-23 10:29:50,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:50,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:50,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-04-23 10:29:51,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:51,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:51,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-04-23 10:29:51,469] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:51,470] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:51,471] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-04-23 10:29:51,594] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:51,594] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:51,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-04-23 10:29:51,822] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:51,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:51,834] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-04-23 10:29:52,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:52,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:52,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-04-23 10:29:53,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:53,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:53,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-04-23 10:29:53,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:29:53,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-23 10:29:53,731] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 85.0, 1.0, 2.0, 0.5290085742871361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739221.3957542346, 739221.3957542346, 188266.0289710891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7938000.0000, 
sim time next is 7938600.0000, 
raw observation next is [27.31666666666666, 85.50000000000001, 1.0, 2.0, 0.5281917196004554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738079.5499012971, 738079.5499012977, 188131.1494959067], 
processed observation next is [1.0, 0.9130434782608695, 0.493680884676145, 0.8550000000000001, 1.0, 1.0, 0.4315562886752475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20502209719480474, 0.2050220971948049, 0.2807927604416518], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.9246192], dtype=float32), 1.5630131]. 
=============================================
[2019-04-23 10:29:53,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:53,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:53,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-04-23 10:29:55,550] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 10:29:55,551] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:29:55,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:55,567] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:29:55,568] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:55,571] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run18
[2019-04-23 10:29:55,568] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:29:55,624] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:55,625] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run18
[2019-04-23 10:29:55,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run18
[2019-04-23 10:29:55,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:29:55,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:55,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-04-23 10:29:55,764] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:29:55,765] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:55,772] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run18
[2019-04-23 10:29:55,773] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:29:55,798] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:29:55,799] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run18
[2019-04-23 10:30:18,931] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:30:18,932] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.6, 81.33333333333334, 1.0, 2.0, 0.4799568484677325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 670656.2848252951, 670656.2848252945, 180515.2154840198]
[2019-04-23 10:30:18,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:30:18,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10926157034053208
[2019-04-23 10:31:14,864] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:31:14,891] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.2, 56.5, 1.0, 2.0, 0.5873254654517597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820743.2288622214, 820743.2288622208, 198408.1037581624]
[2019-04-23 10:31:14,898] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:31:14,917] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43601443684960817
[2019-04-23 10:31:16,917] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:31:16,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.55399443, 69.46518703333334, 1.0, 2.0, 0.5170960380908959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722569.4979690005, 722569.4979690005, 186317.9596279392]
[2019-04-23 10:31:16,939] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:31:16,969] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4201054017817457
[2019-04-23 10:31:20,076] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:31:20,081] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.37193053333333, 79.98783524, 1.0, 2.0, 0.5599631101754355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 782492.3078706999, 782492.3078707006, 193522.4589076538]
[2019-04-23 10:31:20,082] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:31:20,084] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6914358349431804
[2019-04-23 10:31:27,340] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:31:27,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.17500286, 73.94462895000001, 1.0, 2.0, 0.4971313363211403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694662.4935320431, 694662.4935320431, 183147.7163660394]
[2019-04-23 10:31:27,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:31:27,341] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8484146885233408
[2019-04-23 10:31:35,868] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:31:35,905] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.66915599666667, 82.99898049333333, 1.0, 2.0, 0.6227366980418937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870248.039516, 870248.039516, 205056.9067960441]
[2019-04-23 10:31:35,910] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:31:35,923] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6050798006589017
[2019-04-23 10:32:36,089] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:32:36,095] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.55, 78.5, 1.0, 2.0, 0.5476257364434313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765245.8546765589, 765245.8546765583, 191393.4890610529]
[2019-04-23 10:32:36,097] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:32:36,105] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22274965071701103
[2019-04-23 10:32:51,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3175298]
[2019-04-23 10:32:51,589] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.45, 81.16666666666666, 1.0, 2.0, 0.5923853303772122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827816.765517464, 827816.765517464, 199335.1469049395]
[2019-04-23 10:32:51,592] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:32:51,597] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.48100375949928986
[2019-04-23 10:33:02,485] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:33:02,527] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:33:02,571] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:33:02,676] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:33:02,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:33:03,702] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 425000, evaluation results [425000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:33:06,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:06,714] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9358
[2019-04-23 10:33:06,717] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 89.0, 1.0, 2.0, 0.3446772152033712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535782.4271607496, 535782.427160749, 169607.1258193017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 87600.0000, 
sim time next is 88200.0000, 
raw observation next is [22.3, 89.0, 1.0, 2.0, 0.3446691038097438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535770.6797168511, 535770.6797168511, 169606.1960093649], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 1.0, 1.0, 0.2104447033852335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1488251888102364, 0.1488251888102364, 0.25314357613338045], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.736935], dtype=float32), -0.015768006]. 
=============================================
[2019-04-23 10:33:07,455] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:07,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7979
[2019-04-23 10:33:07,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 89.0, 1.0, 2.0, 0.3438353877293951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534082.3449685606, 534082.3449685606, 169459.2737652238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 91200.0000, 
sim time next is 91800.0000, 
raw observation next is [22.35, 89.0, 1.0, 2.0, 0.3444837827850009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534850.1042379659, 534850.1042379659, 169515.0482132377], 
processed observation next is [1.0, 0.043478260869565216, 0.25829383886255936, 0.89, 1.0, 1.0, 0.21022142504216976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14856947339943496, 0.14856947339943496, 0.2530075346466234], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.935212], dtype=float32), -0.057361048]. 
=============================================
[2019-04-23 10:33:08,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:08,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5282
[2019-04-23 10:33:08,476] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.16666666666667, 1.0, 2.0, 0.9119344527621884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1363298.675074063, 1363298.675074063, 286022.8737582292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 126600.0000, 
sim time next is 127200.0000, 
raw observation next is [22.8, 95.33333333333334, 1.0, 2.0, 0.9424317766578285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1407771.218817024, 1407771.218817024, 295150.583107924], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.9533333333333335, 1.0, 1.0, 0.9306406947684681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3910475607825067, 0.3910475607825067, 0.44052325837003575], 
reward next is 0.5595, 
noisyNet noise sample is [array([0.42834875], dtype=float32), -0.34577617]. 
=============================================
[2019-04-23 10:33:11,684] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:11,694] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8066
[2019-04-23 10:33:11,699] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 95.5, 1.0, 2.0, 0.2650526154486161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 431053.1406491161, 431053.1406491161, 162334.7419220064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 275400.0000, 
sim time next is 276000.0000, 
raw observation next is [19.33333333333333, 95.66666666666667, 1.0, 2.0, 0.26342732567264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 428744.169440457, 428744.1694404576, 162181.6441650866], 
processed observation next is [0.0, 0.17391304347826086, 0.11532385466034739, 0.9566666666666667, 1.0, 1.0, 0.11256304297908432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11909560262234917, 0.11909560262234933, 0.24206215547027848], 
reward next is 0.7579, 
noisyNet noise sample is [array([0.3256951], dtype=float32), -0.011950564]. 
=============================================
[2019-04-23 10:33:11,707] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[78.109695]
 [78.1959  ]
 [78.357475]
 [78.464355]
 [78.4465  ]], R is [[77.95713806]
 [77.93527222]
 [77.91337585]
 [77.89142609]
 [77.86946106]].
[2019-04-23 10:33:13,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:13,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8561
[2019-04-23 10:33:13,065] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 95.0, 1.0, 2.0, 0.2704413298915661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 438720.9824950329, 438720.9824950329, 162843.5832058849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 273600.0000, 
sim time next is 274200.0000, 
raw observation next is [19.53333333333333, 95.16666666666667, 1.0, 2.0, 0.268756527409337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 436380.2894901971, 436380.2894901978, 162687.0761892313], 
processed observation next is [0.0, 0.17391304347826086, 0.12480252764612951, 0.9516666666666667, 1.0, 1.0, 0.11898376796305658, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1212167470806103, 0.12121674708061049, 0.24281653162571837], 
reward next is 0.7572, 
noisyNet noise sample is [array([-0.24677804], dtype=float32), 0.72890985]. 
=============================================
[2019-04-23 10:33:13,680] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:13,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6844
[2019-04-23 10:33:13,696] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 90.66666666666667, 1.0, 2.0, 0.2943811953996872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471934.3171387933, 471934.3171387933, 165087.7251199895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 250800.0000, 
sim time next is 251400.0000, 
raw observation next is [20.73333333333333, 90.83333333333334, 1.0, 2.0, 0.293988935910098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471426.8767904231, 471426.8767904231, 165053.0886931898], 
processed observation next is [0.0, 0.9130434782608695, 0.18167456556082143, 0.9083333333333334, 1.0, 1.0, 0.1493842601326482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13095191021956198, 0.13095191021956198, 0.2463478935719251], 
reward next is 0.7537, 
noisyNet noise sample is [array([-1.3097385], dtype=float32), 0.20052166]. 
=============================================
[2019-04-23 10:33:14,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:14,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8323
[2019-04-23 10:33:14,153] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2864150530230906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460166.5194317136, 460166.5194317136, 164280.924340306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 261000.0000, 
sim time next is 261600.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2859187706480828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 459369.1685770766, 459369.1685770772, 164226.5570637658], 
processed observation next is [0.0, 0.0, 0.1706161137440759, 0.92, 1.0, 1.0, 0.13966116945552143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12760254682696573, 0.1276025468269659, 0.2451142642742773], 
reward next is 0.7549, 
noisyNet noise sample is [array([0.481381], dtype=float32), -0.8333257]. 
=============================================
[2019-04-23 10:33:23,284] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:23,287] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-23 10:33:23,289] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.21666666666667, 78.5, 1.0, 2.0, 0.2401342701356224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397080.5500383901, 397080.5500383901, 159864.9659576104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 504600.0000, 
sim time next is 505200.0000, 
raw observation next is [20.13333333333333, 79.0, 1.0, 2.0, 0.2404901571515648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 397739.4148161089, 397739.4148161095, 159893.8413064448], 
processed observation next is [1.0, 0.8695652173913043, 0.15323854660347538, 0.79, 1.0, 1.0, 0.08492790018260818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11048317078225246, 0.11048317078225264, 0.23864752433797734], 
reward next is 0.7614, 
noisyNet noise sample is [array([0.29003015], dtype=float32), 0.28092673]. 
=============================================
[2019-04-23 10:33:31,872] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:31,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9531
[2019-04-23 10:33:31,881] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 53.33333333333334, 1.0, 2.0, 0.5974845082175666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 982588.4422644093, 982588.4422644093, 215072.6719184397], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.5758176870902904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 947255.4411877558, 947255.4411877552, 210515.0070299353], 
processed observation next is [1.0, 0.6086956521739131, 0.3696682464454976, 0.53, 1.0, 1.0, 0.48893697239794015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26312651144104326, 0.2631265114410431, 0.3142015030297542], 
reward next is 0.6858, 
noisyNet noise sample is [array([-0.2086317], dtype=float32), 1.5035336]. 
=============================================
[2019-04-23 10:33:36,637] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:36,643] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9022
[2019-04-23 10:33:36,649] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 83.5, 1.0, 2.0, 0.3008289006429775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478878.6753059169, 478878.6753059163, 165538.9578176553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 852600.0000, 
sim time next is 853200.0000, 
raw observation next is [22.0, 84.0, 1.0, 2.0, 0.3029436973056214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481905.6715583483, 481905.6715583489, 165750.4613694883], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.84, 1.0, 1.0, 0.16017312928388117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13386268654398564, 0.1338626865439858, 0.2473887483126691], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.5955634], dtype=float32), -0.8771648]. 
=============================================
[2019-04-23 10:33:38,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:38,577] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3445
[2019-04-23 10:33:38,581] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 89.0, 1.0, 2.0, 0.3087097295269451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489279.8158706811, 489279.8158706804, 166253.6816097139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 862200.0000, 
sim time next is 862800.0000, 
raw observation next is [21.46666666666667, 89.0, 1.0, 2.0, 0.3079426918569733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488430.3244859311, 488430.3244859317, 166198.4926851338], 
processed observation next is [0.0, 1.0, 0.21642969984202226, 0.89, 1.0, 1.0, 0.16619601428551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13567509013498086, 0.13567509013498102, 0.2480574517688564], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.66301256], dtype=float32), 0.7176681]. 
=============================================
[2019-04-23 10:33:40,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:40,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6030
[2019-04-23 10:33:40,986] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 70.66666666666667, 1.0, 2.0, 0.3135463153491793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493711.9461238421, 493711.9461238421, 166509.8329997223], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 921000.0000, 
sim time next is 921600.0000, 
raw observation next is [24.3, 71.0, 1.0, 2.0, 0.3134652038833068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493740.0374303367, 493740.0374303367, 166515.7331143028], 
processed observation next is [0.0, 0.6956521739130435, 0.3507109004739337, 0.71, 1.0, 1.0, 0.17284964323289972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13715001039731575, 0.13715001039731575, 0.2485309449467206], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.01494381], dtype=float32), -0.07920973]. 
=============================================
[2019-04-23 10:33:41,802] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:41,808] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3036
[2019-04-23 10:33:41,812] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 71.33333333333333, 1.0, 2.0, 0.3014877156297535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478693.274271819, 478693.2742718183, 165503.9728785646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 909600.0000, 
sim time next is 910200.0000, 
raw observation next is [24.05, 70.66666666666667, 1.0, 2.0, 0.3027321731922025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480197.6300833625, 480197.6300833632, 165602.6905125778], 
processed observation next is [0.0, 0.5217391304347826, 0.3388625592417062, 0.7066666666666667, 1.0, 1.0, 0.15991828095446087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1333882305787118, 0.133388230578712, 0.24716819479489224], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.45239156], dtype=float32), 1.7339953]. 
=============================================
[2019-04-23 10:33:43,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:43,445] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6802
[2019-04-23 10:33:43,454] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 71.0, 1.0, 2.0, 0.3134652038833068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493740.0374303367, 493740.0374303367, 166515.7331143028], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 921600.0000, 
sim time next is 922200.0000, 
raw observation next is [24.25, 71.5, 1.0, 2.0, 0.31413214871874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494627.0220118128, 494627.0220118128, 166577.6945062599], 
processed observation next is [0.0, 0.6956521739130435, 0.3483412322274882, 0.715, 1.0, 1.0, 0.17365319122739759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13739639500328132, 0.13739639500328132, 0.2486234246362088], 
reward next is 0.7514, 
noisyNet noise sample is [array([1.4226782], dtype=float32), -0.096761644]. 
=============================================
[2019-04-23 10:33:43,481] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:33:43,490] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3033
[2019-04-23 10:33:43,492] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3433815576369807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531856.691001625, 531856.6910016245, 169237.5690268535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 961200.0000, 
sim time next is 961800.0000, 
raw observation next is [21.81666666666667, 93.83333333333334, 1.0, 2.0, 0.3383488663541128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524192.5131478327, 524192.5131478327, 168626.7849811791], 
processed observation next is [1.0, 0.13043478260869565, 0.2330173775671408, 0.9383333333333335, 1.0, 1.0, 0.20282995946278654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14560903142995352, 0.14560903142995352, 0.2516817686286255], 
reward next is 0.7483, 
noisyNet noise sample is [array([1.3359565], dtype=float32), 1.1070507]. 
=============================================
[2019-04-23 10:33:44,079] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-23 10:33:44,081] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:33:44,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:33:44,082] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:33:44,083] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:33:44,085] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:33:44,085] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:33:44,086] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:33:44,088] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:33:44,084] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:33:44,090] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:33:44,103] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run19
[2019-04-23 10:33:44,117] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run19
[2019-04-23 10:33:44,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run19
[2019-04-23 10:33:44,118] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run19
[2019-04-23 10:33:44,118] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run19
[2019-04-23 10:33:49,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:33:49,945] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.84026917333333, 84.38262542, 1.0, 2.0, 0.308560400940558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490449.606919374, 490449.606919374, 166363.6050112053]
[2019-04-23 10:33:49,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:33:49,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31014749343941916
[2019-04-23 10:34:00,797] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:34:00,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.10047048, 80.78102509, 1.0, 2.0, 0.2544574705037613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 418774.8215554976, 418774.8215554983, 161343.3374845043]
[2019-04-23 10:34:00,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:34:00,805] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2501290969789508
[2019-04-23 10:34:15,335] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:34:15,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.9, 91.5, 1.0, 2.0, 0.5898153121947395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940732.0345272601, 940732.0345272601, 212064.7947096643]
[2019-04-23 10:34:15,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:34:15,353] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2799675822254619
[2019-04-23 10:34:17,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:34:17,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.073199655, 93.936082545, 1.0, 2.0, 0.4651943781877593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664237.9323645019, 664237.9323645013, 180145.5385294919]
[2019-04-23 10:34:17,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:34:17,769] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24981199730595682
[2019-04-23 10:34:32,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:34:32,074] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.55, 88.0, 1.0, 2.0, 0.5122492992232585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715794.5754063062, 715794.5754063068, 185537.7050539116]
[2019-04-23 10:34:32,074] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:34:32,076] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2539684323956576
[2019-04-23 10:34:42,635] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:34:42,636] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.96070900166666, 68.32677862166668, 1.0, 2.0, 0.896686253754587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1253306.805721399, 1253306.805721399, 269031.8570252043]
[2019-04-23 10:34:42,636] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:34:42,659] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6306133968660196
[2019-04-23 10:35:42,620] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:35:42,622] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 75.0, 1.0, 2.0, 0.5983498642354573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836155.0622102371, 836155.0622102377, 200439.0969074108]
[2019-04-23 10:35:42,623] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:35:42,626] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6126717330075225
[2019-04-23 10:35:42,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:35:42,766] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 75.66666666666666, 1.0, 2.0, 0.5716572118280063, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9905488718455837, 6.911200000000001, 6.9112, 168.9129217484468, 1598280.822399276, 1598280.822399275, 349290.9877721622]
[2019-04-23 10:35:42,768] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:35:42,774] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33197144773675735
[2019-04-23 10:35:47,930] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30475184]
[2019-04-23 10:35:47,936] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.2, 67.0, 1.0, 2.0, 0.5916996917706522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826858.2609233127, 826858.2609233127, 199209.7158516677]
[2019-04-23 10:35:47,938] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:35:47,945] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8761467017302849
[2019-04-23 10:37:22,399] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:37:22,664] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:37:22,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:37:22,740] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:37:22,779] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:37:23,793] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 450000, evaluation results [450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:37:24,698] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:37:24,705] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9149
[2019-04-23 10:37:24,731] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3283808777197444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 509139.258063013, 509139.2580630137, 167456.4418894371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 972000.0000, 
sim time next is 972600.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3292132708040946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510337.8854934814, 510337.8854934814, 167546.445712559], 
processed observation next is [1.0, 0.2608695652173913, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19182321783625858, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1417605237481893, 0.1417605237481893, 0.2500693219590433], 
reward next is 0.7499, 
noisyNet noise sample is [array([1.9756188], dtype=float32), 0.7165357]. 
=============================================
[2019-04-23 10:37:37,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:37:37,075] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9361
[2019-04-23 10:37:37,127] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 69.5, 1.0, 2.0, 0.6672565713197245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1028934.593009282, 1028934.593009282, 225863.8729400148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [25.6, 69.0, 1.0, 2.0, 0.7022262944596471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080421.196625199, 1080421.196625199, 233784.8565052438], 
processed observation next is [1.0, 0.6086956521739131, 0.4123222748815167, 0.69, 1.0, 1.0, 0.6412364993489724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30011699906255523, 0.30011699906255523, 0.34893262164961764], 
reward next is 0.6511, 
noisyNet noise sample is [array([0.99398875], dtype=float32), -0.87861013]. 
=============================================
[2019-04-23 10:37:40,912] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:37:40,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4778
[2019-04-23 10:37:40,965] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 91.66666666666667, 1.0, 2.0, 0.5137428404186826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730800.3274593552, 730800.3274593552, 187428.4019170274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1311600.0000, 
sim time next is 1312200.0000, 
raw observation next is [24.55, 91.5, 1.0, 2.0, 0.5153362958853872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733246.0981006918, 733246.0981006911, 187712.6272447009], 
processed observation next is [1.0, 0.17391304347826086, 0.3625592417061612, 0.915, 1.0, 1.0, 0.4160678263679364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2036794716946366, 0.2036794716946364, 0.2801681003652252], 
reward next is 0.7198, 
noisyNet noise sample is [array([1.0258366], dtype=float32), -0.18829465]. 
=============================================
[2019-04-23 10:37:53,518] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:37:53,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5704
[2019-04-23 10:37:53,528] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 95.0, 1.0, 2.0, 0.8139356302280932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1210813.706585817, 1210813.706585817, 257422.0193257338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1333800.0000, 
sim time next is 1334400.0000, 
raw observation next is [22.93333333333333, 95.0, 1.0, 2.0, 0.74319664946453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1106221.01304135, 1106221.01304135, 239407.4658106031], 
processed observation next is [1.0, 0.43478260869565216, 0.28593996840442326, 0.95, 1.0, 1.0, 0.6905983728488313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30728361473370835, 0.30728361473370835, 0.35732457583672106], 
reward next is 0.6427, 
noisyNet noise sample is [array([1.2413374], dtype=float32), -1.6472794]. 
=============================================
[2019-04-23 10:37:55,186] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:37:55,190] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8604
[2019-04-23 10:37:55,195] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 98.0, 1.0, 2.0, 0.3132743445435494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495886.8030634278, 495886.8030634284, 166728.6082702915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1396800.0000, 
sim time next is 1397400.0000, 
raw observation next is [20.51666666666667, 98.0, 1.0, 2.0, 0.3134668458917984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495934.7954368402, 495934.7954368402, 166727.1465853586], 
processed observation next is [0.0, 0.17391304347826086, 0.17140600315955784, 0.98, 1.0, 1.0, 0.17285162155638362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1377596653991223, 0.1377596653991223, 0.24884648744083374], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.4641366], dtype=float32), -1.2738749]. 
=============================================
[2019-04-23 10:38:00,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:00,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3158
[2019-04-23 10:38:00,211] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 57.66666666666667, 1.0, 2.0, 0.3549844035424509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544589.4044788288, 544589.4044788294, 170129.0385365109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1527600.0000, 
sim time next is 1528200.0000, 
raw observation next is [27.55, 58.0, 1.0, 2.0, 0.3522676077483729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541603.219326074, 541603.219326074, 169917.3253098239], 
processed observation next is [0.0, 0.6956521739130435, 0.504739336492891, 0.58, 1.0, 1.0, 0.21959952740767819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15044533870168722, 0.15044533870168722, 0.25360794822361776], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.56325597], dtype=float32), -0.7550651]. 
=============================================
[2019-04-23 10:38:02,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:02,865] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5073
[2019-04-23 10:38:02,869] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3271180828983243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512930.2020848742, 512930.2020848735, 167913.7722458612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1558800.0000, 
sim time next is 1559400.0000, 
raw observation next is [21.7, 91.00000000000001, 1.0, 2.0, 0.3264188989787366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511970.4120501378, 511970.4120501378, 167843.0780645584], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.9100000000000001, 1.0, 1.0, 0.18845650479365855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1422140033472605, 0.1422140033472605, 0.2505120568127737], 
reward next is 0.7495, 
noisyNet noise sample is [array([1.4066857], dtype=float32), 2.8054385]. 
=============================================
[2019-04-23 10:38:03,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:03,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8325
[2019-04-23 10:38:03,159] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 85.0, 1.0, 2.0, 0.808003420546072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1225805.69447403, 1225805.694474031, 258861.4250405442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [23.68333333333333, 85.0, 1.0, 2.0, 0.8268311027735775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253689.908224907, 1253689.908224907, 263925.0723206759], 
processed observation next is [1.0, 0.43478260869565216, 0.32148499210110576, 0.85, 1.0, 1.0, 0.791362774425997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34824719672914084, 0.34824719672914084, 0.3939180183890685], 
reward next is 0.6061, 
noisyNet noise sample is [array([-0.02563554], dtype=float32), 0.5563638]. 
=============================================
[2019-04-23 10:38:09,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:09,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9937
[2019-04-23 10:38:09,780] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 94.0, 1.0, 2.0, 0.612192352997449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857606.3686980192, 857606.3686980198, 203304.8232198185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1736400.0000, 
sim time next is 1737000.0000, 
raw observation next is [24.55, 94.0, 1.0, 2.0, 0.5637764497705771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790397.620084555, 790397.620084555, 194518.8143751693], 
processed observation next is [1.0, 0.08695652173913043, 0.3625592417061612, 0.94, 1.0, 1.0, 0.4744294575549121, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21955489446793194, 0.21955489446793194, 0.2903265886196557], 
reward next is 0.7097, 
noisyNet noise sample is [array([-0.9243638], dtype=float32), 1.9447984]. 
=============================================
[2019-04-23 10:38:09,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[63.25837 ]
 [62.613464]
 [63.46947 ]
 [63.161083]
 [63.151768]], R is [[63.34883118]
 [63.41190338]
 [63.4372406 ]
 [63.53467941]
 [63.63061142]].
[2019-04-23 10:38:11,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:11,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3921
[2019-04-23 10:38:11,737] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 85.0, 1.0, 2.0, 0.5627371642723477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872093.8119475187, 872093.8119475187, 204226.336034002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1774800.0000, 
sim time next is 1775400.0000, 
raw observation next is [22.81666666666667, 84.83333333333334, 1.0, 2.0, 0.5917256669463309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 919469.1522564429, 919469.1522564436, 210267.1149630368], 
processed observation next is [1.0, 0.5652173913043478, 0.28041074249605075, 0.8483333333333334, 1.0, 1.0, 0.5081032131883504, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2554080978490119, 0.25540809784901214, 0.31383151487020416], 
reward next is 0.6862, 
noisyNet noise sample is [array([2.074461], dtype=float32), -0.82722956]. 
=============================================
[2019-04-23 10:38:11,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:11,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-23 10:38:11,919] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.71666666666667, 82.66666666666667, 1.0, 2.0, 0.7788576083047615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1191096.788939322, 1191096.788939322, 252253.2850973595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1768200.0000, 
sim time next is 1768800.0000, 
raw observation next is [23.63333333333334, 83.33333333333334, 1.0, 2.0, 0.7611438823765887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163799.272541831, 1163799.272541831, 247621.4332862092], 
processed observation next is [1.0, 0.4782608695652174, 0.3191153238546607, 0.8333333333333335, 1.0, 1.0, 0.7122215450320346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3232775757060642, 0.3232775757060642, 0.3695842287853869], 
reward next is 0.6304, 
noisyNet noise sample is [array([0.05474343], dtype=float32), -0.18902628]. 
=============================================
[2019-04-23 10:38:13,467] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:13,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6165
[2019-04-23 10:38:13,480] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 92.66666666666667, 1.0, 2.0, 0.5659994561674413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 898306.8904581534, 898306.8904581541, 206835.93483588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1783200.0000, 
sim time next is 1783800.0000, 
raw observation next is [21.0, 93.0, 1.0, 2.0, 0.5709548202188928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 905379.7015858737, 905379.7015858737, 207764.8903681867], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.93, 1.0, 1.0, 0.4830780966492684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2514943615516316, 0.2514943615516316, 0.31009685129580106], 
reward next is 0.6899, 
noisyNet noise sample is [array([0.1836407], dtype=float32), 0.766135]. 
=============================================
[2019-04-23 10:38:13,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:13,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9926
[2019-04-23 10:38:13,656] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 85.83333333333334, 1.0, 2.0, 0.7594696038246377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1162191.58322926, 1162191.58322926, 247302.023874254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1771800.0000, 
sim time next is 1772400.0000, 
raw observation next is [23.16666666666667, 85.66666666666667, 1.0, 2.0, 0.6935849271315306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064085.705241788, 1064085.705241787, 231391.2392456986], 
processed observation next is [1.0, 0.5217391304347826, 0.2969984202211693, 0.8566666666666667, 1.0, 1.0, 0.6308252134114827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29557936256716333, 0.29557936256716305, 0.3453600585756696], 
reward next is 0.6546, 
noisyNet noise sample is [array([1.3170185], dtype=float32), 0.52866733]. 
=============================================
[2019-04-23 10:38:13,815] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:13,820] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2467
[2019-04-23 10:38:13,825] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 95.0, 1.0, 2.0, 0.3684247733673023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557112.2904422659, 557112.2904422659, 170935.6785943345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1836000.0000, 
sim time next is 1836600.0000, 
raw observation next is [22.61666666666667, 94.5, 1.0, 2.0, 0.3781118518647175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570729.9378662504, 570729.937866251, 172088.9409996399], 
processed observation next is [1.0, 0.2608695652173913, 0.2709320695102688, 0.945, 1.0, 1.0, 0.2507371709213464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15853609385173623, 0.1585360938517364, 0.25684916567110433], 
reward next is 0.7432, 
noisyNet noise sample is [array([-1.3103486], dtype=float32), 0.057383254]. 
=============================================
[2019-04-23 10:38:14,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:14,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1023
[2019-04-23 10:38:14,251] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 98.0, 1.0, 2.0, 0.3550032613765913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542418.572054986, 542418.5720549854, 169879.2927229101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1832400.0000, 
sim time next is 1833000.0000, 
raw observation next is [21.91666666666667, 97.5, 1.0, 2.0, 0.3832576915613302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584746.9237708475, 584746.9237708475, 173507.6793236924], 
processed observation next is [1.0, 0.21739130434782608, 0.23775671406003188, 0.975, 1.0, 1.0, 0.2569369777847352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16242970104745763, 0.16242970104745763, 0.2589666855577499], 
reward next is 0.7410, 
noisyNet noise sample is [array([0.14110304], dtype=float32), -0.6638099]. 
=============================================
[2019-04-23 10:38:14,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.6124  ]
 [73.555275]
 [73.55986 ]
 [73.65556 ]
 [73.71559 ]], R is [[73.48207092]
 [73.49370575]
 [73.50543213]
 [73.51708221]
 [73.52854156]].
[2019-04-23 10:38:16,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:38:16,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2482
[2019-04-23 10:38:16,902] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1985576.635537629 W.
[2019-04-23 10:38:16,905] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.8, 86.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.660346945756158, 6.9112, 168.9088369811823, 1985576.635537629, 1454118.970327783, 311346.7943819568], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1857000.0000, 
sim time next is 1857600.0000, 
raw observation next is [25.9, 86.0, 1.0, 2.0, 0.6161198035148419, 0.0, 2.0, 0.0, 1.0, 1.0, 1.029518330470033, 6.911199999999999, 6.9112, 168.9123391805019, 1722693.466142503, 1722693.466142504, 368889.6428958702], 
processed observation next is [1.0, 0.5217391304347826, 0.42654028436018954, 0.86, 1.0, 1.0, 0.5374937391745083, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0359979639878452, -8.881784197001253e-17, 0.0, 0.8294369137797359, 0.478525962817362, 0.4785259628173622, 0.5505815565610003], 
reward next is 0.4494, 
noisyNet noise sample is [array([-0.15392648], dtype=float32), -0.30950275]. 
=============================================
[2019-04-23 10:38:17,891] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-23 10:38:17,896] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:38:17,897] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:38:17,897] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:38:17,898] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:38:17,899] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:38:17,900] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:38:17,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:38:17,901] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:38:17,901] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:38:17,903] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:38:17,912] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run20
[2019-04-23 10:38:17,913] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run20
[2019-04-23 10:38:17,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run20
[2019-04-23 10:38:17,955] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run20
[2019-04-23 10:38:17,992] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run20
[2019-04-23 10:38:23,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:38:23,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.61666666666667, 85.16666666666667, 1.0, 2.0, 0.2397959677748113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395445.2857745519, 395445.2857745519, 159890.7016942344]
[2019-04-23 10:38:23,215] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:38:23,217] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3105317638204962
[2019-04-23 10:38:23,896] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:38:23,896] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.54279999, 92.37456325, 1.0, 2.0, 0.3140460838605987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494384.7537382358, 494384.7537382358, 166557.3916408431]
[2019-04-23 10:38:23,897] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:38:23,899] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3594068878517742
[2019-04-23 10:38:26,516] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:38:26,518] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 70.33333333333334, 1.0, 2.0, 0.2747743962667195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447035.2345021366, 447035.2345021366, 163361.472051933]
[2019-04-23 10:38:26,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:38:26,520] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9899238545823986
[2019-04-23 10:38:29,244] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:38:29,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.59906307333333, 97.08610834333334, 1.0, 2.0, 0.4038841064309314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599645.6290869114, 599645.6290869114, 174416.8776397922]
[2019-04-23 10:38:29,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:38:29,250] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7301263484611316
[2019-04-23 10:38:44,595] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:38:44,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.02745121333334, 95.41451878, 1.0, 2.0, 0.4147604517178956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 614321.5731720757, 614321.5731720764, 175746.2266477763]
[2019-04-23 10:38:44,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:38:44,607] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43476453509251245
[2019-04-23 10:38:52,262] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:38:52,264] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.35219013333333, 79.40454214, 1.0, 2.0, 0.511026582215754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714085.4303115755, 714085.4303115755, 185340.9134155095]
[2019-04-23 10:38:52,264] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:38:52,267] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.953458994293359
[2019-04-23 10:39:09,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:39:09,111] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.33253502666667, 94.41277731000001, 1.0, 2.0, 0.27992854331726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 452053.3028396777, 452053.3028396777, 163736.6675930346]
[2019-04-23 10:39:09,119] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:39:09,124] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7317502230924746
[2019-04-23 10:39:31,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:39:31,004] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.41667618, 71.86257018500001, 1.0, 2.0, 0.8736809347502522, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986130493363, 6.9112, 168.9123159813637, 2118197.738620832, 2050953.45035774, 426891.9279151433]
[2019-04-23 10:39:31,011] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:39:31,020] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5690175256126139
[2019-04-23 10:39:31,025] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2118197.738620832 W.
[2019-04-23 10:40:30,741] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31200606]
[2019-04-23 10:40:30,743] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.46666666666667, 54.33333333333334, 1.0, 2.0, 0.8362001400722303, 1.0, 1.0, 0.8362001400722303, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2338699.475738946, 2338699.475738946, 437607.4853778195]
[2019-04-23 10:40:30,743] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:40:30,745] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05365431225117778
[2019-04-23 10:40:30,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2338699.475738946 W.
[2019-04-23 10:41:39,374] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:41:39,468] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:41:39,504] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:41:39,574] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:41:40,525] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:41:41,552] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 475000, evaluation results [475000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:41:43,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:41:43,938] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1343
[2019-04-23 10:41:44,004] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 94.33333333333334, 1.0, 2.0, 0.4259573929352983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622513.4018254412, 622513.4018254412, 176304.953978988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1917600.0000, 
sim time next is 1918200.0000, 
raw observation next is [23.45, 94.16666666666667, 1.0, 2.0, 0.4203535402130014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 615999.7236060007, 615999.7236060014, 175724.8899134576], 
processed observation next is [1.0, 0.17391304347826086, 0.3104265402843602, 0.9416666666666668, 1.0, 1.0, 0.30163077134096555, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1711110343350002, 0.17111103433500038, 0.2622759550947128], 
reward next is 0.7377, 
noisyNet noise sample is [array([-0.27128032], dtype=float32), 0.58218163]. 
=============================================
[2019-04-23 10:41:46,026] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:41:46,037] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7099
[2019-04-23 10:41:46,039] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.86666666666667, 91.33333333333334, 1.0, 2.0, 0.4351395411081841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 637203.9112639973, 637203.9112639967, 177774.0840418001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1921200.0000, 
sim time next is 1921800.0000, 
raw observation next is [23.98333333333333, 90.66666666666666, 1.0, 2.0, 0.4370288705946396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639250.9888726175, 639250.9888726175, 177959.2335691631], 
processed observation next is [1.0, 0.21739130434782608, 0.3357030015797787, 0.9066666666666666, 1.0, 1.0, 0.3217215308369151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17756971913128264, 0.17756971913128264, 0.2656107963718852], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.71190244], dtype=float32), -0.58563]. 
=============================================
[2019-04-23 10:41:52,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:41:52,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2282
[2019-04-23 10:41:52,116] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 88.66666666666667, 1.0, 2.0, 0.4075703248775858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104158, 591802.6358784868, 591802.6358784862, 173269.187126803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1963200.0000, 
sim time next is 1963800.0000, 
raw observation next is [24.3, 89.0, 1.0, 2.0, 0.4027512366261708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586313.7683562424, 586313.768356243, 172807.5456997983], 
processed observation next is [1.0, 0.7391304347826086, 0.3507109004739337, 0.89, 1.0, 1.0, 0.2804231766580371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1628649356545118, 0.16286493565451196, 0.25792170999969893], 
reward next is 0.7421, 
noisyNet noise sample is [array([-0.9530989], dtype=float32), 0.021467377]. 
=============================================
[2019-04-23 10:42:00,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:00,952] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0515
[2019-04-23 10:42:00,995] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 85.5, 1.0, 2.0, 0.5047805089321843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705354.5522846185, 705354.5522846185, 184348.6271707236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2046600.0000, 
sim time next is 2047200.0000, 
raw observation next is [26.63333333333333, 85.66666666666666, 1.0, 2.0, 0.5032134804873083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703164.1418835776, 703164.1418835782, 184101.2471757254], 
processed observation next is [0.0, 0.6956521739130435, 0.46129541864139006, 0.8566666666666666, 1.0, 1.0, 0.40146202468350395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1953233727454382, 0.19532337274543837, 0.27477798085929167], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.59026676], dtype=float32), -1.0487442]. 
=============================================
[2019-04-23 10:42:12,370] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:12,371] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1792
[2019-04-23 10:42:12,376] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 96.33333333333333, 1.0, 2.0, 0.5916131441029758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826737.2696399585, 826737.2696399585, 199186.5375727023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [24.65, 96.5, 1.0, 2.0, 0.5889505493944133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823015.0414544895, 823015.0414544889, 198698.2665550767], 
processed observation next is [1.0, 0.17391304347826086, 0.3672985781990521, 0.965, 1.0, 1.0, 0.5047596980655581, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22861528929291375, 0.22861528929291358, 0.2965645769478757], 
reward next is 0.7034, 
noisyNet noise sample is [array([-0.16261241], dtype=float32), -0.7354913]. 
=============================================
[2019-04-23 10:42:16,332] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:16,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2667
[2019-04-23 10:42:16,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 95.66666666666667, 1.0, 2.0, 0.6136397413985974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857530.2976508441, 857530.2976508441, 203305.9632519845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2173200.0000, 
sim time next is 2173800.0000, 
raw observation next is [24.83333333333334, 95.83333333333333, 1.0, 2.0, 0.6042654707323429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844425.0241070074, 844425.0241070081, 201535.2731236075], 
processed observation next is [1.0, 0.13043478260869565, 0.3759873617693526, 0.9583333333333333, 1.0, 1.0, 0.523211410520895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23456250669639095, 0.23456250669639114, 0.30079891510986195], 
reward next is 0.6992, 
noisyNet noise sample is [array([-1.0082161], dtype=float32), 0.3032047]. 
=============================================
[2019-04-23 10:42:26,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:26,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3889
[2019-04-23 10:42:26,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2340765.463562347 W.
[2019-04-23 10:42:27,012] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 65.0, 1.0, 2.0, 0.55796344176723, 1.0, 1.0, 0.55796344176723, 1.0, 2.0, 0.9689979770198348, 6.911200000000001, 6.9112, 170.5573041426782, 2340765.463562347, 2340765.463562347, 457542.886619288], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2297400.0000, 
sim time next is 2298000.0000, 
raw observation next is [31.86666666666667, 65.0, 1.0, 2.0, 0.5603057016513162, 1.0, 2.0, 0.5603057016513162, 1.0, 2.0, 0.9730657078413124, 6.911200000000001, 6.9112, 170.5573041426782, 2350600.942071694, 2350600.942071694, 459353.3321479383], 
processed observation next is [1.0, 0.6086956521739131, 0.7093206951026858, 0.65, 1.0, 1.0, 0.47024783331483877, 1.0, 1.0, 0.47024783331483877, 1.0, 1.0, 0.9671533022455028, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6529447061310261, 0.6529447061310261, 0.685601988280505], 
reward next is 0.3144, 
noisyNet noise sample is [array([1.4785832], dtype=float32), 0.069225155]. 
=============================================
[2019-04-23 10:42:27,039] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[52.074577]
 [51.968708]
 [50.952908]
 [51.71962 ]
 [52.502697]], R is [[51.45832062]
 [50.94373703]
 [50.43429947]
 [50.2539978 ]
 [50.08961105]].
[2019-04-23 10:42:46,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:46,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5401
[2019-04-23 10:42:46,423] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 93.0, 1.0, 2.0, 0.552380441864277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771892.4390403318, 771892.4390403318, 192209.9112043129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2498400.0000, 
sim time next is 2499000.0000, 
raw observation next is [26.88333333333333, 93.16666666666667, 1.0, 2.0, 0.5519965502658011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771355.7969124786, 771355.7969124786, 192143.8699131767], 
processed observation next is [1.0, 0.9565217391304348, 0.47314375987361756, 0.9316666666666668, 1.0, 1.0, 0.4602368075491579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21426549914235518, 0.21426549914235518, 0.286781895392801], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.7260115], dtype=float32), -0.8939156]. 
=============================================
[2019-04-23 10:42:46,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.040306]
 [73.08091 ]
 [73.13875 ]
 [73.22186 ]
 [73.16739 ]], R is [[72.98669434]
 [72.96994781]
 [72.95331573]
 [72.93668365]
 [72.91996765]].
[2019-04-23 10:42:50,134] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:50,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6630
[2019-04-23 10:42:50,194] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 93.0, 1.0, 2.0, 0.5563324514544964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777416.9696505311, 777416.9696505311, 192892.673262696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2492400.0000, 
sim time next is 2493000.0000, 
raw observation next is [26.95, 93.0, 1.0, 2.0, 0.5557684444988097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776628.5402283682, 776628.5402283682, 192794.9312219789], 
processed observation next is [1.0, 0.8695652173913043, 0.476303317535545, 0.93, 1.0, 1.0, 0.4647812584323008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21573015006343563, 0.21573015006343563, 0.28775362868952076], 
reward next is 0.7122, 
noisyNet noise sample is [array([-1.0618372], dtype=float32), -0.036060687]. 
=============================================
[2019-04-23 10:42:50,246] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[76.42383 ]
 [76.40843 ]
 [75.8693  ]
 [75.348465]
 [74.83692 ]], R is [[76.44942474]
 [76.39702606]
 [76.34542847]
 [76.29460144]
 [76.24441528]].
[2019-04-23 10:42:54,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:54,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6172
[2019-04-23 10:42:54,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2038811.425236541 W.
[2019-04-23 10:42:54,064] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.25, 69.0, 1.0, 2.0, 0.8169611208082053, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988488696549974, 6.9112, 168.9124966380398, 2038811.425236541, 1983980.313112687, 412719.469680601], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2557800.0000, 
sim time next is 2558400.0000, 
raw observation next is [30.16666666666667, 69.33333333333333, 1.0, 2.0, 0.8771006963930413, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986805740839577, 6.9112, 168.9124439120387, 2122984.301245206, 2069347.149317435, 428489.6204842716], 
processed observation next is [1.0, 0.6086956521739131, 0.6287519747235389, 0.6933333333333332, 1.0, 1.0, 0.8519285498711341, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007560574083957672, 0.0, 0.8294374280595279, 0.5897178614570017, 0.5748186525881764, 0.6395367469914501], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4289813], dtype=float32), -0.6038969]. 
=============================================
[2019-04-23 10:42:54,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:54,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1644
[2019-04-23 10:42:54,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2153455.460382993 W.
[2019-04-23 10:42:54,376] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.08333333333333, 69.66666666666667, 1.0, 2.0, 0.8988704595908251, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986479514634251, 6.9112, 168.9124471152105, 2153455.460382993, 2100049.742838024, 434458.3664265261], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2559000.0000, 
sim time next is 2559600.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7698535222462274, 1.0, 1.0, 0.7698535222462274, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2152953.594772529, 2152953.59477253, 405458.661932932], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.7, 1.0, 1.0, 0.7227150870436474, 1.0, 0.5, 0.7227150870436474, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5980426652145914, 0.5980426652145916, 0.6051621819894507], 
reward next is 0.3948, 
noisyNet noise sample is [array([1.3818737], dtype=float32), -0.6682244]. 
=============================================
[2019-04-23 10:42:59,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:42:59,421] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3139
[2019-04-23 10:42:59,449] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3933527587485803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586941.4729008859, 586941.4729008866, 173337.7128165629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2746200.0000, 
sim time next is 2746800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3923020949218254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585374.0190396183, 585374.0190396183, 173194.7922291267], 
processed observation next is [0.0, 0.8260869565217391, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2678338493034041, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16260389417767177, 0.16260389417767177, 0.25849968989421895], 
reward next is 0.7415, 
noisyNet noise sample is [array([-0.37093717], dtype=float32), 0.56514734]. 
=============================================
[2019-04-23 10:43:01,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:43:01,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3172
[2019-04-23 10:43:01,272] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.486316907442497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679546.2044620784, 679546.2044620784, 181480.2396901032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2636400.0000, 
sim time next is 2637000.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.49055919603352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685476.0028912062, 685476.0028912062, 182130.4274093183], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.815, 1.0, 1.0, 0.3862158988355663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19041000080311282, 0.19041000080311282, 0.27183645881987806], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.60060275], dtype=float32), -0.30332842]. 
=============================================
[2019-04-23 10:43:01,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.856544]
 [77.83962 ]
 [77.84375 ]
 [77.72725 ]
 [77.70714 ]], R is [[77.82923126]
 [77.78007507]
 [77.73215485]
 [77.68532562]
 [77.63919067]].
[2019-04-23 10:43:03,325] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:43:03,356] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4004
[2019-04-23 10:43:03,365] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 86.5, 1.0, 2.0, 0.4660689571230514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655977.7309307858, 655977.7309307863, 179061.9022285829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2619000.0000, 
sim time next is 2619600.0000, 
raw observation next is [25.66666666666666, 85.66666666666667, 1.0, 2.0, 0.4682794274503636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657664.9178185721, 657664.9178185728, 179205.7604229623], 
processed observation next is [0.0, 0.30434782608695654, 0.4154818325434437, 0.8566666666666667, 1.0, 1.0, 0.35937280415706463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18268469939404783, 0.18268469939404802, 0.26747128421337657], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.8839181], dtype=float32), -0.04010423]. 
=============================================
[2019-04-23 10:43:05,140] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:43:05,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3734
[2019-04-23 10:43:05,239] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4792904809376818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669724.8584926792, 669724.8584926798, 180414.8840751948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2634600.0000, 
sim time next is 2635200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4803232599636469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671168.442954809, 671168.4429548084, 180570.5467884914], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3738834457393336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18643567859855806, 0.18643567859855786, 0.26950827878879313], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.9840434], dtype=float32), -0.40146092]. 
=============================================
[2019-04-23 10:43:07,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:43:07,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5555
[2019-04-23 10:43:07,346] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 89.0, 1.0, 2.0, 0.4750088158621437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666012.16823512, 666012.16823512, 180066.1552623873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2656200.0000, 
sim time next is 2656800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4657907796082175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657914.0540058315, 657914.0540058315, 179319.7866438999], 
processed observation next is [0.0, 0.782608695652174, 0.38388625592417064, 0.89, 1.0, 1.0, 0.35637443326291274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18275390389050875, 0.18275390389050875, 0.26764147260283566], 
reward next is 0.7324, 
noisyNet noise sample is [array([-1.1871971], dtype=float32), 0.8924395]. 
=============================================
[2019-04-23 10:43:15,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:43:15,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6408
[2019-04-23 10:43:15,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4083411635024524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601657.3465114563, 601657.3465114563, 174467.6989200493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2830200.0000, 
sim time next is 2830800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4102168239075166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 604419.0189070036, 604419.018907003, 174725.3111859522], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28941786012953813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1678941719186121, 0.16789417191861195, 0.2607840465461973], 
reward next is 0.7392, 
noisyNet noise sample is [array([1.8180101], dtype=float32), -0.8733607]. 
=============================================
[2019-04-23 10:43:15,460] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-23 10:43:15,461] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:43:15,461] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:43:15,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:43:15,462] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:43:15,463] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:43:15,463] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:43:15,464] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:43:15,464] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:43:15,464] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:43:15,465] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:43:15,468] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run21
[2019-04-23 10:43:15,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run21
[2019-04-23 10:43:15,504] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run21
[2019-04-23 10:43:15,506] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run21
[2019-04-23 10:43:15,539] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run21
[2019-04-23 10:43:44,111] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:43:44,121] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.10044206, 83.098607415, 1.0, 2.0, 0.3774640929969967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597805.4745462907, 597805.4745462907, 175000.6055790607]
[2019-04-23 10:43:44,124] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:43:44,130] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.27265173069919035
[2019-04-23 10:43:51,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:43:51,150] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.37651612333334, 99.84540123666667, 1.0, 2.0, 0.5352532559120549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747950.6067922228, 747950.6067922235, 189303.5010662293]
[2019-04-23 10:43:51,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:43:51,159] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7834481685620012
[2019-04-23 10:43:51,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:43:51,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.65, 84.5, 1.0, 2.0, 0.7342333500924266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026135.140452633, 1026135.140452633, 228408.1018651827]
[2019-04-23 10:43:51,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:43:51,840] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7217393576085444
[2019-04-23 10:44:02,747] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:44:02,750] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.34786969, 94.18382473, 1.0, 2.0, 0.2669403716084326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 435348.8448655552, 435348.8448655545, 162577.1120504795]
[2019-04-23 10:44:02,753] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:44:02,755] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43043115101859286
[2019-04-23 10:44:53,803] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:44:53,807] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.14819913, 60.77082846499999, 1.0, 2.0, 0.5306439512552416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741507.42220728, 741507.4222072794, 188535.4469809843]
[2019-04-23 10:44:53,810] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:44:53,817] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0477542444197947
[2019-04-23 10:45:02,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:45:02,637] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.7038382, 83.06213803, 1.0, 2.0, 0.5188935002351494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725082.0575282413, 725082.0575282413, 186607.6908043978]
[2019-04-23 10:45:02,640] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:45:02,646] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2118803330372111
[2019-04-23 10:45:12,823] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:45:12,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.26666666666667, 91.66666666666666, 1.0, 2.0, 0.7796612521139348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104278, 1089655.931503817, 1089655.931503817, 238978.7797241175]
[2019-04-23 10:45:12,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:45:12,832] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.827430150631031
[2019-04-23 10:45:43,346] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.30760333]
[2019-04-23 10:45:43,347] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.9, 78.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.928871899564104, 6.9112, 168.9127802576062, 1466300.553774579, 1453763.513085794, 311353.7698941257]
[2019-04-23 10:45:43,347] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:45:43,348] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.01660292092894533
[2019-04-23 10:46:05,179] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:46:05,715] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:46:05,948] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:46:06,146] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:46:06,172] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:46:07,190] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 500000, evaluation results [500000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:46:09,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:09,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2139
[2019-04-23 10:46:09,672] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 92.33333333333334, 1.0, 2.0, 0.516654771935371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777291.1511361116, 777291.1511361116, 193052.4794744494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2896800.0000, 
sim time next is 2897400.0000, 
raw observation next is [23.0, 93.16666666666667, 1.0, 2.0, 0.5213168263441044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781545.8609867118, 781545.8609867118, 193553.6008072828], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.9316666666666668, 1.0, 1.0, 0.42327328475193293, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21709607249630883, 0.21709607249630883, 0.28888597135415345], 
reward next is 0.7111, 
noisyNet noise sample is [array([0.8029003], dtype=float32), 0.53635603]. 
=============================================
[2019-04-23 10:46:12,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:12,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8939
[2019-04-23 10:46:12,861] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333334, 95.0, 1.0, 2.0, 0.3159479759907824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500268.3602040446, 500268.3602040446, 167057.7114591704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2929200.0000, 
sim time next is 2929800.0000, 
raw observation next is [20.91666666666666, 94.5, 1.0, 2.0, 0.316162499541435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500325.7186357727, 500325.7186357721, 167056.7971203584], 
processed observation next is [1.0, 0.9130434782608695, 0.1903633491311214, 0.945, 1.0, 1.0, 0.17609939703787347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13897936628771465, 0.13897936628771448, 0.249338503164714], 
reward next is 0.7507, 
noisyNet noise sample is [array([1.9056112], dtype=float32), -0.3722863]. 
=============================================
[2019-04-23 10:46:14,006] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:14,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2084
[2019-04-23 10:46:14,050] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3078368523663025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488082.7785863078, 488082.7785863078, 166169.8120022653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2947200.0000, 
sim time next is 2947800.0000, 
raw observation next is [20.83333333333333, 95.0, 1.0, 2.0, 0.3073100318811158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486690.1820198278, 486690.1820198271, 166057.2253637769], 
processed observation next is [1.0, 0.08695652173913043, 0.1864139020537123, 0.95, 1.0, 1.0, 0.1654337733507419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13519171722772994, 0.13519171722772974, 0.24784660502056255], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.36332375], dtype=float32), 0.25619385]. 
=============================================
[2019-04-23 10:46:14,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:14,663] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2744
[2019-04-23 10:46:14,670] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3186198199692449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504778.9420151702, 504778.9420151702, 167401.0737023378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [20.5, 97.0, 1.0, 2.0, 0.3158403466178733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500954.1734709924, 500954.1734709924, 167123.6546730745], 
processed observation next is [1.0, 0.8260869565217391, 0.1706161137440759, 0.97, 1.0, 1.0, 0.17571126098538953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13915393707527568, 0.13915393707527568, 0.2494382905568276], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.5774668], dtype=float32), 1.1840801]. 
=============================================
[2019-04-23 10:46:15,170] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:15,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8262
[2019-04-23 10:46:15,300] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 2.0, 0.3201091971202503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506196.7057556303, 506196.7057556303, 167491.6476772758], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2952600.0000, 
sim time next is 2953200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3103552240950783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490769.21008797, 490769.21008797, 166340.4255789926], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.1691026796326245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13632478057999167, 0.13632478057999167, 0.2482692919089442], 
reward next is 0.7517, 
noisyNet noise sample is [array([-0.9959218], dtype=float32), 0.1830377]. 
=============================================
[2019-04-23 10:46:24,490] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:24,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3691
[2019-04-23 10:46:24,498] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.0, 1.0, 2.0, 0.3604358247515858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570259.3613013607, 570259.3613013614, 172624.7549877047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2999400.0000, 
sim time next is 3000000.0000, 
raw observation next is [20.66666666666667, 96.0, 1.0, 2.0, 0.3155535954409784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499881.1584047105, 499881.1584047111, 167032.9784127846], 
processed observation next is [1.0, 0.7391304347826086, 0.17851500789889443, 0.96, 1.0, 1.0, 0.175365777639733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1388558773346418, 0.13885587733464197, 0.2493029528549024], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.7013146], dtype=float32), 1.7288625]. 
=============================================
[2019-04-23 10:46:24,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.70593 ]
 [78.87142 ]
 [78.542984]
 [78.615486]
 [79.034004]], R is [[81.1568222 ]
 [81.08760071]
 [80.97208405]
 [80.85066986]
 [80.7355423 ]].
[2019-04-23 10:46:31,206] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:31,268] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4029
[2019-04-23 10:46:31,271] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3966181482058797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591816.2734095218, 591816.2734095218, 173784.5850322915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3111600.0000, 
sim time next is 3112200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3930914692769988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586550.9406073227, 586550.9406073234, 173302.0483535628], 
processed observation next is [1.0, 0.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.268784902743372, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16293081683536742, 0.16293081683536761, 0.258659773662034], 
reward next is 0.7413, 
noisyNet noise sample is [array([1.6048608], dtype=float32), -0.34331915]. 
=============================================
[2019-04-23 10:46:44,653] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:44,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5987
[2019-04-23 10:46:44,709] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 78.33333333333334, 1.0, 2.0, 0.5638418887388965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787914.5223730415, 787914.5223730408, 194202.8663751448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3264600.0000, 
sim time next is 3265200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5566302439121694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777833.2561273318, 777833.2561273318, 192944.1488639698], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.79, 1.0, 1.0, 0.46581957097851734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21606479336870327, 0.21606479336870327, 0.28797634158801466], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.56786555], dtype=float32), 1.5270725]. 
=============================================
[2019-04-23 10:46:52,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:46:52,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6205
[2019-04-23 10:46:52,486] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 75.66666666666667, 1.0, 2.0, 0.6083247724871772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850099.9267960817, 850099.9267960817, 202306.4935603956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3341400.0000, 
sim time next is 3342000.0000, 
raw observation next is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5941244627154119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830248.030297597, 830248.030297597, 199656.6287523603], 
processed observation next is [0.0, 0.6956521739130435, 0.6524486571879939, 0.7633333333333334, 1.0, 1.0, 0.5109933285727853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2306244528604436, 0.2306244528604436, 0.2979949682871049], 
reward next is 0.7020, 
noisyNet noise sample is [array([0.50105375], dtype=float32), 1.5940257]. 
=============================================
[2019-04-23 10:46:52,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.87097]
 [73.98366]
 [73.90145]
 [73.90091]
 [73.88972]], R is [[73.93651581]
 [73.89520264]
 [73.85817719]
 [73.82151031]
 [73.78518677]].
[2019-04-23 10:47:20,955] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:47:20,956] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0011
[2019-04-23 10:47:20,956] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2547306.766232487 W.
[2019-04-23 10:47:21,016] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.66666666666667, 1.0, 2.0, 0.910719204158467, 1.0, 2.0, 0.910719204158467, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2547306.766232487, 2547306.766232487, 477372.8424748515], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3595800.0000, 
sim time next is 3596400.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.6076395795631434, 1.0, 2.0, 0.6076395795631434, 1.0, 1.0, 1.03, 6.939606546558568, 6.9112, 170.5573041426782, 2549379.140752942, 2529030.375948604, 490764.6446706169], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 0.5272766018833053, 1.0, 1.0, 0.5272766018833053, 1.0, 0.5, 1.0365853658536586, 0.002840654655856767, 0.0, 0.8375144448122397, 0.7081608724313727, 0.7025084377635011, 0.7324845442845028], 
reward next is 0.1255, 
noisyNet noise sample is [array([-0.09748284], dtype=float32), 0.5167503]. 
=============================================
[2019-04-23 10:47:41,037] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:47:41,061] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6193
[2019-04-23 10:47:41,071] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.83333333333334, 1.0, 2.0, 0.5017033646768905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701053.2888917864, 701053.2888917871, 183862.9431474948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3707400.0000, 
sim time next is 3708000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4975647508796914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695268.3200912824, 695268.320091283, 183214.8986190758], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.74, 1.0, 1.0, 0.394656326361074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1931300889142451, 0.19313008891424527, 0.2734550725657848], 
reward next is 0.7265, 
noisyNet noise sample is [array([-0.47430775], dtype=float32), -0.71332854]. 
=============================================
[2019-04-23 10:47:41,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[70.137375]
 [70.29837 ]
 [70.314064]
 [70.31979 ]
 [70.04536 ]], R is [[70.16690063]
 [70.19081116]
 [70.21360779]
 [70.23495483]
 [70.2550354 ]].
[2019-04-23 10:47:42,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:47:42,368] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4554
[2019-04-23 10:47:42,385] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.5563359687187139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777421.8864615781, 777421.8864615781, 192893.8610922746], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3694200.0000, 
sim time next is 3694800.0000, 
raw observation next is [29.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5576934901660396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779319.5779665861, 779319.5779665867, 193129.2585808034], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590839, 0.7633333333333334, 1.0, 1.0, 0.4671005905614935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21647766054627393, 0.21647766054627407, 0.28825262474746777], 
reward next is 0.7117, 
noisyNet noise sample is [array([0.6542729], dtype=float32), -0.04215768]. 
=============================================
[2019-04-23 10:47:42,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:47:42,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9338
[2019-04-23 10:47:42,883] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 75.66666666666666, 1.0, 2.0, 0.5359869445913245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748976.2079533341, 748976.2079533335, 189426.4703816694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3699600.0000, 
sim time next is 3700200.0000, 
raw observation next is [29.0, 74.83333333333334, 1.0, 2.0, 0.5307761881085955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741692.2709010799, 741692.2709010799, 188558.4893703967], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7483333333333334, 1.0, 1.0, 0.4346701061549343, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2060256308058555, 0.2060256308058555, 0.2814305811498458], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.51341635], dtype=float32), 2.020772]. 
=============================================
[2019-04-23 10:47:50,386] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-23 10:47:50,390] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:47:50,392] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:47:50,395] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:47:50,397] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run22
[2019-04-23 10:47:50,396] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:47:50,398] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:47:50,428] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:47:50,427] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:47:50,433] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:47:50,438] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run22
[2019-04-23 10:47:50,395] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:47:50,485] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:47:50,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run22
[2019-04-23 10:47:50,548] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run22
[2019-04-23 10:47:50,487] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run22
[2019-04-23 10:48:28,797] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3036124]
[2019-04-23 10:48:28,820] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.33333333333334, 86.0, 1.0, 2.0, 0.6291521978505602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894171.1008548035, 894171.1008548035, 208247.7599838983]
[2019-04-23 10:48:28,820] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:48:28,823] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.240972795623415
[2019-04-23 10:48:29,218] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3036124]
[2019-04-23 10:48:29,219] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.1, 89.66666666666666, 1.0, 2.0, 0.4130265372773202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603858.4284902134, 603858.428490214, 174531.818704889]
[2019-04-23 10:48:29,235] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:48:29,237] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.821887464120392
[2019-04-23 10:49:21,152] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3036124]
[2019-04-23 10:49:21,161] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5796733832508436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810045.9411018521, 810045.9411018527, 197020.3200530737]
[2019-04-23 10:49:21,181] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:49:21,190] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31406287373580943
[2019-04-23 10:49:38,006] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3036124]
[2019-04-23 10:49:38,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 84.0, 1.0, 2.0, 0.5913937776984254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826430.6014398711, 826430.6014398711, 199153.3841534985]
[2019-04-23 10:49:38,018] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:49:38,021] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5401454442610983
[2019-04-23 10:50:00,241] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3036124]
[2019-04-23 10:50:00,242] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.7, 81.66666666666666, 1.0, 2.0, 0.5237736778688047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731903.7816338924, 731903.781633893, 187404.6838262411]
[2019-04-23 10:50:00,242] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:50:00,247] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6568346242655854
[2019-04-23 10:50:47,740] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:50:47,818] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:50:48,192] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:50:48,253] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:50:49,038] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:50:50,054] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 525000, evaluation results [525000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:50:59,767] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:50:59,789] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9535
[2019-04-23 10:50:59,797] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 89.83333333333333, 1.0, 2.0, 0.574327543093026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802572.7462452691, 802572.7462452691, 196060.3594296371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3901800.0000, 
sim time next is 3902400.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.576169738306765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805148.0293902752, 805148.0293902752, 196390.060684455], 
processed observation next is [0.0, 0.17391304347826086, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4893611304900783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22365223038618756, 0.22365223038618756, 0.29311949355888806], 
reward next is 0.7069, 
noisyNet noise sample is [array([-0.4392991], dtype=float32), -0.5256629]. 
=============================================
[2019-04-23 10:51:01,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:01,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6296
[2019-04-23 10:51:01,140] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 61.5, 1.0, 2.0, 0.5513510599473078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770453.46650263, 770453.4665026306, 192033.6265716101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3868200.0000, 
sim time next is 3868800.0000, 
raw observation next is [32.33333333333334, 63.33333333333333, 1.0, 2.0, 0.5572060011874463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778638.1123257191, 778638.1123257191, 193045.2620676647], 
processed observation next is [0.0, 0.782608695652174, 0.7314375987361774, 0.6333333333333333, 1.0, 1.0, 0.4665132544427064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21628836453492198, 0.21628836453492198, 0.28812725681741], 
reward next is 0.7119, 
noisyNet noise sample is [array([-0.40440175], dtype=float32), 0.9563429]. 
=============================================
[2019-04-23 10:51:04,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:04,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7663
[2019-04-23 10:51:04,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 92.33333333333334, 1.0, 2.0, 0.5711563119389853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798139.5617838425, 798139.5617838431, 195494.9711987829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3900000.0000, 
sim time next is 3900600.0000, 
raw observation next is [27.5, 91.5, 1.0, 2.0, 0.5719451471865765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 799242.3028045539, 799242.3028045546, 195635.3687388565], 
processed observation next is [0.0, 0.13043478260869565, 0.5023696682464456, 0.915, 1.0, 1.0, 0.4842712616705741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22201175077904275, 0.22201175077904295, 0.29199308766993504], 
reward next is 0.7080, 
noisyNet noise sample is [array([1.4234684], dtype=float32), -0.5443143]. 
=============================================
[2019-04-23 10:51:09,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:09,336] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6561
[2019-04-23 10:51:09,350] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5985336799580335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836412.0338814144, 836412.0338814144, 200473.6040636892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3944400.0000, 
sim time next is 3945000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5984022742334975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836228.3306087685, 836228.3306087685, 200449.1914506043], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5161473183536114, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2322856473913246, 0.2322856473913246, 0.2991778976874691], 
reward next is 0.7008, 
noisyNet noise sample is [array([-1.405732], dtype=float32), -1.0222963]. 
=============================================
[2019-04-23 10:51:09,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.13571]
 [73.10919]
 [73.05698]
 [72.80947]
 [73.08899]], R is [[73.13630676]
 [73.10572815]
 [73.07537079]
 [73.045578  ]
 [73.00489044]].
[2019-04-23 10:51:10,177] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:10,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-23 10:51:10,211] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.33333333333334, 65.0, 1.0, 2.0, 0.6086731510854085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850586.961389273, 850586.961389273, 202373.494847292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3954000.0000, 
sim time next is 3954600.0000, 
raw observation next is [33.0, 67.5, 1.0, 2.0, 0.610975163404368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853805.1888998044, 853805.1888998044, 202809.328919186], 
processed observation next is [0.0, 0.782608695652174, 0.7630331753554502, 0.675, 1.0, 1.0, 0.5312953775956241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23716810802772342, 0.23716810802772342, 0.3027004909241582], 
reward next is 0.6973, 
noisyNet noise sample is [array([-0.01417367], dtype=float32), 0.83188254]. 
=============================================
[2019-04-23 10:51:13,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:13,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5587
[2019-04-23 10:51:13,532] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6345517343025823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886765.9427293071, 886765.9427293071, 207359.2051816386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3971400.0000, 
sim time next is 3972000.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6242372947493807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 872345.9207552606, 872345.9207552613, 205348.4448074379], 
processed observation next is [0.0, 1.0, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5472738490956394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24231831132090573, 0.24231831132090592, 0.3064902161305043], 
reward next is 0.6935, 
noisyNet noise sample is [array([-0.1896107], dtype=float32), -1.0046979]. 
=============================================
[2019-04-23 10:51:13,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.617714]
 [68.71034 ]
 [68.64289 ]
 [68.64046 ]
 [68.61583 ]], R is [[68.65298462]
 [68.65695953]
 [68.66470337]
 [68.67311859]
 [68.68214417]].
[2019-04-23 10:51:15,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:15,272] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4362
[2019-04-23 10:51:15,281] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.011999370684645, 6.9112, 168.9122115008688, 1525314.127889714, 1453803.90048471, 311355.9980509268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3999000.0000, 
sim time next is 3999600.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.048770429416965, 6.9112, 168.9118815942395, 1551418.343200074, 1453821.766844976, 311356.4266586021], 
processed observation next is [1.0, 0.30434782608695654, 0.6208530805687204, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.013757042941696529, 0.0, 0.8294346668217036, 0.4309495397777983, 0.40383937967916, 0.46471108456507776], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06507061], dtype=float32), 1.0330522]. 
=============================================
[2019-04-23 10:51:21,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:21,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0519
[2019-04-23 10:51:21,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2672705.894342578 W.
[2019-04-23 10:51:21,907] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 79.0, 1.0, 2.0, 0.9555042984863301, 1.0, 2.0, 0.9555042984863301, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2672705.894342578, 2672705.894342578, 502684.499656296], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4095600.0000, 
sim time next is 4096200.0000, 
raw observation next is [30.83333333333334, 79.0, 1.0, 2.0, 0.678799858890746, 1.0, 2.0, 0.6599899689596356, 1.0, 1.0, 1.03, 7.005096060604131, 6.9112, 170.5573041426782, 2769260.969806561, 2701999.402148276, 514717.1686179866], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.79, 1.0, 1.0, 0.6130118781816217, 1.0, 1.0, 0.590349360192332, 1.0, 0.5, 1.0365853658536586, 0.009389606060413058, 0.0, 0.8375144448122397, 0.7692391582796002, 0.7505553894856323, 0.7682345800268456], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.39953205], dtype=float32), -0.9951951]. 
=============================================
[2019-04-23 10:51:39,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:39,458] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7426
[2019-04-23 10:51:39,468] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.8641396242585072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1207790.201490409, 1207790.201490408, 260265.107691575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.8512771823772495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1189802.557988619, 1189802.557988619, 256888.0568795293], 
processed observation next is [1.0, 0.2608695652173913, 0.5734597156398105, 0.79, 1.0, 1.0, 0.8208158823822282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33050071055239416, 0.33050071055239416, 0.38341501026795416], 
reward next is 0.6166, 
noisyNet noise sample is [array([0.46092126], dtype=float32), 0.045343257]. 
=============================================
[2019-04-23 10:51:41,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:41,113] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3971
[2019-04-23 10:51:41,115] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6186151948370953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864486.0701509388, 864486.0701509388, 204265.4046309429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4325400.0000, 
sim time next is 4326000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6182377847733076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863958.4425649181, 863958.4425649188, 204193.0585556088], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5400455238232621, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2399884562680328, 0.239988456268033, 0.30476575903822206], 
reward next is 0.6952, 
noisyNet noise sample is [array([0.58323735], dtype=float32), 0.2695048]. 
=============================================
[2019-04-23 10:51:41,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[57.82469 ]
 [57.95121 ]
 [58.067345]
 [58.321644]
 [58.34815 ]], R is [[57.79203033]
 [57.90923691]
 [58.02499008]
 [58.13978195]
 [58.25380325]].
[2019-04-23 10:51:46,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:46,695] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0018
[2019-04-23 10:51:46,745] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6045746050301529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844857.1927000735, 844857.1927000735, 201602.8412600016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4395600.0000, 
sim time next is 4396200.0000, 
raw observation next is [31.0, 79.00000000000001, 1.0, 2.0, 0.6144071240269208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858603.1092581487, 858603.1092581487, 203461.6278172622], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.7900000000000001, 1.0, 1.0, 0.5354302699119527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2385008636828191, 0.2385008636828191, 0.30367407136904806], 
reward next is 0.6963, 
noisyNet noise sample is [array([-1.5446244], dtype=float32), 0.47679827]. 
=============================================
[2019-04-23 10:51:49,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:49,556] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3052
[2019-04-23 10:51:49,591] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6188710837720963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864843.8086701048, 864843.8086701054, 204314.6587111072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4401600.0000, 
sim time next is 4402200.0000, 
raw observation next is [30.16666666666666, 83.16666666666666, 1.0, 2.0, 0.6178550359089731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863423.3514857844, 863423.3514857844, 204119.8234795695], 
processed observation next is [1.0, 0.9565217391304348, 0.6287519747235385, 0.8316666666666666, 1.0, 1.0, 0.5395843806132206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23983981985716232, 0.23983981985716232, 0.3046564529545813], 
reward next is 0.6953, 
noisyNet noise sample is [array([-1.5239602], dtype=float32), -1.6729802]. 
=============================================
[2019-04-23 10:51:52,498] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:52,508] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3188
[2019-04-23 10:51:52,542] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 87.33333333333333, 1.0, 2.0, 0.61397848311866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858003.8631620125, 858003.8631620125, 203379.1088855984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4416000.0000, 
sim time next is 4416600.0000, 
raw observation next is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.6123978910827148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855794.1759007145, 855794.1759007145, 203078.4648208303], 
processed observation next is [0.0, 0.08695652173913043, 0.581358609794629, 0.8816666666666667, 1.0, 1.0, 0.533009507328572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23772060441686513, 0.23772060441686513, 0.3031021862997467], 
reward next is 0.6969, 
noisyNet noise sample is [array([-1.4428853], dtype=float32), 0.92716867]. 
=============================================
[2019-04-23 10:51:58,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:51:58,932] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7420
[2019-04-23 10:51:58,950] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.507745637686434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709499.2557531429, 709499.2557531429, 184818.0670325271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5079039030453599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709720.4820082138, 709720.4820082145, 184843.2213397491], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4071131361992288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19714457833561494, 0.19714457833561513, 0.27588540498470016], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.33452502], dtype=float32), 1.2862039]. 
=============================================
[2019-04-23 10:52:00,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:52:00,143] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9211
[2019-04-23 10:52:00,192] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5123566830453368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715944.6793896905, 715944.6793896911, 185554.6149372619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5126314567395316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716328.7655173802, 716328.7655173802, 185598.6667908782], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.412808984023532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19898021264371674, 0.19898021264371674, 0.27701293550877343], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.254454], dtype=float32), 0.48809847]. 
=============================================
[2019-04-23 10:52:16,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:52:16,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5771
[2019-04-23 10:52:16,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2166132.247871446 W.
[2019-04-23 10:52:16,543] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.83333333333334, 67.5, 1.0, 2.0, 0.5163741248728414, 1.0, 2.0, 0.5163741248728414, 1.0, 1.0, 0.8954115707419299, 6.9112, 6.9112, 170.5573041426782, 2166132.247871446, 2166132.247871446, 426399.7111037135], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4716600.0000, 
sim time next is 4717200.0000, 
raw observation next is [30.66666666666667, 69.0, 1.0, 2.0, 0.7808725648468529, 1.0, 2.0, 0.7808725648468529, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2183800.593364602, 2183800.593364602, 410666.4284994038], 
processed observation next is [1.0, 0.6086956521739131, 0.6524486571879939, 0.69, 1.0, 1.0, 0.7359910419841601, 1.0, 1.0, 0.7359910419841601, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6066112759346116, 0.6066112759346116, 0.6129349679095579], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24403158], dtype=float32), -0.58381325]. 
=============================================
[2019-04-23 10:52:21,452] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-23 10:52:21,455] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:52:21,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:52:21,460] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run23
[2019-04-23 10:52:21,519] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:52:21,521] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:52:21,522] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:52:21,523] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:52:21,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:52:21,525] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:52:21,525] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:52:21,526] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:52:21,528] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run23
[2019-04-23 10:52:21,533] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run23
[2019-04-23 10:52:21,529] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run23
[2019-04-23 10:52:21,776] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run23
[2019-04-23 10:52:36,933] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:52:36,934] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 81.0, 1.0, 2.0, 0.3471023244776215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537758.2981747161, 537758.2981747154, 169720.3508267953]
[2019-04-23 10:52:36,934] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:52:36,936] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2622403063922718
[2019-04-23 10:52:37,764] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:52:37,765] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.93460414, 94.739570205, 1.0, 2.0, 0.3107979332917399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493117.2650736825, 493117.2650736825, 166545.3992115085]
[2019-04-23 10:52:37,767] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:52:37,770] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9029430743348937
[2019-04-23 10:52:42,093] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:52:42,094] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.78333333333333, 53.16666666666666, 1.0, 2.0, 0.2790902040122251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 452947.9165568563, 452947.9165568569, 163773.5117413086]
[2019-04-23 10:52:42,094] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:52:42,098] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18938868236134587
[2019-04-23 10:52:48,621] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:52:48,622] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.03333333333333, 92.66666666666667, 1.0, 2.0, 0.4936282386687215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689765.8788267073, 689765.8788267066, 182603.0420696346]
[2019-04-23 10:52:48,623] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:52:48,625] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2446040741396971
[2019-04-23 10:52:54,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:52:54,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.34836718, 84.58519397500001, 1.0, 2.0, 0.4986945804289277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696847.5966495533, 696847.5966495527, 183391.2141908886]
[2019-04-23 10:52:54,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:52:54,077] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.41270134653384316
[2019-04-23 10:53:16,351] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:53:16,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.304672945, 74.08432190166667, 1.0, 2.0, 0.6876060168836772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 960941.1634507988, 960941.1634507995, 218205.8130043457]
[2019-04-23 10:53:16,360] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:53:16,365] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1333219987030284
[2019-04-23 10:54:17,252] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33308342]
[2019-04-23 10:54:17,268] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.73382579666666, 81.02104880666667, 1.0, 2.0, 0.6038099695821332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843788.235521721, 843788.235521721, 201457.0529207877]
[2019-04-23 10:54:17,271] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:54:17,294] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3976384624653485
[2019-04-23 10:55:00,732] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:55:01,458] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:55:01,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:55:01,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:55:01,801] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:55:02,816] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 550000, evaluation results [550000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:55:05,532] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:05,553] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3188
[2019-04-23 10:55:05,574] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2115037.844353957 W.
[2019-04-23 10:55:05,627] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7563089733938728, 1.0, 1.0, 0.7563089733938728, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2115037.844353957, 2115037.844353957, 399159.287632067], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4785600.0000, 
sim time next is 4786200.0000, 
raw observation next is [30.5, 68.0, 1.0, 2.0, 0.760148598394103, 1.0, 2.0, 0.760148598394103, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2125786.110588436, 2125786.110588437, 400934.4167275555], 
processed observation next is [1.0, 0.391304347826087, 0.6445497630331753, 0.68, 1.0, 1.0, 0.7110224077037385, 1.0, 1.0, 0.7110224077037385, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5904961418301211, 0.5904961418301213, 0.5984095772053067], 
reward next is 0.4016, 
noisyNet noise sample is [array([-0.43487105], dtype=float32), 1.8658823]. 
=============================================
[2019-04-23 10:55:08,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:08,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6233
[2019-04-23 10:55:08,521] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4846000987575116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677146.4880560861, 677146.4880560861, 181218.174291246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4758600.0000, 
sim time next is 4759200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4846202610063223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677174.6703606546, 677174.670360654, 181221.2402177604], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3790605554293039, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18810407510018184, 0.18810407510018168, 0.2704794630115827], 
reward next is 0.7295, 
noisyNet noise sample is [array([0.5732717], dtype=float32), 0.20590451]. 
=============================================
[2019-04-23 10:55:10,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:10,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3198
[2019-04-23 10:55:11,041] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5061244382373603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707233.1157376021, 707233.1157376014, 184561.0802186313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4749600.0000, 
sim time next is 4750200.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5037757394861112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703950.0734656842, 703950.0734656842, 184189.5768226573], 
processed observation next is [1.0, 1.0, 0.5023696682464456, 0.79, 1.0, 1.0, 0.40213944516398936, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19554168707380118, 0.19554168707380118, 0.27490981615321985], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.0973309], dtype=float32), -0.003892571]. 
=============================================
[2019-04-23 10:55:11,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:11,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4142
[2019-04-23 10:55:11,533] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.4984717210444817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696536.0834554569, 696536.0834554569, 183356.6523347963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4751400.0000, 
sim time next is 4752000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4948129326373555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691421.8363920553, 691421.8363920553, 182787.0134091775], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.74, 1.0, 1.0, 0.391340882695609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19206162122001535, 0.19206162122001535, 0.27281643792414556], 
reward next is 0.7272, 
noisyNet noise sample is [array([0.0973309], dtype=float32), -0.003892571]. 
=============================================
[2019-04-23 10:55:11,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.4193  ]
 [71.40903 ]
 [71.40178 ]
 [71.413475]
 [71.432556]], R is [[69.17321777]
 [69.20781708]
 [69.24136353]
 [69.27404022]
 [69.30583191]].
[2019-04-23 10:55:17,788] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:17,813] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9941
[2019-04-23 10:55:17,856] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6864991842597744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959393.647256703, 959393.6472567037, 217958.2614225175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4863600.0000, 
sim time next is 4864200.0000, 
raw observation next is [27.16666666666666, 87.33333333333334, 1.0, 2.0, 0.7132153483236026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 996747.4373574523, 996747.4373574523, 223723.5666201256], 
processed observation next is [1.0, 0.30434782608695654, 0.4865718799368086, 0.8733333333333334, 1.0, 1.0, 0.6544763232814489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2768742881548479, 0.2768742881548479, 0.33391577107481435], 
reward next is 0.6661, 
noisyNet noise sample is [array([1.8042827], dtype=float32), -1.0299714]. 
=============================================
[2019-04-23 10:55:31,493] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:31,494] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4913
[2019-04-23 10:55:31,652] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4896975195386722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684271.5635466398, 684271.5635466403, 181997.5674451162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4910400.0000, 
sim time next is 4911000.0000, 
raw observation next is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4895930152108962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684125.4889527609, 684125.4889527604, 181981.4572746074], 
processed observation next is [1.0, 0.8695652173913043, 0.5181674565560824, 0.7483333333333334, 1.0, 1.0, 0.38505182555529666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19003485804243359, 0.19003485804243342, 0.27161411533523494], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.38525212], dtype=float32), -0.057179473]. 
=============================================
[2019-04-23 10:55:31,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.81199]
 [75.85097]
 [75.97284]
 [76.02641]
 [76.01856]], R is [[75.63910675]
 [75.61107635]
 [75.58290863]
 [75.55464172]
 [75.52626801]].
[2019-04-23 10:55:32,001] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:32,019] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2650
[2019-04-23 10:55:32,032] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4870278561144585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 181588.3701370411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4854182362938947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678290.0615507917, 678290.0615507917, 181342.7604716004], 
processed observation next is [1.0, 0.8695652173913043, 0.4865718799368086, 0.7816666666666667, 1.0, 1.0, 0.3800219714384273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18841390598633104, 0.18841390598633104, 0.27066083652477674], 
reward next is 0.7293, 
noisyNet noise sample is [array([0.7666896], dtype=float32), 0.03875516]. 
=============================================
[2019-04-23 10:55:45,516] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:55:45,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3187
[2019-04-23 10:55:45,585] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1925329.090299422 W.
[2019-04-23 10:55:45,649] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.2, 65.0, 1.0, 2.0, 0.6885325884249356, 1.0, 1.0, 0.6885325884249356, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1925329.090299422, 1925329.090299422, 369247.6763753436], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4968000.0000, 
sim time next is 4968600.0000, 
raw observation next is [30.23333333333333, 65.0, 1.0, 2.0, 0.5357170474579045, 1.0, 2.0, 0.5357170474579045, 1.0, 1.0, 0.9154693412419318, 6.911199999999999, 6.9112, 170.5573041426782, 2247353.585467034, 2247353.585467034, 437793.5014702821], 
processed observation next is [1.0, 0.5217391304347826, 0.6319115323854659, 0.65, 1.0, 1.0, 0.4406229487444632, 1.0, 1.0, 0.4406229487444632, 1.0, 0.5, 0.8969138307828435, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6242648848519539, 0.6242648848519539, 0.6534231365228091], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0734568], dtype=float32), 0.4447706]. 
=============================================
[2019-04-23 10:56:06,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:06,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8256
[2019-04-23 10:56:06,328] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5229393191414531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730737.4756872482, 730737.4756872487, 187267.831458377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5087400.0000, 
sim time next is 5088000.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.521231110208707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728349.6647396589, 728349.6647396584, 186989.0277437185], 
processed observation next is [0.0, 0.9130434782608695, 0.5102685624012641, 0.8066666666666668, 1.0, 1.0, 0.423170012299647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20231935131657192, 0.20231935131657178, 0.27908810111002763], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.15283269], dtype=float32), -0.4494274]. 
=============================================
[2019-04-23 10:56:06,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.89774 ]
 [74.923195]
 [74.84593 ]
 [74.862114]
 [74.871185]], R is [[74.87796783]
 [74.84968567]
 [74.82182312]
 [74.79424286]
 [74.766922  ]].
[2019-04-23 10:56:07,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:07,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0441
[2019-04-23 10:56:07,534] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.516471439461945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721696.412124088, 721696.4121240875, 186216.6336728786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5094600.0000, 
sim time next is 5095200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5170087466958854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722447.478955866, 722447.478955866, 186303.4505701619], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4180828273444402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20067985526551832, 0.20067985526551832, 0.2780648515972566], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.33482713], dtype=float32), -0.27914017]. 
=============================================
[2019-04-23 10:56:17,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:17,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0941
[2019-04-23 10:56:17,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5406710671499779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755524.0242093195, 755524.0242093201, 190213.936129436], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5161800.0000, 
sim time next is 5162400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5409040698056533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755849.733772564, 755849.7337725633, 190253.2434629186], 
processed observation next is [0.0, 0.782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44687237325982326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20995825938126778, 0.20995825938126758, 0.28396006487002773], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.50190437], dtype=float32), 1.5211476]. 
=============================================
[2019-04-23 10:56:22,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:22,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9291
[2019-04-23 10:56:23,071] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.63333333333333, 79.83333333333334, 1.0, 2.0, 0.5467728997040666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764053.6818878799, 764053.6818878804, 191248.7567152311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5259000.0000, 
sim time next is 5259600.0000, 
raw observation next is [28.6, 80.0, 1.0, 2.0, 0.5469442937905147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764293.2720285837, 764293.2720285837, 191277.961218172], 
processed observation next is [1.0, 0.9130434782608695, 0.5545023696682465, 0.8, 1.0, 1.0, 0.45414975155483694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21230368667460658, 0.21230368667460658, 0.2854894943554806], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.3390584], dtype=float32), -1.0996962]. 
=============================================
[2019-04-23 10:56:24,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:24,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1815
[2019-04-23 10:56:24,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2788441.696246434 W.
[2019-04-23 10:56:24,723] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.83333333333334, 60.66666666666666, 1.0, 2.0, 0.6879322408986426, 1.0, 2.0, 0.664556159963584, 1.0, 2.0, 1.03, 7.0050967806437, 6.9112, 170.5573041426782, 2788441.696246434, 2721179.612794517, 517547.8335864523], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5403000.0000, 
sim time next is 5403600.0000, 
raw observation next is [37.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.558437215240727, 6.9112, 170.5573041426782, 3373512.156984277, 2909869.811780732, 550105.5392032852], 
processed observation next is [1.0, 0.5652173913043478, 0.95260663507109, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.06472372152407271, 0.0, 0.8375144448122397, 0.9370867102734103, 0.8082971699390923, 0.8210530435869929], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0237594], dtype=float32), -0.97339803]. 
=============================================
[2019-04-23 10:56:27,786] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:27,786] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4968
[2019-04-23 10:56:27,787] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3192604.374253362 W.
[2019-04-23 10:56:27,865] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.13333333333333, 52.66666666666666, 1.0, 2.0, 0.8803307186167033, 1.0, 2.0, 0.7607553988226143, 1.0, 2.0, 1.03, 7.005111955375662, 6.9112, 170.5573041426782, 3192604.374253362, 3125331.420523673, 584339.2680205575], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5319600.0000, 
sim time next is 5320200.0000, 
raw observation next is [36.11666666666667, 52.83333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.105281807505645, 6.9112, 170.5573041426782, 3048520.380555538, 2909491.687096073, 552638.834171076], 
processed observation next is [1.0, 0.5652173913043478, 0.9107424960505529, 0.5283333333333334, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.019408180750564517, 0.0, 0.8375144448122397, 0.8468112168209828, 0.8081921353044647, 0.8248340808523522], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9705581], dtype=float32), 0.038500175]. 
=============================================
[2019-04-23 10:56:30,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:30,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1580
[2019-04-23 10:56:30,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3104881.690269791 W.
[2019-04-23 10:56:30,466] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 0.8385763117934726, 1.0, 2.0, 0.7398781954109989, 1.0, 2.0, 1.03, 7.005108661314333, 6.9112, 170.5573041426782, 3104881.690269791, 3037611.096210225, 568671.6721989783], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5324400.0000, 
sim time next is 5325000.0000, 
raw observation next is [36.08333333333334, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.835728545324629, 6.9112, 170.5573041426782, 3572378.64385902, 2910101.239426234, 548370.2974198182], 
processed observation next is [1.0, 0.6521739130434783, 0.9091627172195897, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09245285453246285, 0.0, 0.8375144448122397, 0.9923274010719499, 0.8083614553961761, 0.8184631304773407], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.713665], dtype=float32), -0.44449002]. 
=============================================
[2019-04-23 10:56:30,555] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[35.76479 ]
 [35.953312]
 [36.205307]
 [36.202835]
 [37.00612 ]], R is [[34.31994629]
 [33.97674561]
 [33.63697815]
 [33.30060959]
 [32.96760559]].
[2019-04-23 10:56:37,468] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:37,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0983
[2019-04-23 10:56:37,470] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3875062.767475126 W.
[2019-04-23 10:56:37,576] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.46666666666667, 61.33333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.257778861266758, 6.9112, 170.5573041426782, 3875062.767475126, 2910453.553823789, 545530.996968269], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5397600.0000, 
sim time next is 5398200.0000, 
raw observation next is [35.6, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 0.8473615018172839, 1.0, 1.0, 1.03, 7.031576184921018, 6.9112, 170.5573041426782, 3556575.247731196, 3470344.889882137, 651856.3762369129], 
processed observation next is [1.0, 0.4782608695652174, 0.8862559241706162, 0.62, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 0.816098194960583, 1.0, 0.5, 1.0365853658536586, 0.012037618492101831, 0.0, 0.8375144448122397, 0.9879375688142211, 0.963984691633927, 0.9729199645327058], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9798581], dtype=float32), 0.6489672]. 
=============================================
[2019-04-23 10:56:39,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:39,651] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9755
[2019-04-23 10:56:39,703] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 93.0, 1.0, 2.0, 0.5420020716338375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757384.6090214167, 757384.609021416, 190438.5092067846], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5534400.0000, 
sim time next is 5535000.0000, 
raw observation next is [26.5, 93.5, 1.0, 2.0, 0.5424731547390964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758043.1275469636, 758043.1275469636, 190518.1862080742], 
processed observation next is [1.0, 0.043478260869565216, 0.4549763033175356, 0.935, 1.0, 1.0, 0.4487628370350559, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21056753542971213, 0.21056753542971213, 0.2843555018030958], 
reward next is 0.7156, 
noisyNet noise sample is [array([-2.12512], dtype=float32), -0.5939116]. 
=============================================
[2019-04-23 10:56:39,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[56.259296]
 [56.255985]
 [56.408695]
 [56.393333]
 [56.514862]], R is [[56.38669205]
 [56.53858948]
 [56.68886566]
 [56.83743668]
 [56.9841156 ]].
[2019-04-23 10:56:42,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:42,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1772
[2019-04-23 10:56:42,200] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 79.0, 1.0, 2.0, 0.5332175153891272, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9260224865039763, 6.911200000000001, 6.9112, 168.9126953123954, 1490732.8531013, 1490732.853101299, 326764.725378737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5382000.0000, 
sim time next is 5382600.0000, 
raw observation next is [30.8, 78.00000000000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.505606232345007, 6.9112, 168.9097858285181, 1875728.083024098, 1454043.759541865, 311356.1422666765], 
processed observation next is [1.0, 0.30434782608695654, 0.6587677725118484, 0.7800000000000001, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.05944062323450065, 0.0, 0.8294243756525659, 0.521035578617805, 0.4039010443171847, 0.4647106600995172], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2312222], dtype=float32), 1.0118455]. 
=============================================
[2019-04-23 10:56:44,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:44,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9443
[2019-04-23 10:56:44,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3080710.398162557 W.
[2019-04-23 10:56:44,497] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.58333333333333, 68.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.150166274132228, 6.9112, 170.5573041426782, 3080710.398162557, 2909529.135475101, 552393.0364406048], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5417400.0000, 
sim time next is 5418000.0000, 
raw observation next is [30.9, 71.0, 1.0, 2.0, 0.7514264106046058, 1.0, 2.0, 0.6963032448165654, 1.0, 1.0, 1.03, 7.005101787431033, 6.9112, 170.5573041426782, 2921806.535828153, 2854540.865810806, 538063.2187115448], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.71, 1.0, 1.0, 0.7005137477163925, 1.0, 1.0, 0.6341002949597173, 1.0, 0.5, 1.0365853658536586, 0.009390178743103305, 0.0, 0.8375144448122397, 0.8116129266189315, 0.7929280182807794, 0.8030794309127534], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41669226], dtype=float32), -0.49935687]. 
=============================================
[2019-04-23 10:56:44,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[40.50005 ]
 [40.361065]
 [40.040993]
 [40.058064]
 [40.766243]], R is [[40.59979248]
 [40.19379425]
 [39.79185486]
 [39.39393616]
 [38.99999619]].
[2019-04-23 10:56:57,453] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:57,467] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9478
[2019-04-23 10:56:57,473] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.8164064319881933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1141038.651709217, 1141038.651709217, 247977.6576206631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5551800.0000, 
sim time next is 5552400.0000, 
raw observation next is [27.03333333333333, 88.66666666666667, 1.0, 2.0, 0.7850647749198626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1097211.806089695, 1097211.806089695, 240277.8492756816], 
processed observation next is [1.0, 0.2608695652173913, 0.48025276461295413, 0.8866666666666667, 1.0, 1.0, 0.7410418974938103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3047810572471375, 0.3047810572471375, 0.35862365563534565], 
reward next is 0.6414, 
noisyNet noise sample is [array([0.27953202], dtype=float32), -0.3781756]. 
=============================================
[2019-04-23 10:56:59,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:56:59,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1242
[2019-04-23 10:56:59,155] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2429181.258887229 W.
[2019-04-23 10:56:59,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.65, 71.5, 1.0, 2.0, 0.5790184681424014, 1.0, 2.0, 0.5790184681424014, 1.0, 1.0, 1.005563594829884, 6.911199999999999, 6.9112, 170.5573041426782, 2429181.258887229, 2429181.25888723, 474088.4590680886], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5563800.0000, 
sim time next is 5564400.0000, 
raw observation next is [30.86666666666667, 70.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.70027119490171, 6.9112, 168.9086416398212, 2844516.75007015, 2284736.786580725, 474331.9803354464], 
processed observation next is [1.0, 0.391304347826087, 0.6619273301737759, 0.7033333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07890711949017097, 0.0, 0.8294187571622523, 0.7901435416861528, 0.6346491073835347, 0.707958179605144], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0273093], dtype=float32), 0.0512257]. 
=============================================
[2019-04-23 10:57:05,644] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:57:05,646] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2509
[2019-04-23 10:57:05,775] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 70.66666666666667, 1.0, 2.0, 0.5440075490657256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760188.0323666892, 760188.0323666892, 190778.8687070981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5651400.0000, 
sim time next is 5652000.0000, 
raw observation next is [30.5, 70.0, 1.0, 2.0, 0.5451095698350268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761728.5319104239, 761728.5319104233, 190966.1246960469], 
processed observation next is [0.0, 0.43478260869565216, 0.6445497630331753, 0.7, 1.0, 1.0, 0.4519392407650925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21159125886400665, 0.21159125886400648, 0.2850240667105178], 
reward next is 0.7150, 
noisyNet noise sample is [array([0.951479], dtype=float32), -0.09288166]. 
=============================================
[2019-04-23 10:57:05,907] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[78.99456 ]
 [78.94493 ]
 [78.873604]
 [78.81641 ]
 [78.75832 ]], R is [[79.08052063]
 [79.00497437]
 [78.93045807]
 [78.85692596]
 [78.78430939]].
[2019-04-23 10:57:07,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:57:07,772] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-23 10:57:07,782] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 89.5, 1.0, 2.0, 0.5553076913136273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775984.4504366948, 775984.4504366942, 192714.954714195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5607000.0000, 
sim time next is 5607600.0000, 
raw observation next is [27.26666666666667, 89.66666666666667, 1.0, 2.0, 0.5543166012770465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774599.0007298482, 774599.0007298482, 192543.4819928273], 
processed observation next is [1.0, 0.9130434782608695, 0.4913112164297, 0.8966666666666667, 1.0, 1.0, 0.46303204973138135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2151663890916245, 0.2151663890916245, 0.28737833133257806], 
reward next is 0.7126, 
noisyNet noise sample is [array([1.004976], dtype=float32), -0.1837046]. 
=============================================
[2019-04-23 10:57:08,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:57:08,499] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3276
[2019-04-23 10:57:08,518] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 89.0, 1.0, 2.0, 0.5372087543229461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750684.140932508, 750684.1409325086, 189630.9578230292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5792400.0000, 
sim time next is 5793000.0000, 
raw observation next is [26.88333333333333, 89.0, 1.0, 2.0, 0.5368667373309692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750206.0448565988, 750206.0448565994, 189573.5949418262], 
processed observation next is [1.0, 0.043478260869565216, 0.47314375987361756, 0.89, 1.0, 1.0, 0.44200811726622796, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2083905680157219, 0.20839056801572206, 0.2829456640922779], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.02454857], dtype=float32), 0.52408135]. 
=============================================
[2019-04-23 10:57:08,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.49432]
 [75.66124]
 [75.87331]
 [75.97992]
 [75.74163]], R is [[75.10526276]
 [75.07118225]
 [75.03726196]
 [75.00352478]
 [74.97003937]].
[2019-04-23 10:57:09,238] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 10:57:09,248] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:57:09,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:57:09,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:57:09,267] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:57:09,267] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run24
[2019-04-23 10:57:09,276] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:57:09,367] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:57:09,369] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run24
[2019-04-23 10:57:09,371] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run24
[2019-04-23 10:57:09,370] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:57:09,437] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:57:09,439] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run24
[2019-04-23 10:57:09,501] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:57:09,502] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:57:09,503] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run24
[2019-04-23 10:58:09,274] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3414725]
[2019-04-23 10:58:09,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.934033095, 85.94633518500001, 1.0, 2.0, 0.5606625567596952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783470.0748187845, 783470.0748187839, 193645.7543346958]
[2019-04-23 10:58:09,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:58:09,278] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14739427150752993
[2019-04-23 10:58:48,945] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3414725]
[2019-04-23 10:58:48,946] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.85, 84.33333333333333, 1.0, 2.0, 0.4236879052625635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619356.4395952794, 619356.4395952794, 176004.5588267967]
[2019-04-23 10:58:48,946] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:58:48,948] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8229676022529504
[2019-04-23 10:58:50,206] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3414725]
[2019-04-23 10:58:50,210] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.26666666666667, 90.33333333333333, 1.0, 2.0, 0.8942200445298669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1249857.732965268, 1249857.732965267, 268351.2923173835]
[2019-04-23 10:58:50,210] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:58:50,212] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5187374268078541
[2019-04-23 10:58:53,842] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3414725]
[2019-04-23 10:58:53,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.4, 78.33333333333334, 1.0, 2.0, 0.600771912684597, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.926339110238134, 6.9112, 168.9128122029124, 1679746.238110505, 1669006.041435298, 364737.52320743]
[2019-04-23 10:58:53,849] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:58:53,856] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8741460098114638
[2019-04-23 10:58:53,861] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1679746.238110505 W.
[2019-04-23 10:58:59,229] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3414725]
[2019-04-23 10:58:59,230] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.1, 68.0, 1.0, 2.0, 0.7348117909631274, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977457195605, 6.9112, 168.9123160287033, 1923843.931828556, 1856605.796659227, 392392.8525055135]
[2019-04-23 10:58:59,232] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:58:59,234] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42222621151302664
[2019-04-23 10:58:59,236] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1923843.931828556 W.
[2019-04-23 10:59:10,918] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:59:11,113] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:59:11,398] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:59:11,508] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:59:11,570] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:59:12,589] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 575000, evaluation results [575000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:59:30,445] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:59:30,445] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-23 10:59:30,445] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2401912.739977845 W.
[2019-04-23 10:59:30,528] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.7, 71.33333333333333, 1.0, 2.0, 0.5725249978775959, 1.0, 2.0, 0.5725249978775959, 1.0, 2.0, 0.9942865844033484, 6.9112, 6.9112, 170.5573041426782, 2401912.739977845, 2401912.739977845, 468920.7135336191], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5823600.0000, 
sim time next is 5824200.0000, 
raw observation next is [30.85, 70.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.677316433776308, 6.9112, 168.9087906716168, 2827794.014722022, 2284298.053282228, 474365.7782188914], 
processed observation next is [1.0, 0.391304347826087, 0.661137440758294, 0.7066666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07661164337763075, 0.0, 0.8294194889765957, 0.785498337422784, 0.6345272370228411, 0.7080086242073006], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3112562], dtype=float32), 0.21396294]. 
=============================================
[2019-04-23 10:59:33,605] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:59:33,607] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3669
[2019-04-23 10:59:33,653] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 87.0, 1.0, 2.0, 0.5481333624718536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765955.4611601174, 765955.4611601168, 191481.0370986562], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5867400.0000, 
sim time next is 5868000.0000, 
raw observation next is [27.5, 87.0, 1.0, 2.0, 0.5459113170153509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762849.284574155, 762849.2845741556, 191101.8993171383], 
processed observation next is [1.0, 0.9565217391304348, 0.5023696682464456, 0.87, 1.0, 1.0, 0.4529052012233143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2119025790483764, 0.21190257904837656, 0.2852267153987139], 
reward next is 0.7148, 
noisyNet noise sample is [array([-0.9965032], dtype=float32), -0.35339546]. 
=============================================
[2019-04-23 10:59:33,756] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[60.343777]
 [60.466553]
 [60.608173]
 [60.931686]
 [61.026657]], R is [[60.49660492]
 [60.60584641]
 [60.71334839]
 [60.81900024]
 [60.9228096 ]].
[2019-04-23 10:59:40,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:59:40,154] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4420
[2019-04-23 10:59:40,154] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2411008.280947357 W.
[2019-04-23 10:59:40,194] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.95, 74.66666666666667, 1.0, 2.0, 0.5746909407544922, 1.0, 2.0, 0.5746909407544922, 1.0, 1.0, 0.9980481108922623, 6.9112, 6.9112, 170.5573041426782, 2411008.280947357, 2411008.280947357, 470637.8633125536], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5820600.0000, 
sim time next is 5821200.0000, 
raw observation next is [30.1, 74.0, 1.0, 2.0, 0.5645100912460906, 1.0, 2.0, 0.5645100912460906, 1.0, 2.0, 0.9803673421545498, 6.9112, 6.9112, 170.5573041426782, 2368255.9591887, 2368255.9591887, 462621.9883162849], 
processed observation next is [1.0, 0.391304347826087, 0.6255924170616115, 0.74, 1.0, 1.0, 0.47531336294709714, 1.0, 1.0, 0.47531336294709714, 1.0, 1.0, 0.9760577343348168, 0.0, 0.0, 0.8375144448122397, 0.6578488775524167, 0.6578488775524167, 0.6904805795765446], 
reward next is 0.3095, 
noisyNet noise sample is [array([-0.9063889], dtype=float32), 0.22806823]. 
=============================================
[2019-04-23 10:59:55,074] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:59:55,116] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2562
[2019-04-23 10:59:55,168] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 92.0, 1.0, 2.0, 0.6178315982302748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 863390.5851196287, 863390.585119628, 204108.6099141939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5981400.0000, 
sim time next is 5982000.0000, 
raw observation next is [26.5, 91.33333333333334, 1.0, 2.0, 0.6156870231956513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 860392.4285413763, 860392.4285413756, 203698.7310376298], 
processed observation next is [1.0, 0.21739130434782608, 0.4549763033175356, 0.9133333333333334, 1.0, 1.0, 0.5369723171031943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23899789681704897, 0.23899789681704878, 0.30402795677258176], 
reward next is 0.6960, 
noisyNet noise sample is [array([2.181596], dtype=float32), 2.1542535]. 
=============================================
[2019-04-23 10:59:55,199] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[55.598114]
 [55.510674]
 [55.61019 ]
 [55.581783]
 [55.78246 ]], R is [[55.74648666]
 [55.88438416]
 [56.01990128]
 [56.14216614]
 [56.26356125]].
[2019-04-23 10:59:55,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:59:55,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-23 10:59:55,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2409892.685700043 W.
[2019-04-23 10:59:55,822] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.1, 73.0, 1.0, 2.0, 0.5744252822442085, 1.0, 1.0, 0.5744252822442085, 1.0, 2.0, 0.9975867499145114, 6.9112, 6.9112, 170.5573041426782, 2409892.685700043, 2409892.685700043, 470427.7854952058], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5999400.0000, 
sim time next is 6000000.0000, 
raw observation next is [31.26666666666667, 72.33333333333333, 1.0, 2.0, 0.5612653308636544, 1.0, 2.0, 0.5612653308636544, 1.0, 2.0, 0.9747322664289139, 6.911199999999999, 6.9112, 170.5573041426782, 2354630.582385733, 2354630.582385734, 460098.0806841174], 
processed observation next is [1.0, 0.43478260869565216, 0.6808846761453398, 0.7233333333333333, 1.0, 1.0, 0.4714040130887402, 1.0, 1.0, 0.4714040130887402, 1.0, 1.0, 0.969185690766968, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6540640506627036, 0.6540640506627039, 0.6867135532598767], 
reward next is 0.3133, 
noisyNet noise sample is [array([-0.49852815], dtype=float32), -0.13439856]. 
=============================================
[2019-04-23 10:59:55,902] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[38.06039 ]
 [37.866665]
 [38.37579 ]
 [36.05625 ]
 [35.29027 ]], R is [[37.85034943]
 [37.47184753]
 [37.09712982]
 [37.1066246 ]
 [37.04814529]].
[2019-04-23 10:59:58,062] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:59:58,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6577
[2019-04-23 10:59:58,123] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 82.0, 1.0, 2.0, 0.3948178612004901, 1.0, 2.0, 0.3948178612004901, 1.0, 1.0, 0.6620248918475671, 6.9112, 6.9112, 170.5573041426782, 1655823.158874787, 1655823.158874787, 346683.4407224692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6010800.0000, 
sim time next is 6011400.0000, 
raw observation next is [25.83333333333334, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.784458076741846, 6.9112, 168.9081811272105, 2073681.119065653, 1454179.298797786, 311346.1556082925], 
processed observation next is [1.0, 0.5652173913043478, 0.42338072669826254, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.08732580767418456, 0.0, 0.8294164958345102, 0.5760225330737925, 0.4039386941104961, 0.4646957546392425], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0412694], dtype=float32), -0.92354107]. 
=============================================
[2019-04-23 11:00:20,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:00:20,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1620
[2019-04-23 11:00:20,322] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 88.0, 1.0, 2.0, 0.5367055306939107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749980.6986402537, 749980.6986402532, 189547.0545906634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6132600.0000, 
sim time next is 6133200.0000, 
raw observation next is [27.16666666666667, 88.33333333333333, 1.0, 2.0, 0.5388578802352892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752989.4113689464, 752989.4113689458, 189908.2768996689], 
processed observation next is [1.0, 1.0, 0.4865718799368091, 0.8833333333333333, 1.0, 1.0, 0.4444070846208304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091637253802629, 0.20916372538026273, 0.2834451894024909], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.8951005], dtype=float32), -1.138609]. 
=============================================
[2019-04-23 11:01:04,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:01:04,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6718
[2019-04-23 11:01:04,281] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.88333333333333, 67.5, 1.0, 2.0, 0.2302218105956912, 1.0, 1.0, 0.2302218105956912, 1.0, 2.0, 0.3923498949263221, 6.911199999999999, 6.9112, 170.5573041426782, 965214.4622594756, 965214.4622594763, 278332.2489544182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6455400.0000, 
sim time next is 6456000.0000, 
raw observation next is [29.76666666666667, 68.0, 1.0, 2.0, 0.4759441985218537, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564975152, 665047.5487652187, 665047.5487652187, 179915.0364663518], 
processed observation next is [1.0, 0.7391304347826086, 0.6097946287519749, 0.68, 1.0, 1.0, 0.36860746809861894, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399450888802, 0.18473543021256075, 0.18473543021256075, 0.2685299051736594], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0848497], dtype=float32), -1.305882]. 
=============================================
[2019-04-23 11:01:04,339] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[48.25315 ]
 [42.44948 ]
 [42.214886]
 [42.425644]
 [41.455322]], R is [[51.5618782 ]
 [51.04626083]
 [50.53579712]
 [50.03044128]
 [49.54138947]].
[2019-04-23 11:01:28,430] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-23 11:01:28,449] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:01:28,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:01:28,466] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run25
[2019-04-23 11:01:28,616] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:01:28,617] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:01:28,635] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run25
[2019-04-23 11:01:28,641] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:01:28,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:01:28,824] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:01:28,826] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run25
[2019-04-23 11:01:28,960] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:01:28,989] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:01:28,989] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:01:28,990] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run25
[2019-04-23 11:01:29,133] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run25
[2019-04-23 11:02:21,124] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36249882]
[2019-04-23 11:02:21,127] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.88333333333333, 96.33333333333333, 1.0, 2.0, 0.3623963527946852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555663.2742153206, 555663.2742153206, 171050.950247655]
[2019-04-23 11:02:21,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:02:21,135] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6106939321463247
[2019-04-23 11:02:27,461] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36249882]
[2019-04-23 11:02:27,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.9, 93.16666666666666, 1.0, 2.0, 0.3993069385917264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599172.2455126302, 599172.2455126296, 174553.9317165279]
[2019-04-23 11:02:27,471] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:02:27,497] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4935580662431436
[2019-04-23 11:03:05,793] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36249882]
[2019-04-23 11:03:05,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.41020808666667, 87.05697017666667, 1.0, 2.0, 0.5104850564481924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713328.4726269646, 713328.4726269646, 185254.8195324763]
[2019-04-23 11:03:05,796] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:03:05,798] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5265250884554925
[2019-04-23 11:03:26,491] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36249882]
[2019-04-23 11:03:26,492] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.51666666666667, 78.33333333333334, 1.0, 2.0, 0.5453361878470017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762045.3183621697, 762045.3183621702, 191003.2064775471]
[2019-04-23 11:03:26,493] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:03:26,494] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07180041195681397
[2019-04-23 11:03:35,029] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36249882]
[2019-04-23 11:03:35,030] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.26666666666667, 51.0, 1.0, 2.0, 0.9786643947533393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1367962.244172892, 1367962.244172892, 292493.176317346]
[2019-04-23 11:03:35,030] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:03:35,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0809682320219538
[2019-04-23 11:03:40,473] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36249882]
[2019-04-23 11:03:40,474] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.0, 55.0, 1.0, 2.0, 0.8513237951756709, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984733933433, 6.9112, 168.9123159782387, 2086905.480240012, 2019662.182742088, 420928.318459025]
[2019-04-23 11:03:40,475] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:03:40,476] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6198393304072868
[2019-04-23 11:03:40,477] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2086905.480240012 W.
[2019-04-23 11:03:48,779] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:03:49,712] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:03:49,912] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:03:50,022] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:03:50,070] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:03:51,095] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 600000, evaluation results [600000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:04:03,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:03,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7862
[2019-04-23 11:04:03,864] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 91.33333333333334, 1.0, 2.0, 0.7108302742604002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 993412.6403899976, 993412.6403899969, 223198.3808237764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6679200.0000, 
sim time next is 6679800.0000, 
raw observation next is [25.95, 91.0, 1.0, 2.0, 0.723737547369012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1011459.641126569, 1011459.641126569, 226050.5412515593], 
processed observation next is [1.0, 0.30434782608695654, 0.42890995260663506, 0.91, 1.0, 1.0, 0.66715367152893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28096101142404695, 0.28096101142404695, 0.33738886753964076], 
reward next is 0.6626, 
noisyNet noise sample is [array([0.887856], dtype=float32), -1.8978682]. 
=============================================
[2019-04-23 11:04:06,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:06,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3424
[2019-04-23 11:04:06,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1854261.493821668 W.
[2019-04-23 11:04:06,633] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.73333333333334, 65.66666666666667, 1.0, 2.0, 0.6850877577047334, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.971309390119771, 6.9112, 168.9125982432377, 1854261.493821668, 1811617.914113091, 382617.6976111993], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6711600.0000, 
sim time next is 6712200.0000, 
raw observation next is [29.7, 66.0, 1.0, 2.0, 0.6320207054275887, 1.0, 1.0, 0.6320207054275887, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1767175.795239608, 1767175.795239609, 346372.4091975461], 
processed observation next is [1.0, 0.6956521739130435, 0.6066350710900474, 0.66, 1.0, 1.0, 0.556651452322396, 1.0, 0.5, 0.556651452322396, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.49088216534433554, 0.4908821653443358, 0.5169737450709643], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4008198], dtype=float32), -0.01688489]. 
=============================================
[2019-04-23 11:04:19,733] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:19,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8735
[2019-04-23 11:04:19,750] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 52.33333333333333, 1.0, 2.0, 0.3407713090822146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530194.1966820222, 530194.1966820229, 169169.2460751638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6864600.0000, 
sim time next is 6865200.0000, 
raw observation next is [28.4, 51.0, 1.0, 2.0, 0.337884572356129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 526686.61259816, 526686.6125981595, 168914.812118924], 
processed observation next is [0.0, 0.4782608695652174, 0.5450236966824644, 0.51, 1.0, 1.0, 0.2022705691037699, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14630183683282225, 0.14630183683282208, 0.25211165987899103], 
reward next is 0.7479, 
noisyNet noise sample is [array([0.5310679], dtype=float32), -0.515635]. 
=============================================
[2019-04-23 11:04:25,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:25,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-23 11:04:25,835] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 57.0, 1.0, 2.0, 0.4769893388404018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666508.4050141387, 666508.4050141381, 180069.4859981938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6967800.0000, 
sim time next is 6968400.0000, 
raw observation next is [30.66666666666667, 58.66666666666666, 1.0, 2.0, 0.4781170296412782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668084.6493509321, 668084.6493509315, 180238.6457860113], 
processed observation next is [0.0, 0.6521739130434783, 0.6524486571879939, 0.5866666666666666, 1.0, 1.0, 0.37122533691720266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1855790692641478, 0.18557906926414763, 0.2690129041582258], 
reward next is 0.7310, 
noisyNet noise sample is [array([-1.1091942], dtype=float32), -0.8040408]. 
=============================================
[2019-04-23 11:04:26,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:26,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7542
[2019-04-23 11:04:26,815] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 60.5, 1.0, 2.0, 0.4765513060145896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665896.1396225395, 665896.1396225395, 180003.3291291465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6971400.0000, 
sim time next is 6972000.0000, 
raw observation next is [30.0, 60.0, 1.0, 2.0, 0.4731713967734117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662880.5402011555, 662880.5402011562, 179719.9022907576], 
processed observation next is [0.0, 0.6956521739130435, 0.6208530805687204, 0.6, 1.0, 1.0, 0.365266743100496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18413348338920985, 0.18413348338921004, 0.2682386601354591], 
reward next is 0.7318, 
noisyNet noise sample is [array([-0.5187044], dtype=float32), -0.06529274]. 
=============================================
[2019-04-23 11:04:26,821] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:26,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4877
[2019-04-23 11:04:26,856] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 87.33333333333334, 1.0, 2.0, 0.4244092032906673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621184.5767911512, 621184.5767911518, 176202.0838894907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6921600.0000, 
sim time next is 6922200.0000, 
raw observation next is [24.35, 87.66666666666667, 1.0, 2.0, 0.4255335402604308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622937.6104078983, 622937.6104078983, 176374.3481268774], 
processed observation next is [0.0, 0.08695652173913043, 0.35308056872037924, 0.8766666666666667, 1.0, 1.0, 0.307871735253531, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17303822511330508, 0.17303822511330508, 0.2632452957117573], 
reward next is 0.7368, 
noisyNet noise sample is [array([-0.9841084], dtype=float32), 0.36308038]. 
=============================================
[2019-04-23 11:04:26,858] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.67716 ]
 [75.7268  ]
 [75.786194]
 [75.87107 ]
 [75.78532 ]], R is [[75.60910797]
 [75.58435822]
 [75.55937195]
 [75.53427124]
 [75.50934601]].
[2019-04-23 11:04:30,113] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:30,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4723
[2019-04-23 11:04:30,146] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 60.0, 1.0, 2.0, 0.4731713967734117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662880.5402011555, 662880.5402011562, 179719.9022907576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6972000.0000, 
sim time next is 6972600.0000, 
raw observation next is [30.0, 59.5, 1.0, 2.0, 0.4690658186551107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659796.1706858793, 659796.1706858799, 179454.4124499215], 
processed observation next is [0.0, 0.6956521739130435, 0.6208530805687204, 0.595, 1.0, 1.0, 0.3603202634398925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18327671407941093, 0.18327671407941107, 0.2678424066416739], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.5137521], dtype=float32), 0.8936317]. 
=============================================
[2019-04-23 11:04:35,760] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:35,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4648
[2019-04-23 11:04:35,794] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 85.33333333333333, 1.0, 2.0, 0.6777628954659383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965422.0169614085, 965422.0169614085, 218538.9102346063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7011600.0000, 
sim time next is 7012200.0000, 
raw observation next is [25.33333333333334, 85.66666666666667, 1.0, 2.0, 0.6654128531676959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 947290.1127677842, 947290.1127677842, 215851.9789133081], 
processed observation next is [1.0, 0.13043478260869565, 0.3996840442338076, 0.8566666666666667, 1.0, 1.0, 0.5968829556237301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2631361424354956, 0.2631361424354956, 0.32216713270643], 
reward next is 0.6778, 
noisyNet noise sample is [array([-0.8071391], dtype=float32), 1.4695797]. 
=============================================
[2019-04-23 11:04:38,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:38,945] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2556
[2019-04-23 11:04:39,001] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 79.33333333333334, 1.0, 2.0, 0.4756000108491052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675383.9255396517, 675383.9255396517, 181250.010066206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7111200.0000, 
sim time next is 7111800.0000, 
raw observation next is [26.5, 78.5, 1.0, 2.0, 0.478114743326414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677855.911743498, 677855.9117434974, 181493.3092074225], 
processed observation next is [1.0, 0.30434782608695654, 0.4549763033175356, 0.785, 1.0, 1.0, 0.3712225823209807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18829330881763834, 0.18829330881763814, 0.2708855361304814], 
reward next is 0.7291, 
noisyNet noise sample is [array([-1.4428056], dtype=float32), -0.9620466]. 
=============================================
[2019-04-23 11:04:41,717] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:41,726] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5189
[2019-04-23 11:04:41,732] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 79.33333333333334, 1.0, 2.0, 0.4756000108491052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675383.9255396517, 675383.9255396517, 181250.010066206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7111200.0000, 
sim time next is 7111800.0000, 
raw observation next is [26.5, 78.5, 1.0, 2.0, 0.478114743326414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677855.911743498, 677855.9117434974, 181493.3092074225], 
processed observation next is [1.0, 0.30434782608695654, 0.4549763033175356, 0.785, 1.0, 1.0, 0.3712225823209807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18829330881763834, 0.18829330881763814, 0.2708855361304814], 
reward next is 0.7291, 
noisyNet noise sample is [array([-0.25950196], dtype=float32), -0.52312446]. 
=============================================
[2019-04-23 11:04:56,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:56,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2168
[2019-04-23 11:04:56,134] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333333, 91.33333333333334, 1.0, 2.0, 0.3594736064731612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551913.1091430961, 551913.1091430968, 170755.0267024983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7249200.0000, 
sim time next is 7249800.0000, 
raw observation next is [22.41666666666667, 91.16666666666667, 1.0, 2.0, 0.3577087760983002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549808.5951016503, 549808.5951016503, 170595.6592206569], 
processed observation next is [1.0, 0.9130434782608695, 0.26145339652448685, 0.9116666666666667, 1.0, 1.0, 0.2261551519256629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15272460975045843, 0.15272460975045843, 0.25462038689650285], 
reward next is 0.7454, 
noisyNet noise sample is [array([-0.19841832], dtype=float32), 1.554534]. 
=============================================
[2019-04-23 11:04:58,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:04:58,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-23 11:04:58,761] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333333, 76.33333333333334, 1.0, 2.0, 0.3805926029902567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575320.9897914657, 575320.9897914657, 172520.3311500463], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7337400.0000, 
sim time next is 7338000.0000, 
raw observation next is [24.96666666666667, 76.66666666666667, 1.0, 2.0, 0.3793609202977866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573736.363949075, 573736.3639490744, 172388.2977895759], 
processed observation next is [1.0, 0.9565217391304348, 0.3823064770932071, 0.7666666666666667, 1.0, 1.0, 0.2522420726479357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15937121220807637, 0.15937121220807624, 0.2572959668501133], 
reward next is 0.7427, 
noisyNet noise sample is [array([1.716358], dtype=float32), 0.66614217]. 
=============================================
[2019-04-23 11:04:58,785] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.60389]
 [83.65193]
 [83.55001]
 [83.4906 ]
 [83.31236]], R is [[83.51219177]
 [83.41957855]
 [83.32770538]
 [83.23665619]
 [83.14655304]].
[2019-04-23 11:05:07,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:05:07,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4293
[2019-04-23 11:05:07,456] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 87.33333333333334, 1.0, 2.0, 0.3024593106594867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482888.18072191, 482888.1807219094, 165845.5775294041], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7420800.0000, 
sim time next is 7421400.0000, 
raw observation next is [21.35, 88.0, 1.0, 2.0, 0.3049986694153428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486589.7910919671, 486589.7910919671, 166108.2189702462], 
processed observation next is [1.0, 0.9130434782608695, 0.2109004739336494, 0.88, 1.0, 1.0, 0.16264899929559376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13516383085887976, 0.13516383085887976, 0.24792271488096448], 
reward next is 0.7521, 
noisyNet noise sample is [array([-1.8870918], dtype=float32), -0.0063239713]. 
=============================================
[2019-04-23 11:05:13,601] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:05:13,710] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8077
[2019-04-23 11:05:13,756] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 83.66666666666667, 1.0, 2.0, 0.2860117294206158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461116.4562357008, 461116.4562357002, 164349.0661752169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7413000.0000, 
sim time next is 7413600.0000, 
raw observation next is [21.43333333333334, 83.33333333333334, 1.0, 2.0, 0.2868760058878072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462313.4609190621, 462313.4609190621, 164430.9176967341], 
processed observation next is [1.0, 0.8260869565217391, 0.21484992101105885, 0.8333333333333335, 1.0, 1.0, 0.1408144649250689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12842040581085057, 0.12842040581085057, 0.24541928014437925], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.6486055], dtype=float32), 0.528097]. 
=============================================
[2019-04-23 11:05:20,441] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:05:20,463] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1937
[2019-04-23 11:05:20,516] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 87.0, 1.0, 2.0, 0.4043482749404768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597252.8594319754, 597252.8594319754, 174104.4724803634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7506000.0000, 
sim time next is 7506600.0000, 
raw observation next is [24.15, 87.5, 1.0, 2.0, 0.4043904469570637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597081.3529848715, 597081.3529848715, 174081.5188522395], 
processed observation next is [0.0, 0.9130434782608695, 0.34360189573459715, 0.875, 1.0, 1.0, 0.2823981288639322, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16585593138468652, 0.16585593138468652, 0.2598231624660291], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.81269085], dtype=float32), 2.0493813]. 
=============================================
[2019-04-23 11:05:23,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:05:23,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6917
[2019-04-23 11:05:23,637] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 79.0, 1.0, 2.0, 0.4107915108411754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 601354.902592889, 601354.9025928896, 174320.5427892415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7482600.0000, 
sim time next is 7483200.0000, 
raw observation next is [25.66666666666666, 78.66666666666667, 1.0, 2.0, 0.4119781442281258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602733.0706916737, 602733.0706916737, 174438.6517716226], 
processed observation next is [0.0, 0.6086956521739131, 0.4154818325434437, 0.7866666666666667, 1.0, 1.0, 0.2915399328049708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16742585296990936, 0.16742585296990936, 0.26035619667406357], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.2250036], dtype=float32), -0.44695702]. 
=============================================
[2019-04-23 11:05:28,399] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-23 11:05:28,407] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:05:28,407] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:05:28,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run26
[2019-04-23 11:05:28,498] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:05:28,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:05:28,505] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:05:28,501] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:05:28,505] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:05:28,506] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run26
[2019-04-23 11:05:28,509] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:05:28,578] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run26
[2019-04-23 11:05:28,502] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:05:28,643] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:05:28,576] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run26
[2019-04-23 11:05:28,705] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run26
[2019-04-23 11:06:06,660] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38585994]
[2019-04-23 11:06:06,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.46666666666667, 92.16666666666667, 1.0, 2.0, 0.4831536105225981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687491.0278524919, 687491.0278524926, 182582.7446476806]
[2019-04-23 11:06:06,670] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:06:06,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5871916009058461
[2019-04-23 11:06:46,275] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38585994]
[2019-04-23 11:06:46,278] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.03333333333333, 59.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.220503175254982, 6.9112, 168.9107142798229, 1673332.617245777, 1453905.212148164, 311347.9564602989]
[2019-04-23 11:06:46,278] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:06:46,280] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8212278696439581
[2019-04-23 11:06:46,281] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1673332.617245777 W.
[2019-04-23 11:07:05,680] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38585994]
[2019-04-23 11:07:05,682] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.06666666666667, 47.33333333333333, 1.0, 2.0, 0.5965750984172447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 833673.9689499894, 833673.9689499901, 200109.9380469998]
[2019-04-23 11:07:05,683] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:07:05,690] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5480906981774016
[2019-04-23 11:07:42,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38585994]
[2019-04-23 11:07:42,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.6, 88.0, 1.0, 2.0, 0.8229685654645147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1150215.090218088, 1150215.090218088, 249630.8066591812]
[2019-04-23 11:07:42,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:07:42,838] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23601390376093845
[2019-04-23 11:08:25,886] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:08:26,043] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:08:26,061] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:08:26,104] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:08:26,240] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:08:27,252] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 625000, evaluation results [625000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:08:29,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:08:29,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9753
[2019-04-23 11:08:29,039] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.5, 1.0, 2.0, 0.4819867045926162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673493.5551222712, 673493.5551222712, 180821.801901838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7597800.0000, 
sim time next is 7598400.0000, 
raw observation next is [25.13333333333333, 92.0, 1.0, 2.0, 0.4824504968940798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674141.8307629039, 674141.8307629032, 180891.9996816422], 
processed observation next is [0.0, 0.9565217391304348, 0.3902053712480251, 0.92, 1.0, 1.0, 0.37644638180009615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1872616196563622, 0.187261619656362, 0.2699880592263316], 
reward next is 0.7300, 
noisyNet noise sample is [array([-1.1621456], dtype=float32), -1.36471]. 
=============================================
[2019-04-23 11:08:34,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:08:34,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0621
[2019-04-23 11:08:34,340] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.71666666666667, 91.16666666666667, 1.0, 2.0, 0.6029276688474788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854008.1614414913, 854008.1614414913, 202772.713782027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7631400.0000, 
sim time next is 7632000.0000, 
raw observation next is [24.8, 91.0, 1.0, 2.0, 0.5626204026534609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 794781.8612301777, 794781.861230177, 195089.2512603661], 
processed observation next is [1.0, 0.34782608695652173, 0.3744075829383887, 0.91, 1.0, 1.0, 0.47303662970296495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22077273923060492, 0.22077273923060473, 0.2911779869557703], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.61707616], dtype=float32), 0.7666779]. 
=============================================
[2019-04-23 11:08:34,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.79374]
 [66.95072]
 [67.23425]
 [67.27656]
 [67.2421 ]], R is [[67.1561203 ]
 [67.18191528]
 [67.21196747]
 [67.25065613]
 [67.29051208]].
[2019-04-23 11:08:39,051] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:08:39,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4339
[2019-04-23 11:08:39,062] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 90.33333333333334, 1.0, 2.0, 0.527664243299526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737342.2142358475, 737342.2142358475, 188043.839472033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7771200.0000, 
sim time next is 7771800.0000, 
raw observation next is [26.45, 90.66666666666667, 1.0, 2.0, 0.5268233517355035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736166.7703692763, 736166.7703692763, 187905.3444407823], 
processed observation next is [1.0, 0.9565217391304348, 0.45260663507109006, 0.9066666666666667, 1.0, 1.0, 0.4299076526933777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2044907695470212, 0.2044907695470212, 0.28045573797131684], 
reward next is 0.7195, 
noisyNet noise sample is [array([-1.5543876], dtype=float32), -1.0610809]. 
=============================================
[2019-04-23 11:08:39,134] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:08:39,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9553
[2019-04-23 11:08:39,146] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2060903.387124989 W.
[2019-04-23 11:08:39,151] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 68.0, 1.0, 2.0, 0.7369698254995538, 1.0, 1.0, 0.7369698254995538, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2060903.387124989, 2060903.387124989, 390349.9670120849], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7826400.0000, 
sim time next is 7827000.0000, 
raw observation next is [30.41666666666666, 68.33333333333334, 1.0, 2.0, 0.522933698277636, 1.0, 2.0, 0.522933698277636, 1.0, 1.0, 0.9032708073656437, 6.911199999999999, 6.9112, 170.5573041426782, 2193677.108601026, 2193677.108601027, 430438.6466089208], 
processed observation next is [1.0, 0.6086956521739131, 0.6406003159557659, 0.6833333333333335, 1.0, 1.0, 0.4252213232260675, 1.0, 1.0, 0.4252213232260675, 1.0, 0.5, 0.8820375699581021, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6093547523891738, 0.6093547523891742, 0.6424457412073445], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59591645], dtype=float32), -1.2692274]. 
=============================================
[2019-04-23 11:08:39,163] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[52.979973]
 [52.87578 ]
 [52.1502  ]
 [49.371117]
 [47.001743]], R is [[51.36179352]
 [50.84817505]
 [50.33969498]
 [49.8362999 ]
 [49.3379364 ]].
[2019-04-23 11:08:39,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:08:39,683] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7027
[2019-04-23 11:08:39,688] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 85.33333333333334, 1.0, 2.0, 0.9537145800155455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333065.875792265, 1333065.875792265, 285137.617281171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7783800.0000, 
sim time next is 7784400.0000, 
raw observation next is [26.3, 85.66666666666667, 1.0, 2.0, 0.8737783464958102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1221269.793485835, 1221269.793485835, 262823.7158245348], 
processed observation next is [1.0, 0.08695652173913043, 0.4454976303317536, 0.8566666666666667, 1.0, 1.0, 0.8479257186696508, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33924160930162084, 0.33924160930162084, 0.3922742027231863], 
reward next is 0.6077, 
noisyNet noise sample is [array([-0.10058795], dtype=float32), 0.7333458]. 
=============================================
[2019-04-23 11:08:40,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:40,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:40,280] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-04-23 11:08:47,814] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:08:47,814] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6343
[2019-04-23 11:08:47,814] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1678884.984554454 W.
[2019-04-23 11:08:47,855] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 71.0, 1.0, 2.0, 0.6004641235982704, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.911411222838447, 6.9112, 168.9129075871723, 1678884.984554454, 1678735.135846531, 365200.5179545556], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7909200.0000, 
sim time next is 7909800.0000, 
raw observation next is [30.03333333333333, 70.83333333333334, 1.0, 2.0, 0.3926503809198016, 1.0, 1.0, 0.3926503809198016, 1.0, 2.0, 0.675414142270344, 6.911199999999999, 6.9112, 170.5573041426782, 1646725.998355243, 1646725.998355243, 347968.3167860223], 
processed observation next is [1.0, 0.5652173913043478, 0.622432859399684, 0.7083333333333335, 1.0, 1.0, 0.26825347098771274, 1.0, 0.5, 0.26825347098771274, 1.0, 1.0, 0.6041635881345658, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4574238884320119, 0.4574238884320119, 0.5193556966955556], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0404963], dtype=float32), -1.7297336]. 
=============================================
[2019-04-23 11:08:51,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:51,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:51,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-04-23 11:08:51,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:51,718] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:51,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-04-23 11:08:51,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:51,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:51,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-04-23 11:08:52,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:52,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:52,008] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-04-23 11:08:52,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:52,154] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:52,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-04-23 11:08:53,861] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:53,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:53,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-04-23 11:08:53,963] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:53,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:53,974] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-04-23 11:08:54,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:54,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:54,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-04-23 11:08:54,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:54,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:54,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-04-23 11:08:54,457] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:54,458] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:54,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run4
[2019-04-23 11:08:55,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:55,527] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:55,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-04-23 11:08:56,321] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:56,322] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:56,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-04-23 11:08:56,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:56,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:56,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-04-23 11:08:56,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:08:56,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:08:56,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-04-23 11:09:00,077] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:09:00,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6064
[2019-04-23 11:09:00,138] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 70.0, 1.0, 2.0, 0.3856228047450939, 1.0, 1.0, 0.3856228047450939, 1.0, 1.0, 0.6520813968669501, 6.9112, 6.9112, 170.5573041426782, 1654906.100241671, 1654906.100241671, 345904.9571333519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 60000.0000, 
sim time next is 60600.0000, 
raw observation next is [26.68333333333333, 70.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.355505531262357, 6.9112, 168.9103166098346, 1834110.544301288, 1518909.834881781, 321587.5698565537], 
processed observation next is [1.0, 0.6956521739130435, 0.4636650868878356, 0.705, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.04443055312623567, 0.0, 0.8294269820318226, 0.5094751511948022, 0.4219193985782725, 0.47998144754709504], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3191174], dtype=float32), -0.21056704]. 
=============================================
[2019-04-23 11:09:01,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:09:01,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:09:01,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-04-23 11:09:22,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:09:22,413] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4550
[2019-04-23 11:09:22,442] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 87.0, 1.0, 2.0, 0.310256711402336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491622.1101814056, 491622.1101814062, 166423.4228649743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 231000.0000, 
sim time next is 231600.0000, 
raw observation next is [21.73333333333333, 87.0, 1.0, 2.0, 0.3089933277960151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 489982.389484485, 489982.3894844856, 166309.8875247809], 
processed observation next is [0.0, 0.6956521739130435, 0.22906793048973137, 0.87, 1.0, 1.0, 0.1674618407180905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13610621930124583, 0.136106219301246, 0.2482237127235536], 
reward next is 0.7518, 
noisyNet noise sample is [array([-0.00645493], dtype=float32), -1.080187]. 
=============================================
[2019-04-23 11:09:24,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:09:24,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8585
[2019-04-23 11:09:24,371] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 94.0, 1.0, 2.0, 0.2814696938757787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 453563.7445835401, 453563.7445835401, 163838.4135973949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [20.01666666666667, 94.16666666666667, 1.0, 2.0, 0.2797420541595443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 451222.5037970109, 451222.5037970115, 163681.6926408365], 
processed observation next is [0.0, 0.13043478260869565, 0.14770932069510287, 0.9416666666666668, 1.0, 1.0, 0.13221934236089672, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1253395843880586, 0.12533958438805876, 0.2443010337922933], 
reward next is 0.7557, 
noisyNet noise sample is [array([-0.91621387], dtype=float32), 0.2311948]. 
=============================================
[2019-04-23 11:09:26,810] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:09:26,862] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-23 11:09:26,877] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 78.0, 1.0, 2.0, 0.3052345561612678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483650.6060567775, 483650.6060567775, 165841.4262128047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 297000.0000, 
sim time next is 297600.0000, 
raw observation next is [23.06666666666667, 77.66666666666666, 1.0, 2.0, 0.3048790734057678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482884.8895118933, 482884.889511894, 165781.7774566349], 
processed observation next is [0.0, 0.43478260869565216, 0.29225908372827825, 0.7766666666666666, 1.0, 1.0, 0.16250490771779252, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13413469153108148, 0.13413469153108168, 0.24743548874124613], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.3645112], dtype=float32), 0.19921635]. 
=============================================
[2019-04-23 11:09:54,514] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-23 11:09:54,551] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:09:54,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:09:54,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run27
[2019-04-23 11:09:54,602] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:09:54,602] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:09:54,604] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run27
[2019-04-23 11:09:54,638] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:09:54,639] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:09:54,639] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:09:54,641] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run27
[2019-04-23 11:09:54,642] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:09:54,672] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:09:54,677] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run27
[2019-04-23 11:09:54,742] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:09:54,744] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run27
[2019-04-23 11:10:09,628] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:10:09,631] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.93333333333334, 71.0, 1.0, 2.0, 0.3157376207322793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501470.9354559084, 501470.9354559084, 167172.8447355299]
[2019-04-23 11:10:09,636] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:10:09,638] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17606563858273883
[2019-04-23 11:10:15,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:10:15,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.68333333333333, 95.16666666666667, 1.0, 2.0, 0.3863456947753357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598072.0404958345, 598072.0404958345, 174882.9244018733]
[2019-04-23 11:10:15,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:10:15,411] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6263870065537653
[2019-04-23 11:11:02,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:11:02,513] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 94.0, 1.0, 2.0, 0.4399813768221698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631967.518960236, 631967.5189602353, 176938.3436916701]
[2019-04-23 11:11:02,513] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:11:02,542] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9453494160219946
[2019-04-23 11:11:26,095] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:11:26,100] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 66.0, 1.0, 2.0, 0.8559188813399997, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.974107739991187, 6.9112, 168.9125314346131, 2093336.932677975, 2048708.129121259, 423217.2878351987]
[2019-04-23 11:11:26,102] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:11:26,107] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34279268844186195
[2019-04-23 11:11:26,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2093336.932677975 W.
[2019-04-23 11:11:35,155] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:11:35,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.19361222, 83.25061265666668, 1.0, 2.0, 0.5693570474481219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795624.309849986, 795624.309849986, 195177.1253412096]
[2019-04-23 11:11:35,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:11:35,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8049882308248366
[2019-04-23 11:11:56,820] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:11:56,822] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 68.66666666666667, 1.0, 2.0, 0.8613698158822611, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.000726896739739, 6.9112, 168.912424111681, 2100966.31712703, 2037453.055490707, 423763.7757128793]
[2019-04-23 11:11:56,823] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:11:56,825] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11615094423949757
[2019-04-23 11:11:56,827] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2100966.31712703 W.
[2019-04-23 11:11:57,081] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:11:57,081] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.78656396666667, 77.01539770166667, 1.0, 2.0, 0.8356714624904091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1167978.952023525, 1167978.952023525, 252862.9259845432]
[2019-04-23 11:11:57,082] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:11:57,085] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08514348498153523
[2019-04-23 11:12:12,088] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:12,092] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.50829864333333, 75.74781942333334, 1.0, 2.0, 0.8711370125633451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104286, 1270639.44343338, 1270639.44343338, 269746.0310328713]
[2019-04-23 11:12:12,093] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:12:12,117] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10002997475775299
[2019-04-23 11:12:13,250] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:13,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.68333333333333, 90.0, 1.0, 2.0, 0.5361237086903345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749167.3864504865, 749167.3864504859, 189449.0325888569]
[2019-04-23 11:12:13,257] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:12:13,263] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.678969728594165
[2019-04-23 11:12:20,118] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:20,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.25, 94.5, 1.0, 2.0, 0.5137712496333934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717922.0004436873, 717922.0004436866, 185780.9826875968]
[2019-04-23 11:12:20,139] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:12:20,142] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10385040535129264
[2019-04-23 11:12:20,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:20,320] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 86.0, 1.0, 2.0, 0.5786117407569856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 808561.8182402804, 808561.8182402797, 196829.9783471324]
[2019-04-23 11:12:20,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:12:20,323] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20446868876346858
[2019-04-23 11:12:20,773] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:20,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.0, 76.0, 1.0, 2.0, 0.9043500772425433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264024.980357338, 1264024.980357338, 271139.6984128628]
[2019-04-23 11:12:20,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:12:20,777] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5133336253791856
[2019-04-23 11:12:27,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:27,973] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.95, 62.83333333333333, 1.0, 2.0, 0.5292061733555125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739497.6111338534, 739497.611133854, 188298.0382992323]
[2019-04-23 11:12:27,976] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:12:27,988] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.433944389259715
[2019-04-23 11:12:36,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:36,319] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.73333333333333, 68.33333333333334, 1.0, 2.0, 0.3963354577669586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591054.166666376, 591054.1666663768, 173704.2671741604]
[2019-04-23 11:12:36,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:12:36,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13616787482027526
[2019-04-23 11:12:57,969] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:12:58,043] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:12:58,186] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:12:58,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:12:58,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3886404]
[2019-04-23 11:12:58,395] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.88155492, 81.87079155, 1.0, 2.0, 0.5365966667606632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749828.5208419813, 749828.5208419813, 189525.8256541776]
[2019-04-23 11:12:58,395] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:12:58,395] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19610400643703574
[2019-04-23 11:12:58,412] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:12:59,426] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 650000, evaluation results [650000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:13:01,109] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:01,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2016
[2019-04-23 11:13:01,130] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 55.5, 1.0, 2.0, 0.6145650588639401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010260.390179418, 1010260.390179418, 218778.4486695058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 559800.0000, 
sim time next is 560400.0000, 
raw observation next is [24.56666666666666, 55.0, 1.0, 2.0, 0.633009382082244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1039117.682835463, 1039117.682835462, 222885.9367263796], 
processed observation next is [1.0, 0.4782608695652174, 0.3633491311216427, 0.55, 1.0, 1.0, 0.5578426290147518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2886438007876286, 0.28864380078762836, 0.3326655772035516], 
reward next is 0.6673, 
noisyNet noise sample is [array([1.2701154], dtype=float32), 1.7920365]. 
=============================================
[2019-04-23 11:13:13,634] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:13,649] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1648
[2019-04-23 11:13:13,654] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 83.0, 1.0, 2.0, 0.2991464842670608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476535.71579723, 476535.7157972306, 165377.3005016374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 852000.0000, 
sim time next is 852600.0000, 
raw observation next is [22.03333333333333, 83.5, 1.0, 2.0, 0.3008289006429775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478878.6753059169, 478878.6753059163, 165538.9578176553], 
processed observation next is [0.0, 0.8695652173913043, 0.2432859399684044, 0.835, 1.0, 1.0, 0.15762518149756327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1330218542516436, 0.13302185425164342, 0.24707307136963477], 
reward next is 0.7529, 
noisyNet noise sample is [array([0.7636702], dtype=float32), -0.09615041]. 
=============================================
[2019-04-23 11:13:14,756] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:14,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1684
[2019-04-23 11:13:14,771] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2939414835487605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469210.9502239273, 469210.9502239273, 164875.3876123086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 895800.0000, 
sim time next is 896400.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2941999846469605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469623.3579305353, 469623.3579305353, 164904.2378586496], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14963853571922953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13045093275848202, 0.13045093275848202, 0.2461257281472382], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.02689231], dtype=float32), 0.60933894]. 
=============================================
[2019-04-23 11:13:14,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:14,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9973
[2019-04-23 11:13:14,955] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 67.5, 1.0, 2.0, 0.2984486034324834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 475367.6815119153, 475367.681511916, 165293.4735364873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 832200.0000, 
sim time next is 832800.0000, 
raw observation next is [24.3, 68.0, 1.0, 2.0, 0.3007735365567235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478765.4867511411, 478765.4867511405, 165530.4651628454], 
processed observation next is [0.0, 0.6521739130434783, 0.3507109004739337, 0.68, 1.0, 1.0, 0.15755847777918494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13299041298642808, 0.1329904129864279, 0.24706039576544087], 
reward next is 0.7529, 
noisyNet noise sample is [array([-0.634102], dtype=float32), 0.116441615]. 
=============================================
[2019-04-23 11:13:16,627] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:16,639] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6675
[2019-04-23 11:13:16,643] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333333, 70.33333333333333, 1.0, 2.0, 0.3137929996768977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493946.926019905, 493946.926019905, 166523.4966483734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 920400.0000, 
sim time next is 921000.0000, 
raw observation next is [24.36666666666667, 70.66666666666667, 1.0, 2.0, 0.3135463153491793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493711.9461238421, 493711.9461238421, 166509.8329997223], 
processed observation next is [0.0, 0.6521739130434783, 0.3538704581358612, 0.7066666666666667, 1.0, 1.0, 0.1729473678905775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1371422072566228, 0.1371422072566228, 0.24852213880555568], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.30150962], dtype=float32), 0.30548617]. 
=============================================
[2019-04-23 11:13:16,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[80.373245]
 [80.34737 ]
 [80.320114]
 [80.297516]
 [80.30994 ]], R is [[80.33174133]
 [80.27988434]
 [80.22846985]
 [80.1774826 ]
 [80.12693024]].
[2019-04-23 11:13:19,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:19,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2977
[2019-04-23 11:13:19,651] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.654795584693358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008446.270262785, 1008446.270262785, 222905.818017141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 987000.0000, 
sim time next is 987600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.584112130816609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 899525.1845637473, 899525.1845637478, 207878.2152914389], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4989302780922999, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24986810682326313, 0.2498681068232633, 0.31026599297229684], 
reward next is 0.6897, 
noisyNet noise sample is [array([-0.2786653], dtype=float32), -0.87134826]. 
=============================================
[2019-04-23 11:13:26,400] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:26,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8136
[2019-04-23 11:13:26,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 78.0, 1.0, 2.0, 0.6821492543553236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1071185.225632032, 1071185.225632032, 231280.6963054911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1155600.0000, 
sim time next is 1156200.0000, 
raw observation next is [23.58333333333333, 77.16666666666667, 1.0, 2.0, 0.6333834938920408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 992957.2007269862, 992957.2007269862, 219962.0367019851], 
processed observation next is [1.0, 0.391304347826087, 0.31674565560821466, 0.7716666666666667, 1.0, 1.0, 0.5582933661349889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27582144464638503, 0.27582144464638503, 0.3283015473163957], 
reward next is 0.6717, 
noisyNet noise sample is [array([1.5492424], dtype=float32), 0.38417974]. 
=============================================
[2019-04-23 11:13:27,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:27,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0886
[2019-04-23 11:13:27,747] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 93.5, 1.0, 2.0, 0.5559673665526376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792256.6378224494, 792256.63782245, 194794.0729365264], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1305000.0000, 
sim time next is 1305600.0000, 
raw observation next is [24.26666666666667, 93.33333333333334, 1.0, 2.0, 0.5487567883739879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782188.4906874401, 782188.4906874401, 193552.1787014624], 
processed observation next is [1.0, 0.08695652173913043, 0.34913112164297017, 0.9333333333333335, 1.0, 1.0, 0.4563334799686601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21727458074651115, 0.21727458074651115, 0.2888838488081528], 
reward next is 0.7111, 
noisyNet noise sample is [array([-0.90552956], dtype=float32), -1.6912316]. 
=============================================
[2019-04-23 11:13:28,995] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:29,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1950
[2019-04-23 11:13:29,007] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 58.66666666666667, 1.0, 2.0, 0.3293212876343562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508207.406258325, 508207.4062583257, 167303.9016481442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1185600.0000, 
sim time next is 1186200.0000, 
raw observation next is [27.15, 59.5, 1.0, 2.0, 0.3250169273635997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501528.1998844551, 501528.1998844551, 166786.9665317235], 
processed observation next is [1.0, 0.7391304347826086, 0.485781990521327, 0.595, 1.0, 1.0, 0.18676738236578277, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1393133888567931, 0.1393133888567931, 0.2489357709428709], 
reward next is 0.7511, 
noisyNet noise sample is [array([-1.2875336], dtype=float32), 0.3665822]. 
=============================================
[2019-04-23 11:13:30,172] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:30,181] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9259
[2019-04-23 11:13:30,185] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 80.0, 1.0, 2.0, 0.908203050069434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1323374.684152444, 1323374.684152444, 280119.690900178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1242000.0000, 
sim time next is 1242600.0000, 
raw observation next is [25.8, 79.16666666666667, 1.0, 2.0, 0.889961885578344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1292420.933340215, 1292420.933340215, 274253.1123152329], 
processed observation next is [1.0, 0.391304347826087, 0.42180094786729866, 0.7916666666666667, 1.0, 1.0, 0.8674239585281253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35900581481672633, 0.35900581481672633, 0.40933300345557155], 
reward next is 0.5907, 
noisyNet noise sample is [array([-0.49989724], dtype=float32), -1.3067558]. 
=============================================
[2019-04-23 11:13:31,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:31,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2158
[2019-04-23 11:13:31,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2149495.750375871 W.
[2019-04-23 11:13:31,103] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.45, 73.0, 1.0, 2.0, 0.5124122033291668, 1.0, 1.0, 0.5124122033291668, 1.0, 1.0, 0.8711177128353235, 6.9112, 6.9112, 170.5573041426782, 2149495.750375871, 2149495.750375871, 420303.2814726993], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1258200.0000, 
sim time next is 1258800.0000, 
raw observation next is [28.46666666666667, 73.0, 1.0, 2.0, 0.6853771774785086, 1.0, 2.0, 0.6853771774785086, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1916497.789908993, 1916497.789908993, 367919.0520875534], 
processed observation next is [1.0, 0.5652173913043478, 0.5481832543443919, 0.73, 1.0, 1.0, 0.6209363584078417, 1.0, 1.0, 0.6209363584078417, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5323604971969426, 0.5323604971969426, 0.5491329135635126], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16909462], dtype=float32), 0.6398217]. 
=============================================
[2019-04-23 11:13:33,482] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:33,488] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7559
[2019-04-23 11:13:33,491] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 94.0, 1.0, 2.0, 0.4656736474320637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679141.4078438018, 679141.4078438018, 181995.2526545957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1321200.0000, 
sim time next is 1321800.0000, 
raw observation next is [23.51666666666667, 94.16666666666667, 1.0, 2.0, 0.5283210951349117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772385.0555037736, 772385.0555037736, 192449.4217741873], 
processed observation next is [1.0, 0.30434782608695654, 0.31358609794628767, 0.9416666666666668, 1.0, 1.0, 0.43171216281314656, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21455140430660377, 0.21455140430660377, 0.2872379429465482], 
reward next is 0.7128, 
noisyNet noise sample is [array([-0.5231663], dtype=float32), 0.8167898]. 
=============================================
[2019-04-23 11:13:36,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:13:36,376] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2993
[2019-04-23 11:13:36,379] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.68333333333333, 97.5, 1.0, 2.0, 0.3156211437177996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498074.5004025648, 498074.5004025642, 166860.1739553702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1401000.0000, 
sim time next is 1401600.0000, 
raw observation next is [20.76666666666667, 97.0, 1.0, 2.0, 0.316605561680278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499321.9557494573, 499321.9557494573, 166946.7863432244], 
processed observation next is [0.0, 0.21739130434782608, 0.18325434439178534, 0.97, 1.0, 1.0, 0.1766332068437084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13870054326373812, 0.13870054326373812, 0.2491743079749618], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.39791527], dtype=float32), 0.86742747]. 
=============================================
[2019-04-23 11:13:43,823] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-23 11:13:43,828] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:13:43,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:13:43,831] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run28
[2019-04-23 11:13:43,882] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:13:43,883] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:13:43,900] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:13:43,900] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:13:43,902] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run28
[2019-04-23 11:13:43,934] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:13:43,936] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:13:43,937] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run28
[2019-04-23 11:13:44,012] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run28
[2019-04-23 11:13:44,074] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:13:44,076] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:13:44,078] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run28
[2019-04-23 11:14:00,425] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41820353]
[2019-04-23 11:14:00,430] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.68475351333333, 60.9099791, 1.0, 2.0, 0.3105967080991675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499619.0090008504, 499619.0090008504, 167076.4686652351]
[2019-04-23 11:14:00,436] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:14:00,440] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33501301484579793
[2019-04-23 11:15:51,555] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41820353]
[2019-04-23 11:15:51,573] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.259992635, 88.95697168000001, 1.0, 2.0, 0.6522846953894476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 911557.8286736037, 911557.8286736037, 210881.3379453744]
[2019-04-23 11:15:51,575] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:15:51,592] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46817751885785264
[2019-04-23 11:16:08,815] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41820353]
[2019-04-23 11:16:08,821] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.2, 73.0, 1.0, 2.0, 0.6056682560105386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846386.1140950363, 846386.1140950363, 201805.9833369838]
[2019-04-23 11:16:08,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:16:08,825] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24228897474307065
[2019-04-23 11:16:18,843] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41820353]
[2019-04-23 11:16:18,846] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.6, 47.0, 1.0, 2.0, 0.9989048343682547, 1.0, 2.0, 0.9989048343682547, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2794240.242157866, 2794240.242157865, 528323.9155589325]
[2019-04-23 11:16:18,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:16:18,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7363270324266636
[2019-04-23 11:16:18,848] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2794240.242157866 W.
[2019-04-23 11:16:25,399] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41820353]
[2019-04-23 11:16:25,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.5, 1.0, 2.0, 0.5563401364978955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777427.7126325896, 777427.7126325902, 192894.3859055719]
[2019-04-23 11:16:25,412] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:16:25,427] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.35186849858727276
[2019-04-23 11:17:31,236] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:17:31,373] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:17:31,711] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:17:31,741] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:17:31,832] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:17:32,843] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 675000, evaluation results [675000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:17:50,482] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:17:50,497] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0886
[2019-04-23 11:17:50,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 96.0, 1.0, 2.0, 0.4002887534625481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590660.1738623303, 590660.1738623303, 173474.1237320452], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1618800.0000, 
sim time next is 1619400.0000, 
raw observation next is [23.08333333333334, 96.0, 1.0, 2.0, 0.4046036908317722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596656.7503834943, 596656.7503834943, 174019.6105307531], 
processed observation next is [1.0, 0.7391304347826086, 0.2930489731437602, 0.96, 1.0, 1.0, 0.2826550491949063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1657379862176373, 0.1657379862176373, 0.25973076198619865], 
reward next is 0.7403, 
noisyNet noise sample is [array([-0.35273784], dtype=float32), 0.904819]. 
=============================================
[2019-04-23 11:17:50,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:17:50,955] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7700
[2019-04-23 11:17:51,002] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 85.0, 1.0, 2.0, 0.748747083551946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1136475.048702974, 1136475.048702975, 243473.5066725575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1593000.0000, 
sim time next is 1593600.0000, 
raw observation next is [23.66666666666667, 85.0, 1.0, 2.0, 0.808003420546072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1225805.69447403, 1225805.694474031, 258861.4250405442], 
processed observation next is [1.0, 0.43478260869565216, 0.3206951026856243, 0.85, 1.0, 1.0, 0.7686788199350265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34050158179834167, 0.34050158179834195, 0.3863603358814093], 
reward next is 0.6136, 
noisyNet noise sample is [array([-0.9235939], dtype=float32), -0.41985658]. 
=============================================
[2019-04-23 11:18:10,050] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:10,055] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4152
[2019-04-23 11:18:10,066] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 93.66666666666667, 1.0, 2.0, 0.332688531179466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521559.1821129513, 521559.1821129513, 168582.9201708599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1806000.0000, 
sim time next is 1806600.0000, 
raw observation next is [21.4, 93.83333333333334, 1.0, 2.0, 0.3328415724815518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521531.8351264926, 521531.835126492, 168574.6074704665], 
processed observation next is [1.0, 0.9130434782608695, 0.21327014218009477, 0.9383333333333335, 1.0, 1.0, 0.19619466564042382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1448699542018035, 0.14486995420180335, 0.2516038917469649], 
reward next is 0.7484, 
noisyNet noise sample is [array([1.0301285], dtype=float32), 0.21045163]. 
=============================================
[2019-04-23 11:18:15,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:15,706] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1183
[2019-04-23 11:18:15,723] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333333, 88.33333333333334, 1.0, 2.0, 0.4619212663082412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655440.5529497884, 655440.5529497884, 179131.7906215914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1890600.0000, 
sim time next is 1891200.0000, 
raw observation next is [24.96666666666667, 88.66666666666667, 1.0, 2.0, 0.4590128919104424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651900.7031326913, 651900.703132692, 178778.1598062251], 
processed observation next is [1.0, 0.9130434782608695, 0.3823064770932071, 0.8866666666666667, 1.0, 1.0, 0.3482083035065572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1810835286479698, 0.18108352864797, 0.2668330743376494], 
reward next is 0.7332, 
noisyNet noise sample is [array([-1.6715281], dtype=float32), 0.11432569]. 
=============================================
[2019-04-23 11:18:23,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:23,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-23 11:18:23,920] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 93.33333333333334, 1.0, 2.0, 0.4731049993552706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661551.4432295057, 661551.4432295051, 179550.3527253309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2068800.0000, 
sim time next is 2069400.0000, 
raw observation next is [24.65, 93.66666666666667, 1.0, 2.0, 0.4727235308802371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661211.2445657701, 661211.2445657701, 179518.6265454997], 
processed observation next is [0.0, 0.9565217391304348, 0.3672985781990521, 0.9366666666666668, 1.0, 1.0, 0.36472714563883984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18366979015715837, 0.18366979015715837, 0.2679382485753727], 
reward next is 0.7321, 
noisyNet noise sample is [array([1.3101785], dtype=float32), -0.5335366]. 
=============================================
[2019-04-23 11:18:28,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:28,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4002
[2019-04-23 11:18:28,729] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 94.0, 1.0, 2.0, 0.5066144268144847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707918.0295500153, 707918.029550016, 184638.4407051234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2167200.0000, 
sim time next is 2167800.0000, 
raw observation next is [25.25, 94.16666666666667, 1.0, 2.0, 1.030350846668959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1440257.792198818, 1440257.792198818, 308307.3099489983], 
processed observation next is [1.0, 0.08695652173913043, 0.39573459715639814, 0.9416666666666668, 1.0, 1.0, 1.036567285143324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4000716089441161, 0.4000716089441161, 0.4601601641029826], 
reward next is 0.5398, 
noisyNet noise sample is [array([-0.17910135], dtype=float32), 0.6081255]. 
=============================================
[2019-04-23 11:18:31,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:31,363] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5040
[2019-04-23 11:18:31,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2464205.793693491 W.
[2019-04-23 11:18:31,374] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.9, 67.0, 1.0, 2.0, 0.8810380134981054, 1.0, 2.0, 0.8810380134981054, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2464205.793693491, 2464205.793693492, 461254.779699341], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2217600.0000, 
sim time next is 2218200.0000, 
raw observation next is [31.88333333333333, 67.16666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.75028627096061, 6.9112, 168.9023868521408, 3589511.496942637, 2284882.041615187, 471267.1959338016], 
processed observation next is [1.0, 0.6956521739130435, 0.7101105845181673, 0.6716666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.18390862709606096, 0.0, 0.8293880432912873, 0.9970865269285103, 0.6346894560042187, 0.7033838745280621], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.183207], dtype=float32), 0.67951775]. 
=============================================
[2019-04-23 11:18:35,461] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:35,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4628
[2019-04-23 11:18:35,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 85.00000000000001, 1.0, 2.0, 0.5256722112767287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734557.6458683417, 734557.6458683423, 187716.0619513254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2243400.0000, 
sim time next is 2244000.0000, 
raw observation next is [27.2, 85.0, 1.0, 2.0, 0.5227447013851051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730465.4300006226, 730465.430000622, 187236.4340996137], 
processed observation next is [1.0, 1.0, 0.4881516587677725, 0.85, 1.0, 1.0, 0.4249936161266326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20290706388906185, 0.20290706388906166, 0.2794573643277816], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.9697944], dtype=float32), 0.2332971]. 
=============================================
[2019-04-23 11:18:35,481] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.450485]
 [67.467766]
 [67.38298 ]
 [67.36892 ]
 [67.33857 ]], R is [[67.52618408]
 [67.57074738]
 [67.6142807 ]
 [67.65711975]
 [67.6993103 ]].
[2019-04-23 11:18:35,909] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:35,913] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8481
[2019-04-23 11:18:35,919] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 82.0, 1.0, 2.0, 0.6353367714674457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887863.465847924, 887863.465847924, 207504.9926998152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2349000.0000, 
sim time next is 2349600.0000, 
raw observation next is [27.16666666666667, 82.0, 1.0, 2.0, 0.6655486001863871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930102.0482365907, 930102.0482365907, 213582.6950682274], 
processed observation next is [1.0, 0.17391304347826086, 0.4865718799368091, 0.82, 1.0, 1.0, 0.5970465062486591, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25836168006571963, 0.25836168006571963, 0.3187801418928767], 
reward next is 0.6812, 
noisyNet noise sample is [array([-0.32269153], dtype=float32), 0.04678477]. 
=============================================
[2019-04-23 11:18:36,174] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:18:36,179] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-23 11:18:36,182] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 82.0, 1.0, 2.0, 0.7558563743247565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1056369.644282446, 1056369.644282446, 233362.6827635489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2343600.0000, 
sim time next is 2344200.0000, 
raw observation next is [27.55, 82.00000000000001, 1.0, 2.0, 0.7674307300008373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1072553.899814714, 1072553.899814714, 236072.1416464229], 
processed observation next is [1.0, 0.13043478260869565, 0.504739336492891, 0.8200000000000002, 1.0, 1.0, 0.7197960602419726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2979316388374206, 0.2979316388374206, 0.3523464800692879], 
reward next is 0.6477, 
noisyNet noise sample is [array([-1.5888485], dtype=float32), 1.5558467]. 
=============================================
[2019-04-23 11:18:38,644] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 11:18:38,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:18:38,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:18:38,646] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:18:38,646] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:18:38,647] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:18:38,647] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:18:38,648] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:18:38,648] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:18:38,648] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:18:38,650] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:18:38,661] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run29
[2019-04-23 11:18:38,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run29
[2019-04-23 11:18:38,662] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run29
[2019-04-23 11:18:38,662] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run29
[2019-04-23 11:18:38,741] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run29
[2019-04-23 11:18:41,951] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:18:41,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.20680285, 94.86694651333335, 1.0, 2.0, 0.221638366133515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 369886.7552660154, 369886.7552660161, 157514.4134488395]
[2019-04-23 11:18:41,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:18:41,954] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.318165258644163
[2019-04-23 11:18:45,205] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:18:45,206] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.6, 72.0, 1.0, 2.0, 0.4270096543093645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702212.8281481456, 702212.8281481449, 183554.951037988]
[2019-04-23 11:18:45,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:18:45,211] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8750020497841506
[2019-04-23 11:18:50,313] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:18:50,314] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.26666666666667, 97.0, 1.0, 2.0, 0.3765383844681099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569045.7232164071, 569045.7232164064, 171962.2030524945]
[2019-04-23 11:18:50,315] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:18:50,318] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5064897448581205
[2019-04-23 11:18:53,261] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:18:53,263] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.55, 41.5, 1.0, 2.0, 0.3706878679216251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612961.3719222895, 612961.3719222895, 175245.9929756392]
[2019-04-23 11:18:53,264] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:18:53,267] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4405872496770956
[2019-04-23 11:19:04,088] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:19:04,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.06666889, 85.54278835, 1.0, 2.0, 0.8208920487877776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1147311.296208281, 1147311.296208281, 249110.5268874061]
[2019-04-23 11:19:04,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:19:04,093] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37502196550452316
[2019-04-23 11:20:23,255] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:20:23,256] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.1, 90.0, 1.0, 2.0, 0.591103486020586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826024.7817945741, 826024.7817945741, 199099.5312959793]
[2019-04-23 11:20:23,257] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:20:23,260] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5455144248012309
[2019-04-23 11:20:23,839] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.41324198]
[2019-04-23 11:20:23,846] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.03548955, 72.066430095, 1.0, 2.0, 0.6044002065618507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844613.384271516, 844613.3842715167, 201569.658299437]
[2019-04-23 11:20:23,848] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:20:23,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.003274880742125119
[2019-04-23 11:21:11,768] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:21:12,808] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:21:13,409] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:21:13,696] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:21:13,808] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:21:14,830] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 700000, evaluation results [700000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:21:15,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:21:15,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2317
[2019-04-23 11:21:15,963] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1770397.568788861 W.
[2019-04-23 11:21:15,978] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.75, 78.0, 1.0, 2.0, 0.6331720039389583, 1.0, 2.0, 0.6331720039389583, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1770397.568788861, 1770397.568788862, 346827.0647607725], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2547000.0000, 
sim time next is 2547600.0000, 
raw observation next is [28.86666666666667, 77.33333333333334, 1.0, 2.0, 0.6763153726623562, 1.0, 2.0, 0.6763153726623562, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1891136.198415885, 1891136.198415885, 364149.2237748885], 
processed observation next is [1.0, 0.4782608695652174, 0.567140600315956, 0.7733333333333334, 1.0, 1.0, 0.6100185212799472, 1.0, 1.0, 0.6100185212799472, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5253156106710791, 0.5253156106710791, 0.5435063041416246], 
reward next is 0.4565, 
noisyNet noise sample is [array([-2.0166953], dtype=float32), -0.4067163]. 
=============================================
[2019-04-23 11:21:17,865] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:21:17,865] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3620
[2019-04-23 11:21:17,866] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2105046.506800886 W.
[2019-04-23 11:21:17,888] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.55, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.828641230438564, 6.9112, 168.9081154799867, 2105046.506800886, 1454200.775753411, 311351.2622795781], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2427000.0000, 
sim time next is 2427600.0000, 
raw observation next is [28.5, 80.33333333333334, 1.0, 2.0, 0.6237861462713691, 1.0, 1.0, 0.6237861462713691, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1744132.631819448, 1744132.631819448, 343204.3841896984], 
processed observation next is [1.0, 0.08695652173913043, 0.5497630331753555, 0.8033333333333335, 1.0, 1.0, 0.5467302967124928, 1.0, 0.5, 0.5467302967124928, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48448128661651335, 0.48448128661651335, 0.5122453495368633], 
reward next is 0.4878, 
noisyNet noise sample is [array([0.06144296], dtype=float32), 1.0414816]. 
=============================================
[2019-04-23 11:21:26,030] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:21:26,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3124
[2019-04-23 11:21:26,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1815873.633789693 W.
[2019-04-23 11:21:26,041] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.75, 87.5, 1.0, 2.0, 0.6494224745049163, 1.0, 2.0, 0.6494224745049163, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1815873.633789693, 1815873.633789693, 353219.5045752018], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2471400.0000, 
sim time next is 2472000.0000, 
raw observation next is [26.86666666666667, 87.0, 1.0, 2.0, 0.698296124676941, 1.0, 2.0, 0.698296124676941, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1952655.5393651, 1952655.5393651, 373391.3466185024], 
processed observation next is [1.0, 0.6086956521739131, 0.4723538704581361, 0.87, 1.0, 1.0, 0.6365013550324591, 1.0, 1.0, 0.6365013550324591, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5424043164903055, 0.5424043164903055, 0.5573005173410484], 
reward next is 0.4427, 
noisyNet noise sample is [array([-0.01853539], dtype=float32), -0.26254097]. 
=============================================
[2019-04-23 11:21:26,167] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[52.276466]
 [50.997776]
 [50.115692]
 [50.0753  ]
 [50.16961 ]], R is [[52.41399765]
 [52.36266327]
 [52.30289459]
 [51.77986526]
 [51.26206589]].
[2019-04-23 11:21:43,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:21:43,361] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0761
[2019-04-23 11:21:43,363] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 92.0, 1.0, 2.0, 0.4379643431404646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 632869.7156598038, 632869.7156598032, 177129.8350404189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2602800.0000, 
sim time next is 2603400.0000, 
raw observation next is [24.08333333333334, 92.0, 1.0, 2.0, 0.4366324199952233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 631501.1169488276, 631501.1169488283, 177008.9383551589], 
processed observation next is [0.0, 0.13043478260869565, 0.34044233807267016, 0.92, 1.0, 1.0, 0.3212438795123172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1754169769302299, 0.1754169769302301, 0.26419244530620734], 
reward next is 0.7358, 
noisyNet noise sample is [array([-0.88394636], dtype=float32), 0.04416868]. 
=============================================
[2019-04-23 11:21:49,292] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:21:49,321] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7883
[2019-04-23 11:21:49,411] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5061442769512821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707260.8465983135, 707260.8465983128, 184564.6073105778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2642400.0000, 
sim time next is 2643000.0000, 
raw observation next is [27.0, 83.16666666666667, 1.0, 2.0, 0.5042421916762652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704602.0857174951, 704602.0857174951, 184263.4967873398], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.8316666666666667, 1.0, 1.0, 0.4027014357545363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19572280158819308, 0.19572280158819308, 0.27502014445871614], 
reward next is 0.7250, 
noisyNet noise sample is [array([-0.20051116], dtype=float32), -3.0861197]. 
=============================================
[2019-04-23 11:21:49,545] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.04447]
 [76.94067]
 [76.92809]
 [76.90028]
 [76.88003]], R is [[76.99060059]
 [76.94522858]
 [76.90031433]
 [76.85586548]
 [76.81191254]].
[2019-04-23 11:21:57,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:21:57,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7225
[2019-04-23 11:21:57,985] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3954311956380803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590041.9324671689, 590041.9324671696, 173621.4593269038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2671800.0000, 
sim time next is 2672400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3953053316863929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589854.1961933773, 589854.1961933773, 173604.2388289574], 
processed observation next is [0.0, 0.9565217391304348, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2714522068510758, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1638483878314937, 0.1638483878314937, 0.25911080422232446], 
reward next is 0.7409, 
noisyNet noise sample is [array([-1.1677405], dtype=float32), -0.97937876]. 
=============================================
[2019-04-23 11:22:03,644] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:03,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6695
[2019-04-23 11:22:03,722] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 100.0, 1.0, 2.0, 0.4157842069799606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610144.6145788797, 610144.6145788797, 175191.8476672653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [22.5, 100.0, 1.0, 2.0, 0.409181697380103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604254.4559888901, 604254.4559888901, 174749.847841923], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 1.0, 1.0, 1.0, 0.28817071973506386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16784845999691392, 0.16784845999691392, 0.2608206684207806], 
reward next is 0.7392, 
noisyNet noise sample is [array([0.17673989], dtype=float32), -0.31871042]. 
=============================================
[2019-04-23 11:22:08,153] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:08,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1348
[2019-04-23 11:22:08,226] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4755042199235282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664432.5644943232, 664432.5644943238, 179847.056403567], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2700000.0000, 
sim time next is 2700600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4758230458241832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664878.2063941116, 664878.206394111, 179894.6935287791], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.3684615009929918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.184688390665031, 0.18468839066503084, 0.2684995425802673], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.2672494], dtype=float32), 0.3537943]. 
=============================================
[2019-04-23 11:22:13,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:13,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8062
[2019-04-23 11:22:13,868] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4102168239075166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 604419.0189070036, 604419.018907003, 174725.3111859522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2830800.0000, 
sim time next is 2831400.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.412121919392743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607224.7099600784, 607224.7099600784, 174988.1797634892], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.29171315589487107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1686735305444662, 0.1686735305444662, 0.2611763877067003], 
reward next is 0.7388, 
noisyNet noise sample is [array([1.9934347], dtype=float32), 0.75659657]. 
=============================================
[2019-04-23 11:22:24,465] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:24,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6518
[2019-04-23 11:22:24,537] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3578061486825639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551189.273692111, 551189.273692111, 170746.0739588929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2877000.0000, 
sim time next is 2877600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3449059683428146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531314.878465243, 531314.8784652436, 169107.2311510369], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2107300823407405, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1475874662403453, 0.14758746624034544, 0.25239885246423416], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.7133907], dtype=float32), 0.24827321]. 
=============================================
[2019-04-23 11:22:29,368] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:29,415] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1184
[2019-04-23 11:22:29,442] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3381740776411795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 525244.7149331778, 525244.7149331785, 168748.9619089661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2966400.0000, 
sim time next is 2967000.0000, 
raw observation next is [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3597844510134063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558262.3869942317, 558262.3869942317, 171442.0311372862], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.9900000000000001, 1.0, 1.0, 0.2286559650763931, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1550728852761755, 0.1550728852761755, 0.2558836285631137], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.9505715], dtype=float32), -0.042236704]. 
=============================================
[2019-04-23 11:22:29,691] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[75.18438]
 [75.07751]
 [75.05345]
 [75.00793]
 [75.00461]], R is [[75.028862  ]
 [75.02671051]
 [75.0244751 ]
 [75.02223969]
 [75.01973724]].
[2019-04-23 11:22:30,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:30,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2577
[2019-04-23 11:22:30,391] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.5355604728846093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 841524.837482649, 841524.837482649, 200166.0786956559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [22.0, 88.00000000000001, 1.0, 2.0, 0.532402841094277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836546.0507181135, 836546.0507181135, 199571.6985395206], 
processed observation next is [1.0, 0.4782608695652174, 0.2417061611374408, 0.8800000000000001, 1.0, 1.0, 0.4366299290292494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23237390297725377, 0.23237390297725377, 0.2978682067754039], 
reward next is 0.7021, 
noisyNet noise sample is [array([-0.45157], dtype=float32), 0.6195343]. 
=============================================
[2019-04-23 11:22:33,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:33,176] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-23 11:22:33,226] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.3120817245335519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494987.5331758604, 494987.5331758604, 166680.0862831681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3000600.0000, 
sim time next is 3001200.0000, 
raw observation next is [20.33333333333333, 98.0, 1.0, 2.0, 0.3118763279392696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495249.3745411828, 495249.3745411834, 166709.224319104], 
processed observation next is [1.0, 0.7391304347826086, 0.16271721958925733, 0.98, 1.0, 1.0, 0.1709353348665899, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13756927070588412, 0.1375692707058843, 0.24881973778970745], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.00751402], dtype=float32), 0.90709203]. 
=============================================
[2019-04-23 11:22:56,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:22:56,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2240
[2019-04-23 11:22:56,485] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.499523373175637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698006.0852411458, 698006.0852411451, 183521.4434408496], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3178800.0000, 
sim time next is 3179400.0000, 
raw observation next is [25.83333333333334, 89.83333333333334, 1.0, 2.0, 0.4980560036117437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695954.9932257273, 695954.9932257273, 183292.0319990777], 
processed observation next is [1.0, 0.8260869565217391, 0.42338072669826254, 0.8983333333333334, 1.0, 1.0, 0.3952481971225828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19332083145159093, 0.19332083145159093, 0.2735701970135488], 
reward next is 0.7264, 
noisyNet noise sample is [array([-1.1066933], dtype=float32), 0.8341846]. 
=============================================
[2019-04-23 11:23:07,504] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:23:07,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2135
[2019-04-23 11:23:07,580] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5809147418937404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811781.300739505, 811781.300739505, 197244.8818838761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3254400.0000, 
sim time next is 3255000.0000, 
raw observation next is [32.83333333333334, 64.33333333333333, 1.0, 2.0, 0.5874568731785758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820926.9322557616, 820926.9322557609, 198432.8783152514], 
processed observation next is [0.0, 0.6956521739130435, 0.7551342812006324, 0.6433333333333333, 1.0, 1.0, 0.5029600881669587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22803525895993376, 0.22803525895993357, 0.29616847509739014], 
reward next is 0.7038, 
noisyNet noise sample is [array([0.1008988], dtype=float32), -1.2432078]. 
=============================================
[2019-04-23 11:23:07,609] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[72.78058]
 [72.68198]
 [72.67333]
 [72.6538 ]
 [72.62478]], R is [[72.70689392]
 [72.68543243]
 [72.66415405]
 [72.6431427 ]
 [72.62244415]].
[2019-04-23 11:23:16,945] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-23 11:23:16,953] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:23:16,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:23:16,955] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run30
[2019-04-23 11:23:17,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:23:17,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:23:17,035] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run30
[2019-04-23 11:23:17,180] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:23:17,181] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:23:17,181] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:23:17,182] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:23:17,182] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:23:17,184] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:23:17,186] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run30
[2019-04-23 11:23:17,247] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run30
[2019-04-23 11:23:17,295] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run30
[2019-04-23 11:23:58,034] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:23:58,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.8, 93.66666666666667, 1.0, 2.0, 0.3790008955934643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570699.7466036596, 570699.7466036596, 172043.4656388914]
[2019-04-23 11:23:58,036] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:23:58,040] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9898142096950651
[2019-04-23 11:24:02,702] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:24:02,703] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.84246317, 82.120666335, 1.0, 2.0, 0.5536664141145028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773690.1018330551, 773690.1018330551, 192432.4383498808]
[2019-04-23 11:24:02,704] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:24:02,707] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7980619261110627
[2019-04-23 11:24:03,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:24:03,580] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.46666666666667, 57.33333333333333, 1.0, 2.0, 0.5632005230683276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787017.9438259731, 787017.9438259738, 194090.6405838508]
[2019-04-23 11:24:03,581] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:24:03,582] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8144480299008419
[2019-04-23 11:24:27,948] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:24:27,951] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [41.73381311666667, 48.83908387666667, 1.0, 2.0, 0.8711689995244186, 1.0, 1.0, 0.8711689995244186, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 2436544.217505763, 2436544.217505763, 456530.7405437656]
[2019-04-23 11:24:27,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:24:27,962] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7662395180427004
[2019-04-23 11:24:27,966] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2436544.217505763 W.
[2019-04-23 11:24:38,945] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:24:38,949] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 85.0, 1.0, 2.0, 0.6388565772129519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892784.3527778083, 892784.3527778083, 208207.8018326514]
[2019-04-23 11:24:38,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:24:38,959] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15686835070100535
[2019-04-23 11:25:28,176] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:25:28,188] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.024509901603679, 6.9112, 168.9118808378079, 1609439.996020496, 1529054.563911314, 323158.4444182154]
[2019-04-23 11:25:28,190] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:25:28,212] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8405415724087115
[2019-04-23 11:25:32,079] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:25:32,082] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.06666666666667, 94.33333333333334, 1.0, 2.0, 0.3181200173644202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 501764.1874958618, 501764.1874958611, 167131.322666799]
[2019-04-23 11:25:32,085] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:25:32,090] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8379302457484117
[2019-04-23 11:25:34,955] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.4337729]
[2019-04-23 11:25:34,961] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.9, 61.0, 1.0, 2.0, 0.3973506042377368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595692.9165795903, 595692.9165795903, 174222.2274148657]
[2019-04-23 11:25:34,962] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:25:34,964] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.049502956601768844
[2019-04-23 11:25:42,535] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:25:43,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:25:43,562] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:25:43,567] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:25:43,690] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:25:44,705] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 725000, evaluation results [725000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:25:48,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:25:48,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-23 11:25:48,870] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5375003237314502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751091.7178992851, 751091.7178992851, 189680.1092237539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3367800.0000, 
sim time next is 3368400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5387947595640994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752901.1765240583, 752901.176524059, 189897.467371552], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44433103561939685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091392157011273, 0.2091392157011275, 0.2834290557784358], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.8959203], dtype=float32), 1.2305692]. 
=============================================
[2019-04-23 11:25:54,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:25:54,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1420
[2019-04-23 11:25:54,276] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 87.33333333333334, 1.0, 2.0, 0.9440242841929092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1319512.732405236, 1319512.732405236, 282332.0423234189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [26.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8807175721614448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1230974.290583841, 1230974.290583841, 264685.8386431032], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271754, 0.8566666666666667, 1.0, 1.0, 0.856286231519813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34193730293995583, 0.34193730293995583, 0.3950534905120943], 
reward next is 0.6049, 
noisyNet noise sample is [array([-0.6814827], dtype=float32), 1.5032427]. 
=============================================
[2019-04-23 11:26:07,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:26:08,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3805
[2019-04-23 11:26:08,005] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 79.0, 1.0, 2.0, 0.5393362461112337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753658.1075837769, 753658.1075837775, 189988.5131985938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3533400.0000, 
sim time next is 3534000.0000, 
raw observation next is [28.33333333333333, 79.0, 1.0, 2.0, 0.5347848399878199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747295.8228908767, 747295.8228908767, 189225.2850913939], 
processed observation next is [1.0, 0.9130434782608695, 0.541864139020537, 0.79, 1.0, 1.0, 0.4394998072142408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2075821730252435, 0.2075821730252435, 0.28242579864387146], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.36407536], dtype=float32), 0.23402128]. 
=============================================
[2019-04-23 11:26:08,081] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[63.730293]
 [63.64911 ]
 [63.347084]
 [63.26868 ]
 [63.19179 ]], R is [[63.95707321]
 [64.03394318]
 [64.10887146]
 [64.18188477]
 [64.25337219]].
[2019-04-23 11:26:14,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:26:14,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6256
[2019-04-23 11:26:14,887] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5191391345342314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725425.4147654794, 725425.4147654788, 186649.0593393132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3624000.0000, 
sim time next is 3624600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5192437556441211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725571.6582813936, 725571.6582813943, 186666.0397329655], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4207756092097844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20154768285594266, 0.20154768285594285, 0.2786060294521873], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.5746876], dtype=float32), 0.59770644]. 
=============================================
[2019-04-23 11:26:23,650] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:26:23,651] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9008
[2019-04-23 11:26:23,712] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4974998686123532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695177.6276825194, 695177.62768252, 183204.9238889904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3634800.0000, 
sim time next is 3635400.0000, 
raw observation next is [27.0, 79.83333333333334, 1.0, 2.0, 0.4936814575882907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689840.2678328209, 689840.2678328209, 182611.5835546549], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.7983333333333335, 1.0, 1.0, 0.3899776597449286, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19162229662022803, 0.19162229662022803, 0.2725546023203805], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.34031093], dtype=float32), -3.2007816]. 
=============================================
[2019-04-23 11:27:00,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:00,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7373
[2019-04-23 11:27:00,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2174702.1479959 W.
[2019-04-23 11:27:00,522] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.9140495357402171, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005988652380127, 6.9112, 168.9123159599315, 2174702.1479959, 2107456.070634858, 437972.7946728321], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3985200.0000, 
sim time next is 3985800.0000, 
raw observation next is [29.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4670047930758067, 1.0, 1.0, 0.4670047930758067, 1.0, 2.0, 0.8110328847276116, 6.9112, 6.9112, 170.5573041426782, 1958844.024446022, 1958844.024446022, 393079.7618276294], 
processed observation next is [1.0, 0.13043478260869565, 0.6129541864139019, 0.7983333333333335, 1.0, 1.0, 0.35783710009133346, 1.0, 0.5, 0.35783710009133346, 1.0, 1.0, 0.7695522984483069, 0.0, 0.0, 0.8375144448122397, 0.544123340123895, 0.544123340123895, 0.5866862116830289], 
reward next is 0.4133, 
noisyNet noise sample is [array([0.5615777], dtype=float32), -0.3565053]. 
=============================================
[2019-04-23 11:27:04,981] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:04,995] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5215
[2019-04-23 11:27:04,998] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.929289861679488, 6.9112, 168.9126525651582, 1466597.264269796, 1453763.716882355, 311355.5335554228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3998400.0000, 
sim time next is 3999000.0000, 
raw observation next is [29.83333333333333, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.011999370684645, 6.9112, 168.9122115008688, 1525314.127889714, 1453803.90048471, 311355.9980509268], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.010079937068464506, 0.0, 0.8294362868142855, 0.4236983688582539, 0.4038344168013083, 0.46471044485212953], 
reward next is 0.0313, 
noisyNet noise sample is [array([1.6603765], dtype=float32), -0.85342264]. 
=============================================
[2019-04-23 11:27:05,111] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[52.01234 ]
 [52.43501 ]
 [53.14297 ]
 [53.149998]
 [53.901962]], R is [[50.93307114]
 [50.86857986]
 [50.91226959]
 [50.97653961]
 [51.02799606]].
[2019-04-23 11:27:10,892] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:10,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7634
[2019-04-23 11:27:10,961] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5491538588682363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767382.0065831868, 767382.0065831868, 191655.8233684453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4044600.0000, 
sim time next is 4045200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5477939674306601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765481.0234148199, 765481.0234148204, 191423.2188579557], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.45517345473573506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21263361761522773, 0.2126336176152279, 0.285706296802919], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.8324578], dtype=float32), 1.7822058]. 
=============================================
[2019-04-23 11:27:11,176] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:11,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3024
[2019-04-23 11:27:11,195] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5867293978527562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819909.9476621918, 819909.9476621918, 198299.2891513203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3981000.0000, 
sim time next is 3981600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5860023686779927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818893.5877447528, 818893.5877447533, 198166.8269466856], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5012076731060153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2274704410402091, 0.22747044104020925, 0.29577138350251586], 
reward next is 0.7042, 
noisyNet noise sample is [array([0.40392774], dtype=float32), 0.060078338]. 
=============================================
[2019-04-23 11:27:14,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:14,309] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3516
[2019-04-23 11:27:14,310] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2661598.916088771 W.
[2019-04-23 11:27:14,320] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 71.0, 1.0, 2.0, 0.634358486201973, 1.0, 2.0, 0.634358486201973, 1.0, 2.0, 1.03, 6.991773596876727, 6.9112, 170.5573041426782, 2661598.916088771, 2603880.771121603, 500773.9145169079], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4102200.0000, 
sim time next is 4102800.0000, 
raw observation next is [32.66666666666667, 71.0, 1.0, 2.0, 0.6010180499848635, 1.0, 2.0, 0.6010180499848635, 1.0, 2.0, 1.03, 6.926679133967625, 6.9112, 170.5573041426782, 2521570.183817159, 2510481.850505237, 488349.3319631939], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458138, 0.71, 1.0, 1.0, 0.51929885540345, 1.0, 1.0, 0.51929885540345, 1.0, 1.0, 1.0365853658536586, 0.001547913396762457, 0.0, 0.8375144448122397, 0.700436162171433, 0.697356069584788, 0.7288795999450655], 
reward next is 0.1937, 
noisyNet noise sample is [array([-0.3172154], dtype=float32), 0.26736504]. 
=============================================
[2019-04-23 11:27:14,481] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:14,481] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3012
[2019-04-23 11:27:14,605] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 88.5, 1.0, 2.0, 0.7727494187434324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079991.028715407, 1079991.028715407, 237331.9339852326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4077000.0000, 
sim time next is 4077600.0000, 
raw observation next is [27.06666666666667, 88.66666666666666, 1.0, 2.0, 0.7809082167690075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1091399.587096579, 1091399.587096579, 239278.5963305474], 
processed observation next is [1.0, 0.17391304347826086, 0.48183254344391807, 0.8866666666666666, 1.0, 1.0, 0.7360339961072379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3031665519712719, 0.3031665519712719, 0.35713223332917526], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.3652654], dtype=float32), 0.99314445]. 
=============================================
[2019-04-23 11:27:14,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:27:14,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8250
[2019-04-23 11:27:14,886] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2225871.285584567 W.
[2019-04-23 11:27:14,906] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666666, 79.83333333333334, 1.0, 2.0, 0.9506082917187195, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005990672409787, 6.9112, 168.9123931444886, 2225871.285584567, 2158623.744421615, 448466.6218703049], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4092600.0000, 
sim time next is 4093200.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5053936746472322, 1.0, 1.0, 0.5053936746472322, 1.0, 2.0, 0.877701676620045, 6.9112, 6.9112, 170.5573041426782, 2120024.893724183, 2120024.893724183, 418896.5077589661], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.79, 1.0, 1.0, 0.4040887646352195, 1.0, 0.5, 0.4040887646352195, 1.0, 1.0, 0.8508557031951768, 0.0, 0.0, 0.8375144448122397, 0.5888958038122731, 0.5888958038122731, 0.6252186682969644], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3998827], dtype=float32), -1.7028708]. 
=============================================
[2019-04-23 11:27:31,286] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-23 11:27:31,288] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:27:31,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:27:31,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run31
[2019-04-23 11:27:31,336] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:27:31,338] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:27:31,339] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:27:31,363] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run31
[2019-04-23 11:27:31,365] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:27:31,377] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:27:31,375] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:27:31,437] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:27:31,439] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run31
[2019-04-23 11:27:31,436] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run31
[2019-04-23 11:27:31,615] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:27:31,617] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run31
[2019-04-23 11:27:37,754] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:27:37,754] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.81925467, 86.84957730500001, 1.0, 2.0, 0.3024687761068579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487273.9516556647, 487273.9516556647, 166177.956807582]
[2019-04-23 11:27:37,755] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:27:37,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10611603125427138
[2019-04-23 11:27:41,173] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:27:41,175] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.243054735, 74.53289978000001, 1.0, 2.0, 0.2858749776328243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470055.053319014, 470055.0533190133, 164665.6255503657]
[2019-04-23 11:27:41,176] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:27:41,179] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8556891071285405
[2019-04-23 11:28:16,626] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:28:16,630] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.66666666666667, 93.66666666666667, 1.0, 2.0, 0.3881070356700207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586388.5229972394, 586388.5229972394, 173502.4704056913]
[2019-04-23 11:28:16,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:28:16,645] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33778669459286736
[2019-04-23 11:28:48,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:28:48,129] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.81666666666666, 53.83333333333334, 1.0, 2.0, 0.8424740790766857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1177491.923188244, 1177491.923188244, 254602.5013269725]
[2019-04-23 11:28:48,129] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:28:48,131] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4376728737906971
[2019-04-23 11:29:11,657] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:29:11,661] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.6, 70.33333333333334, 1.0, 2.0, 0.5605375199566821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783295.2839166016, 783295.2839166023, 193624.1886749356]
[2019-04-23 11:29:11,663] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:29:11,679] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9378860329761053
[2019-04-23 11:29:38,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:29:38,562] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.25, 88.0, 1.0, 2.0, 0.5482636191396113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766137.5460209971, 766137.5460209965, 191502.8268794776]
[2019-04-23 11:29:38,563] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:29:38,584] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06233564033114403
[2019-04-23 11:29:42,144] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:29:42,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.45, 75.5, 1.0, 2.0, 0.5540362727117968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774207.1283577326, 774207.128357732, 192495.0459336667]
[2019-04-23 11:29:42,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:29:42,149] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.474992869585957
[2019-04-23 11:30:11,974] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:30:11,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.53333333333333, 87.66666666666667, 1.0, 2.0, 0.5908555148733337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 825678.1251900012, 825678.1251900005, 199054.2660556567]
[2019-04-23 11:30:11,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:30:11,978] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9998167462120537
[2019-04-23 11:30:17,661] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39670703]
[2019-04-23 11:30:17,673] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.43101732, 100.0, 1.0, 2.0, 0.463296381208004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647369.0837911976, 647369.0837911976, 178046.6976171726]
[2019-04-23 11:30:17,674] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:30:17,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.009040309697140536
[2019-04-23 11:30:24,581] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:30:24,665] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:30:24,693] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:30:24,940] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:30:24,987] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:30:26,022] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 750000, evaluation results [750000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:30:27,882] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:30:27,893] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9931
[2019-04-23 11:30:27,912] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3385745.530764027 W.
[2019-04-23 11:30:27,924] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.0, 1.0, 2.0, 0.9722520692358061, 1.0, 2.0, 0.8067160741321655, 1.0, 2.0, 1.03, 7.005119208804803, 6.9112, 170.5573041426782, 3385745.530764027, 3318467.381107992, 621127.3936475031], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4287600.0000, 
sim time next is 4288200.0000, 
raw observation next is [38.0, 50.0, 1.0, 2.0, 0.8889057652913162, 1.0, 2.0, 0.7650429221599205, 1.0, 2.0, 1.03, 7.005112631929873, 6.9112, 170.5573041426782, 3210620.627215636, 3143347.188842644, 587638.5156567065], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.5, 1.0, 1.0, 0.8661515244473689, 1.0, 1.0, 0.7169191833252054, 1.0, 1.0, 1.0365853658536586, 0.009391263192987331, 0.0, 0.8375144448122397, 0.8918390631154545, 0.8731519969007344, 0.8770724114279201], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.02153], dtype=float32), 1.1198441]. 
=============================================
[2019-04-23 11:30:28,462] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:30:28,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0547
[2019-04-23 11:30:28,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2789773.8538416 W.
[2019-04-23 11:30:28,486] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 52.5, 1.0, 2.0, 0.6885665063530277, 1.0, 2.0, 0.6648732926907763, 1.0, 2.0, 1.03, 7.005096830652964, 6.9112, 170.5573041426782, 2789773.8538416, 2722511.734566013, 517742.5835300553], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4205400.0000, 
sim time next is 4206000.0000, 
raw observation next is [36.0, 52.0, 1.0, 2.0, 0.995122441330484, 1.0, 2.0, 0.995122441330484, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2783647.951673088, 2783647.951673087, 526049.0485100127], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.52, 1.0, 1.0, 0.9941234232897398, 1.0, 1.0, 0.9941234232897398, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7732355421314133, 0.7732355421314131, 0.7851478335970339], 
reward next is 0.2149, 
noisyNet noise sample is [array([0.17602265], dtype=float32), -0.87518793]. 
=============================================
[2019-04-23 11:30:28,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[41.70616 ]
 [40.424324]
 [39.71083 ]
 [38.915203]
 [38.213455]], R is [[41.78419495]
 [41.36635208]
 [40.95269012]
 [40.5431633 ]
 [40.13773346]].
[2019-04-23 11:30:45,992] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:30:45,998] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5080
[2019-04-23 11:30:46,007] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 84.0, 1.0, 2.0, 0.5319539987084964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743338.6874111893, 743338.6874111893, 188753.3489196623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4488000.0000, 
sim time next is 4488600.0000, 
raw observation next is [27.16666666666666, 84.0, 1.0, 2.0, 0.5266037821918546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735859.8443584514, 735859.844358452, 187868.5678555646], 
processed observation next is [0.0, 0.9565217391304348, 0.4865718799368086, 0.84, 1.0, 1.0, 0.4296431110745235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20440551232179208, 0.20440551232179222, 0.2804008475456188], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.01129266], dtype=float32), -0.2524242]. 
=============================================
[2019-04-23 11:30:53,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:30:53,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7257
[2019-04-23 11:30:53,841] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.54855875032214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766550.1078980914, 766550.1078980914, 191553.7051416827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4574400.0000, 
sim time next is 4575000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5492822316496979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767561.4582137185, 767561.4582137185, 191677.4982692783], 
processed observation next is [0.0, 0.9565217391304348, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45696654415626253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21321151617047737, 0.21321151617047737, 0.2860858183123557], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.47443902], dtype=float32), -1.0626652]. 
=============================================
[2019-04-23 11:30:53,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.90124]
 [72.89804]
 [72.89532]
 [72.89159]
 [72.9283 ]], R is [[72.88061523]
 [72.86590576]
 [72.85155487]
 [72.83751678]
 [72.82375336]].
[2019-04-23 11:30:58,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:30:58,304] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7468
[2019-04-23 11:30:58,356] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5069772357225876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708425.1698864554, 708425.169886456, 184696.6834716284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4669200.0000, 
sim time next is 4669800.0000, 
raw observation next is [27.0, 84.83333333333333, 1.0, 2.0, 0.5101189259999527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712816.6870251385, 712816.6870251378, 185196.9682904144], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.8483333333333333, 1.0, 1.0, 0.40978183855415984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1980046352847607, 0.1980046352847605, 0.2764133855080812], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.718593], dtype=float32), 0.46582994]. 
=============================================
[2019-04-23 11:31:14,296] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:31:14,338] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1396
[2019-04-23 11:31:14,377] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7545645603778484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1054563.334669906, 1054563.334669906, 233060.318162231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4762200.0000, 
sim time next is 4762800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7526746236852463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051920.691221194, 1051920.691221194, 232622.368919132], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.702017618897887, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29220019200588726, 0.29220019200588726, 0.3471975655509433], 
reward next is 0.6528, 
noisyNet noise sample is [array([-0.8172621], dtype=float32), 0.49435076]. 
=============================================
[2019-04-23 11:31:23,105] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:31:23,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7252
[2019-04-23 11:31:23,181] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4868762882625725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680328.0955097442, 680328.095509745, 181565.058558997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4843200.0000, 
sim time next is 4843800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4859292428987306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679004.3352443101, 679004.3352443101, 181420.5453945243], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3806376420466634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18861231534564168, 0.18861231534564168, 0.27077693342466314], 
reward next is 0.7292, 
noisyNet noise sample is [array([-0.40735817], dtype=float32), 1.0054274]. 
=============================================
[2019-04-23 11:31:58,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:31:58,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4495
[2019-04-23 11:31:58,840] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5183896779389829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724377.795350636, 724377.7953506367, 186526.8721398202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5098200.0000, 
sim time next is 5098800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162498980852747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721386.7339989959, 721386.7339989964, 186180.6622535018], 
processed observation next is [0.0, 0.0, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.41716855190996943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20038520388860995, 0.20038520388861011, 0.27788158545298774], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.40554753], dtype=float32), -0.647556]. 
=============================================
[2019-04-23 11:32:01,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:32:01,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7867
[2019-04-23 11:32:01,780] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 72.66666666666667, 1.0, 2.0, 0.5296395360667537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740103.3907889776, 740103.3907889769, 188370.2028500197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5165400.0000, 
sim time next is 5166000.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.52713651146288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736604.5218805672, 736604.5218805678, 187956.8702118441], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4302849535697349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20461236718904643, 0.2046123671890466, 0.28053264210723], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.19455053], dtype=float32), -0.70935136]. 
=============================================
[2019-04-23 11:32:01,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.51017 ]
 [74.45926 ]
 [74.39177 ]
 [74.30911 ]
 [74.234886]], R is [[74.61083984]
 [74.58358002]
 [74.55603027]
 [74.52822876]
 [74.50012207]].
[2019-04-23 11:32:06,207] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:32:06,232] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9026
[2019-04-23 11:32:06,252] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 64.33333333333334, 1.0, 2.0, 0.5341207986210382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746367.5810222572, 746367.5810222572, 189115.5088919639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5138400.0000, 
sim time next is 5139000.0000, 
raw observation next is [31.5, 65.0, 1.0, 2.0, 0.541680318533968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756934.8364356133, 756934.8364356133, 190385.0663148909], 
processed observation next is [0.0, 0.4782608695652174, 0.6919431279620853, 0.65, 1.0, 1.0, 0.44780761269152775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21025967678767038, 0.21025967678767038, 0.28415681539535953], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.308325], dtype=float32), -0.54513144]. 
=============================================
[2019-04-23 11:32:06,320] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.91197 ]
 [71.920135]
 [71.950615]
 [71.84197 ]
 [71.83867 ]], R is [[71.90234375]
 [71.90105438]
 [71.90149689]
 [71.90348053]
 [71.90585327]].
[2019-04-23 11:32:11,150] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-23 11:32:11,184] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:32:11,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:32:11,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run32
[2019-04-23 11:32:11,193] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:32:11,259] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:32:11,260] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:32:11,260] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:32:11,262] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run32
[2019-04-23 11:32:11,334] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run32
[2019-04-23 11:32:11,401] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:32:11,403] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:32:11,404] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:32:11,405] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run32
[2019-04-23 11:32:11,458] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:32:11,460] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run32
[2019-04-23 11:32:24,085] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3898773]
[2019-04-23 11:32:24,089] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.63333333333333, 80.33333333333334, 1.0, 2.0, 0.4209195753817015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611345.3501203498, 611345.3501203491, 175120.8625157861]
[2019-04-23 11:32:24,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:32:24,093] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13963057136729418
[2019-04-23 11:32:25,640] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3898773]
[2019-04-23 11:32:25,640] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.81666666666667, 60.66666666666666, 1.0, 2.0, 0.7596594214233054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1242601.624228484, 1242601.624228484, 254190.4673776444]
[2019-04-23 11:32:25,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:32:25,648] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7807685731982521
[2019-04-23 11:32:54,545] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3898773]
[2019-04-23 11:32:54,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.91666666666666, 94.00000000000001, 1.0, 2.0, 0.4056492453141737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607118.0543921894, 607118.0543921894, 175242.4147494001]
[2019-04-23 11:32:54,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:32:54,548] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5591651846101396
[2019-04-23 11:33:39,955] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3898773]
[2019-04-23 11:33:39,956] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5744246551545814, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9854748126892707, 6.9112, 6.9112, 168.9129462683313, 1606024.095626132, 1606024.095626132, 348901.2264053582]
[2019-04-23 11:33:39,959] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:33:39,962] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9102082576594512
[2019-04-23 11:33:49,426] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3898773]
[2019-04-23 11:33:49,435] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.05098295, 68.79593609, 1.0, 2.0, 0.794103062051233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1109850.378346437, 1109850.378346438, 242465.7451275071]
[2019-04-23 11:33:49,439] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:33:49,445] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8026418056890972
[2019-04-23 11:33:56,015] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3898773]
[2019-04-23 11:33:56,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.86666666666667, 87.66666666666667, 1.0, 2.0, 0.6067548752978722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847905.2073800185, 847905.2073800185, 202010.5279271824]
[2019-04-23 11:33:56,041] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:33:56,074] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03817940431747624
[2019-04-23 11:34:56,521] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:34:56,937] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:34:57,498] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:34:57,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:34:57,686] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:34:58,702] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 775000, evaluation results [775000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:35:20,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:35:20,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-23 11:35:20,516] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.9, 71.66666666666667, 1.0, 2.0, 0.2976229641470205, 1.0, 2.0, 0.2976229641470205, 1.0, 1.0, 0.5168726633051026, 6.911199999999999, 6.9112, 170.5573041426782, 1247961.037072815, 1247961.037072816, 303460.835607431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5418600.0000, 
sim time next is 5419200.0000, 
raw observation next is [30.9, 72.33333333333334, 1.0, 2.0, 0.5481961799706523, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956449349, 766043.2732869881, 766043.2732869874, 191494.2738703299], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7233333333333334, 1.0, 1.0, 0.45565804815741234, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439944852362, 0.2127897981352745, 0.2127897981352743, 0.28581234906019387], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5152811], dtype=float32), -0.42832553]. 
=============================================
[2019-04-23 11:35:31,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:35:31,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1393
[2019-04-23 11:35:31,691] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3223344.232564006 W.
[2019-04-23 11:35:31,696] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.11666666666667, 63.83333333333334, 1.0, 2.0, 0.8949616397914971, 1.0, 2.0, 0.768070859410011, 1.0, 1.0, 1.03, 7.00511310973814, 6.9112, 170.5573041426782, 3223344.232564006, 3156070.451917517, 589984.7567596799], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5483400.0000, 
sim time next is 5484000.0000, 
raw observation next is [35.23333333333333, 62.66666666666667, 1.0, 2.0, 0.7587614529216443, 1.0, 2.0, 0.6999707659750848, 1.0, 2.0, 1.03, 7.005102365899402, 6.9112, 170.5573041426782, 2937214.170937197, 2869948.086539427, 540532.9532069933], 
processed observation next is [1.0, 0.4782608695652174, 0.8688783570300155, 0.6266666666666667, 1.0, 1.0, 0.7093511480983666, 1.0, 1.0, 0.6385189951507045, 1.0, 1.0, 1.0365853658536586, 0.009390236589940227, 0.0, 0.8375144448122397, 0.8158928252603325, 0.7972078018165074, 0.8067656018014825], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.59011847], dtype=float32), -1.6067852]. 
=============================================
[2019-04-23 11:35:31,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[27.064737]
 [26.274267]
 [27.150713]
 [27.223106]
 [27.226442]], R is [[28.39052963]
 [28.1066246 ]
 [27.82555771]
 [27.54730225]
 [27.27182961]].
[2019-04-23 11:35:33,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:35:33,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3860
[2019-04-23 11:35:33,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3031698.28536246 W.
[2019-04-23 11:35:33,878] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.26666666666667, 65.0, 1.0, 2.0, 0.8037400941244258, 1.0, 2.0, 0.7224600865764755, 1.0, 2.0, 1.03, 7.005105913392093, 6.9112, 170.5573041426782, 3031698.28536246, 2964429.659751373, 556096.537048478], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5416800.0000, 
sim time next is 5417400.0000, 
raw observation next is [31.58333333333333, 68.0, 1.0, 2.0, 0.7826072994697351, 1.0, 2.0, 0.7118936892491302, 1.0, 2.0, 1.03, 7.005104246568966, 6.9112, 170.5573041426782, 2987304.916122216, 2920037.484524337, 548689.0350835291], 
processed observation next is [1.0, 0.6956521739130435, 0.6958925750394943, 0.68, 1.0, 1.0, 0.7380810836984759, 1.0, 1.0, 0.6528839629507592, 1.0, 1.0, 1.0365853658536586, 0.009390424656896634, 0.0, 0.8375144448122397, 0.8298069211450599, 0.8111215234789825, 0.8189388583336255], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2843399], dtype=float32), -0.30840778]. 
=============================================
[2019-04-23 11:36:13,803] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:36:13,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8539
[2019-04-23 11:36:13,908] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.4, 68.0, 1.0, 2.0, 0.5145613744208694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719026.4607476827, 719026.4607476827, 185913.0687880251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5851800.0000, 
sim time next is 5852400.0000, 
raw observation next is [31.2, 69.0, 1.0, 2.0, 0.5264994225176146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735713.9648746852, 735713.9648746852, 187855.3627484682], 
processed observation next is [1.0, 0.7391304347826086, 0.6777251184834123, 0.69, 1.0, 1.0, 0.4295173765272465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2043649902429681, 0.2043649902429681, 0.28038113843054957], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.43278557], dtype=float32), 0.5060309]. 
=============================================
[2019-04-23 11:36:19,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:36:19,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7135
[2019-04-23 11:36:19,637] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 80.5, 1.0, 2.0, 0.9795041303634713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1369136.770771229, 1369136.77077123, 292746.1710240605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5904600.0000, 
sim time next is 5905200.0000, 
raw observation next is [29.53333333333333, 80.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.987940680208244, 6.9112, 168.8954401013694, 3637628.799626471, 1455109.420327927, 306342.9021693337], 
processed observation next is [1.0, 0.34782608695652173, 0.598736176935229, 0.8, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.30767406802082437, 0.0, 0.8293539315647634, 1.0104524443406864, 0.40419706120220195, 0.4572282121930354], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4999474], dtype=float32), -0.3410466]. 
=============================================
[2019-04-23 11:36:27,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:36:27,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3813
[2019-04-23 11:36:27,868] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 91.66666666666667, 1.0, 2.0, 0.5293834369824261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739745.4005211208, 739745.4005211202, 188327.7587514756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5878200.0000, 
sim time next is 5878800.0000, 
raw observation next is [26.3, 92.0, 1.0, 2.0, 0.5278209957178638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737561.3314175853, 737561.3314175847, 188069.7574341381], 
processed observation next is [1.0, 0.043478260869565216, 0.4454976303317536, 0.92, 1.0, 1.0, 0.43110963339501657, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048781476159959, 0.20487814761599574, 0.2807011304987136], 
reward next is 0.7193, 
noisyNet noise sample is [array([-0.7355732], dtype=float32), -0.46851754]. 
=============================================
[2019-04-23 11:36:51,976] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-23 11:36:51,978] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:36:51,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:36:51,980] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:36:51,982] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:36:51,983] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:36:51,983] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:36:51,986] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run33
[2019-04-23 11:36:51,990] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run33
[2019-04-23 11:36:52,006] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:36:52,147] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:36:52,148] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run33
[2019-04-23 11:36:52,026] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run33
[2019-04-23 11:36:52,008] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:36:52,295] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:36:52,298] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run33
[2019-04-23 11:36:56,860] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:36:56,862] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.30476393, 94.61347305333332, 1.0, 2.0, 0.435218192522946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686443.9792391141, 686443.9792391134, 183190.1513197563]
[2019-04-23 11:36:56,864] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:36:56,866] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37048868965342463
[2019-04-23 11:37:14,993] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:37:14,998] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.39331832666667, 46.76607891, 1.0, 2.0, 0.3287190889843549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543262.2304831769, 543262.2304831769, 169690.0869976933]
[2019-04-23 11:37:14,999] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:37:15,005] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8533463351387636
[2019-04-23 11:37:27,552] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:37:27,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.68152703333333, 69.98857015333334, 1.0, 2.0, 0.5469531563854855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764305.6609661015, 764305.6609661022, 191277.9189038687]
[2019-04-23 11:37:27,555] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:37:27,557] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.207889219166556
[2019-04-23 11:37:43,722] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:37:43,737] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.292211115, 96.20916713, 1.0, 2.0, 0.3349426723348732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524446.0970154592, 524446.0970154598, 168795.522305429]
[2019-04-23 11:37:43,742] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:37:43,743] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.30579762441843683
[2019-04-23 11:38:38,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:38:38,519] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.31613600666667, 95.00908838000001, 1.0, 2.0, 0.5471245735969497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764545.28348989, 764545.2834898906, 191308.4489024116]
[2019-04-23 11:38:38,521] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:38:38,526] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6702538342574677
[2019-04-23 11:38:56,136] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:38:56,138] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.8, 77.0, 1.0, 2.0, 0.6209373814670733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867732.5440008627, 867732.5440008627, 204710.5525078101]
[2019-04-23 11:38:56,139] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:38:56,140] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4561445899402865
[2019-04-23 11:39:06,112] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:39:06,119] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.12078819, 58.74380839, 1.0, 2.0, 0.8773306019090635, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986369226093, 6.9112, 168.9122968531047, 2123306.097501635, 2056061.647488983, 427859.3028849016]
[2019-04-23 11:39:06,122] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:39:06,127] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.022980963097188778
[2019-04-23 11:39:06,142] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2123306.097501635 W.
[2019-04-23 11:39:26,130] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39310876]
[2019-04-23 11:39:26,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.73333333333333, 84.16666666666667, 1.0, 2.0, 0.5189932476683724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725221.4883810722, 725221.4883810715, 186624.3060876347]
[2019-04-23 11:39:26,133] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:39:26,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8772711938796046
[2019-04-23 11:39:49,454] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:39:49,657] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:39:49,706] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:39:49,784] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:39:50,306] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:39:51,323] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 800000, evaluation results [800000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:39:57,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:39:57,787] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6868
[2019-04-23 11:39:57,798] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 86.66666666666667, 1.0, 2.0, 0.5376333720016498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751277.702518975, 751277.702518975, 189702.2476683482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6126000.0000, 
sim time next is 6126600.0000, 
raw observation next is [27.28333333333333, 86.83333333333333, 1.0, 2.0, 0.5380119582735724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751806.918324652, 751806.918324652, 189765.8916780368], 
processed observation next is [1.0, 0.9130434782608695, 0.49210110584518163, 0.8683333333333333, 1.0, 1.0, 0.44338790153442453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2088352550901811, 0.2088352550901811, 0.2832326741463236], 
reward next is 0.7168, 
noisyNet noise sample is [array([0.2896368], dtype=float32), 1.4750363]. 
=============================================
[2019-04-23 11:40:03,769] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:40:03,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0670
[2019-04-23 11:40:03,884] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 68.0, 1.0, 2.0, 0.5401834855089805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754842.4443038256, 754842.444303825, 190131.8280194779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6262200.0000, 
sim time next is 6262800.0000, 
raw observation next is [30.66666666666666, 67.66666666666667, 1.0, 2.0, 0.5385036327451984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752494.2175479198, 752494.2175479204, 189849.0417139439], 
processed observation next is [0.0, 0.4782608695652174, 0.6524486571879934, 0.6766666666666667, 1.0, 1.0, 0.44398028041590165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20902617154108885, 0.20902617154108902, 0.28335677867752823], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.41901845], dtype=float32), 0.004190847]. 
=============================================
[2019-04-23 11:40:23,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:40:23,376] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3281
[2019-04-23 11:40:23,406] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 70.0, 1.0, 2.0, 0.5023734663010554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701989.9622427048, 701989.9622427048, 183969.3246355279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6458400.0000, 
sim time next is 6459000.0000, 
raw observation next is [29.2, 70.5, 1.0, 2.0, 0.5075011301951279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709157.4787359153, 709157.4787359147, 184779.8942074853], 
processed observation next is [1.0, 0.782608695652174, 0.5829383886255924, 0.705, 1.0, 1.0, 0.40662786770497333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19698818853775424, 0.19698818853775407, 0.2757908868768437], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.38564178], dtype=float32), 0.6530177]. 
=============================================
[2019-04-23 11:40:23,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[65.63659]
 [64.92327]
 [62.43765]
 [58.9163 ]
 [54.16959]], R is [[66.0819931 ]
 [66.14659119]
 [66.21308899]
 [66.28166199]
 [66.35161591]].
[2019-04-23 11:40:34,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:40:34,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6860
[2019-04-23 11:40:34,858] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 87.0, 1.0, 2.0, 0.5200690135913529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726725.237111787, 726725.237111787, 186800.3863176981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6566400.0000, 
sim time next is 6567000.0000, 
raw observation next is [26.83333333333334, 87.33333333333333, 1.0, 2.0, 0.5196885430402731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726193.3998804901, 726193.3998804907, 186738.5075661945], 
processed observation next is [1.0, 0.0, 0.4707740916271725, 0.8733333333333333, 1.0, 1.0, 0.42131149763888326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017203888556917, 0.20172038885569188, 0.2787141903973052], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.5061526], dtype=float32), -0.6369986]. 
=============================================
[2019-04-23 11:40:34,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[54.648315]
 [58.588013]
 [58.649643]
 [58.704414]
 [58.770126]], R is [[53.7673378 ]
 [53.95085907]
 [54.13252258]
 [54.3123436 ]
 [54.49035263]].
[2019-04-23 11:40:56,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:40:56,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0182
[2019-04-23 11:40:56,437] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 92.66666666666666, 1.0, 2.0, 0.6301144569834896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 880562.4176982759, 880562.4176982765, 206481.4075119509], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6676800.0000, 
sim time next is 6677400.0000, 
raw observation next is [25.6, 92.33333333333333, 1.0, 2.0, 0.629559288602593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879786.2684104894, 879786.2684104894, 206373.2390313194], 
processed observation next is [1.0, 0.2608695652173913, 0.4123222748815167, 0.9233333333333333, 1.0, 1.0, 0.5536858898826422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24438507455846928, 0.24438507455846928, 0.3080197597482379], 
reward next is 0.6920, 
noisyNet noise sample is [array([0.6942523], dtype=float32), 1.0453691]. 
=============================================
[2019-04-23 11:40:58,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:40:58,776] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-23 11:40:58,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1966649.831278552 W.
[2019-04-23 11:40:58,829] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.93333333333334, 63.66666666666667, 1.0, 2.0, 0.4688640561904273, 1.0, 2.0, 0.4688640561904273, 1.0, 2.0, 0.7951715924895524, 6.9112, 6.9112, 170.5573041426782, 1966649.831278552, 1966649.831278552, 391023.6379026044], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6708000.0000, 
sim time next is 6708600.0000, 
raw observation next is [29.9, 64.0, 1.0, 2.0, 0.4811418315642679, 1.0, 2.0, 0.4811418315642679, 1.0, 2.0, 0.8167031902513886, 6.9112, 6.9112, 170.5573041426782, 2018197.45622517, 2018197.45622517, 399034.2025737654], 
processed observation next is [1.0, 0.6521739130434783, 0.6161137440758293, 0.64, 1.0, 1.0, 0.3748696765834553, 1.0, 1.0, 0.3748696765834553, 1.0, 1.0, 0.7764673051846203, 0.0, 0.0, 0.8375144448122397, 0.5606104045069917, 0.5606104045069917, 0.5955734366772618], 
reward next is 0.4044, 
noisyNet noise sample is [array([-0.08220769], dtype=float32), -0.4405431]. 
=============================================
[2019-04-23 11:41:08,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:41:08,023] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3063
[2019-04-23 11:41:08,060] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 78.0, 1.0, 2.0, 0.3824561498131069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576316.7930854935, 576316.7930854928, 172554.259451626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6854400.0000, 
sim time next is 6855000.0000, 
raw observation next is [25.1, 76.33333333333333, 1.0, 2.0, 0.3825835284722049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 576844.8355181072, 576844.8355181077, 172611.3858539963], 
processed observation next is [0.0, 0.34782608695652173, 0.38862559241706174, 0.7633333333333333, 1.0, 1.0, 0.25612473309904205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16023467653280754, 0.1602346765328077, 0.25762893411044224], 
reward next is 0.7424, 
noisyNet noise sample is [array([-0.4254127], dtype=float32), 1.2309357]. 
=============================================
[2019-04-23 11:41:08,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.5532  ]
 [83.4167  ]
 [83.435875]
 [83.44043 ]
 [83.44939 ]], R is [[83.40551758]
 [83.31391907]
 [83.22353363]
 [83.13439941]
 [83.04652405]].
[2019-04-23 11:41:33,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:41:33,873] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5042
[2019-04-23 11:41:33,894] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 76.66666666666667, 1.0, 2.0, 0.4449989567746867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637915.7614895086, 637915.7614895086, 177500.9528324651], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6996000.0000, 
sim time next is 6996600.0000, 
raw observation next is [26.45, 77.0, 1.0, 2.0, 0.446562342492944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 640099.1347765512, 640099.1347765506, 177719.6724194971], 
processed observation next is [0.0, 1.0, 0.45260663507109006, 0.77, 1.0, 1.0, 0.33320764155776383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17780531521570866, 0.1778053152157085, 0.26525324241715986], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.0448834], dtype=float32), 2.6432242]. 
=============================================
[2019-04-23 11:41:38,614] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:41:38,615] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4069
[2019-04-23 11:41:38,669] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.48333333333333, 76.66666666666667, 1.0, 2.0, 0.4743345089592749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662797.591599204, 662797.591599204, 179673.0550942314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7063800.0000, 
sim time next is 7064400.0000, 
raw observation next is [27.36666666666667, 77.33333333333334, 1.0, 2.0, 0.4763694024420677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665641.8819275071, 665641.8819275071, 179976.8854039479], 
processed observation next is [1.0, 0.782608695652174, 0.49605055292259104, 0.7733333333333334, 1.0, 1.0, 0.3691197619783948, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18490052275764088, 0.18490052275764088, 0.2686222170208178], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.09389934], dtype=float32), 1.3452489]. 
=============================================
[2019-04-23 11:41:42,315] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 11:41:42,333] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:41:42,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:41:42,335] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:41:42,341] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:41:42,341] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:41:42,336] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:41:42,343] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:41:42,344] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run34
[2019-04-23 11:41:42,336] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:41:42,374] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:41:42,374] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:41:42,377] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run34
[2019-04-23 11:41:42,404] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run34
[2019-04-23 11:41:42,405] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run34
[2019-04-23 11:41:42,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run34
[2019-04-23 11:41:49,104] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36919647]
[2019-04-23 11:41:49,111] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.03333333333333, 74.0, 1.0, 2.0, 0.2480492543083349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 409379.503844773, 409379.5038447736, 160673.2015517791]
[2019-04-23 11:41:49,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:41:49,121] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5042336582831349
[2019-04-23 11:43:22,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36919647]
[2019-04-23 11:43:22,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.5, 84.0, 1.0, 2.0, 0.592702732294271, 0.0, 2.0, 0.0, 1.0, 2.0, 1.029328636206374, 6.9112, 6.9112, 168.9128283783926, 1657167.357001077, 1657167.357001077, 363060.628871663]
[2019-04-23 11:43:22,124] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:43:22,126] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3890236985429286
[2019-04-23 11:43:22,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1657167.357001077 W.
[2019-04-23 11:43:35,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36919647]
[2019-04-23 11:43:35,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.755672425, 77.26953495833334, 1.0, 2.0, 0.5648185251564316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 789279.7843955184, 789279.7843955177, 194375.070227058]
[2019-04-23 11:43:35,990] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:43:35,993] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5106242642912222
[2019-04-23 11:43:56,186] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36919647]
[2019-04-23 11:43:56,188] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 81.16666666666666, 1.0, 2.0, 0.5891208125267886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823253.063857509, 823253.063857509, 198736.6960920513]
[2019-04-23 11:43:56,189] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:43:56,191] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.48543468014534674
[2019-04-23 11:44:00,048] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36919647]
[2019-04-23 11:44:00,049] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.45, 93.0, 1.0, 2.0, 0.6197460395986303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866067.0176168857, 866067.0176168857, 204481.8795784632]
[2019-04-23 11:44:00,050] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:44:00,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.28042934249548335
[2019-04-23 11:44:21,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.36919647]
[2019-04-23 11:44:21,694] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.56666666666667, 87.0, 1.0, 2.0, 0.643800110652795, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.990768631133193, 6.9112, 168.9118289286079, 1796488.178001936, 1740039.829359711, 373516.1139972314]
[2019-04-23 11:44:21,694] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:44:21,699] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03577450970036644
[2019-04-23 11:44:21,700] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1796488.178001936 W.
[2019-04-23 11:44:52,682] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:44:53,499] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:44:53,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:44:53,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:44:54,050] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:44:55,073] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 825000, evaluation results [825000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:45:01,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:01,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5347
[2019-04-23 11:45:01,277] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 74.33333333333333, 1.0, 2.0, 0.9691403403198644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1357649.012873665, 1357649.012873665, 290126.5411376889], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7130400.0000, 
sim time next is 7131000.0000, 
raw observation next is [27.31666666666667, 75.16666666666667, 1.0, 2.0, 0.9636726043885497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1348991.688292092, 1348991.688292093, 288357.5841332554], 
processed observation next is [1.0, 0.5217391304347826, 0.4936808846761455, 0.7516666666666667, 1.0, 1.0, 0.9562320534801804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37471991341447, 0.3747199134144703, 0.43038445393023195], 
reward next is 0.5696, 
noisyNet noise sample is [array([1.6024203], dtype=float32), 0.71472573]. 
=============================================
[2019-04-23 11:45:01,322] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[53.19133 ]
 [52.75916 ]
 [51.342133]
 [49.8902  ]
 [50.2711  ]], R is [[54.65728378]
 [54.67768478]
 [54.13090897]
 [53.58959961]
 [53.53901291]].
[2019-04-23 11:45:01,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:01,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7278
[2019-04-23 11:45:01,813] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 84.66666666666667, 1.0, 2.0, 0.4683753990956585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659578.6357287432, 659578.6357287432, 179448.9864789623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7166400.0000, 
sim time next is 7167000.0000, 
raw observation next is [25.71666666666667, 84.83333333333334, 1.0, 2.0, 0.4687918454652567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659956.7714684583, 659956.771468459, 179484.0627182439], 
processed observation next is [1.0, 0.9565217391304348, 0.41785150078988953, 0.8483333333333334, 1.0, 1.0, 0.35999017525934546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1833213254079051, 0.1833213254079053, 0.2678866607734983], 
reward next is 0.7321, 
noisyNet noise sample is [array([0.16696164], dtype=float32), 0.61714685]. 
=============================================
[2019-04-23 11:45:01,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.89844 ]
 [72.87529 ]
 [72.862595]
 [72.84816 ]
 [72.89448 ]], R is [[72.91470337]
 [72.91771698]
 [72.92062378]
 [72.92327881]
 [72.92560577]].
[2019-04-23 11:45:23,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:23,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8663
[2019-04-23 11:45:23,161] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 93.0, 1.0, 2.0, 0.6163464047426903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 972426.3776905333, 972426.3776905339, 216825.7324020278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7387200.0000, 
sim time next is 7387800.0000, 
raw observation next is [21.16666666666667, 92.83333333333333, 1.0, 2.0, 0.6387313323200511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1008690.98886403, 1008690.988864029, 221827.4207823641], 
processed observation next is [1.0, 0.5217391304347826, 0.2022116903633494, 0.9283333333333332, 1.0, 1.0, 0.5647365449639169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28019194135111947, 0.28019194135111913, 0.3310857026602449], 
reward next is 0.6689, 
noisyNet noise sample is [array([-1.9627665], dtype=float32), -0.61854225]. 
=============================================
[2019-04-23 11:45:24,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:24,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8866
[2019-04-23 11:45:24,272] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.65, 90.5, 1.0, 2.0, 0.6322900661969493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015932.425856937, 1015932.425856936, 221778.3248422539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7399800.0000, 
sim time next is 7400400.0000, 
raw observation next is [20.63333333333333, 90.33333333333334, 1.0, 2.0, 0.6114700420070801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983143.4715254193, 983143.4715254187, 217231.662735039], 
processed observation next is [1.0, 0.6521739130434783, 0.17693522906793036, 0.9033333333333334, 1.0, 1.0, 0.5318916168760001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2730954087570609, 0.27309540875706073, 0.32422636229110297], 
reward next is 0.6758, 
noisyNet noise sample is [array([-1.1738927], dtype=float32), 1.456125]. 
=============================================
[2019-04-23 11:45:27,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:27,080] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2021
[2019-04-23 11:45:27,091] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 82.66666666666667, 1.0, 2.0, 0.2874157617010465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462797.0075401866, 462797.0075401873, 164463.9532688753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7414800.0000, 
sim time next is 7415400.0000, 
raw observation next is [21.63333333333333, 82.33333333333334, 1.0, 2.0, 0.2874782655347496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462706.1998459483, 462706.1998459483, 164457.4963176636], 
processed observation next is [1.0, 0.8260869565217391, 0.2243285939968403, 0.8233333333333335, 1.0, 1.0, 0.14154007895752962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12852949995720786, 0.12852949995720786, 0.24545894972785612], 
reward next is 0.7545, 
noisyNet noise sample is [array([1.2027107], dtype=float32), 0.6720225]. 
=============================================
[2019-04-23 11:45:34,073] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:34,074] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6252
[2019-04-23 11:45:34,170] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.0, 1.0, 2.0, 0.4051850920970963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596411.0850560442, 596411.0850560435, 173962.7340567657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7509600.0000, 
sim time next is 7510200.0000, 
raw observation next is [23.85, 90.33333333333333, 1.0, 2.0, 0.4051719434541921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596370.3290199444, 596370.3290199444, 173958.2841457388], 
processed observation next is [0.0, 0.9565217391304348, 0.3293838862559243, 0.9033333333333333, 1.0, 1.0, 0.2833396909086652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16565842472776235, 0.16565842472776235, 0.2596392300682669], 
reward next is 0.7404, 
noisyNet noise sample is [array([1.2242151], dtype=float32), 0.3708478]. 
=============================================
[2019-04-23 11:45:58,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:58,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7436
[2019-04-23 11:45:58,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2433965.487224025 W.
[2019-04-23 11:45:58,217] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.8, 61.33333333333334, 1.0, 2.0, 0.5801577249578369, 1.0, 1.0, 0.5801577249578369, 1.0, 2.0, 0.9906837745636212, 6.911199999999999, 6.9112, 170.5573041426782, 2433965.487224025, 2433965.487224025, 471378.3890615738], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7747800.0000, 
sim time next is 7748400.0000, 
raw observation next is [30.7, 61.66666666666667, 1.0, 2.0, 0.7912347477960416, 1.0, 2.0, 0.7912347477960416, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2212809.546192726, 2212809.546192726, 415617.3611159306], 
processed observation next is [1.0, 0.6956521739130435, 0.6540284360189573, 0.6166666666666667, 1.0, 1.0, 0.748475599754267, 1.0, 1.0, 0.748475599754267, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6146693183868683, 0.6146693183868683, 0.6203244195760158], 
reward next is 0.3797, 
noisyNet noise sample is [array([-0.78885955], dtype=float32), -0.52692986]. 
=============================================
[2019-04-23 11:45:58,444] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:45:58,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7795
[2019-04-23 11:45:58,490] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 94.0, 1.0, 2.0, 0.4779205471834551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667810.0132973973, 667810.0132973973, 180208.7063805271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7695600.0000, 
sim time next is 7696200.0000, 
raw observation next is [24.65, 94.5, 1.0, 2.0, 0.47821504555122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668221.6524615595, 668221.6524615588, 180252.9526438478], 
processed observation next is [1.0, 0.043478260869565216, 0.3672985781990521, 0.945, 1.0, 1.0, 0.3713434283749639, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18561712568376654, 0.18561712568376634, 0.26903425767738476], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.61196226], dtype=float32), 0.69608927]. 
=============================================
[2019-04-23 11:46:00,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:46:00,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8941
[2019-04-23 11:46:00,774] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666666, 82.5, 1.0, 2.0, 0.7792158234416188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1089033.080385263, 1089033.080385263, 238872.0986803748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7804200.0000, 
sim time next is 7804800.0000, 
raw observation next is [27.7, 82.0, 1.0, 2.0, 0.7750000565356212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083138.113945953, 1083138.113945952, 237866.0188770856], 
processed observation next is [1.0, 0.34782608695652173, 0.5118483412322274, 0.82, 1.0, 1.0, 0.7289157307658086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3008716983183203, 0.30087169831832, 0.35502390877176954], 
reward next is 0.6450, 
noisyNet noise sample is [array([0.3237054], dtype=float32), -0.7937389]. 
=============================================
[2019-04-23 11:46:05,687] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:46:05,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7078
[2019-04-23 11:46:05,703] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 84.50000000000001, 1.0, 2.0, 0.7565199476128736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1057297.502628378, 1057297.502628377, 233516.4975966799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7801800.0000, 
sim time next is 7802400.0000, 
raw observation next is [27.16666666666666, 84.0, 1.0, 2.0, 0.6687241109450456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 934541.766919579, 934541.766919579, 214237.9198633613], 
processed observation next is [1.0, 0.30434782608695654, 0.4865718799368086, 0.84, 1.0, 1.0, 0.6008724228253561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2595949352554386, 0.2595949352554386, 0.31975808934830047], 
reward next is 0.6802, 
noisyNet noise sample is [array([0.7457465], dtype=float32), 0.40488288]. 
=============================================
[2019-04-23 11:46:07,673] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:07,673] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:07,674] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-04-23 11:46:21,519] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:21,519] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:21,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-04-23 11:46:22,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:22,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:22,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-04-23 11:46:23,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:23,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:23,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-04-23 11:46:24,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:24,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:24,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-04-23 11:46:24,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:24,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:24,707] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-04-23 11:46:25,356] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:25,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:25,367] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-04-23 11:46:26,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:26,327] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:26,328] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-04-23 11:46:26,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:26,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:26,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-04-23 11:46:26,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:26,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:26,966] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-04-23 11:46:27,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:27,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:27,320] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-04-23 11:46:28,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:28,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:28,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-04-23 11:46:29,010] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:46:29,033] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8965
[2019-04-23 11:46:29,068] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:29,068] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:29,082] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 85.33333333333334, 1.0, 2.0, 0.2839427060194353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 456876.1405156444, 456876.1405156438, 164060.34452536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 15600.0000, 
sim time next is 16200.0000, 
raw observation next is [21.3, 85.5, 1.0, 2.0, 0.2843686877177798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457095.9576430458, 457095.9576430458, 164073.2743378893], 
processed observation next is [1.0, 0.17391304347826086, 0.2085308056872039, 0.855, 1.0, 1.0, 0.13779359965997565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1269710993452905, 0.1269710993452905, 0.24488548408640196], 
reward next is 0.7551, 
noisyNet noise sample is [array([0.91040134], dtype=float32), 0.12679613]. 
=============================================
[2019-04-23 11:46:29,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run5
[2019-04-23 11:46:30,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:30,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:30,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-04-23 11:46:34,623] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:34,623] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:34,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-04-23 11:46:35,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:46:35,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0601
[2019-04-23 11:46:35,693] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.93333333333334, 73.66666666666667, 1.0, 2.0, 0.3790941638036186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563968.195839067, 563968.195839067, 171219.2365498463], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 63600.0000, 
sim time next is 64200.0000, 
raw observation next is [25.76666666666667, 74.33333333333333, 1.0, 2.0, 0.3815274811478772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568727.3474694667, 568727.3474694673, 171679.5541916547], 
processed observation next is [1.0, 0.7391304347826086, 0.42022116903633505, 0.7433333333333333, 1.0, 1.0, 0.2548523869251533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15797981874151854, 0.1579798187415187, 0.25623814058455924], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.9601064], dtype=float32), -0.13485347]. 
=============================================
[2019-04-23 11:46:37,295] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-23 11:46:37,347] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:46:37,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:37,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run35
[2019-04-23 11:46:37,438] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:46:37,439] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:37,441] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run35
[2019-04-23 11:46:37,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:46:37,464] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:37,439] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:46:37,471] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:37,472] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run35
[2019-04-23 11:46:37,470] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run35
[2019-04-23 11:46:37,439] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:46:37,528] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:37,530] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run35
[2019-04-23 11:46:37,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 11:46:37,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:46:37,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-04-23 11:47:04,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:47:04,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.25, 92.0, 1.0, 2.0, 0.4135541638997961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653610.659338267, 653610.659338267, 180023.4618226911]
[2019-04-23 11:47:04,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:47:04,223] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2020320197372637
[2019-04-23 11:47:31,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:47:31,060] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.3, 96.0, 1.0, 2.0, 0.6786099245043262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 948363.3571559621, 948363.3571559621, 216296.8486666579]
[2019-04-23 11:47:31,064] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:47:31,076] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3357020035772109
[2019-04-23 11:47:31,392] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:47:31,392] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.16170872, 96.33593433333334, 1.0, 2.0, 0.6240893034767919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872139.0241326219, 872139.0241326219, 205319.4209693897]
[2019-04-23 11:47:31,393] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:47:31,400] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.38147214310287025
[2019-04-23 11:47:40,805] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:47:40,812] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.72782320333333, 97.28987315833334, 1.0, 2.0, 0.3192628112950021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504459.2728728415, 504459.2728728409, 167352.9453420782]
[2019-04-23 11:47:40,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:47:40,861] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5699041780368245
[2019-04-23 11:47:49,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:47:49,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.26144292166667, 74.47892351333333, 1.0, 2.0, 0.5788593933166362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808908.0238893094, 808908.0238893094, 196872.9432084556]
[2019-04-23 11:47:49,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:47:49,801] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.45127629869920205
[2019-04-23 11:48:06,610] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:48:06,610] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [38.486911535, 63.01858828, 1.0, 2.0, 0.8876057597490422, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987000371642, 6.9112, 168.9123159668296, 2137688.141220856, 2070443.2358443, 430734.8777808572]
[2019-04-23 11:48:06,610] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:48:06,612] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.007619344465683686
[2019-04-23 11:48:06,614] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2137688.141220856 W.
[2019-04-23 11:48:30,128] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34648558]
[2019-04-23 11:48:30,137] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.82201118166667, 85.57382828166668, 1.0, 2.0, 0.5188942955621453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725083.1692672656, 725083.169267265, 186608.9107372342]
[2019-04-23 11:48:30,159] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:48:30,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4266111616295174
[2019-04-23 11:49:30,840] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:49:31,626] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:49:31,810] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:49:32,279] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:49:32,545] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:49:33,561] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 850000, evaluation results [850000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:49:39,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:49:39,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-23 11:49:39,746] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.41666666666667, 89.16666666666667, 1.0, 2.0, 0.5309476239929853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822520.922189616, 822520.922189616, 198162.0159274481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 94200.0000, 
sim time next is 94800.0000, 
raw observation next is [22.43333333333333, 89.33333333333334, 1.0, 2.0, 0.4399588967573597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680774.4790538431, 680774.4790538425, 182658.1238073049], 
processed observation next is [1.0, 0.08695652173913043, 0.2622432859399683, 0.8933333333333334, 1.0, 1.0, 0.32525168284019246, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18910402195940085, 0.18910402195940068, 0.27262406538403716], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.38543418], dtype=float32), -0.67148405]. 
=============================================
[2019-04-23 11:50:06,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:50:06,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4840
[2019-04-23 11:50:06,401] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 78.0, 1.0, 2.0, 0.5088114363844702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815972.3848522342, 815972.3848522342, 196591.9670968222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 402000.0000, 
sim time next is 402600.0000, 
raw observation next is [22.35, 78.5, 1.0, 2.0, 0.5127419440580625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821956.079211097, 821956.079211097, 197287.736454933], 
processed observation next is [1.0, 0.6521739130434783, 0.25829383886255936, 0.785, 1.0, 1.0, 0.4129421012747741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2283211331141936, 0.2283211331141936, 0.2944593081416911], 
reward next is 0.7055, 
noisyNet noise sample is [array([-0.7033327], dtype=float32), 0.1369847]. 
=============================================
[2019-04-23 11:50:22,442] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:50:22,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2565
[2019-04-23 11:50:22,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.86666666666667, 87.0, 1.0, 2.0, 0.2331460808770074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 386578.0685461686, 386578.0685461686, 159110.4564403699], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 516000.0000, 
sim time next is 516600.0000, 
raw observation next is [18.85, 87.0, 1.0, 2.0, 0.2325930176599129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385719.082205185, 385719.082205185, 159052.3526245966], 
processed observation next is [1.0, 1.0, 0.09241706161137453, 0.87, 1.0, 1.0, 0.07541327428905166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10714418950144028, 0.10714418950144028, 0.23739157108148745], 
reward next is 0.7626, 
noisyNet noise sample is [array([1.1751171], dtype=float32), 0.39858]. 
=============================================
[2019-04-23 11:50:26,618] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:50:26,618] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9982
[2019-04-23 11:50:26,697] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 72.0, 1.0, 2.0, 0.4270096543093645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702212.8281481456, 702212.8281481449, 183554.951037988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 552000.0000, 
sim time next is 552600.0000, 
raw observation next is [21.8, 70.5, 1.0, 2.0, 0.4084047101630429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671773.8923314483, 671773.8923314483, 180715.5563780493], 
processed observation next is [1.0, 0.391304347826087, 0.23222748815165886, 0.705, 1.0, 1.0, 0.28723459055788303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18660385898095785, 0.18660385898095785, 0.2697247110120139], 
reward next is 0.7303, 
noisyNet noise sample is [array([0.38991937], dtype=float32), -0.58816975]. 
=============================================
[2019-04-23 11:50:34,584] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:50:34,584] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1064
[2019-04-23 11:50:34,635] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 58.66666666666667, 1.0, 2.0, 0.5545895935895162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908492.3237519702, 908492.3237519709, 206181.3662040037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [24.06666666666667, 58.33333333333334, 1.0, 2.0, 0.5886806297422197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964628.6644423761, 964628.6644423761, 213178.2764068074], 
processed observation next is [1.0, 0.4782608695652174, 0.3396524486571882, 0.5833333333333335, 1.0, 1.0, 0.5044344936653249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2679524067895489, 0.2679524067895489, 0.3181765319504588], 
reward next is 0.6818, 
noisyNet noise sample is [array([-0.7550057], dtype=float32), 0.04013325]. 
=============================================
[2019-04-23 11:50:49,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:50:49,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1689
[2019-04-23 11:50:49,504] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 54.5, 1.0, 2.0, 0.2284614883460498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377813.0948803165, 377813.0948803165, 158775.7680412834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 754200.0000, 
sim time next is 754800.0000, 
raw observation next is [23.73333333333333, 56.0, 1.0, 2.0, 0.2310126852527967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381839.9661477196, 381839.9661477196, 159022.8178168577], 
processed observation next is [1.0, 0.7391304347826086, 0.3238546603475513, 0.56, 1.0, 1.0, 0.0735092593407189, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10606665726325544, 0.10606665726325544, 0.2373474892788921], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.07471632], dtype=float32), 1.8698069]. 
=============================================
[2019-04-23 11:50:57,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:50:57,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8771
[2019-04-23 11:50:57,717] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 63.0, 1.0, 2.0, 0.2904307138657254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 465128.1484360214, 465128.1484360207, 164610.1315868421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 824400.0000, 
sim time next is 825000.0000, 
raw observation next is [24.76666666666667, 63.0, 1.0, 2.0, 0.2898091305429463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464338.8374976719, 464338.8374976725, 164557.6143666503], 
processed observation next is [0.0, 0.5652173913043478, 0.3728278041074251, 0.63, 1.0, 1.0, 0.14434835005174254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12898301041601998, 0.12898301041602014, 0.24560837965171686], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.95917916], dtype=float32), -2.382121]. 
=============================================
[2019-04-23 11:50:57,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.523224]
 [78.4164  ]
 [78.41269 ]
 [78.399   ]
 [78.38376 ]], R is [[78.46994019]
 [78.43955231]
 [78.40965271]
 [78.38018799]
 [78.35101318]].
[2019-04-23 11:51:12,872] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:51:12,872] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4111
[2019-04-23 11:51:13,031] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:51:13,031] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3740
[2019-04-23 11:51:13,070] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 91.5, 1.0, 2.0, 0.3404024194911415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 526792.0208482681, 526792.0208482675, 168817.1136385364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 942600.0000, 
sim time next is 943200.0000, 
raw observation next is [22.1, 92.0, 1.0, 2.0, 0.3410723180482942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 527645.53783535, 527645.5378353494, 168880.0675782755], 
processed observation next is [0.0, 0.9565217391304348, 0.24644549763033188, 0.92, 1.0, 1.0, 0.20611122656420985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1465682049542639, 0.14656820495426373, 0.2520598023556351], 
reward next is 0.7479, 
noisyNet noise sample is [array([-1.4941642], dtype=float32), -0.18367629]. 
=============================================
[2019-04-23 11:51:13,105] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 93.16666666666667, 1.0, 2.0, 0.499242761645745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773201.4396798844, 773201.4396798844, 192457.7354956286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 983400.0000, 
sim time next is 984000.0000, 
raw observation next is [21.93333333333333, 93.33333333333334, 1.0, 2.0, 0.5372174152532702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 831187.1474571837, 831187.1474571843, 199218.4408116657], 
processed observation next is [1.0, 0.391304347826087, 0.23854660347551332, 0.9333333333333335, 1.0, 1.0, 0.44243062078707246, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23088531873810658, 0.23088531873810675, 0.29734095643532193], 
reward next is 0.7027, 
noisyNet noise sample is [array([1.498081], dtype=float32), 2.2054462]. 
=============================================
[2019-04-23 11:51:13,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[69.568375]
 [69.47054 ]
 [69.197464]
 [69.47369 ]
 [70.01542 ]], R is [[69.25353241]
 [69.27375031]
 [69.28805542]
 [69.29071045]
 [69.2976532 ]].
[2019-04-23 11:51:15,633] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-23 11:51:15,665] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:51:15,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:51:15,688] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run36
[2019-04-23 11:51:15,690] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:51:15,714] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:51:15,715] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run36
[2019-04-23 11:51:15,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:51:15,856] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:51:15,858] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run36
[2019-04-23 11:51:15,691] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:51:15,900] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:51:15,691] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:51:15,915] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:51:15,916] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run36
[2019-04-23 11:51:16,039] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run36
[2019-04-23 11:51:35,454] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:51:35,459] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.826184565, 93.733716885, 1.0, 2.0, 0.2299573768278516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381930.0273335099, 381930.0273335099, 158729.0320162886]
[2019-04-23 11:51:35,461] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:51:35,464] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6985574394782033
[2019-04-23 11:51:55,032] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:51:55,035] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.117536595, 94.644279055, 1.0, 2.0, 0.4426704101737423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657364.5550411836, 657364.5550411836, 179979.5922835821]
[2019-04-23 11:51:55,036] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:51:55,038] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.5, 43.5, 1.0, 2.0, 0.3554135655839094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586169.3937069981, 586169.3937069988, 173229.4609677024]
[2019-04-23 11:51:55,040] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:51:55,037] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:51:55,043] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.051506420893586924
[2019-04-23 11:51:55,045] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7811380022105593
[2019-04-23 11:53:25,695] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:53:25,696] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.68333333333334, 61.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.708670124198843, 6.9112, 168.9089176603737, 2019881.682176302, 1454142.456887679, 311355.2969256254]
[2019-04-23 11:53:25,703] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:53:25,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5413077469946826
[2019-04-23 11:53:25,719] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2019881.682176302 W.
[2019-04-23 11:53:39,501] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:53:39,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.74600040666667, 85.33365031666668, 1.0, 2.0, 0.5644542340418484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788770.5334446575, 788770.5334446569, 194309.1224253619]
[2019-04-23 11:53:39,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:53:39,504] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26009090510474897
[2019-04-23 11:53:40,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:53:40,582] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.05, 63.66666666666666, 1.0, 2.0, 0.5644348237983475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788743.3994244533, 788743.3994244526, 194307.1171380955]
[2019-04-23 11:53:40,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:53:40,585] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8001853410070857
[2019-04-23 11:54:08,811] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.32304534]
[2019-04-23 11:54:08,813] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.01682688, 63.45662046, 1.0, 2.0, 0.8187589499554283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1144328.386168984, 1144328.386168984, 248574.3281895916]
[2019-04-23 11:54:08,814] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 11:54:08,816] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8918903707891725
[2019-04-23 11:54:14,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:54:15,056] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:54:15,480] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:54:15,837] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:54:15,892] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:54:16,910] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 875000, evaluation results [875000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:54:47,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:54:47,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3365
[2019-04-23 11:54:47,048] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1780997.064492748 W.
[2019-04-23 11:54:47,050] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.01666666666667, 74.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.329390880512875, 6.9112, 168.9110677347442, 1780997.064492748, 1484321.378286425, 316201.3668282327], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1246200.0000, 
sim time next is 1246800.0000, 
raw observation next is [27.23333333333334, 73.66666666666667, 1.0, 2.0, 0.586624993963457, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9821851794979696, 6.911200000000001, 6.9112, 168.9126160012299, 1654672.817811926, 1654672.817811925, 352071.2666277591], 
processed observation next is [1.0, 0.43478260869565216, 0.4897314375987366, 0.7366666666666667, 1.0, 1.0, 0.5019578240523578, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9782746091438652, 8.881784197001253e-17, 0.0, 0.8294382730962395, 0.45963133828109054, 0.45963133828109026, 0.5254795024294912], 
reward next is 0.4745, 
noisyNet noise sample is [array([-0.35765323], dtype=float32), -0.6099881]. 
=============================================
[2019-04-23 11:54:49,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:54:49,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9486
[2019-04-23 11:54:49,756] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 63.66666666666667, 1.0, 2.0, 0.3406652638625843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525829.1067234937, 525829.1067234937, 168698.0445456251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1189200.0000, 
sim time next is 1189800.0000, 
raw observation next is [26.2, 64.5, 1.0, 2.0, 0.3437963493509814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530918.9951119735, 530918.9951119741, 169115.5353538451], 
processed observation next is [1.0, 0.782608695652174, 0.44075829383886256, 0.645, 1.0, 1.0, 0.20939319198913423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14747749864221485, 0.14747749864221502, 0.25241124679678373], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.5071147], dtype=float32), -0.61543965]. 
=============================================
[2019-04-23 11:54:55,449] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:54:55,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5583
[2019-04-23 11:54:55,507] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 94.0, 1.0, 2.0, 0.4570678034078495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649487.8429542875, 649487.8429542875, 178537.2880311009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1301400.0000, 
sim time next is 1302000.0000, 
raw observation next is [24.23333333333333, 94.0, 1.0, 2.0, 0.4560888869921874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648564.8309758569, 648564.8309758569, 178453.6880327009], 
processed observation next is [1.0, 0.043478260869565216, 0.3475513428120062, 0.94, 1.0, 1.0, 0.3446854060146836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18015689749329358, 0.18015689749329358, 0.2663487881085088], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.698224], dtype=float32), -0.570989]. 
=============================================
[2019-04-23 11:54:55,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.69804]
 [68.63571]
 [68.5201 ]
 [68.48042]
 [68.24551]], R is [[68.76241302]
 [68.80831909]
 [68.8536377 ]
 [68.89837646]
 [68.94258118]].
[2019-04-23 11:55:03,043] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:03,043] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7825
[2019-04-23 11:55:03,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 98.0, 1.0, 2.0, 0.3142556772399356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496983.3143755, 496983.3143754994, 166801.1078839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1398000.0000, 
sim time next is 1398600.0000, 
raw observation next is [20.55, 98.0, 1.0, 2.0, 0.314633678927182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497383.7763782715, 497383.7763782715, 166826.9482481077], 
processed observation next is [0.0, 0.17391304347826086, 0.17298578199052142, 0.98, 1.0, 1.0, 0.17425744449058075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13816216010507543, 0.13816216010507543, 0.24899544514642943], 
reward next is 0.7510, 
noisyNet noise sample is [array([1.1540725], dtype=float32), 0.78450775]. 
=============================================
[2019-04-23 11:55:12,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:12,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8092
[2019-04-23 11:55:12,432] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 96.0, 1.0, 2.0, 0.3419572955289975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 530467.4887371445, 530467.4887371439, 169148.6773291997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [21.46666666666667, 96.0, 1.0, 2.0, 0.3413674513089504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530039.9030655492, 530039.9030655499, 169127.8221798184], 
processed observation next is [0.0, 0.043478260869565216, 0.21642969984202226, 0.96, 1.0, 1.0, 0.20646680880596435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.147233306407097, 0.1472333064070972, 0.2524295853430126], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.46474597], dtype=float32), -0.6201405]. 
=============================================
[2019-04-23 11:55:14,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:14,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7278
[2019-04-23 11:55:14,112] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 70.16666666666667, 1.0, 2.0, 0.3682610225889591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558880.3561709941, 558880.3561709941, 171152.7829510424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1501800.0000, 
sim time next is 1502400.0000, 
raw observation next is [26.13333333333334, 68.33333333333334, 1.0, 2.0, 0.3658982396448303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 555886.3358045889, 555886.3358045883, 170914.7795841877], 
processed observation next is [0.0, 0.391304347826087, 0.43759873617693557, 0.6833333333333335, 1.0, 1.0, 0.23602197547569917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15441287105683024, 0.15441287105683008, 0.2550966859465488], 
reward next is 0.7449, 
noisyNet noise sample is [array([1.4137033], dtype=float32), -0.382891]. 
=============================================
[2019-04-23 11:55:14,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:14,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9357
[2019-04-23 11:55:14,282] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 69.16666666666667, 1.0, 2.0, 0.4303329792422638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619344.9523717558, 619344.9523717551, 175728.4968300429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [27.73333333333333, 69.33333333333334, 1.0, 2.0, 0.4339430502299369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622305.1670256971, 622305.1670256971, 175954.0529619521], 
processed observation next is [0.0, 0.6521739130434783, 0.513428120063191, 0.6933333333333335, 1.0, 1.0, 0.3180036749758276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17286254639602697, 0.17286254639602697, 0.2626179894954509], 
reward next is 0.7374, 
noisyNet noise sample is [array([1.140794], dtype=float32), 0.023784045]. 
=============================================
[2019-04-23 11:55:15,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:15,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1709
[2019-04-23 11:55:15,551] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333333, 96.66666666666666, 1.0, 2.0, 0.3322407999722555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 519339.8942545966, 519339.8942545972, 168372.2554917981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1478400.0000, 
sim time next is 1479000.0000, 
raw observation next is [21.06666666666667, 96.83333333333334, 1.0, 2.0, 0.3302525080932052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516865.1005311398, 516865.1005311398, 168194.9085333298], 
processed observation next is [0.0, 0.08695652173913043, 0.19747235387045833, 0.9683333333333334, 1.0, 1.0, 0.19307531095566893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14357363903642772, 0.14357363903642772, 0.25103717691541766], 
reward next is 0.7490, 
noisyNet noise sample is [array([1.6783803], dtype=float32), 0.5486464]. 
=============================================
[2019-04-23 11:55:15,663] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.803986]
 [76.96837 ]
 [77.23863 ]
 [77.44044 ]
 [76.29534 ]], R is [[76.62174988]
 [76.60422516]
 [76.58677673]
 [76.56941223]
 [76.5520401 ]].
[2019-04-23 11:55:16,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:16,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6983
[2019-04-23 11:55:17,032] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 57.0, 1.0, 2.0, 0.3597016180012442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549558.4744311062, 549558.4744311069, 170475.6977692278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1526400.0000, 
sim time next is 1527000.0000, 
raw observation next is [27.85, 57.33333333333333, 1.0, 2.0, 0.3572748588014932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 170285.8858871213], 
processed observation next is [0.0, 0.6956521739130435, 0.5189573459715641, 0.5733333333333333, 1.0, 1.0, 0.225632360001799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15191858677415285, 0.15191858677415285, 0.2541580386374945], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.32691666], dtype=float32), -0.64202124]. 
=============================================
[2019-04-23 11:55:17,169] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.51567 ]
 [74.42917 ]
 [74.434814]
 [74.44211 ]
 [74.451645]], R is [[74.46609497]
 [74.46699524]
 [74.46760559]
 [74.46803284]
 [74.46842194]].
[2019-04-23 11:55:35,349] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:35,374] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1867
[2019-04-23 11:55:35,462] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.0, 1.0, 2.0, 0.4165743051441188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610975.3894626108, 610975.3894626114, 175261.1882086786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1634400.0000, 
sim time next is 1635000.0000, 
raw observation next is [23.1, 97.16666666666667, 1.0, 2.0, 0.4168699389410615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611053.8734260148, 611053.8734260142, 175258.3735891297], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.9716666666666667, 1.0, 1.0, 0.2974336613747729, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16973718706278187, 0.16973718706278174, 0.26157966207332795], 
reward next is 0.7384, 
noisyNet noise sample is [array([-1.5038928], dtype=float32), 0.5009716]. 
=============================================
[2019-04-23 11:55:35,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.31384 ]
 [69.2522  ]
 [69.271385]
 [69.15057 ]
 [69.0372  ]], R is [[69.31421661]
 [69.35949707]
 [69.40436554]
 [69.44876862]
 [69.49265289]].
[2019-04-23 11:55:37,756] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:37,756] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3621
[2019-04-23 11:55:37,800] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.81666666666667, 97.33333333333334, 1.0, 2.0, 0.5338018664416964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759378.0177450682, 759378.0177450682, 190781.882326112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1671000.0000, 
sim time next is 1671600.0000, 
raw observation next is [23.93333333333333, 96.66666666666667, 1.0, 2.0, 0.7759857961140745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1102290.8804797, 1102290.8804797, 240568.0929563844], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333332, 0.9666666666666667, 1.0, 1.0, 0.7301033688121379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30619191124436107, 0.30619191124436107, 0.3590568551587827], 
reward next is 0.6409, 
noisyNet noise sample is [array([0.6665802], dtype=float32), 0.5598046]. 
=============================================
[2019-04-23 11:55:42,773] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:42,800] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3978
[2019-04-23 11:55:42,879] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5047705006445073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705340.5625705486, 705340.5625705493, 184347.0012442233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [25.43333333333334, 94.0, 1.0, 2.0, 0.503261366934298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703231.0780435816, 703231.0780435816, 184108.6900725193], 
processed observation next is [1.0, 1.0, 0.40442338072669864, 0.94, 1.0, 1.0, 0.4015197191979494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1953419661232171, 0.1953419661232171, 0.27478908966047655], 
reward next is 0.7252, 
noisyNet noise sample is [array([2.192417], dtype=float32), -0.51328564]. 
=============================================
[2019-04-23 11:55:45,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:45,538] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4456
[2019-04-23 11:55:45,580] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5047705006445073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705340.5625705486, 705340.5625705493, 184347.0012442233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [25.43333333333334, 94.0, 1.0, 2.0, 0.503261366934298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703231.0780435816, 703231.0780435816, 184108.6900725193], 
processed observation next is [1.0, 1.0, 0.40442338072669864, 0.94, 1.0, 1.0, 0.4015197191979494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1953419661232171, 0.1953419661232171, 0.27478908966047655], 
reward next is 0.7252, 
noisyNet noise sample is [array([-1.2680551], dtype=float32), 0.77276105]. 
=============================================
[2019-04-23 11:55:50,423] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:50,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-23 11:55:50,468] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 87.33333333333333, 1.0, 2.0, 0.3240488823107133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510889.4066064937, 510889.4066064943, 167819.2239146206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1793400.0000, 
sim time next is 1794000.0000, 
raw observation next is [21.9, 87.66666666666667, 1.0, 2.0, 0.3220858664158496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507877.6350391586, 507877.6350391586, 167591.0391723086], 
processed observation next is [1.0, 0.782608695652174, 0.23696682464454974, 0.8766666666666667, 1.0, 1.0, 0.1832359836335537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1410771208442107, 0.1410771208442107, 0.25013587936165466], 
reward next is 0.7499, 
noisyNet noise sample is [array([0.14268605], dtype=float32), -2.1116762]. 
=============================================
[2019-04-23 11:55:50,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.370705]
 [73.41031 ]
 [73.24022 ]
 [73.159805]
 [72.92923 ]], R is [[73.39117432]
 [73.40679169]
 [73.42293549]
 [73.43956757]
 [73.45657349]].
[2019-04-23 11:55:57,804] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:55:57,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1019
[2019-04-23 11:55:57,858] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 97.0, 1.0, 2.0, 0.4695583185565638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658112.1932833813, 658112.1932833806, 179221.2954939674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2007000.0000, 
sim time next is 2007600.0000, 
raw observation next is [24.3, 96.66666666666667, 1.0, 2.0, 0.4723316300925153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660230.6871954617, 660230.6871954617, 179404.5169101899], 
processed observation next is [0.0, 0.21739130434782608, 0.3507109004739337, 0.9666666666666667, 1.0, 1.0, 0.36425497601507867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18339741310985047, 0.18339741310985047, 0.2677679356868506], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.34723043], dtype=float32), 1.0485902]. 
=============================================
[2019-04-23 11:56:10,394] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-23 11:56:10,400] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 11:56:10,450] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:56:10,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run37
[2019-04-23 11:56:10,455] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 11:56:10,545] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:56:10,546] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 11:56:10,550] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:56:10,551] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run37
[2019-04-23 11:56:10,547] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 11:56:10,646] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:56:10,648] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run37
[2019-04-23 11:56:10,548] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run37
[2019-04-23 11:56:10,825] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 11:56:10,901] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 11:56:10,904] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run37
[2019-04-23 11:56:33,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:56:33,924] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.7, 90.0, 1.0, 2.0, 0.4531547825765369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647094.4249835579, 647094.4249835585, 178369.7568584493]
[2019-04-23 11:56:33,927] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:56:33,934] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4767219146153211
[2019-04-23 11:56:35,755] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:56:35,766] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.13333333333333, 68.66666666666667, 1.0, 2.0, 0.3401414681967084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 530031.9003612985, 530031.9003612985, 169177.4426886227]
[2019-04-23 11:56:35,768] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:56:35,770] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34663259167342053
[2019-04-23 11:56:40,925] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:56:40,929] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.6, 92.0, 1.0, 2.0, 0.3030265565729944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485758.1914098757, 485758.1914098751, 166067.9871633174]
[2019-04-23 11:56:40,932] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 11:56:40,937] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7686957490607538
[2019-04-23 11:56:44,260] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:56:44,266] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.8, 51.0, 1.0, 2.0, 0.2056515171260029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343724.4516716925, 343724.4516716925, 155849.2810393436]
[2019-04-23 11:56:44,268] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:56:44,269] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7707943631246871
[2019-04-23 11:56:45,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:56:45,410] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.41559604333333, 97.85456573333335, 1.0, 2.0, 0.4431860026993182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 641491.415482043, 641491.4154820438, 178018.2243413065]
[2019-04-23 11:56:45,410] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:56:45,411] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.294139563090528
[2019-04-23 11:57:15,725] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:57:15,742] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.38333333333333, 95.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.359698216321023, 6.9112, 169.6265963146485, 1773492.994362928, 1453968.645634479, 311482.866855632]
[2019-04-23 11:57:15,745] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:57:15,749] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1122143628999992
[2019-04-23 11:57:15,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1773492.994362928 W.
[2019-04-23 11:58:00,629] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:58:00,633] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.26666666666667, 55.0, 1.0, 2.0, 0.6390373012883875, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.984059332747542, 6.9112, 168.91195459364, 1786825.576383869, 1735136.964703811, 372659.9073403471]
[2019-04-23 11:58:00,677] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 11:58:00,678] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06841405176522342
[2019-04-23 11:58:00,679] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1786825.576383869 W.
[2019-04-23 11:59:11,743] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:59:11,744] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.53333333333333, 64.0, 1.0, 2.0, 0.5973931094300245, 0.0, 2.0, 0.0, 1.0, 2.0, 1.009319509531932, 6.911199999999999, 6.9112, 168.9129380240929, 1707101.923828308, 1707101.923828308, 362456.8611218095]
[2019-04-23 11:59:11,753] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 11:59:11,758] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6542440675418779
[2019-04-23 11:59:11,763] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1707101.923828308 W.
[2019-04-23 11:59:24,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33490968]
[2019-04-23 11:59:24,636] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.14144134666667, 59.05814553333333, 1.0, 2.0, 0.3748069420324616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566956.8351335231, 566956.8351335231, 171795.8862279889]
[2019-04-23 11:59:24,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 11:59:24,658] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9004916677643441
[2019-04-23 11:59:29,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 11:59:29,987] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 11:59:30,326] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 11:59:31,023] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 11:59:31,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 11:59:32,384] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 900000, evaluation results [900000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 11:59:36,923] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:59:36,928] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7273
[2019-04-23 11:59:36,983] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 94.0, 1.0, 2.0, 0.5037625457317685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703931.6310894974, 703931.6310894967, 184188.0953561375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2016000.0000, 
sim time next is 2016600.0000, 
raw observation next is [25.6, 94.00000000000001, 1.0, 2.0, 0.5044118205129783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704839.1949613115, 704839.1949613115, 184290.5711192366], 
processed observation next is [0.0, 0.34782608695652173, 0.4123222748815167, 0.9400000000000002, 1.0, 1.0, 0.4029058078469618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19578866526703098, 0.19578866526703098, 0.2750605539093084], 
reward next is 0.7249, 
noisyNet noise sample is [array([0.05336741], dtype=float32), 1.4565604]. 
=============================================
[2019-04-23 11:59:37,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:59:37,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2989
[2019-04-23 11:59:37,318] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 97.0, 1.0, 2.0, 0.4695583185565638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658112.1932833813, 658112.1932833806, 179221.2954939674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2007000.0000, 
sim time next is 2007600.0000, 
raw observation next is [24.3, 96.66666666666667, 1.0, 2.0, 0.4723316300925153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660230.6871954617, 660230.6871954617, 179404.5169101899], 
processed observation next is [0.0, 0.21739130434782608, 0.3507109004739337, 0.9666666666666667, 1.0, 1.0, 0.36425497601507867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18339741310985047, 0.18339741310985047, 0.2677679356868506], 
reward next is 0.7322, 
noisyNet noise sample is [array([1.0446856], dtype=float32), 1.5345246]. 
=============================================
[2019-04-23 11:59:54,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 11:59:54,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3025
[2019-04-23 11:59:54,520] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 93.66666666666666, 1.0, 2.0, 0.5759891990431278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804895.6455888246, 804895.6455888246, 196352.5197220974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2180400.0000, 
sim time next is 2181000.0000, 
raw observation next is [25.58333333333333, 92.83333333333334, 1.0, 2.0, 0.5880963522836973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821820.902071321, 821820.902071321, 198543.1472285961], 
processed observation next is [1.0, 0.21739130434782608, 0.41153238546603454, 0.9283333333333335, 1.0, 1.0, 0.5037305449201172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2282835839087003, 0.2282835839087003, 0.2963330555650688], 
reward next is 0.7037, 
noisyNet noise sample is [array([1.0721397], dtype=float32), 0.31301004]. 
=============================================
[2019-04-23 11:59:54,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.309555]
 [62.566612]
 [62.687405]
 [63.08706 ]
 [62.754436]], R is [[62.11481476]
 [62.20060349]
 [62.28898239]
 [62.37903214]
 [62.47322464]].
[2019-04-23 12:00:19,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:00:19,568] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7685
[2019-04-23 12:00:19,569] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1982154.271773207 W.
[2019-04-23 12:00:19,616] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.56666666666666, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.655525986406969, 6.9112, 168.9088531500964, 1982154.271773207, 1454116.627091182, 311353.5761791127], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2379000.0000, 
sim time next is 2379600.0000, 
raw observation next is [32.6, 63.0, 1.0, 2.0, 0.6604691209025139, 1.0, 1.0, 0.6604691209025139, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1846788.179629952, 1846788.179629952, 357667.2727059143], 
processed observation next is [1.0, 0.5652173913043478, 0.7440758293838864, 0.63, 1.0, 1.0, 0.5909266516897758, 1.0, 0.5, 0.5909266516897758, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5129967165638756, 0.5129967165638756, 0.5338317503073348], 
reward next is 0.4662, 
noisyNet noise sample is [array([1.251286], dtype=float32), -1.2716993]. 
=============================================
[2019-04-23 12:00:46,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:00:46,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1238
[2019-04-23 12:00:46,912] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4755530615936887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664629.3345159743, 664629.3345159743, 179870.6752010286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2625600.0000, 
sim time next is 2626200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4754706219922958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664514.1948421964, 664514.194842197, 179858.3732863566], 
processed observation next is [0.0, 0.391304347826087, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3680368939666215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18458727634505456, 0.18458727634505473, 0.2684453332632188], 
reward next is 0.7316, 
noisyNet noise sample is [array([1.2617798], dtype=float32), 0.470135]. 
=============================================
[2019-04-23 12:00:51,655] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:00:51,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3517
[2019-04-23 12:00:51,714] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3977482839684885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593474.40523567, 593474.40523567, 173936.5160771536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3971945146211823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592670.2684337221, 592670.2684337215, 173863.0354437851], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27372833086889437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16463063012047835, 0.16463063012047818, 0.2594970678265449], 
reward next is 0.7405, 
noisyNet noise sample is [array([0.6246695], dtype=float32), 0.6055665]. 
=============================================
[2019-04-23 12:00:56,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:00:56,754] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9993
[2019-04-23 12:00:56,769] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4754706219922958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664514.1948421964, 664514.194842197, 179858.3732863566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2626200.0000, 
sim time next is 2626800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.475462305375623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664502.6212817535, 664502.6212817541, 179857.1377831047], 
processed observation next is [0.0, 0.391304347826087, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3680268739465338, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18458406146715375, 0.18458406146715392, 0.26844348922851446], 
reward next is 0.7316, 
noisyNet noise sample is [array([0.3032815], dtype=float32), -0.6676743]. 
=============================================
[2019-04-23 12:01:01,061] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:01:01,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7198
[2019-04-23 12:01:01,064] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3923020949218254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585374.0190396183, 585374.0190396183, 173194.7922291267], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911062162407326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584174.1603886123, 584174.1603886123, 173103.7404002603], 
processed observation next is [0.0, 0.8260869565217391, 0.2812006319115327, 0.95, 1.0, 1.0, 0.2663930316153405, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16227060010794786, 0.16227060010794786, 0.2583637916421796], 
reward next is 0.7416, 
noisyNet noise sample is [array([-0.5344745], dtype=float32), 0.30134678]. 
=============================================
[2019-04-23 12:01:10,295] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:01:10,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2904
[2019-04-23 12:01:10,339] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.7839933921886968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1147675.696847003, 1147675.696847003, 247138.7163522834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2816400.0000, 
sim time next is 2817000.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.8153726605854218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1193582.024290414, 1193582.024290413, 255233.7849349417], 
processed observation next is [1.0, 0.6086956521739131, 0.38388625592417064, 0.83, 1.0, 1.0, 0.7775574223920745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3315505623028928, 0.3315505623028925, 0.3809459476640921], 
reward next is 0.6191, 
noisyNet noise sample is [array([-0.11205141], dtype=float32), 0.22750866]. 
=============================================
[2019-04-23 12:01:10,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.623604]
 [71.02344 ]
 [71.3558  ]
 [70.888405]
 [71.05217 ]], R is [[70.2995224 ]
 [70.22766113]
 [70.17306519]
 [70.13385773]
 [70.07404327]].
[2019-04-23 12:01:11,602] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:01:11,603] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-23 12:01:11,649] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3540312321264625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545380.3656194228, 545380.3656194228, 170261.4877199658], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2787000.0000, 
sim time next is 2787600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3377006207281725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520218.5628121088, 520218.5628121095, 168216.6743281917], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20204894063635237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1445051563366969, 0.14450515633669708, 0.2510696631764055], 
reward next is 0.7489, 
noisyNet noise sample is [array([-1.2363026], dtype=float32), -0.31159112]. 
=============================================
[2019-04-23 12:01:18,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:01:18,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8158
[2019-04-23 12:01:18,090] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 89.83333333333334, 1.0, 2.0, 0.705100973192129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1075383.100681327, 1075383.100681328, 233407.1101784439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2905800.0000, 
sim time next is 2906400.0000, 
raw observation next is [22.66666666666667, 90.66666666666667, 1.0, 2.0, 0.6911292910529765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1055892.195042201, 1055892.195042202, 230315.1323058094], 
processed observation next is [1.0, 0.6521739130434783, 0.27330173775671435, 0.9066666666666667, 1.0, 1.0, 0.6278666157264777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2933033875117225, 0.2933033875117228, 0.3437539288146409], 
reward next is 0.6562, 
noisyNet noise sample is [array([0.901465], dtype=float32), 0.7723425]. 
=============================================
[2019-04-23 12:01:18,445] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-23 12:01:18,488] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:01:18,489] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:01:18,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run38
[2019-04-23 12:01:18,553] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:01:18,554] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:01:18,569] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run38
[2019-04-23 12:01:18,565] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:01:18,678] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:01:18,679] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run38
[2019-04-23 12:01:18,588] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:01:18,778] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:01:18,780] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run38
[2019-04-23 12:01:18,830] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:01:18,831] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:01:18,832] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run38
[2019-04-23 12:03:00,444] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33704567]
[2019-04-23 12:03:00,462] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.25086736, 83.44898030333334, 1.0, 2.0, 0.6640957935167747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928070.8684503237, 928070.8684503237, 213295.5398801241]
[2019-04-23 12:03:00,465] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:03:00,476] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34682587981176305
[2019-04-23 12:03:45,548] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33704567]
[2019-04-23 12:03:45,570] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.8, 56.0, 1.0, 2.0, 0.994527669809015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1390150.207464365, 1390150.207464365, 297265.2365483171]
[2019-04-23 12:03:45,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:03:45,589] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6450256029509547
[2019-04-23 12:04:31,503] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:04:32,657] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:04:32,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:04:32,762] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:04:32,843] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:04:33,857] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 925000, evaluation results [925000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:04:36,467] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:04:36,467] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9815
[2019-04-23 12:04:36,469] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 92.0, 1.0, 2.0, 0.5896047368670424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906798.5690595781, 906798.5690595781, 208860.9060423482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2887200.0000, 
sim time next is 2887800.0000, 
raw observation next is [22.36666666666667, 91.66666666666667, 1.0, 2.0, 0.6015080193165437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 924546.5356235106, 924546.5356235106, 211234.4279068697], 
processed observation next is [1.0, 0.43478260869565216, 0.2590837282780413, 0.9166666666666667, 1.0, 1.0, 0.5198891798994503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25681848211764186, 0.25681848211764186, 0.31527526553264135], 
reward next is 0.6847, 
noisyNet noise sample is [array([1.1187717], dtype=float32), -0.7236293]. 
=============================================
[2019-04-23 12:04:44,529] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:04:44,557] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2670
[2019-04-23 12:04:44,571] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3071500491668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485706.0995626082, 485706.0995626082, 165969.880051621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2959200.0000, 
sim time next is 2959800.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3258918714676126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514351.9279243697, 514351.9279243691, 168095.8772717536], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.95, 1.0, 1.0, 0.18782153188868989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14287553553454713, 0.142875535534547, 0.2508893690623188], 
reward next is 0.7491, 
noisyNet noise sample is [array([-1.1263747], dtype=float32), 0.4594615]. 
=============================================
[2019-04-23 12:04:52,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:04:52,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0778
[2019-04-23 12:04:52,999] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3835702333057812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577730.0896667422, 577730.0896667422, 172672.2091281781], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3102600.0000, 
sim time next is 3103200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3837926002778032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578065.3288533741, 578065.3288533748, 172702.1588169946], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2575814461178352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16057370245927058, 0.16057370245927077, 0.2577644161447681], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.63048387], dtype=float32), 0.08920364]. 
=============================================
[2019-04-23 12:05:04,559] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:05:04,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1500
[2019-04-23 12:05:04,599] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 94.0, 1.0, 2.0, 0.4969973463677968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694475.2025412312, 694475.2025412312, 183127.1426194583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3187200.0000, 
sim time next is 3187800.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5015079647209113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700780.157443193, 700780.157443193, 183832.9873011234], 
processed observation next is [1.0, 0.9130434782608695, 0.40758293838862564, 0.94, 1.0, 1.0, 0.39940718641073647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1946611548453314, 0.1946611548453314, 0.27437759298675135], 
reward next is 0.7256, 
noisyNet noise sample is [array([0.8479168], dtype=float32), 0.36452606]. 
=============================================
[2019-04-23 12:05:06,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:05:06,857] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5445
[2019-04-23 12:05:06,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 77.33333333333334, 1.0, 2.0, 0.4801970682299169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670992.0562059153, 670992.0562059153, 180551.6786721637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3226800.0000, 
sim time next is 3227400.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4813187504993471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672559.9086517622, 672559.9086517622, 180721.0391912017], 
processed observation next is [0.0, 0.34782608695652173, 0.5023696682464456, 0.765, 1.0, 1.0, 0.3750828319269242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1868221968477117, 0.1868221968477117, 0.26973289431522646], 
reward next is 0.7303, 
noisyNet noise sample is [array([1.4825671], dtype=float32), -0.5610363]. 
=============================================
[2019-04-23 12:05:31,398] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:05:31,410] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8884
[2019-04-23 12:05:31,416] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5166829660210659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721992.0912784077, 721992.0912784083, 186251.3456363576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3441600.0000, 
sim time next is 3442200.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.51689380709869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722286.8123248415, 722286.8123248421, 186285.4157394324], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41794434590203616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006352256457893, 0.20063522564578948, 0.27803793393945136], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.7719136], dtype=float32), 0.50796545]. 
=============================================
[2019-04-23 12:05:41,226] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:05:41,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-23 12:05:41,326] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 79.0, 1.0, 2.0, 0.765143346344006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1069355.464498483, 1069355.464498483, 235532.7141016354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3566400.0000, 
sim time next is 3567000.0000, 
raw observation next is [27.83333333333334, 79.0, 1.0, 2.0, 0.7631696739617206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1066595.697582479, 1066595.697582479, 235069.5729072058], 
processed observation next is [1.0, 0.2608695652173913, 0.5181674565560824, 0.79, 1.0, 1.0, 0.7146622577852055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29627658266179974, 0.29627658266179974, 0.35085010881672507], 
reward next is 0.6491, 
noisyNet noise sample is [array([-0.4192231], dtype=float32), -1.4168719]. 
=============================================
[2019-04-23 12:05:41,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[54.8255  ]
 [54.800476]
 [55.04658 ]
 [55.39524 ]
 [56.439915]], R is [[54.98173141]
 [55.08037567]
 [55.16901779]
 [55.26324844]
 [55.35926056]].
[2019-04-23 12:05:43,989] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:05:43,992] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0298
[2019-04-23 12:05:44,011] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.00000000000001, 1.0, 2.0, 0.5529439566125797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772680.1768895868, 772680.1768895861, 192306.9920530774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3528600.0000, 
sim time next is 3529200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.550857715978353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769763.821872252, 769763.821872252, 191948.1791811568], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4588647180462084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21382328385340335, 0.21382328385340335, 0.28648981967336834], 
reward next is 0.7135, 
noisyNet noise sample is [array([1.4449265], dtype=float32), -0.24545193]. 
=============================================
[2019-04-23 12:05:55,047] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:05:55,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0052
[2019-04-23 12:05:55,069] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.5, 61.0, 1.0, 2.0, 0.616858181621116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862029.7291866334, 862029.7291866334, 203929.5574139173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3848400.0000, 
sim time next is 3849000.0000, 
raw observation next is [34.58333333333334, 60.83333333333334, 1.0, 2.0, 0.6432020043697867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898859.5364505397, 898859.5364505397, 209070.0666631784], 
processed observation next is [0.0, 0.5652173913043478, 0.8380726698262247, 0.6083333333333334, 1.0, 1.0, 0.5701228968310683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24968320456959436, 0.24968320456959436, 0.3120448756166842], 
reward next is 0.6880, 
noisyNet noise sample is [array([-0.12982436], dtype=float32), 2.6034355]. 
=============================================
[2019-04-23 12:05:55,166] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.25885 ]
 [73.147896]
 [73.124176]
 [73.093605]
 [73.061806]], R is [[73.02090454]
 [72.9863205 ]
 [72.95211792]
 [72.91830444]
 [72.88484955]].
[2019-04-23 12:06:07,045] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 12:06:07,047] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:06:07,048] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:06:07,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:06:07,049] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:06:07,050] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:06:07,050] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:06:07,050] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:06:07,051] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:06:07,051] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:06:07,052] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:06:07,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run39
[2019-04-23 12:06:07,080] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run39
[2019-04-23 12:06:07,081] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run39
[2019-04-23 12:06:07,105] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run39
[2019-04-23 12:06:07,132] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run39
[2019-04-23 12:06:12,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:06:12,215] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.777937685, 78.73254029, 1.0, 2.0, 0.3969760910682033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611147.7550559776, 611147.7550559776, 175994.9756319255]
[2019-04-23 12:06:12,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:06:12,218] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20977846455675708
[2019-04-23 12:06:17,633] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:06:17,634] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.8, 95.0, 1.0, 2.0, 0.3431374915803272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529872.0780038765, 529872.0780038759, 169030.1340209157]
[2019-04-23 12:06:17,637] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:06:17,639] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32218866836839144
[2019-04-23 12:06:53,354] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:06:53,354] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.2, 72.33333333333333, 1.0, 2.0, 0.3403609114690315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534925.8034039647, 534925.8034039647, 169669.9589828275]
[2019-04-23 12:06:53,355] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:06:53,377] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8940175600261041
[2019-04-23 12:07:17,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:17,660] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.63162181666667, 67.16559678333334, 1.0, 2.0, 0.4671352870673741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654994.388808141, 654994.388808141, 178899.2775182342]
[2019-04-23 12:07:17,664] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:07:17,669] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9772239295072127
[2019-04-23 12:07:30,346] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:30,348] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.34344194333333, 83.44222072333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.382036359857772, 6.9112, 168.9104254642658, 2618008.330142564, 2283985.81669855, 475102.3618268062]
[2019-04-23 12:07:30,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:07:30,353] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16883647087654152
[2019-04-23 12:07:30,353] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2618008.330142564 W.
[2019-04-23 12:07:33,711] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:33,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.83333333333334, 48.33333333333333, 1.0, 2.0, 0.4454534457089321, 1.0, 2.0, 0.4454534457089321, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1245217.159633947, 1245217.159633947, 284196.4411472306]
[2019-04-23 12:07:33,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:07:33,715] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42093866704929317
[2019-04-23 12:07:35,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:35,311] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.0, 46.0, 1.0, 2.0, 0.5301166339375395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740770.3063986235, 740770.3063986229, 188448.9844826283]
[2019-04-23 12:07:35,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:07:35,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5805342591259031
[2019-04-23 12:07:37,540] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:37,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.10202132666667, 85.04964319666666, 1.0, 2.0, 0.5839271054650711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815992.4518188693, 815992.4518188693, 197787.6257454043]
[2019-04-23 12:07:37,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:07:37,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.828423396015453
[2019-04-23 12:07:50,499] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:50,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.91536662666667, 91.99648029333333, 1.0, 2.0, 0.5066401000940551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707953.9160799773, 707953.9160799773, 184643.3137675929]
[2019-04-23 12:07:50,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:07:50,505] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1684568096322312
[2019-04-23 12:07:53,348] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:07:53,366] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.785653965, 85.14843048, 1.0, 2.0, 0.6648360860862905, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.945917365844732, 6.9112, 168.9127561194202, 1848929.693847958, 1824300.02885779, 381740.6532804682]
[2019-04-23 12:07:53,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:07:53,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6224723929578446
[2019-04-23 12:07:53,387] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1848929.693847958 W.
[2019-04-23 12:08:10,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:08:10,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.21666666666667, 84.5, 1.0, 2.0, 0.8557499177315474, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.998291786171195, 6.9112, 168.9124382053084, 2093100.445319304, 2031314.72433001, 422371.2230468337]
[2019-04-23 12:08:10,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:08:10,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13903608181873306
[2019-04-23 12:08:10,762] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2093100.445319304 W.
[2019-04-23 12:08:14,216] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.31626353]
[2019-04-23 12:08:14,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.06666666666667, 79.33333333333333, 1.0, 2.0, 0.5638031385575702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787860.3526588776, 787860.3526588776, 194195.8003676014]
[2019-04-23 12:08:14,220] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:08:14,222] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.25300413814844536
[2019-04-23 12:09:01,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:09:03,010] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:09:04,583] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:09:04,619] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:09:04,735] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:09:05,750] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 950000, evaluation results [950000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:09:20,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:09:20,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7116
[2019-04-23 12:09:20,777] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5875621490323545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821074.1042301796, 821074.1042301796, 198451.2058671792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3980400.0000, 
sim time next is 3981000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5867293978527562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819909.9476621918, 819909.9476621918, 198299.2891513203], 
processed observation next is [1.0, 0.043478260869565216, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5020836118707906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22775276323949772, 0.22775276323949772, 0.2959690882855527], 
reward next is 0.7040, 
noisyNet noise sample is [array([0.86442685], dtype=float32), -1.1966106]. 
=============================================
[2019-04-23 12:09:20,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[68.426575]
 [68.587585]
 [68.6901  ]
 [68.7495  ]
 [68.85601 ]], R is [[68.30802155]
 [68.32874298]
 [68.34903717]
 [68.36867523]
 [68.38787079]].
[2019-04-23 12:09:23,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:09:23,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4301
[2019-04-23 12:09:23,421] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5854427739261425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 818111.295625029, 818111.2956250283, 198065.5999001538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3927600.0000, 
sim time next is 3928200.0000, 
raw observation next is [33.16666666666666, 62.5, 1.0, 2.0, 0.6090831864898364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851160.1927159972, 851160.1927159972, 202449.2426094315], 
processed observation next is [0.0, 0.4782608695652174, 0.7709320695102682, 0.625, 1.0, 1.0, 0.5290158873371522, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2364333868655548, 0.2364333868655548, 0.3021630486707933], 
reward next is 0.6978, 
noisyNet noise sample is [array([0.8720559], dtype=float32), -1.2599889]. 
=============================================
[2019-04-23 12:09:26,976] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:09:26,977] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3660
[2019-04-23 12:09:27,046] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.929289861679488, 6.9112, 168.9126525651582, 1466597.264269796, 1453763.716882355, 311355.5335554228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3998400.0000, 
sim time next is 3999000.0000, 
raw observation next is [29.83333333333333, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.011999370684645, 6.9112, 168.9122115008688, 1525314.127889714, 1453803.90048471, 311355.9980509268], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.010079937068464506, 0.0, 0.8294362868142855, 0.4236983688582539, 0.4038344168013083, 0.46471044485212953], 
reward next is 0.0313, 
noisyNet noise sample is [array([0.06418109], dtype=float32), -0.770282]. 
=============================================
[2019-04-23 12:09:27,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[49.81887 ]
 [50.597908]
 [50.78811 ]
 [49.857674]
 [49.96067 ]], R is [[48.56130981]
 [48.52053833]
 [48.58770752]
 [48.67522049]
 [48.74969101]].
[2019-04-23 12:09:47,757] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:09:47,757] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1938
[2019-04-23 12:09:47,832] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.7562996446112641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1056989.458223951, 1056989.45822395, 233469.5705925064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4164600.0000, 
sim time next is 4165200.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.8130366652437334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1136326.4258456, 1136326.4258456, 247138.7285865187], 
processed observation next is [1.0, 0.21739130434782608, 0.5260663507109005, 0.89, 1.0, 1.0, 0.7747429701731727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31564622940155557, 0.31564622940155557, 0.3688637740097294], 
reward next is 0.6311, 
noisyNet noise sample is [array([-0.5974547], dtype=float32), -1.854755]. 
=============================================
[2019-04-23 12:10:09,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:10:09,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5815
[2019-04-23 12:10:09,175] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.6171029704802917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862371.9487547625, 862371.9487547625, 203975.584667055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4414800.0000, 
sim time next is 4415400.0000, 
raw observation next is [29.5, 86.5, 1.0, 2.0, 0.615433754593709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860038.3547904453, 860038.3547904453, 203656.5748547878], 
processed observation next is [0.0, 0.08695652173913043, 0.5971563981042655, 0.865, 1.0, 1.0, 0.5366671742092879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23889954299734592, 0.23889954299734592, 0.3039650370966982], 
reward next is 0.6960, 
noisyNet noise sample is [array([1.4884148], dtype=float32), 0.0014782753]. 
=============================================
[2019-04-23 12:10:20,270] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:10:20,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6947
[2019-04-23 12:10:20,387] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.632694677132829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884169.6839489749, 884169.6839489749, 206995.7315317388], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.633447829405717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 885222.6277207539, 885222.6277207533, 207143.3702792489], 
processed observation next is [0.0, 0.4782608695652174, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5583708788020686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24589517436687608, 0.2458951743668759, 0.30916920937201325], 
reward next is 0.6908, 
noisyNet noise sample is [array([-0.12829338], dtype=float32), 1.0860853]. 
=============================================
[2019-04-23 12:10:20,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.710205]
 [77.64955 ]
 [77.56523 ]
 [77.480484]
 [77.3765  ]], R is [[77.77633667]
 [77.68962097]
 [77.60387421]
 [77.51880646]
 [77.43376923]].
[2019-04-23 12:10:31,014] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:10:31,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3857
[2019-04-23 12:10:31,080] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 58.66666666666667, 1.0, 2.0, 0.5479223699667236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765660.516317923, 765660.516317923, 191445.165749903], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4540800.0000, 
sim time next is 4541400.0000, 
raw observation next is [33.0, 56.5, 1.0, 2.0, 0.5417849326905646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757081.0745698228, 757081.0745698235, 190402.0466983519], 
processed observation next is [0.0, 0.5652173913043478, 0.7630331753554502, 0.565, 1.0, 1.0, 0.4479336538440537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21030029849161744, 0.21030029849161763, 0.2841821592512715], 
reward next is 0.7158, 
noisyNet noise sample is [array([1.8099755], dtype=float32), 0.20991056]. 
=============================================
[2019-04-23 12:10:41,356] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:10:41,358] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0510
[2019-04-23 12:10:41,381] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5578854881691763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779587.9739847527, 779587.9739847521, 193163.1672444347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4646400.0000, 
sim time next is 4647000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5586445498791025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780649.0749579842, 780649.0749579842, 193295.1489402235], 
processed observation next is [1.0, 0.782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.46824644563747286, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2168469652661067, 0.2168469652661067, 0.28850022229884104], 
reward next is 0.7115, 
noisyNet noise sample is [array([1.1099869], dtype=float32), -0.71686685]. 
=============================================
[2019-04-23 12:10:41,431] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.679474]
 [68.96636 ]
 [69.12811 ]
 [69.30045 ]
 [69.69819 ]], R is [[68.12671661]
 [68.15715027]
 [68.18684387]
 [68.21639252]
 [68.24712372]].
[2019-04-23 12:10:47,856] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:10:47,881] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6270
[2019-04-23 12:10:47,881] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2111817.57427498 W.
[2019-04-23 12:10:47,895] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.66666666666666, 1.0, 2.0, 0.8691226024714537, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.99553945214427, 6.9112, 168.9124543905596, 2111817.57427498, 2051984.442047675, 426029.8688921233], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4708200.0000, 
sim time next is 4708800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4962380938857126, 1.0, 1.0, 0.4962380938857126, 1.0, 2.0, 0.8586655409506717, 6.911200000000001, 6.9112, 170.5573041426782, 2081581.757398481, 2081581.75739848, 411993.4487356929], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3930579444406176, 1.0, 0.5, 0.3930579444406176, 1.0, 1.0, 0.82764090359838, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5782171548329114, 0.5782171548329111, 0.6149155951278998], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1220946], dtype=float32), -0.88597065]. 
=============================================
[2019-04-23 12:10:48,587] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:10:48,595] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1191
[2019-04-23 12:10:48,599] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2371622.003534102 W.
[2019-04-23 12:10:48,602] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8479675182684997, 1.0, 2.0, 0.8479675182684997, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2371622.003534102, 2371622.003534102, 443895.8735768858], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4715400.0000, 
sim time next is 4716000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8482164440174547, 1.0, 2.0, 0.8482164440174547, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2372318.867667431, 2372318.867667431, 444024.0806784402], 
processed observation next is [1.0, 0.6086956521739131, 0.6682464454976303, 0.66, 1.0, 1.0, 0.8171282458041622, 1.0, 1.0, 0.8171282458041622, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6589774632409531, 0.6589774632409531, 0.6627225084752839], 
reward next is 0.3373, 
noisyNet noise sample is [array([0.00351785], dtype=float32), -0.19323303]. 
=============================================
[2019-04-23 12:10:48,702] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[39.68158 ]
 [39.421288]
 [39.63228 ]
 [40.002857]
 [42.074623]], R is [[39.91467667]
 [39.85299683]
 [39.79194641]
 [39.7107048 ]
 [39.66150284]].
[2019-04-23 12:10:50,668] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-23 12:10:50,669] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:10:50,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:10:50,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run40
[2019-04-23 12:10:50,720] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:10:50,788] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:10:50,789] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run40
[2019-04-23 12:10:50,869] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:10:50,870] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:10:50,892] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run40
[2019-04-23 12:10:50,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:10:50,995] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:10:51,007] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:10:51,007] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:10:51,008] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run40
[2019-04-23 12:10:51,070] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run40
[2019-04-23 12:10:56,255] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:10:56,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.62774199, 83.45225304, 1.0, 2.0, 0.353737637331385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544637.1901858897, 544637.1901858903, 170191.9583735768]
[2019-04-23 12:10:56,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:10:56,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9654582623305417
[2019-04-23 12:11:06,202] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:11:06,203] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.630366795, 56.39757033333333, 1.0, 2.0, 0.4081619966950621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668215.9899492998, 668215.9899492991, 180655.0510121605]
[2019-04-23 12:11:06,205] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:11:06,207] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16188919582991512
[2019-04-23 12:12:02,641] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:12:02,643] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.6, 55.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.354738868345503, 6.9112, 168.9050238931743, 2478476.500030592, 1454428.438426374, 310786.7325605908]
[2019-04-23 12:12:02,643] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:12:02,646] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.603049218513281
[2019-04-23 12:12:02,646] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2478476.500030592 W.
[2019-04-23 12:12:10,745] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:12:10,749] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.16666666666667, 70.16666666666667, 1.0, 2.0, 1.002382431044912, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005895078701013, 6.9112, 168.9122827188167, 2298336.680833987, 2231157.000830357, 464029.0006871568]
[2019-04-23 12:12:10,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:12:10,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8589445041762295
[2019-04-23 12:12:10,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2298336.680833987 W.
[2019-04-23 12:12:13,449] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:12:13,453] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.77338334333334, 79.35406885, 1.0, 2.0, 0.520895256820398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727880.1944638384, 727880.1944638384, 186934.1104876544]
[2019-04-23 12:12:13,455] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:12:13,458] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04129488240616086
[2019-04-23 12:13:29,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:13:29,104] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.239377895, 70.78982274500001, 1.0, 2.0, 0.6825178173920826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953827.124429888, 953827.1244298874, 217115.5085900968]
[2019-04-23 12:13:29,106] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:13:29,112] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3026347498237969
[2019-04-23 12:13:30,370] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.34054703]
[2019-04-23 12:13:30,418] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.93333333333333, 66.33333333333333, 1.0, 2.0, 0.7693812572364792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1075281.318358757, 1075281.318358757, 236534.5036058335]
[2019-04-23 12:13:30,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:13:30,433] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7505209016506134
[2019-04-23 12:14:02,868] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:14:03,282] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:14:03,566] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:14:04,169] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:14:04,177] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:14:05,190] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 975000, evaluation results [975000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:14:11,537] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:14:11,550] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9928
[2019-04-23 12:14:11,573] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 70.0, 1.0, 2.0, 0.506334780575076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707527.1355426661, 707527.1355426661, 184595.3005569134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [29.33333333333334, 70.0, 1.0, 2.0, 0.5041326307496193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704448.9401545095, 704448.9401545101, 184246.6101248209], 
processed observation next is [1.0, 0.782608695652174, 0.5892575039494474, 0.7, 1.0, 1.0, 0.4025694346380956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19568026115403042, 0.1956802611540306, 0.2749949404848073], 
reward next is 0.7250, 
noisyNet noise sample is [array([1.2098006], dtype=float32), -0.19539924]. 
=============================================
[2019-04-23 12:14:20,129] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:14:20,129] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3415
[2019-04-23 12:14:20,165] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5061648481581459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707289.6013497357, 707289.6013497364, 184567.5614508101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4930200.0000, 
sim time next is 4930800.0000, 
raw observation next is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5048256229970798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705417.6133108041, 705417.6133108041, 184355.5239324152], 
processed observation next is [1.0, 0.043478260869565216, 0.44707740916271754, 0.8733333333333333, 1.0, 1.0, 0.4034043650567226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19594933703077894, 0.19594933703077894, 0.2751574984065899], 
reward next is 0.7248, 
noisyNet noise sample is [array([0.9841417], dtype=float32), 2.7387683]. 
=============================================
[2019-04-23 12:14:25,492] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:14:25,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3938
[2019-04-23 12:14:25,552] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 77.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.987081761574328, 6.9112, 168.9126192848853, 1507624.818489466, 1453791.792654888, 311349.3201276031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4868400.0000, 
sim time next is 4869000.0000, 
raw observation next is [28.5, 76.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.561204040691552, 6.9112, 168.9093175616714, 1915196.085861738, 1454070.782374474, 311349.5056219258], 
processed observation next is [1.0, 0.34782608695652173, 0.5497630331753555, 0.765, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.06500040406915523, 0.0, 0.8294220762479751, 0.5319989127393717, 0.4039085506595761, 0.46470075465959076], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.63468564], dtype=float32), 0.12067931]. 
=============================================
[2019-04-23 12:14:25,581] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[55.523964]
 [57.92608 ]
 [58.445957]
 [58.85188 ]
 [58.832253]], R is [[52.07569885]
 [51.7108345 ]
 [51.82858658]
 [51.97028351]
 [52.13123322]].
[2019-04-23 12:14:36,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:14:36,982] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2338
[2019-04-23 12:14:37,017] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666666, 64.0, 1.0, 2.0, 0.5124171257713278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716029.1678607106, 716029.1678607112, 185564.9163644828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5049600.0000, 
sim time next is 5050200.0000, 
raw observation next is [30.83333333333334, 63.5, 1.0, 2.0, 0.5143577971479303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718741.8941740311, 718741.8941740311, 185876.7264951668], 
processed observation next is [0.0, 0.43478260869565216, 0.6603475513428123, 0.635, 1.0, 1.0, 0.414888912226422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19965052615945308, 0.19965052615945308, 0.27742794999278625], 
reward next is 0.7226, 
noisyNet noise sample is [array([0.14018415], dtype=float32), -0.80122614]. 
=============================================
[2019-04-23 12:14:42,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:14:42,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1947
[2019-04-23 12:14:42,765] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5069270928919654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708355.0793879728, 708355.0793879734, 184688.7124087242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5076443043177187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709357.6101144654, 709357.6101144648, 184802.5896972851], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4068003666478538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19704378058735147, 0.19704378058735134, 0.27582476074221657], 
reward next is 0.7242, 
noisyNet noise sample is [array([-1.0549637], dtype=float32), -2.020834]. 
=============================================
[2019-04-23 12:14:44,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:14:44,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8592
[2019-04-23 12:14:44,964] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5104633257084197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713298.0968898302, 713298.0968898295, 185251.679123224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5006400.0000, 
sim time next is 5007000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.511770668521239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715125.5328461556, 715125.5328461563, 185460.7396174659], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41177188978462526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19864598134615433, 0.19864598134615452, 0.27680707405591926], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.08637793], dtype=float32), 1.6338327]. 
=============================================
[2019-04-23 12:14:44,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.743904]
 [67.76949 ]
 [67.80152 ]
 [67.82672 ]
 [67.941734]], R is [[67.76234436]
 [67.80822754]
 [67.85397339]
 [67.89946747]
 [67.94449615]].
[2019-04-23 12:15:10,686] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:15:10,690] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5365
[2019-04-23 12:15:10,761] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8229685654645147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1150215.090218088, 1150215.090218088, 249630.8066591812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5291400.0000, 
sim time next is 5292000.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.9495685690163033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1327267.117137036, 1327267.117137035, 283940.5516541216], 
processed observation next is [1.0, 0.2608695652173913, 0.5545023696682465, 0.88, 1.0, 1.0, 0.9392392397786786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36868531031584334, 0.368685310315843, 0.42379186814048003], 
reward next is 0.5762, 
noisyNet noise sample is [array([1.8871803], dtype=float32), -0.10009305]. 
=============================================
[2019-04-23 12:15:10,785] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:15:10,920] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2994
[2019-04-23 12:15:10,921] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2410848.737292514 W.
[2019-04-23 12:15:11,025] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 67.33333333333334, 1.0, 2.0, 0.8619794225686022, 1.0, 2.0, 0.8619794225686022, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2410848.737292514, 2410848.737292514, 451171.0069239861], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5221200.0000, 
sim time next is 5221800.0000, 
raw observation next is [31.0, 68.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.465502463664198, 6.9112, 168.9098544125039, 2682956.960295081, 2289722.936855431, 475167.2509896771], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.68, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.05543024636641976, 0.0, 0.8294247124313341, 0.7452658223041893, 0.6360341491265087, 0.7092048522233987], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6272354], dtype=float32), -1.2238882]. 
=============================================
[2019-04-23 12:15:11,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[51.55535 ]
 [51.465237]
 [51.42417 ]
 [51.5154  ]
 [51.060684]], R is [[50.82919312]
 [50.94831848]
 [51.06521225]
 [51.17938614]
 [51.2946167 ]].
[2019-04-23 12:15:25,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:15:25,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1539
[2019-04-23 12:15:25,634] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.46666666666667, 85.16666666666667, 1.0, 2.0, 0.6027389735796208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842290.9893702389, 842290.9893702389, 201257.5864045632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5361000.0000, 
sim time next is 5361600.0000, 
raw observation next is [29.43333333333333, 85.33333333333334, 1.0, 2.0, 0.6028608298990676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 842461.3436861585, 842461.3436861578, 201280.3608809893], 
processed observation next is [1.0, 0.043478260869565216, 0.5939968404423379, 0.8533333333333334, 1.0, 1.0, 0.5215190721675513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23401703991282183, 0.23401703991282163, 0.30041844907610343], 
reward next is 0.6996, 
noisyNet noise sample is [array([0.5822594], dtype=float32), 0.17592184]. 
=============================================
[2019-04-23 12:15:29,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:15:29,496] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7589
[2019-04-23 12:15:29,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.63333333333333, 77.66666666666667, 1.0, 2.0, 1.039472012486992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129385239251, 1453016.379419023, 1453016.379419022, 311188.3186773755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5470800.0000, 
sim time next is 5471400.0000, 
raw observation next is [30.81666666666667, 76.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.217645542502184, 6.9112, 168.9112068695867, 1671304.583730655, 1453903.820750489, 311355.5728236293], 
processed observation next is [1.0, 0.30434782608695654, 0.6595576619273303, 0.7683333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03064455425021837, 0.0, 0.829431353614768, 0.4642512732585153, 0.40386217243069134, 0.46470981018452134], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6475648], dtype=float32), 0.5494844]. 
=============================================
[2019-04-23 12:15:37,675] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:15:37,681] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4387
[2019-04-23 12:15:37,838] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 95.0, 1.0, 2.0, 0.8695910168799726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1215413.86196063, 1215413.86196063, 261709.2825793039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5542200.0000, 
sim time next is 5542800.0000, 
raw observation next is [25.9, 95.0, 1.0, 2.0, 0.8628775180420613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206025.179706315, 1206025.179706315, 259929.5579927708], 
processed observation next is [1.0, 0.13043478260869565, 0.42654028436018954, 0.95, 1.0, 1.0, 0.8347921904121219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3350069943628653, 0.3350069943628653, 0.38795456416831464], 
reward next is 0.6120, 
noisyNet noise sample is [array([0.8001971], dtype=float32), -1.3264723]. 
=============================================
[2019-04-23 12:15:58,077] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-23 12:15:58,079] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:15:58,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:15:58,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run41
[2019-04-23 12:15:58,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:15:58,144] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:15:58,155] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:15:58,174] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run41
[2019-04-23 12:15:58,216] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:15:58,217] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:15:58,224] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run41
[2019-04-23 12:15:58,220] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:15:58,263] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run41
[2019-04-23 12:15:58,221] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:15:58,306] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:15:58,308] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run41
[2019-04-23 12:16:16,843] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3205611]
[2019-04-23 12:16:16,844] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.6, 92.0, 1.0, 2.0, 0.2149129967709744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 358225.1785073982, 358225.1785073988, 157091.67273694]
[2019-04-23 12:16:16,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:16:16,847] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.030901026497566875
[2019-04-23 12:16:27,874] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3205611]
[2019-04-23 12:16:27,875] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.78333333333333, 89.0, 1.0, 2.0, 0.9460413016240383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1349931.384088655, 1349931.384088655, 287099.2215068976]
[2019-04-23 12:16:27,875] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:16:27,877] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.560012144883192
[2019-04-23 12:16:55,211] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3205611]
[2019-04-23 12:16:55,215] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.26304616833333, 100.0, 1.0, 2.0, 0.3196953213860161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503518.9846426797, 503518.9846426803, 167247.1398901582]
[2019-04-23 12:16:55,219] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:16:55,225] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8217174297084147
[2019-04-23 12:18:10,238] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3205611]
[2019-04-23 12:18:10,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.83333333333333, 94.0, 1.0, 2.0, 0.5009612852500567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700016.0053627033, 700016.0053627033, 183745.6149736889]
[2019-04-23 12:18:10,245] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:18:10,266] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.033775053896200724
[2019-04-23 12:19:07,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:19:08,457] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:19:08,555] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:19:08,986] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:19:09,336] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:19:10,348] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1000000, evaluation results [1000000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:19:12,695] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:12,696] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-23 12:19:12,716] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.1, 60.0, 1.0, 2.0, 0.5354387577611922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748209.9141096384, 748209.9141096377, 189335.2411248458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5673600.0000, 
sim time next is 5674200.0000, 
raw observation next is [32.06666666666667, 59.83333333333333, 1.0, 2.0, 0.5413215084498287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756433.2628154951, 756433.2628154951, 190323.2964620353], 
processed observation next is [0.0, 0.6956521739130435, 0.7187993680884678, 0.5983333333333333, 1.0, 1.0, 0.4473753113853358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.210120350782082, 0.210120350782082, 0.2840646215851273], 
reward next is 0.7159, 
noisyNet noise sample is [array([-0.11773036], dtype=float32), -0.34387723]. 
=============================================
[2019-04-23 12:19:28,569] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:28,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0343
[2019-04-23 12:19:28,650] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 95.0, 1.0, 2.0, 0.7774507243969447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1086564.913465274, 1086564.913465275, 238450.4391852209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5886000.0000, 
sim time next is 5886600.0000, 
raw observation next is [25.86666666666667, 95.0, 1.0, 2.0, 0.9543050645449079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333891.751063515, 1333891.751063515, 285311.5487068324], 
processed observation next is [1.0, 0.13043478260869565, 0.42496050552922615, 0.95, 1.0, 1.0, 0.9449458608974795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37052548640653193, 0.37052548640653193, 0.4258381323982573], 
reward next is 0.5742, 
noisyNet noise sample is [array([0.92675805], dtype=float32), -1.7269194]. 
=============================================
[2019-04-23 12:19:31,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:31,754] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1629
[2019-04-23 12:19:31,835] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 91.66666666666667, 1.0, 2.0, 0.5293834369824261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739745.4005211208, 739745.4005211202, 188327.7587514756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5878200.0000, 
sim time next is 5878800.0000, 
raw observation next is [26.3, 92.0, 1.0, 2.0, 0.5278209957178638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737561.3314175853, 737561.3314175847, 188069.7574341381], 
processed observation next is [1.0, 0.043478260869565216, 0.4454976303317536, 0.92, 1.0, 1.0, 0.43110963339501657, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048781476159959, 0.20487814761599574, 0.2807011304987136], 
reward next is 0.7193, 
noisyNet noise sample is [array([1.593955], dtype=float32), -0.42123356]. 
=============================================
[2019-04-23 12:19:39,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:39,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4924
[2019-04-23 12:19:39,145] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 86.5, 1.0, 2.0, 0.7215532050365911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008405.463406446, 1008405.463406446, 225567.9730546769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5987400.0000, 
sim time next is 5988000.0000, 
raw observation next is [28.1, 86.0, 1.0, 2.0, 0.7260829362993917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1014739.006210879, 1014739.00621088, 226577.8577497597], 
processed observation next is [1.0, 0.30434782608695654, 0.5308056872037916, 0.86, 1.0, 1.0, 0.6699794413245683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2818719461696886, 0.2818719461696889, 0.3381759070891936], 
reward next is 0.6618, 
noisyNet noise sample is [array([1.1652163], dtype=float32), -0.29675385]. 
=============================================
[2019-04-23 12:19:39,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[43.693584]
 [43.263126]
 [42.64515 ]
 [42.795116]
 [43.37206 ]], R is [[43.99595642]
 [44.21932602]
 [44.4340477 ]
 [44.63558197]
 [44.84206772]].
[2019-04-23 12:19:43,582] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:43,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2750
[2019-04-23 12:19:43,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2301902.186668315 W.
[2019-04-23 12:19:43,596] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.26666666666667, 78.33333333333334, 1.0, 2.0, 0.8230623096724073, 1.0, 2.0, 0.8230623096724073, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2301902.186668315, 2301902.186668315, 431261.9869413031], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5934000.0000, 
sim time next is 5934600.0000, 
raw observation next is [30.3, 78.5, 1.0, 2.0, 0.8247014677623421, 1.0, 2.0, 0.8247014677623421, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2306490.737829812, 2306490.737829811, 432083.3374926724], 
processed observation next is [1.0, 0.6956521739130435, 0.6350710900473934, 0.785, 1.0, 1.0, 0.7887969491112555, 1.0, 1.0, 0.7887969491112555, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6406918716193922, 0.6406918716193919, 0.6449005037204065], 
reward next is 0.3551, 
noisyNet noise sample is [array([0.4887345], dtype=float32), 0.6405601]. 
=============================================
[2019-04-23 12:19:45,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:45,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9554
[2019-04-23 12:19:45,360] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 91.33333333333334, 1.0, 2.0, 0.954837199340371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129051603066, 1334636.016869436, 1334636.016869436, 285466.4439005664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5970000.0000, 
sim time next is 5970600.0000, 
raw observation next is [26.3, 91.5, 1.0, 2.0, 0.8767697159339759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564976088, 1225453.208459404, 1225453.208459404, 263626.620912572], 
processed observation next is [1.0, 0.08695652173913043, 0.4454976303317536, 0.915, 1.0, 1.0, 0.8515297782337059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399450893398, 0.34040366901650115, 0.34040366901650115, 0.3934725685262269], 
reward next is 0.6065, 
noisyNet noise sample is [array([0.61257243], dtype=float32), 0.6742505]. 
=============================================
[2019-04-23 12:19:48,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:48,791] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2024
[2019-04-23 12:19:48,815] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 90.16666666666667, 1.0, 2.0, 0.5199750205467105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726593.849781271, 726593.8497812704, 186785.2032186275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6225000.0000, 
sim time next is 6225600.0000, 
raw observation next is [26.46666666666667, 90.33333333333334, 1.0, 2.0, 0.5199932660851658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726619.3541415257, 726619.3541415257, 186788.186377479], 
processed observation next is [0.0, 0.043478260869565216, 0.45339652448657203, 0.9033333333333334, 1.0, 1.0, 0.42167863383754917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20183870948375712, 0.20183870948375712, 0.2787883378768343], 
reward next is 0.7212, 
noisyNet noise sample is [array([-0.7863802], dtype=float32), -2.051978]. 
=============================================
[2019-04-23 12:19:56,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:19:56,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1608
[2019-04-23 12:19:56,291] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 91.66666666666667, 1.0, 2.0, 0.5401712434587672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754825.3314103496, 754825.331410349, 190129.4420498498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6141000.0000, 
sim time next is 6141600.0000, 
raw observation next is [26.7, 92.0, 1.0, 2.0, 0.5419737015091537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757344.9509518993, 757344.9509518993, 190433.7015636399], 
processed observation next is [1.0, 0.08695652173913043, 0.46445497630331756, 0.92, 1.0, 1.0, 0.44816108615560685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2103735974866387, 0.2103735974866387, 0.2842294053188655], 
reward next is 0.7158, 
noisyNet noise sample is [array([1.3110514], dtype=float32), -0.50508124]. 
=============================================
[2019-04-23 12:20:08,417] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:08,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3992
[2019-04-23 12:20:08,486] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 85.0, 1.0, 2.0, 0.5348530469362281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747391.1672156662, 747391.1672156662, 189236.9176902414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6300000.0000, 
sim time next is 6300600.0000, 
raw observation next is [27.46666666666667, 85.00000000000001, 1.0, 2.0, 0.5341131603184139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746356.9036917143, 746356.9036917136, 189113.392622222], 
processed observation next is [0.0, 0.9565217391304348, 0.500789889415482, 0.8500000000000001, 1.0, 1.0, 0.4386905546004987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2073213621365873, 0.2073213621365871, 0.2822587949585403], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.9502926], dtype=float32), 0.35866743]. 
=============================================
[2019-04-23 12:20:09,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:09,685] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0597
[2019-04-23 12:20:09,728] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 85.00000000000001, 1.0, 2.0, 0.5341131603184139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746356.9036917143, 746356.9036917136, 189113.392622222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6300600.0000, 
sim time next is 6301200.0000, 
raw observation next is [27.43333333333334, 85.0, 1.0, 2.0, 0.5331760247438307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745046.9136892973, 745046.9136892973, 188957.1926558465], 
processed observation next is [0.0, 0.9565217391304348, 0.49921011058451853, 0.85, 1.0, 1.0, 0.43756147559497677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2069574760248048, 0.2069574760248048, 0.28202566068036794], 
reward next is 0.7180, 
noisyNet noise sample is [array([0.3704683], dtype=float32), -0.6651948]. 
=============================================
[2019-04-23 12:20:20,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:20,306] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5330
[2019-04-23 12:20:20,360] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 82.5, 1.0, 2.0, 0.512316550059083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715888.5804238338, 715888.5804238344, 185548.0190812189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6391800.0000, 
sim time next is 6392400.0000, 
raw observation next is [27.13333333333333, 82.66666666666666, 1.0, 2.0, 0.5122698657261385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715823.3238133806, 715823.3238133806, 185540.5564482097], 
processed observation next is [0.0, 1.0, 0.484992101105845, 0.8266666666666665, 1.0, 1.0, 0.41237333220016686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1988398121703835, 0.1988398121703835, 0.27692620365404436], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.35445464], dtype=float32), 0.49962503]. 
=============================================
[2019-04-23 12:20:32,735] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:32,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2129
[2019-04-23 12:20:32,751] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1927592.231533322 W.
[2019-04-23 12:20:32,754] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.8, 66.0, 1.0, 2.0, 0.6893412013509227, 1.0, 2.0, 0.6893412013509227, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1927592.231533322, 1927592.231533322, 369587.0652844765], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6516000.0000, 
sim time next is 6516600.0000, 
raw observation next is [30.01666666666667, 64.5, 1.0, 2.0, 0.4531322659481128, 1.0, 2.0, 0.4531322659481128, 1.0, 1.0, 0.7705838316527899, 6.9112, 6.9112, 170.5573041426782, 1900604.322633997, 1900604.322633997, 381570.6067591298], 
processed observation next is [1.0, 0.43478260869565216, 0.6216429699842023, 0.645, 1.0, 1.0, 0.3411232119856781, 1.0, 1.0, 0.3411232119856781, 1.0, 0.5, 0.7202241849424266, 0.0, 0.0, 0.8375144448122397, 0.5279456451761103, 0.5279456451761103, 0.5695083682972086], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0663108], dtype=float32), -0.7521305]. 
=============================================
[2019-04-23 12:20:36,637] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:36,637] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6016
[2019-04-23 12:20:36,637] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2026215.565056316 W.
[2019-04-23 12:20:36,685] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 69.0, 1.0, 2.0, 0.4830515568423187, 1.0, 2.0, 0.4830515568423187, 1.0, 1.0, 0.830119929296773, 6.9112, 6.9112, 170.5573041426782, 2026215.565056316, 2026215.565056316, 402092.0557616889], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6438000.0000, 
sim time next is 6438600.0000, 
raw observation next is [29.9, 69.0, 1.0, 2.0, 0.6883366289658333, 1.0, 2.0, 0.6883366289658333, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1924780.640805345, 1924780.640805345, 369167.9003879545], 
processed observation next is [1.0, 0.5217391304347826, 0.6161137440758293, 0.69, 1.0, 1.0, 0.6245019626094377, 1.0, 1.0, 0.6245019626094377, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5346612891125958, 0.5346612891125958, 0.5509968662506783], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5831201], dtype=float32), -0.46532494]. 
=============================================
[2019-04-23 12:20:39,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:39,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1680
[2019-04-23 12:20:39,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2058822.569252195 W.
[2019-04-23 12:20:39,230] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.25, 56.5, 1.0, 2.0, 0.8312590545113494, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.96693322206187, 6.9112, 168.9126243196446, 2058822.569252195, 2019283.581091103, 416981.641390119], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6521400.0000, 
sim time next is 6522000.0000, 
raw observation next is [31.3, 56.33333333333333, 1.0, 2.0, 0.7078336037417519, 1.0, 1.0, 0.7078336037417519, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1979349.966460121, 1979349.966460121, 377485.5384319857], 
processed observation next is [1.0, 0.4782608695652174, 0.6824644549763034, 0.5633333333333332, 1.0, 1.0, 0.6479922936647613, 1.0, 0.5, 0.6479922936647613, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5498194351278114, 0.5498194351278114, 0.5634112513910234], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25791264], dtype=float32), 0.69905263]. 
=============================================
[2019-04-23 12:20:39,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[30.85379 ]
 [30.603079]
 [30.575556]
 [30.24703 ]
 [29.8059  ]], R is [[30.58489418]
 [30.2790451 ]
 [30.38249969]
 [30.2035675 ]
 [30.03181076]].
[2019-04-23 12:20:53,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:20:53,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1298
[2019-04-23 12:20:53,247] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 88.66666666666667, 1.0, 2.0, 0.499840822466859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698449.8169956735, 698449.8169956729, 183571.178597833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6652200.0000, 
sim time next is 6652800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4989342556943306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697182.6152101486, 697182.6152101492, 183429.3100557103], 
processed observation next is [1.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3963063321618441, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19366183755837463, 0.19366183755837477, 0.2737750896353885], 
reward next is 0.7262, 
noisyNet noise sample is [array([0.17608188], dtype=float32), 0.52066696]. 
=============================================
[2019-04-23 12:20:55,898] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 12:20:55,900] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:20:55,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:20:55,921] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:20:55,922] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:20:55,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:20:55,931] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:20:55,933] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:20:55,933] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:20:55,935] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run42
[2019-04-23 12:20:55,931] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run42
[2019-04-23 12:20:55,977] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run42
[2019-04-23 12:20:56,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:20:55,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run42
[2019-04-23 12:20:56,123] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:20:56,205] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run42
[2019-04-23 12:21:29,282] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3373955]
[2019-04-23 12:21:29,282] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.434154025, 78.015042405, 1.0, 2.0, 0.5625035613032282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786043.6471882334, 786043.6471882334, 193968.3475531701]
[2019-04-23 12:21:29,283] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:21:29,284] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11555639439575305
[2019-04-23 12:21:53,476] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3373955]
[2019-04-23 12:21:53,482] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.586725941692012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819905.1160749003, 819905.1160749003, 198298.557284411]
[2019-04-23 12:21:53,485] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:21:53,486] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6027298834251598
[2019-04-23 12:22:42,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3373955]
[2019-04-23 12:22:42,245] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.67495816, 94.55815998333333, 1.0, 2.0, 0.5509626043773536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769910.445135577, 769910.4451355777, 191966.1238523488]
[2019-04-23 12:22:42,245] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:22:42,256] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7947323957262438
[2019-04-23 12:23:20,871] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:23:21,138] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:23:22,213] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:23:22,271] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:23:22,407] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:23:23,428] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1025000, evaluation results [1025000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:23:26,351] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:23:26,352] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2235
[2019-04-23 12:23:26,352] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2063612.77096188 W.
[2019-04-23 12:23:26,455] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 61.5, 1.0, 2.0, 0.8346816063774071, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.962313245891694, 6.9112, 168.9126092888636, 2063612.77096188, 2027351.349684165, 417968.0889280863], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6705000.0000, 
sim time next is 6705600.0000, 
raw observation next is [30.06666666666667, 62.0, 1.0, 2.0, 0.7353283877810582, 1.0, 1.0, 0.7353283877810582, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2056308.773922379, 2056308.773922379, 389603.6894937343], 
processed observation next is [1.0, 0.6086956521739131, 0.6240126382306479, 0.62, 1.0, 1.0, 0.6811185394952509, 1.0, 0.5, 0.6811185394952509, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5711968816451053, 0.5711968816451053, 0.5814980440204989], 
reward next is 0.4185, 
noisyNet noise sample is [array([0.08827245], dtype=float32), 0.76488006]. 
=============================================
[2019-04-23 12:23:48,156] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:23:48,157] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9365
[2019-04-23 12:23:48,208] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 50.66666666666667, 1.0, 2.0, 0.3253137773952314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503269.5484397367, 503269.5484397367, 166966.0851520679], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6801600.0000, 
sim time next is 6802200.0000, 
raw observation next is [28.65, 51.0, 1.0, 2.0, 0.3186079987081588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493879.8458758697, 493879.8458758697, 166286.6180074074], 
processed observation next is [1.0, 0.7391304347826086, 0.5568720379146919, 0.51, 1.0, 1.0, 0.17904578157609494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13718884607663046, 0.13718884607663046, 0.24818898210060808], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.5149832], dtype=float32), -0.68282306]. 
=============================================
[2019-04-23 12:23:57,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:23:57,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6408
[2019-04-23 12:23:57,725] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.41666666666666, 67.83333333333333, 1.0, 2.0, 0.4183159584869597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612532.4056348385, 612532.4056348385, 175380.4542522756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6990600.0000, 
sim time next is 6991200.0000, 
raw observation next is [27.3, 69.0, 1.0, 2.0, 0.4224405810370953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616696.7005535706, 616696.7005535706, 175725.2670841848], 
processed observation next is [0.0, 0.9565217391304348, 0.4928909952606636, 0.69, 1.0, 1.0, 0.3041452783579462, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17130463904265852, 0.17130463904265852, 0.2622765180360967], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.7646518], dtype=float32), 2.1108751]. 
=============================================
[2019-04-23 12:24:26,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:24:26,530] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1391
[2019-04-23 12:24:26,646] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 71.16666666666667, 1.0, 2.0, 0.4315112207539734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625496.4139470931, 625496.4139470936, 176455.85753759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6940200.0000, 
sim time next is 6940800.0000, 
raw observation next is [27.3, 70.0, 1.0, 2.0, 0.4317475600795346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 625741.220320253, 625741.2203202525, 176477.1497145662], 
processed observation next is [0.0, 0.34782608695652173, 0.4928909952606636, 0.7, 1.0, 1.0, 0.3153585061199212, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17381700564451474, 0.17381700564451458, 0.263398730917263], 
reward next is 0.7366, 
noisyNet noise sample is [array([-1.7594342], dtype=float32), 0.4211981]. 
=============================================
[2019-04-23 12:24:33,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:24:33,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8956
[2019-04-23 12:24:33,605] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 74.0, 1.0, 2.0, 0.7689866624988516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1100967.114886573, 1100967.114886574, 240056.2035162981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7024200.0000, 
sim time next is 7024800.0000, 
raw observation next is [27.1, 73.0, 1.0, 2.0, 0.7580408675962303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1085756.157478967, 1085756.157478967, 237480.9454318413], 
processed observation next is [1.0, 0.30434782608695654, 0.4834123222748816, 0.73, 1.0, 1.0, 0.7084829730075064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3015989326330464, 0.3015989326330464, 0.3544491722863303], 
reward next is 0.6456, 
noisyNet noise sample is [array([0.09308026], dtype=float32), -0.5141705]. 
=============================================
[2019-04-23 12:24:37,754] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:24:37,782] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2281
[2019-04-23 12:24:37,805] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1883527.813445367 W.
[2019-04-23 12:24:37,818] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.15, 61.0, 1.0, 2.0, 0.6967165408837853, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.950089619710062, 6.9112, 168.9127241445889, 1883527.813445367, 1855938.216699437, 387201.6696135513], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7032600.0000, 
sim time next is 7033200.0000, 
raw observation next is [29.3, 60.33333333333333, 1.0, 2.0, 0.6122211826127557, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.91448109698433, 6.9112, 168.9128803570791, 1745570.620343843, 1743242.898265955, 370524.4242004151], 
processed observation next is [1.0, 0.391304347826087, 0.5876777251184835, 0.6033333333333333, 1.0, 1.0, 0.5327966055575369, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0003281096984330212, 0.0, 0.8294395712044818, 0.48488072787328973, 0.48423413840720975, 0.553021528657336], 
reward next is 0.4306, 
noisyNet noise sample is [array([-0.6839947], dtype=float32), -0.94657254]. 
=============================================
[2019-04-23 12:24:45,432] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:24:45,433] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7238
[2019-04-23 12:24:45,500] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.31666666666667, 75.16666666666667, 1.0, 2.0, 0.9636922615561508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1348991.68817825, 1348991.688178249, 288359.1163473554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7131000.0000, 
sim time next is 7131600.0000, 
raw observation next is [27.2, 76.0, 1.0, 2.0, 0.9351983227775986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1308127.459114284, 1308127.459114284, 279945.2081732283], 
processed observation next is [1.0, 0.5652173913043478, 0.4881516587677725, 0.76, 1.0, 1.0, 0.9219256900934923, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3633687386428567, 0.3633687386428567, 0.41782866891526615], 
reward next is 0.5822, 
noisyNet noise sample is [array([-0.7591772], dtype=float32), 1.0667324]. 
=============================================
[2019-04-23 12:24:45,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:24:45,944] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4446
[2019-04-23 12:24:46,008] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 90.33333333333334, 1.0, 2.0, 0.5211679546692625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728261.3832011225, 728261.3832011219, 186978.3157707294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7190400.0000, 
sim time next is 7191000.0000, 
raw observation next is [26.2, 90.0, 1.0, 2.0, 0.5590150052231301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 781166.9390698414, 781166.9390698408, 193355.4766735776], 
processed observation next is [1.0, 0.21739130434782608, 0.44075829383886256, 0.9, 1.0, 1.0, 0.4686927773772651, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21699081640828927, 0.21699081640828913, 0.2885902636919069], 
reward next is 0.7114, 
noisyNet noise sample is [array([2.187839], dtype=float32), -0.8688308]. 
=============================================
[2019-04-23 12:24:46,146] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.13329 ]
 [69.666115]
 [70.57579 ]
 [70.57555 ]
 [70.529366]], R is [[69.75430298]
 [69.77768707]
 [69.77828979]
 [69.80780029]
 [69.83720398]].
[2019-04-23 12:25:03,764] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:25:03,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5302
[2019-04-23 12:25:03,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:25:03,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4493
[2019-04-23 12:25:03,822] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 62.66666666666667, 1.0, 2.0, 0.8641420552727646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1298066.380070984, 1298066.380070985, 272888.5347951196], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7316400.0000, 
sim time next is 7317000.0000, 
raw observation next is [27.5, 63.0, 1.0, 2.0, 0.896584692574569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1345635.751955853, 1345635.751955853, 282170.7008800743], 
processed observation next is [1.0, 0.6956521739130435, 0.5023696682464456, 0.63, 1.0, 1.0, 0.8754032440657458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3737877088766259, 0.3737877088766259, 0.4211502998210064], 
reward next is 0.5788, 
noisyNet noise sample is [array([-1.2461337], dtype=float32), 1.7197909]. 
=============================================
[2019-04-23 12:25:03,823] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 76.0, 1.0, 2.0, 0.3686010774412307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561849.5578281358, 561849.5578281365, 171483.4946955817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7344000.0000, 
sim time next is 7344600.0000, 
raw observation next is [24.81666666666667, 75.66666666666667, 1.0, 2.0, 0.3669082671492171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559822.073719512, 559822.0737195127, 171325.6498865445], 
processed observation next is [1.0, 0.0, 0.37519747235387063, 0.7566666666666667, 1.0, 1.0, 0.23723887608339408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15550613158875332, 0.1555061315887535, 0.25570992520379776], 
reward next is 0.7443, 
noisyNet noise sample is [array([-0.26754987], dtype=float32), -0.5946821]. 
=============================================
[2019-04-23 12:25:03,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[63.71957 ]
 [64.017914]
 [63.77852 ]
 [63.208725]
 [62.438835]], R is [[63.71329117]
 [63.6688652 ]
 [63.64303207]
 [63.6148262 ]
 [63.55940247]].
[2019-04-23 12:25:07,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:25:07,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-23 12:25:07,273] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666666, 93.0, 1.0, 2.0, 0.6971100003796143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1101304.569605126, 1101304.569605127, 235486.3110821837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7382400.0000, 
sim time next is 7383000.0000, 
raw observation next is [21.23333333333333, 93.0, 1.0, 2.0, 0.6994920625006624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1103318.787937988, 1103318.787937988, 235905.5648011823], 
processed observation next is [1.0, 0.43478260869565216, 0.2053712480252764, 0.93, 1.0, 1.0, 0.6379422439767017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3064774410938856, 0.3064774410938856, 0.3520978579122124], 
reward next is 0.6479, 
noisyNet noise sample is [array([-0.670378], dtype=float32), -0.63295424]. 
=============================================
[2019-04-23 12:25:07,310] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.56339]
 [71.78736]
 [72.18389]
 [72.80291]
 [73.37626]], R is [[71.3279953 ]
 [71.26324463]
 [71.19937134]
 [71.14529419]
 [71.11303711]].
[2019-04-23 12:25:10,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:25:10,924] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6162
[2019-04-23 12:25:10,977] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 92.33333333333333, 1.0, 2.0, 0.3164641826768507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499628.4358157842, 499628.4358157848, 166981.1764986522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7433400.0000, 
sim time next is 7434000.0000, 
raw observation next is [21.3, 92.0, 1.0, 2.0, 0.3159488283097837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498901.1413231689, 498901.1413231689, 166928.5734579208], 
processed observation next is [0.0, 0.043478260869565216, 0.2085308056872039, 0.92, 1.0, 1.0, 0.1758419618190165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1385836503675469, 0.1385836503675469, 0.2491471245640609], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.2815547], dtype=float32), 0.98041254]. 
=============================================
[2019-04-23 12:25:11,046] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.585655]
 [81.45844 ]
 [81.33584 ]
 [81.39401 ]
 [81.41435 ]], R is [[81.78347778]
 [81.71641541]
 [81.64993286]
 [81.58403778]
 [81.51878357]].
[2019-04-23 12:25:18,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:25:18,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9636
[2019-04-23 12:25:18,818] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 93.0, 1.0, 2.0, 0.4081216371613072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599604.4694083417, 599604.469408341, 174223.9876948501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7518600.0000, 
sim time next is 7519200.0000, 
raw observation next is [23.53333333333333, 93.0, 1.0, 2.0, 0.4079176848427963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599679.8845819202, 599679.8845819196, 174242.5415096132], 
processed observation next is [0.0, 0.0, 0.3143759873617693, 0.93, 1.0, 1.0, 0.28664781306361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16657774571720008, 0.1665777457171999, 0.2600634947904675], 
reward next is 0.7399, 
noisyNet noise sample is [array([-0.80014294], dtype=float32), -0.34882107]. 
=============================================
[2019-04-23 12:25:18,929] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:25:18,958] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7726
[2019-04-23 12:25:18,961] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 93.66666666666667, 1.0, 2.0, 0.3452236390077541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532743.9537262849, 532743.9537262843, 169251.9696557469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7461600.0000, 
sim time next is 7462200.0000, 
raw observation next is [22.1, 93.33333333333333, 1.0, 2.0, 0.3469143773739593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534445.9834828508, 534445.9834828508, 169362.8886042231], 
processed observation next is [0.0, 0.34782608695652173, 0.24644549763033188, 0.9333333333333332, 1.0, 1.0, 0.2131498522577823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14845721763412523, 0.14845721763412523, 0.2527804307525718], 
reward next is 0.7472, 
noisyNet noise sample is [array([-0.12400281], dtype=float32), -0.5208974]. 
=============================================
[2019-04-23 12:25:28,718] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 12:25:28,720] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:25:28,720] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:25:28,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run43
[2019-04-23 12:25:28,786] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:25:28,788] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:25:28,790] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run43
[2019-04-23 12:25:28,834] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:25:28,834] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:25:28,836] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run43
[2019-04-23 12:25:28,905] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:25:28,905] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:25:28,926] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:25:28,927] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:25:28,929] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run43
[2019-04-23 12:25:28,942] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run43
[2019-04-23 12:25:45,134] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:25:45,138] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.63333333333334, 65.0, 1.0, 2.0, 0.6438746507568369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 951669.9090683988, 951669.9090683981, 215843.360477736]
[2019-04-23 12:25:45,143] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:25:45,148] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9881626292507888
[2019-04-23 12:25:58,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:25:58,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.26696950166667, 78.11904656333334, 1.0, 2.0, 0.4834054834423286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675476.684947163, 675476.6849471637, 181037.0163570801]
[2019-04-23 12:25:58,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:25:58,789] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2793748337839099
[2019-04-23 12:26:10,642] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:26:10,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.45, 88.5, 1.0, 2.0, 0.4201363266885031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610519.9239026884, 610519.9239026884, 175051.2595800947]
[2019-04-23 12:26:10,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:26:10,653] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4205787465038381
[2019-04-23 12:26:18,215] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:26:18,216] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.36666666666667, 67.66666666666667, 1.0, 2.0, 0.5727318801043572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 800342.1060434849, 800342.1060434855, 195775.3767119978]
[2019-04-23 12:26:18,217] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:26:18,219] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6609153941773123
[2019-04-23 12:26:25,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:26:25,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.16666666666666, 74.16666666666667, 1.0, 2.0, 0.9794615561824253, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.001594289778387, 6.9112, 168.9123455239921, 2266255.078203781, 2202126.489868321, 457213.606345479]
[2019-04-23 12:26:25,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:26:25,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8584834864925458
[2019-04-23 12:26:25,125] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2266255.078203781 W.
[2019-04-23 12:26:38,950] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:26:38,951] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.06591156, 85.85977815000001, 1.0, 2.0, 0.5834400034919905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815311.5036663819, 815311.5036663826, 197702.1564510608]
[2019-04-23 12:26:38,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:26:38,956] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3175188401599145
[2019-04-23 12:26:40,203] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3554675]
[2019-04-23 12:26:40,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.6804918828114739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 950994.5880500379, 950994.5880500379, 216689.0901414799]
[2019-04-23 12:26:40,205] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:26:40,207] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8955314602221193
[2019-04-23 12:27:52,207] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:27:52,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:27:52,304] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:27:52,420] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:27:52,608] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:27:53,625] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1050000, evaluation results [1050000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:27:54,994] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:27:55,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7108
[2019-04-23 12:27:55,021] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 95.0, 1.0, 2.0, 0.4728800869518402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695344.3513718158, 695344.3513718158, 183804.3188588586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7620000.0000, 
sim time next is 7620600.0000, 
raw observation next is [23.23333333333333, 95.0, 1.0, 2.0, 0.4742532649757413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698227.8911076504, 698227.8911076504, 184124.8974606107], 
processed observation next is [1.0, 0.17391304347826086, 0.3001579778830963, 0.95, 1.0, 1.0, 0.3665701987659533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19395219197434732, 0.19395219197434732, 0.27481327979195624], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.49880493], dtype=float32), -0.47940966]. 
=============================================
[2019-04-23 12:28:09,543] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:09,553] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9472
[2019-04-23 12:28:09,557] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 95.0, 1.0, 2.0, 0.5683668133518734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794240.0310704067, 794240.0310704067, 194994.3968573486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7699800.0000, 
sim time next is 7700400.0000, 
raw observation next is [24.6, 95.0, 1.0, 2.0, 0.5593320930472424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781610.2002654336, 781610.2002654336, 193408.544843594], 
processed observation next is [1.0, 0.13043478260869565, 0.36492890995260674, 0.95, 1.0, 1.0, 0.469074810900292, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.217113944518176, 0.217113944518176, 0.28866946991581194], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.19407591], dtype=float32), -1.6711495]. 
=============================================
[2019-04-23 12:28:10,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:10,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0187
[2019-04-23 12:28:10,499] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 85.0, 1.0, 2.0, 0.5165411808302021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721793.8990162755, 721793.899016275, 186228.7630499495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7761600.0000, 
sim time next is 7762200.0000, 
raw observation next is [27.15, 85.33333333333334, 1.0, 2.0, 0.5179917238877224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723821.5202064664, 723821.5202064664, 186463.3624524275], 
processed observation next is [1.0, 0.8695652173913043, 0.485781990521327, 0.8533333333333334, 1.0, 1.0, 0.41926713721412334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2010615333906851, 0.2010615333906851, 0.27830352604839925], 
reward next is 0.7217, 
noisyNet noise sample is [array([-1.2026141], dtype=float32), -0.6360766]. 
=============================================
[2019-04-23 12:28:14,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:14,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:14,355] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-04-23 12:28:14,585] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:14,630] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8398
[2019-04-23 12:28:14,658] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 88.00000000000001, 1.0, 2.0, 0.5267284110989366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736034.0572764135, 736034.0572764135, 187890.0591313179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7852200.0000, 
sim time next is 7852800.0000, 
raw observation next is [26.9, 88.0, 1.0, 2.0, 0.5243820498853013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732754.1935141945, 732754.193514195, 187504.6804352729], 
processed observation next is [1.0, 0.9130434782608695, 0.4739336492890995, 0.88, 1.0, 1.0, 0.42696632516301364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20354283153172067, 0.20354283153172084, 0.2798577319929446], 
reward next is 0.7201, 
noisyNet noise sample is [array([0.0476638], dtype=float32), 2.438782]. 
=============================================
[2019-04-23 12:28:26,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:26,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5049
[2019-04-23 12:28:26,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2188621.014202786 W.
[2019-04-23 12:28:26,720] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.7, 63.0, 1.0, 2.0, 0.5217296452054592, 1.0, 1.0, 0.5217296452054592, 1.0, 2.0, 0.8925779254208254, 6.9112, 6.9112, 170.5573041426782, 2188621.014202786, 2188621.014202786, 427922.2345473462], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7921800.0000, 
sim time next is 7922400.0000, 
raw observation next is [30.73333333333333, 62.66666666666666, 1.0, 2.0, 0.7420454300183064, 1.0, 2.0, 0.7420454300183064, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2075110.837048598, 2075110.837048599, 392636.5660939049], 
processed observation next is [1.0, 0.6956521739130435, 0.6556082148499209, 0.6266666666666666, 1.0, 1.0, 0.689211361467839, 1.0, 1.0, 0.689211361467839, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5764196769579439, 0.5764196769579442, 0.5860247255132909], 
reward next is 0.4140, 
noisyNet noise sample is [array([0.09091468], dtype=float32), -1.243211]. 
=============================================
[2019-04-23 12:28:27,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:27,702] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:27,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-04-23 12:28:29,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:29,446] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3491
[2019-04-23 12:28:29,539] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.51666666666667, 90.16666666666667, 1.0, 2.0, 0.378453223783551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582541.3871756506, 582541.38717565, 173434.9363647899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 101400.0000, 
sim time next is 102000.0000, 
raw observation next is [22.53333333333333, 90.33333333333334, 1.0, 2.0, 0.3699564899225927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 568852.5160799492, 568852.5160799492, 172223.4007100372], 
processed observation next is [1.0, 0.17391304347826086, 0.26698262243285936, 0.9033333333333334, 1.0, 1.0, 0.24091143364167797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1580145877999859, 0.1580145877999859, 0.2570498518060257], 
reward next is 0.7430, 
noisyNet noise sample is [array([-0.15802313], dtype=float32), 0.75507903]. 
=============================================
[2019-04-23 12:28:29,597] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.98406 ]
 [64.96705 ]
 [64.96816 ]
 [65.00606 ]
 [64.985214]], R is [[64.87670898]
 [64.96908569]
 [65.06026459]
 [65.15020752]
 [65.23869324]].
[2019-04-23 12:28:29,601] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:29,601] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:29,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-04-23 12:28:29,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:29,648] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:29,651] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-04-23 12:28:29,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:29,931] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:30,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-04-23 12:28:30,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:30,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:30,988] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-04-23 12:28:32,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:32,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:32,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-04-23 12:28:34,781] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:34,782] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:34,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-04-23 12:28:35,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:35,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:35,020] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-04-23 12:28:35,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:35,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:35,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-04-23 12:28:36,343] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1060242: loss 1.0455
[2019-04-23 12:28:36,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1060242: learning rate 0.0005
[2019-04-23 12:28:37,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:37,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3557
[2019-04-23 12:28:37,669] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 65.5, 1.0, 2.0, 0.3411127325826715, 1.0, 2.0, 0.3411127325826715, 1.0, 1.0, 0.5810370910657098, 6.9112, 6.9112, 170.5573041426782, 1479315.271175075, 1479315.271175075, 325697.8044699628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 54600.0000, 
sim time next is 55200.0000, 
raw observation next is [27.36666666666667, 66.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.301330427503867, 6.9112, 168.9110093270553, 1802440.205469504, 1525671.443498234, 322623.5961379743], 
processed observation next is [1.0, 0.6521739130434783, 0.49605055292259104, 0.66, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.03901304275038671, 0.0, 0.8294303835905036, 0.50067783485264, 0.4237976231939539, 0.48152775542981235], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5539542], dtype=float32), -0.9515853]. 
=============================================
[2019-04-23 12:28:38,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:38,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:38,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-04-23 12:28:38,551] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:38,554] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:38,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-04-23 12:28:41,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:41,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:41,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run6
[2019-04-23 12:28:41,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:41,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:41,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-04-23 12:28:41,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:41,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6249
[2019-04-23 12:28:41,236] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:41,236] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:41,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-04-23 12:28:41,537] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 65.33333333333334, 1.0, 2.0, 1.021612509034909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1550388.656170097, 1550388.656170097, 324055.4728581236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 42600.0000, 
sim time next is 43200.0000, 
raw observation next is [26.8, 65.0, 1.0, 2.0, 0.9816847687986171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1487998.301291125, 1487998.301291124, 310516.4289071359], 
processed observation next is [1.0, 0.5217391304347826, 0.4691943127962086, 0.65, 1.0, 1.0, 0.9779334563838761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4133328614697569, 0.4133328614697567, 0.4634573565778148], 
reward next is 0.5365, 
noisyNet noise sample is [array([-0.12275346], dtype=float32), 0.4251957]. 
=============================================
[2019-04-23 12:28:41,897] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 12:28:41,897] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:28:41,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-04-23 12:28:45,490] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:28:45,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9448
[2019-04-23 12:28:45,511] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.21666666666667, 89.0, 1.0, 2.0, 0.3415944473847044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532224.9514514953, 532224.9514514946, 169351.5759151763], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 83400.0000, 
sim time next is 84000.0000, 
raw observation next is [22.23333333333333, 89.0, 1.0, 2.0, 0.341691003898236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532156.9568278198, 532156.9568278198, 169340.608502679], 
processed observation next is [1.0, 1.0, 0.25276461295418634, 0.89, 1.0, 1.0, 0.20685663120269396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14782137689661662, 0.14782137689661662, 0.2527471768696702], 
reward next is 0.7473, 
noisyNet noise sample is [array([-0.95073766], dtype=float32), 0.49943742]. 
=============================================
[2019-04-23 12:28:45,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.86592]
 [72.90608]
 [72.75887]
 [72.74171]
 [72.72539]], R is [[72.8888855 ]
 [72.90723419]
 [72.92532349]
 [72.94297791]
 [72.96020508]].
[2019-04-23 12:28:50,010] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1062698: loss 0.7340
[2019-04-23 12:28:50,011] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1062698: learning rate 0.0005
[2019-04-23 12:28:51,032] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1062901: loss 0.6126
[2019-04-23 12:28:51,058] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1062901: learning rate 0.0005
[2019-04-23 12:28:51,470] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1062991: loss 0.6117
[2019-04-23 12:28:51,506] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1062993: learning rate 0.0005
[2019-04-23 12:28:52,157] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1063089: loss 0.3974
[2019-04-23 12:28:52,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1063091: learning rate 0.0005
[2019-04-23 12:28:54,545] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1063453: loss 0.1288
[2019-04-23 12:28:54,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1063453: learning rate 0.0005
[2019-04-23 12:28:56,218] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1063682: loss 0.3497
[2019-04-23 12:28:56,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1063692: learning rate 0.0005
[2019-04-23 12:28:59,067] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1063940: loss 0.3151
[2019-04-23 12:28:59,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1063940: learning rate 0.0005
[2019-04-23 12:29:01,013] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1064216: loss 0.2584
[2019-04-23 12:29:01,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1064217: learning rate 0.0005
[2019-04-23 12:29:01,864] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1064342: loss 0.2170
[2019-04-23 12:29:01,873] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1064342: learning rate 0.0005
[2019-04-23 12:29:06,325] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1064932: loss 0.2694
[2019-04-23 12:29:06,350] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1064932: learning rate 0.0005
[2019-04-23 12:29:08,709] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1065294: loss 0.2042
[2019-04-23 12:29:08,711] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1065294: learning rate 0.0005
[2019-04-23 12:29:10,193] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1065508: loss 0.1594
[2019-04-23 12:29:10,249] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1065518: learning rate 0.0005
[2019-04-23 12:29:10,313] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1065528: loss 0.1528
[2019-04-23 12:29:10,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1065528: learning rate 0.0005
[2019-04-23 12:29:10,694] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1065584: loss 0.1838
[2019-04-23 12:29:10,697] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1065585: learning rate 0.0005
[2019-04-23 12:29:11,566] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1065752: loss 0.0624
[2019-04-23 12:29:11,599] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1065752: learning rate 0.0005
[2019-04-23 12:29:23,984] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1068015: loss 0.0024
[2019-04-23 12:29:23,986] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1068015: learning rate 0.0005
[2019-04-23 12:29:37,681] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1070791: loss 0.0198
[2019-04-23 12:29:37,703] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1070797: learning rate 0.0005
[2019-04-23 12:29:37,962] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1070857: loss 0.0068
[2019-04-23 12:29:37,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1070859: learning rate 0.0005
[2019-04-23 12:29:37,999] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1070866: loss 0.0023
[2019-04-23 12:29:38,000] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1070866: learning rate 0.0005
[2019-04-23 12:29:38,461] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1071006: loss 0.0150
[2019-04-23 12:29:38,462] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1071006: learning rate 0.0005
[2019-04-23 12:29:39,748] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1071321: loss 0.0359
[2019-04-23 12:29:39,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1071321: learning rate 0.0005
[2019-04-23 12:29:40,988] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1071620: loss 0.0335
[2019-04-23 12:29:41,008] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1071621: learning rate 0.0005
[2019-04-23 12:29:42,048] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:29:42,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2069
[2019-04-23 12:29:42,117] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.5, 1.0, 2.0, 0.2290585576323127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378771.8010226543, 378771.8010226543, 158832.2920210429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 448200.0000, 
sim time next is 448800.0000, 
raw observation next is [19.7, 82.33333333333334, 1.0, 2.0, 0.2289946747030891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 378745.9979576253, 378745.9979576246, 158820.4414377433], 
processed observation next is [1.0, 0.17391304347826086, 0.1327014218009479, 0.8233333333333335, 1.0, 1.0, 0.07107792132902299, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10520722165489592, 0.10520722165489572, 0.23704543498170644], 
reward next is 0.7630, 
noisyNet noise sample is [array([-0.6518242], dtype=float32), 0.70467734]. 
=============================================
[2019-04-23 12:29:43,110] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1072023: loss 0.0083
[2019-04-23 12:29:43,155] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1072030: loss 0.0068
[2019-04-23 12:29:43,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1072030: learning rate 0.0005
[2019-04-23 12:29:43,169] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1072030: learning rate 0.0005
[2019-04-23 12:29:45,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:29:45,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8791
[2019-04-23 12:29:45,260] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.83333333333333, 87.0, 1.0, 2.0, 0.2321318764183155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385011.7829843339, 385011.7829843339, 159002.7278875467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
processed observation next is [1.0, 1.0, 0.09083728278041096, 0.87, 1.0, 1.0, 0.07366329259156455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10650673415106725, 0.10650673415106725, 0.23717051313799883], 
reward next is 0.7628, 
noisyNet noise sample is [array([-1.3072008], dtype=float32), -0.6591359]. 
=============================================
[2019-04-23 12:29:45,315] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1072512: loss 0.0225
[2019-04-23 12:29:45,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1072522: learning rate 0.0005
[2019-04-23 12:29:47,085] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1072926: loss 0.0032
[2019-04-23 12:29:47,088] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1072926: learning rate 0.0005
[2019-04-23 12:29:47,914] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1073130: loss 0.0057
[2019-04-23 12:29:47,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1073132: learning rate 0.0005
[2019-04-23 12:29:49,713] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1073470: loss 0.0408
[2019-04-23 12:29:49,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1073470: learning rate 0.0005
[2019-04-23 12:29:50,084] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1073524: loss 0.0134
[2019-04-23 12:29:50,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1073524: learning rate 0.0005
[2019-04-23 12:29:50,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:29:50,194] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9510
[2019-04-23 12:29:50,292] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333333, 67.66666666666667, 1.0, 2.0, 0.4671205029577186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766549.5944158662, 766549.5944158655, 190072.9993363132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 470400.0000, 
sim time next is 471000.0000, 
raw observation next is [22.56666666666667, 66.83333333333333, 1.0, 2.0, 0.4749960563640614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779425.2878549513, 779425.2878549506, 191417.6931008899], 
processed observation next is [1.0, 0.43478260869565216, 0.26856240126382325, 0.6683333333333333, 1.0, 1.0, 0.3674651281494715, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21650702440415315, 0.21650702440415295, 0.2856980494043133], 
reward next is 0.7143, 
noisyNet noise sample is [array([-1.4170483], dtype=float32), 1.7430907]. 
=============================================
[2019-04-23 12:29:50,414] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.41616]
 [73.46404]
 [73.30518]
 [73.60017]
 [73.39634]], R is [[73.34893036]
 [73.33174896]
 [73.31640625]
 [73.29697418]
 [73.28469086]].
[2019-04-23 12:29:50,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:29:50,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2024
[2019-04-23 12:29:50,884] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 55.0, 1.0, 2.0, 0.633009382082244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1039117.682835463, 1039117.682835462, 222885.9367263796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [24.78333333333333, 54.5, 1.0, 2.0, 0.6412729187898063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051154.283686693, 1051154.283686693, 224749.5555619333], 
processed observation next is [1.0, 0.4782608695652174, 0.37361769352290675, 0.545, 1.0, 1.0, 0.567798697337116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2919873010240814, 0.2919873010240814, 0.33544709785363175], 
reward next is 0.6646, 
noisyNet noise sample is [array([0.7385842], dtype=float32), -0.26065388]. 
=============================================
[2019-04-23 12:29:50,930] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.97965]
 [74.31121]
 [74.53685]
 [74.70641]
 [76.03812]], R is [[73.70370483]
 [73.63400269]
 [73.57112885]
 [73.50972748]
 [73.44936371]].
[2019-04-23 12:29:51,284] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1073761: loss 0.0041
[2019-04-23 12:29:51,304] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1073764: learning rate 0.0005
[2019-04-23 12:29:51,764] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1073886: loss 0.0102
[2019-04-23 12:29:51,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1073886: learning rate 0.0005
[2019-04-23 12:29:57,601] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-23 12:29:57,604] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:29:57,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:29:57,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run44
[2019-04-23 12:29:57,676] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:29:57,686] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:29:57,694] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run44
[2019-04-23 12:29:57,685] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:29:57,718] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:29:57,720] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run44
[2019-04-23 12:29:57,747] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:29:57,748] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:29:57,757] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run44
[2019-04-23 12:29:57,902] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:29:57,907] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:29:57,908] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run44
[2019-04-23 12:30:30,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:30:30,702] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.41959089666667, 91.51911023333332, 1.0, 2.0, 0.2692094474000211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 441322.2061252267, 441322.2061252273, 162863.4615464451]
[2019-04-23 12:30:30,702] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:30:30,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7818738822371338
[2019-04-23 12:30:44,563] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:30:44,563] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.51666666666667, 93.33333333333334, 1.0, 2.0, 0.4608808574322754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677225.1522928596, 677225.1522928596, 181884.2629197574]
[2019-04-23 12:30:44,564] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:30:44,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6813882663808813
[2019-04-23 12:30:45,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:30:45,371] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.86403685, 93.28703803000002, 1.0, 2.0, 0.3636772934522894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564970.4821977094, 564970.4821977094, 172025.2304322634]
[2019-04-23 12:30:45,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:30:45,379] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9400229389029263
[2019-04-23 12:30:56,012] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:30:56,012] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.20415984, 84.99614676, 1.0, 2.0, 0.468391072432978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749699.4782778741, 749699.4782778741, 189408.9037700424]
[2019-04-23 12:30:56,012] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:30:56,016] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7225492807299884
[2019-04-23 12:31:03,908] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:03,910] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.9, 77.66666666666667, 1.0, 2.0, 0.5169224866816707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722326.901657219, 722326.9016572183, 186289.3409876456]
[2019-04-23 12:31:03,911] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:31:03,912] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4903819580089458
[2019-04-23 12:31:07,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:07,075] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 69.33333333333334, 1.0, 2.0, 0.5098388306968414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104235, 712425.1635039157, 712425.1635039157, 185153.5532844848]
[2019-04-23 12:31:07,075] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:31:07,088] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9616661386331857
[2019-04-23 12:31:19,139] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:19,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.0, 71.0, 1.0, 2.0, 0.5530210374218548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772787.9282779845, 772787.9282779839, 192319.3631264513]
[2019-04-23 12:31:19,142] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:31:19,143] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7941106471247481
[2019-04-23 12:31:25,594] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:25,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [38.23333333333333, 41.0, 1.0, 2.0, 0.5892051614192796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823370.980926029, 823370.9809260285, 198751.3329785534]
[2019-04-23 12:31:25,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:31:25,598] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18221171748819553
[2019-04-23 12:31:36,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:36,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.91666666666666, 92.16666666666667, 1.0, 2.0, 0.6296859982597083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879963.4139140464, 879963.4139140464, 206406.7424695602]
[2019-04-23 12:31:36,132] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:31:36,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6202292685703671
[2019-04-23 12:31:37,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:37,727] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.35, 81.5, 1.0, 2.0, 1.025154290993451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9128688028365, 1432988.975954305, 1432988.975954305, 306687.1170428929]
[2019-04-23 12:31:37,728] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:31:37,731] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8115299751248252
[2019-04-23 12:31:54,103] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38033018]
[2019-04-23 12:31:54,105] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.40000000000001, 58.0, 1.0, 2.0, 0.6186143411483624, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.961500249954915, 6.9112, 168.9122149297214, 1729673.969492025, 1693989.397640131, 368078.5301379533]
[2019-04-23 12:31:54,107] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:31:54,112] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2865424761926806
[2019-04-23 12:31:54,115] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1729673.969492025 W.
[2019-04-23 12:32:10,382] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:32:11,637] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:32:12,144] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:32:12,177] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:32:12,521] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:32:13,538] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1075000, evaluation results [1075000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:32:17,446] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1075977: loss 1.5075
[2019-04-23 12:32:17,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1075977: learning rate 0.0005
[2019-04-23 12:32:20,537] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:20,556] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4161
[2019-04-23 12:32:20,558] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 67.5, 1.0, 2.0, 0.2573000799277182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 421632.5319611416, 421632.531961141, 161635.2801093342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 583800.0000, 
sim time next is 584400.0000, 
raw observation next is [22.43333333333333, 68.0, 1.0, 2.0, 0.2573014350957554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 421930.9457295403, 421930.9457295403, 161637.8520224646], 
processed observation next is [1.0, 0.782608695652174, 0.2622432859399683, 0.68, 1.0, 1.0, 0.10518245192259688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11720304048042786, 0.11720304048042786, 0.24125052540666356], 
reward next is 0.7587, 
noisyNet noise sample is [array([-0.5492392], dtype=float32), 0.40514955]. 
=============================================
[2019-04-23 12:32:21,468] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:21,468] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2592
[2019-04-23 12:32:21,564] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 58.0, 1.0, 2.0, 0.6372104790311179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044985.718161123, 1044985.718161123, 223828.8376568352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 567600.0000, 
sim time next is 568200.0000, 
raw observation next is [23.98333333333333, 58.5, 1.0, 2.0, 0.6424025593383613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053425.796239427, 1053425.796239427, 225013.9144379167], 
processed observation next is [1.0, 0.5652173913043478, 0.3357030015797787, 0.585, 1.0, 1.0, 0.5691597100462185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2926182767331742, 0.2926182767331742, 0.3358416633401742], 
reward next is 0.6642, 
noisyNet noise sample is [array([-0.45479393], dtype=float32), 0.8893972]. 
=============================================
[2019-04-23 12:32:23,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:23,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4548
[2019-04-23 12:32:23,282] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 80.66666666666667, 1.0, 2.0, 0.2415022161778914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399074.9289187366, 399074.9289187372, 160012.917123301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 681000.0000, 
sim time next is 681600.0000, 
raw observation next is [19.9, 81.33333333333334, 1.0, 2.0, 0.2408704641226388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398089.9218956504, 398089.9218956504, 159948.9190431294], 
processed observation next is [1.0, 0.9130434782608695, 0.14218009478672985, 0.8133333333333335, 1.0, 1.0, 0.08538610135257685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11058053385990288, 0.11058053385990288, 0.2387297299151185], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.55763865], dtype=float32), 0.38025552]. 
=============================================
[2019-04-23 12:32:23,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:23,479] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5791
[2019-04-23 12:32:23,572] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.2, 91.0, 1.0, 2.0, 0.2048236585511596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342264.3359998221, 342264.3359998221, 155835.6189262077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 612000.0000, 
sim time next is 612600.0000, 
raw observation next is [17.18333333333333, 91.16666666666667, 1.0, 2.0, 0.245768296161783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410712.8942951671, 410712.8942951671, 159480.1815492728], 
processed observation next is [1.0, 0.08695652173913043, 0.013428120063191062, 0.9116666666666667, 1.0, 1.0, 0.0912871038093771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11408691508199087, 0.11408691508199087, 0.23803012171533253], 
reward next is 0.7620, 
noisyNet noise sample is [array([0.6336735], dtype=float32), 0.902721]. 
=============================================
[2019-04-23 12:32:30,144] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1078679: loss 1.2607
[2019-04-23 12:32:30,146] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1078681: learning rate 0.0005
[2019-04-23 12:32:30,339] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1078725: loss 1.4064
[2019-04-23 12:32:30,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1078729: learning rate 0.0005
[2019-04-23 12:32:31,358] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1079013: loss 1.3467
[2019-04-23 12:32:31,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1079016: learning rate 0.0005
[2019-04-23 12:32:31,662] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1079102: loss 1.0957
[2019-04-23 12:32:31,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1079102: learning rate 0.0005
[2019-04-23 12:32:32,681] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1079387: loss 1.2518
[2019-04-23 12:32:32,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1079387: learning rate 0.0005
[2019-04-23 12:32:33,458] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1079591: loss 1.2219
[2019-04-23 12:32:33,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1079591: learning rate 0.0005
[2019-04-23 12:32:36,141] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1080094: loss 0.7904
[2019-04-23 12:32:36,167] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1080094: learning rate 0.0005
[2019-04-23 12:32:36,260] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1080119: loss 0.8797
[2019-04-23 12:32:36,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1080119: learning rate 0.0005
[2019-04-23 12:32:37,879] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1080411: loss 0.6472
[2019-04-23 12:32:37,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1080411: learning rate 0.0005
[2019-04-23 12:32:40,207] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1080897: loss 0.6300
[2019-04-23 12:32:40,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1080897: learning rate 0.0005
[2019-04-23 12:32:41,224] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1081143: loss 0.6202
[2019-04-23 12:32:41,229] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1081143: learning rate 0.0005
[2019-04-23 12:32:41,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:41,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5357
[2019-04-23 12:32:41,686] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 74.66666666666667, 1.0, 2.0, 0.3109348703224167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491277.5409015107, 491277.5409015114, 166369.0214944375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 840000.0000, 
sim time next is 840600.0000, 
raw observation next is [23.45, 75.5, 1.0, 2.0, 0.3105343506142487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490927.7073007232, 490927.7073007232, 166349.4056229615], 
processed observation next is [0.0, 0.7391304347826086, 0.3104265402843602, 0.755, 1.0, 1.0, 0.1693184947159623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13636880758353423, 0.13636880758353423, 0.24828269495964403], 
reward next is 0.7517, 
noisyNet noise sample is [array([-1.4635478], dtype=float32), 0.39017633]. 
=============================================
[2019-04-23 12:32:42,439] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:42,440] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5974
[2019-04-23 12:32:42,450] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 93.0, 1.0, 2.0, 0.2612251365279191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 426841.1356951171, 426841.1356951177, 162012.107858232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 794400.0000, 
sim time next is 795000.0000, 
raw observation next is [19.4, 93.0, 1.0, 2.0, 0.2613040230641719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 426970.1042571639, 426970.1042571633, 162020.1651532058], 
processed observation next is [0.0, 0.17391304347826086, 0.11848341232227487, 0.93, 1.0, 1.0, 0.11000484706526732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11860280673810109, 0.11860280673810092, 0.24182114201971014], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.03420807], dtype=float32), -2.1948154]. 
=============================================
[2019-04-23 12:32:42,585] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.71782 ]
 [74.686066]
 [74.60979 ]
 [74.55653 ]
 [74.518585]], R is [[74.72151184]
 [74.73249054]
 [74.74333191]
 [74.75404358]
 [74.76464844]].
[2019-04-23 12:32:43,063] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1081538: loss 0.5275
[2019-04-23 12:32:43,066] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1081539: loss 0.5086
[2019-04-23 12:32:43,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1081539: learning rate 0.0005
[2019-04-23 12:32:43,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1081538: learning rate 0.0005
[2019-04-23 12:32:44,166] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1081774: loss 0.5971
[2019-04-23 12:32:44,208] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1081774: learning rate 0.0005
[2019-04-23 12:32:45,635] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1082020: loss 0.4506
[2019-04-23 12:32:45,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1082020: learning rate 0.0005
[2019-04-23 12:32:55,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1084007: loss 0.0428
[2019-04-23 12:32:55,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1084012: learning rate 0.0005
[2019-04-23 12:32:57,550] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:32:57,551] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2703
[2019-04-23 12:32:57,647] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.0, 1.0, 2.0, 0.3241191120042129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504031.8680298316, 504031.8680298316, 167111.298592803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 968400.0000, 
sim time next is 969000.0000, 
raw observation next is [21.9, 92.16666666666667, 1.0, 2.0, 0.3322008264389752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516494.7689646671, 516494.7689646671, 168073.3060268565], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9216666666666667, 1.0, 1.0, 0.1954226824565966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14347076915685197, 0.14347076915685197, 0.25085568063709923], 
reward next is 0.7491, 
noisyNet noise sample is [array([-1.805755], dtype=float32), -0.0655521]. 
=============================================
[2019-04-23 12:32:57,686] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.57768 ]
 [73.513596]
 [73.49901 ]
 [73.55771 ]
 [73.53054 ]], R is [[73.53253174]
 [73.54779053]
 [73.56290436]
 [73.57779694]
 [73.59229279]].
[2019-04-23 12:33:07,139] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1086748: loss 0.0094
[2019-04-23 12:33:07,180] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1086748: learning rate 0.0005
[2019-04-23 12:33:07,572] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1086883: loss 0.0041
[2019-04-23 12:33:07,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1086883: learning rate 0.0005
[2019-04-23 12:33:07,674] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1086897: loss 0.0036
[2019-04-23 12:33:07,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1086897: learning rate 0.0005
[2019-04-23 12:33:08,354] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1087068: loss 0.0046
[2019-04-23 12:33:08,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1087068: learning rate 0.0005
[2019-04-23 12:33:09,206] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1087219: loss 0.0071
[2019-04-23 12:33:09,225] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1087219: learning rate 0.0005
[2019-04-23 12:33:10,429] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1087493: loss 0.0035
[2019-04-23 12:33:10,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1087493: learning rate 0.0005
[2019-04-23 12:33:12,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:33:12,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0729
[2019-04-23 12:33:12,314] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 67.0, 1.0, 2.0, 0.7717465397206623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1190770.815690985, 1190770.815690985, 251603.0383437812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [25.66666666666666, 67.5, 1.0, 2.0, 0.4232138857200688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653470.7032023642, 653470.7032023648, 179974.1591324478], 
processed observation next is [1.0, 0.7391304347826086, 0.4154818325434437, 0.675, 1.0, 1.0, 0.3050769707470708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1815196397784345, 0.1815196397784347, 0.2686181479588773], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.24343425], dtype=float32), 0.20522854]. 
=============================================
[2019-04-23 12:33:12,480] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1088052: loss 0.0033
[2019-04-23 12:33:12,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1088052: learning rate 0.0005
[2019-04-23 12:33:12,801] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1088142: loss 0.0158
[2019-04-23 12:33:12,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1088142: learning rate 0.0005
[2019-04-23 12:33:13,812] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1088463: loss 0.0033
[2019-04-23 12:33:13,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1088463: learning rate 0.0005
[2019-04-23 12:33:16,090] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1089175: loss 0.0063
[2019-04-23 12:33:16,119] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1089175: learning rate 0.0005
[2019-04-23 12:33:16,384] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1089250: loss 0.0034
[2019-04-23 12:33:16,392] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1089250: learning rate 0.0005
[2019-04-23 12:33:16,719] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1089341: loss 0.0036
[2019-04-23 12:33:16,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1089341: learning rate 0.0005
[2019-04-23 12:33:17,314] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1089504: loss 0.0118
[2019-04-23 12:33:17,316] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1089504: learning rate 0.0005
[2019-04-23 12:33:18,369] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1089772: loss 0.0179
[2019-04-23 12:33:18,383] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1089773: learning rate 0.0005
[2019-04-23 12:33:19,091] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1089968: loss 0.0089
[2019-04-23 12:33:19,092] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1089968: learning rate 0.0005
[2019-04-23 12:33:21,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:33:21,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3062
[2019-04-23 12:33:21,561] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.61666666666667, 68.83333333333333, 1.0, 2.0, 0.6240442131922198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 959612.0107939396, 959612.0107939389, 216011.5901053671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [25.63333333333334, 68.66666666666667, 1.0, 2.0, 0.6260929258838839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 963043.2271748716, 963043.2271748709, 216480.3294812276], 
processed observation next is [1.0, 0.6086956521739131, 0.4139020537124806, 0.6866666666666668, 1.0, 1.0, 0.5495095492576914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26751200754857546, 0.26751200754857524, 0.3231049693749666], 
reward next is 0.6769, 
noisyNet noise sample is [array([3.4844353], dtype=float32), -0.7175057]. 
=============================================
[2019-04-23 12:33:30,925] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1092003: loss 0.9192
[2019-04-23 12:33:30,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1092003: learning rate 0.0005
[2019-04-23 12:33:38,046] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:33:38,048] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3814
[2019-04-23 12:33:38,092] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 60.33333333333334, 1.0, 2.0, 0.9746844197968386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1478238.822412414, 1478238.822412414, 308355.7544564822], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1174800.0000, 
sim time next is 1175400.0000, 
raw observation next is [27.6, 60.0, 1.0, 2.0, 0.9793493051604505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1487928.539479675, 1487928.539479675, 310223.311881785], 
processed observation next is [1.0, 0.6086956521739131, 0.5071090047393366, 0.6, 1.0, 1.0, 0.975119644771627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4133134831887986, 0.4133134831887986, 0.4630198684802761], 
reward next is 0.5370, 
noisyNet noise sample is [array([0.01439393], dtype=float32), 1.7127419]. 
=============================================
[2019-04-23 12:33:44,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:33:44,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0168
[2019-04-23 12:33:44,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1925412.76549144 W.
[2019-04-23 12:33:44,636] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.23333333333333, 72.16666666666667, 1.0, 2.0, 0.7359328538937335, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.965207195840017, 6.9112, 168.9126409261011, 1925412.76549144, 1887098.273562368, 393950.6034812281], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1253400.0000, 
sim time next is 1254000.0000, 
raw observation next is [28.26666666666667, 72.33333333333334, 1.0, 2.0, 0.4666083369286121, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7822926544542996, 6.911199999999999, 6.9112, 168.9129125354993, 1304397.083153039, 1304397.083153039, 285853.6399262592], 
processed observation next is [1.0, 0.5217391304347826, 0.53870458135861, 0.7233333333333334, 1.0, 1.0, 0.3573594420826652, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7345032371393898, -8.881784197001253e-17, 0.0, 0.8294397292152562, 0.3623325230980664, 0.3623325230980664, 0.4266472237705361], 
reward next is 0.5734, 
noisyNet noise sample is [array([0.17543772], dtype=float32), -1.0798424]. 
=============================================
[2019-04-23 12:33:44,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[52.408607]
 [53.19966 ]
 [53.395187]
 [52.98472 ]
 [55.960968]], R is [[56.17427826]
 [55.61253738]
 [55.05641174]
 [54.91807175]
 [54.84407425]].
[2019-04-23 12:33:45,532] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:33:45,533] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0848
[2019-04-23 12:33:45,533] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1916497.789908995 W.
[2019-04-23 12:33:45,605] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.46666666666667, 73.0, 1.0, 2.0, 0.4569181183190063, 1.0, 2.0, 0.4569181183190063, 1.0, 2.0, 0.7774095244669507, 6.9112, 6.9112, 170.5573041426782, 1916497.789908995, 1916497.789908995, 383971.0979605867], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1258800.0000, 
sim time next is 1259400.0000, 
raw observation next is [28.48333333333333, 73.0, 1.0, 2.0, 0.6788633572514087, 1.0, 2.0, 0.6788633572514087, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1898267.271730465, 1898267.271730465, 365199.0588535006], 
processed observation next is [1.0, 0.5652173913043478, 0.5489731437598735, 0.73, 1.0, 1.0, 0.6130883822306128, 1.0, 1.0, 0.6130883822306128, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5272964643695737, 0.5272964643695737, 0.5450732221694039], 
reward next is 0.4549, 
noisyNet noise sample is [array([-0.1749746], dtype=float32), -0.038403586]. 
=============================================
[2019-04-23 12:33:46,378] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1094758: loss 0.9108
[2019-04-23 12:33:46,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1094758: learning rate 0.0005
[2019-04-23 12:33:46,933] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1094878: loss 0.8395
[2019-04-23 12:33:46,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1094878: learning rate 0.0005
[2019-04-23 12:33:47,066] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1094904: loss 0.8103
[2019-04-23 12:33:47,083] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1094904: learning rate 0.0005
[2019-04-23 12:33:48,061] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1095107: loss 0.7018
[2019-04-23 12:33:48,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1095107: learning rate 0.0005
[2019-04-23 12:33:48,567] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1095216: loss 0.6677
[2019-04-23 12:33:48,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1095216: learning rate 0.0005
[2019-04-23 12:33:48,843] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1095294: loss 0.5930
[2019-04-23 12:33:48,844] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1095294: learning rate 0.0005
[2019-04-23 12:33:52,174] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1095999: loss 0.6768
[2019-04-23 12:33:52,206] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1096003: learning rate 0.0005
[2019-04-23 12:33:53,205] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1096230: loss 0.3901
[2019-04-23 12:33:53,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1096230: learning rate 0.0005
[2019-04-23 12:33:54,686] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1096524: loss 0.5030
[2019-04-23 12:33:54,746] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1096526: learning rate 0.0005
[2019-04-23 12:33:57,104] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1097043: loss 0.5891
[2019-04-23 12:33:57,124] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1097043: learning rate 0.0005
[2019-04-23 12:33:58,497] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1097272: loss 0.3130
[2019-04-23 12:33:58,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1097272: learning rate 0.0005
[2019-04-23 12:33:58,725] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1097344: loss 0.4305
[2019-04-23 12:33:58,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1097344: learning rate 0.0005
[2019-04-23 12:34:00,483] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1097650: loss 0.5185
[2019-04-23 12:34:00,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1097650: learning rate 0.0005
[2019-04-23 12:34:00,761] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1097702: loss 0.5138
[2019-04-23 12:34:00,776] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1097705: learning rate 0.0005
[2019-04-23 12:34:01,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:34:01,385] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6535
[2019-04-23 12:34:01,443] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333333, 93.16666666666667, 1.0, 2.0, 0.3151404346517996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500459.3537640856, 500459.3537640856, 167096.4078086852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1361400.0000, 
sim time next is 1362000.0000, 
raw observation next is [20.96666666666667, 93.33333333333334, 1.0, 2.0, 0.3179549541097438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504321.7038382572, 504321.7038382572, 167376.3973238359], 
processed observation next is [1.0, 0.782608695652174, 0.1927330173775673, 0.9333333333333335, 1.0, 1.0, 0.178258980855113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14008936217729365, 0.14008936217729365, 0.24981551839378494], 
reward next is 0.7502, 
noisyNet noise sample is [array([-0.54511255], dtype=float32), -0.83301425]. 
=============================================
[2019-04-23 12:34:01,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.549904]
 [75.615486]
 [75.58333 ]
 [75.66732 ]
 [75.50448 ]], R is [[75.40466309]
 [75.40122223]
 [75.39865875]
 [75.39672852]
 [75.39537048]].
[2019-04-23 12:34:01,590] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1097886: loss 0.4630
[2019-04-23 12:34:01,596] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1097886: learning rate 0.0005
[2019-04-23 12:34:12,211] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1099963: loss 0.0260
[2019-04-23 12:34:12,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1099963: learning rate 0.0005
[2019-04-23 12:34:12,411] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-23 12:34:12,430] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:34:12,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:34:12,433] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:34:12,439] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:34:12,433] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:34:12,446] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:34:12,447] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run45
[2019-04-23 12:34:12,444] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run45
[2019-04-23 12:34:12,434] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:34:12,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run45
[2019-04-23 12:34:12,487] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:34:12,433] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:34:12,544] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run45
[2019-04-23 12:34:12,567] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:34:12,615] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run45
[2019-04-23 12:35:06,046] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3911101]
[2019-04-23 12:35:06,053] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.73688085, 94.89629333500001, 1.0, 2.0, 0.3819181864595209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 572521.370289546, 572521.3702895466, 172122.9740137527]
[2019-04-23 12:35:06,056] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:35:06,061] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5668652263912303
[2019-04-23 12:35:55,866] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3911101]
[2019-04-23 12:35:55,869] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.19361222, 83.25061265666668, 1.0, 2.0, 0.5693570474481219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795624.309849986, 795624.309849986, 195177.1253412096]
[2019-04-23 12:35:55,876] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:35:55,878] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9242216355222397
[2019-04-23 12:36:55,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3911101]
[2019-04-23 12:36:55,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 57.0, 1.0, 2.0, 0.3933394035745594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 592211.3528626972, 592211.3528626978, 173973.8248621355]
[2019-04-23 12:36:55,428] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:36:55,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8334924402165834
[2019-04-23 12:36:58,233] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3911101]
[2019-04-23 12:36:58,235] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.63333333333334, 62.0, 1.0, 2.0, 0.3262123713433607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515460.1671022695, 515460.1671022695, 168191.9568614688]
[2019-04-23 12:36:58,235] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:36:58,237] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9704779953658719
[2019-04-23 12:36:59,913] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:37:00,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:37:00,322] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:37:00,432] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:37:00,528] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:37:01,544] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1100000, evaluation results [1100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:37:03,086] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:03,098] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4109
[2019-04-23 12:37:03,103] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 96.33333333333333, 1.0, 2.0, 0.3344182530572016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521447.604229324, 521447.6042293247, 168504.2985355485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [21.2, 96.5, 1.0, 2.0, 0.333327726455469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520395.3048499784, 520395.3048499784, 168438.5593902614], 
processed observation next is [0.0, 0.08695652173913043, 0.20379146919431282, 0.965, 1.0, 1.0, 0.19678039331984215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14455425134721622, 0.14455425134721622, 0.2514008349108379], 
reward next is 0.7486, 
noisyNet noise sample is [array([0.4181388], dtype=float32), -0.010772608]. 
=============================================
[2019-04-23 12:37:04,256] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:04,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2572
[2019-04-23 12:37:04,264] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 63.66666666666667, 1.0, 2.0, 0.345033003962754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 533590.4046474983, 533590.4046474977, 169354.2843945845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1532400.0000, 
sim time next is 1533000.0000, 
raw observation next is [26.1, 64.83333333333333, 1.0, 2.0, 0.3440100814731696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531925.144956696, 531925.1449566953, 169216.9894561035], 
processed observation next is [0.0, 0.7391304347826086, 0.4360189573459717, 0.6483333333333333, 1.0, 1.0, 0.20965070057008384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14775698471019333, 0.14775698471019313, 0.2525626708300052], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.5534901], dtype=float32), -0.5548869]. 
=============================================
[2019-04-23 12:37:04,277] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.92717 ]
 [74.96658 ]
 [75.00825 ]
 [75.04378 ]
 [75.116356]], R is [[74.89337158]
 [74.89167023]
 [74.89002228]
 [74.88844299]
 [74.88696289]].
[2019-04-23 12:37:05,363] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:05,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-23 12:37:05,374] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.11666666666667, 95.0, 1.0, 2.0, 0.4101477262998694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606834.6002153656, 606834.6002153651, 175024.2289283059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1624200.0000, 
sim time next is 1624800.0000, 
raw observation next is [23.13333333333333, 95.0, 1.0, 2.0, 0.4109285298736459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607633.886693007, 607633.8866930075, 175088.9145583835], 
processed observation next is [1.0, 0.8260869565217391, 0.29541864139020524, 0.95, 1.0, 1.0, 0.29027533719716375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16878719074805748, 0.16878719074805765, 0.26132673814684104], 
reward next is 0.7387, 
noisyNet noise sample is [array([0.09547531], dtype=float32), -0.6949279]. 
=============================================
[2019-04-23 12:37:05,821] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1102715: loss 0.0061
[2019-04-23 12:37:05,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1102715: learning rate 0.0005
[2019-04-23 12:37:06,124] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1102901: loss 0.0025
[2019-04-23 12:37:06,125] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1102901: learning rate 0.0005
[2019-04-23 12:37:06,189] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1102943: loss 0.0067
[2019-04-23 12:37:06,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1102943: learning rate 0.0005
[2019-04-23 12:37:06,278] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1102999: loss 0.0066
[2019-04-23 12:37:06,281] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1103000: learning rate 0.0005
[2019-04-23 12:37:06,577] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1103181: loss 0.0179
[2019-04-23 12:37:06,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1103182: learning rate 0.0005
[2019-04-23 12:37:06,799] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1103314: loss 0.0039
[2019-04-23 12:37:06,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1103315: learning rate 0.0005
[2019-04-23 12:37:06,880] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:06,887] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-23 12:37:06,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 85.0, 1.0, 2.0, 0.4809180021870427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734962.9368181376, 734962.9368181371, 188286.2661040828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1585200.0000, 
sim time next is 1585800.0000, 
raw observation next is [23.45, 85.0, 1.0, 2.0, 0.5584358961648915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852950.3846322736, 852950.3846322736, 202073.3479479912], 
processed observation next is [1.0, 0.34782608695652173, 0.3104265402843602, 0.85, 1.0, 1.0, 0.4679950556203512, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23693066239785376, 0.23693066239785376, 0.30160201186267344], 
reward next is 0.6984, 
noisyNet noise sample is [array([0.65545374], dtype=float32), -0.426167]. 
=============================================
[2019-04-23 12:37:07,732] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1103882: loss 0.0161
[2019-04-23 12:37:07,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1103882: learning rate 0.0005
[2019-04-23 12:37:08,437] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1104309: loss 0.0013
[2019-04-23 12:37:08,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1104309: learning rate 0.0005
[2019-04-23 12:37:08,808] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1104537: loss 0.0484
[2019-04-23 12:37:08,810] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1104537: learning rate 0.0005
[2019-04-23 12:37:09,563] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1105022: loss 0.0014
[2019-04-23 12:37:09,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1105023: learning rate 0.0005
[2019-04-23 12:37:09,860] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1105212: loss 0.0184
[2019-04-23 12:37:09,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1105213: learning rate 0.0005
[2019-04-23 12:37:09,967] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1105275: loss 0.0012
[2019-04-23 12:37:09,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1105276: learning rate 0.0005
[2019-04-23 12:37:10,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1105659: loss 0.0138
[2019-04-23 12:37:10,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1105659: learning rate 0.0005
[2019-04-23 12:37:10,686] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1105737: loss 0.0104
[2019-04-23 12:37:10,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1105739: learning rate 0.0005
[2019-04-23 12:37:10,912] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1105884: loss 0.0479
[2019-04-23 12:37:10,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1105884: learning rate 0.0005
[2019-04-23 12:37:14,389] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1108019: loss -213.1439
[2019-04-23 12:37:14,392] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1108019: learning rate 0.0005
[2019-04-23 12:37:15,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:15,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5508
[2019-04-23 12:37:15,873] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 84.0, 1.0, 2.0, 0.7829286534486163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196817.775381536, 1196817.775381536, 253267.13430812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1769400.0000, 
sim time next is 1770000.0000, 
raw observation next is [23.46666666666667, 84.66666666666666, 1.0, 2.0, 0.7629865618473617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1166042.850806639, 1166042.850806639, 248028.8281951654], 
processed observation next is [1.0, 0.4782608695652174, 0.31121642969984215, 0.8466666666666666, 1.0, 1.0, 0.7144416407799539, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32390079189073306, 0.32390079189073306, 0.3701922808883066], 
reward next is 0.6298, 
noisyNet noise sample is [array([0.8244479], dtype=float32), 0.21600248]. 
=============================================
[2019-04-23 12:37:15,887] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[65.19392 ]
 [65.4423  ]
 [65.27361 ]
 [65.45923 ]
 [65.237656]], R is [[65.23201752]
 [65.20168304]
 [65.18008423]
 [65.1517868 ]
 [65.13568115]].
[2019-04-23 12:37:16,650] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:16,657] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-23 12:37:16,662] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 92.0, 1.0, 2.0, 0.4513999253238355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644504.4327146407, 644504.4327146407, 178102.8765773601], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1897200.0000, 
sim time next is 1897800.0000, 
raw observation next is [24.38333333333333, 92.16666666666667, 1.0, 2.0, 0.4517948544353128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645023.1309467133, 645023.1309467133, 178154.6848170711], 
processed observation next is [1.0, 1.0, 0.3546603475513427, 0.9216666666666667, 1.0, 1.0, 0.33951187281362993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17917309192964256, 0.17917309192964256, 0.26590251465234493], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.90603083], dtype=float32), 0.9757078]. 
=============================================
[2019-04-23 12:37:17,006] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:17,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0589
[2019-04-23 12:37:17,018] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 88.66666666666667, 1.0, 2.0, 0.8853274306964721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253347.294663158, 1253347.294663158, 268275.8140768221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1852800.0000, 
sim time next is 1853400.0000, 
raw observation next is [25.18333333333334, 88.33333333333334, 1.0, 2.0, 0.880329486664621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1242475.176727639, 1242475.176727639, 266352.1982722688], 
processed observation next is [1.0, 0.43478260869565216, 0.39257503949447115, 0.8833333333333334, 1.0, 1.0, 0.8558186586320734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3451319935354553, 0.3451319935354553, 0.3975405944362221], 
reward next is 0.6025, 
noisyNet noise sample is [array([1.0133274], dtype=float32), -0.121488094]. 
=============================================
[2019-04-23 12:37:19,054] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1110835: loss -145.7717
[2019-04-23 12:37:19,057] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1110835: learning rate 0.0005
[2019-04-23 12:37:19,282] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1110985: loss -103.4931
[2019-04-23 12:37:19,284] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1110985: learning rate 0.0005
[2019-04-23 12:37:19,311] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1111000: loss -162.1833
[2019-04-23 12:37:19,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1111000: learning rate 0.0005
[2019-04-23 12:37:19,378] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1111053: loss -153.0953
[2019-04-23 12:37:19,380] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1111054: learning rate 0.0005
[2019-04-23 12:37:19,566] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1111158: loss -176.0372
[2019-04-23 12:37:19,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1111158: learning rate 0.0005
[2019-04-23 12:37:19,851] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1111317: loss -17.6782
[2019-04-23 12:37:19,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1111317: learning rate 0.0005
[2019-04-23 12:37:20,887] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1111925: loss -150.5832
[2019-04-23 12:37:20,889] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1111927: learning rate 0.0005
[2019-04-23 12:37:22,031] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1112334: loss -25.8302
[2019-04-23 12:37:22,040] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1112334: learning rate 0.0005
[2019-04-23 12:37:23,367] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1112628: loss -10.0399
[2019-04-23 12:37:23,367] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1112628: learning rate 0.0005
[2019-04-23 12:37:24,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1113051: loss -87.0645
[2019-04-23 12:37:24,989] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1113051: learning rate 0.0005
[2019-04-23 12:37:25,497] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1113176: loss -65.5690
[2019-04-23 12:37:25,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1113176: learning rate 0.0005
[2019-04-23 12:37:25,875] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1113255: loss -53.2901
[2019-04-23 12:37:25,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1113255: learning rate 0.0005
[2019-04-23 12:37:27,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1113767: loss -11.3887
[2019-04-23 12:37:27,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1113770: learning rate 0.0005
[2019-04-23 12:37:27,589] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1113783: loss -122.2090
[2019-04-23 12:37:27,600] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1113783: learning rate 0.0005
[2019-04-23 12:37:28,058] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1113884: loss -97.5857
[2019-04-23 12:37:28,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1113884: learning rate 0.0005
[2019-04-23 12:37:34,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:34,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0580
[2019-04-23 12:37:34,257] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.55, 73.83333333333334, 1.0, 2.0, 0.5683691952604154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794243.3608116453, 794243.3608116459, 195001.7289771804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2131800.0000, 
sim time next is 2132400.0000, 
raw observation next is [30.6, 73.66666666666667, 1.0, 2.0, 0.5692102775244059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795419.1355337119, 795419.1355337119, 195150.646355685], 
processed observation next is [0.0, 0.6956521739130435, 0.6492890995260664, 0.7366666666666667, 1.0, 1.0, 0.4809762379812119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22094975987047552, 0.22094975987047552, 0.2912696214263955], 
reward next is 0.7087, 
noisyNet noise sample is [array([-0.24655516], dtype=float32), 1.1983905]. 
=============================================
[2019-04-23 12:37:34,332] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1115846: loss 4.4590
[2019-04-23 12:37:34,335] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1115846: learning rate 0.0005
[2019-04-23 12:37:46,243] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1118697: loss 2.7912
[2019-04-23 12:37:46,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1118697: learning rate 0.0005
[2019-04-23 12:37:47,154] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1118962: loss 2.7493
[2019-04-23 12:37:47,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1118962: learning rate 0.0005
[2019-04-23 12:37:47,308] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1119004: loss 2.9449
[2019-04-23 12:37:47,310] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1119004: learning rate 0.0005
[2019-04-23 12:37:47,701] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1119127: loss 3.0654
[2019-04-23 12:37:47,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1119128: learning rate 0.0005
[2019-04-23 12:37:47,836] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1119151: loss 3.3482
[2019-04-23 12:37:47,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1119159: learning rate 0.0005
[2019-04-23 12:37:48,634] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1119372: loss 2.7638
[2019-04-23 12:37:48,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1119374: learning rate 0.0005
[2019-04-23 12:37:50,800] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1119881: loss 2.6222
[2019-04-23 12:37:50,814] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1119885: learning rate 0.0005
[2019-04-23 12:37:51,390] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1119992: loss 2.0076
[2019-04-23 12:37:51,402] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1119992: learning rate 0.0005
[2019-04-23 12:37:51,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:51,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8289
[2019-04-23 12:37:51,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2310308.109013662 W.
[2019-04-23 12:37:51,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.06666666666667, 62.33333333333334, 1.0, 2.0, 0.550710089553516, 1.0, 1.0, 0.550710089553516, 1.0, 2.0, 0.9564013029448454, 6.9112, 6.9112, 170.5573041426782, 2310308.109013662, 2310308.109013662, 451983.9764785335], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2287200.0000, 
sim time next is 2287800.0000, 
raw observation next is [32.05, 62.5, 1.0, 2.0, 0.5441440502464763, 1.0, 2.0, 0.5441440502464763, 1.0, 2.0, 0.9449982640908975, 6.911199999999999, 6.9112, 170.5573041426782, 2282737.471555361, 2282737.471555362, 447014.5022145892], 
processed observation next is [1.0, 0.4782608695652174, 0.7180094786729857, 0.625, 1.0, 1.0, 0.450775964152381, 1.0, 1.0, 0.450775964152381, 1.0, 1.0, 0.9329247123059724, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6340937420987114, 0.6340937420987116, 0.6671858242008795], 
reward next is 0.3328, 
noisyNet noise sample is [array([1.4295924], dtype=float32), -0.9955351]. 
=============================================
[2019-04-23 12:37:52,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:37:52,903] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9365
[2019-04-23 12:37:52,926] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.13333333333333, 70.66666666666667, 1.0, 2.0, 0.5390484970333709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753255.8699807245, 753255.8699807245, 189943.1877018184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2223600.0000, 
sim time next is 2224200.0000, 
raw observation next is [30.96666666666667, 71.33333333333333, 1.0, 2.0, 0.5483779519453399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766297.3710551697, 766297.371055169, 191525.0280127278], 
processed observation next is [1.0, 0.7391304347826086, 0.6666666666666667, 0.7133333333333333, 1.0, 1.0, 0.4558770505365541, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21286038084865827, 0.21286038084865808, 0.2858582507652654], 
reward next is 0.7141, 
noisyNet noise sample is [array([-0.5843791], dtype=float32), 0.18237846]. 
=============================================
[2019-04-23 12:37:54,333] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1120739: loss 2.1631
[2019-04-23 12:37:54,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1120739: learning rate 0.0005
[2019-04-23 12:37:55,128] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1120970: loss 1.9122
[2019-04-23 12:37:55,160] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1120978: learning rate 0.0005
[2019-04-23 12:37:55,767] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1121160: loss 1.6122
[2019-04-23 12:37:55,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1121165: learning rate 0.0005
[2019-04-23 12:37:55,880] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1121202: loss 1.6915
[2019-04-23 12:37:55,893] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1121203: learning rate 0.0005
[2019-04-23 12:37:57,894] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1121697: loss 1.7452
[2019-04-23 12:37:57,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1121697: learning rate 0.0005
[2019-04-23 12:37:58,861] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1121869: loss 1.6085
[2019-04-23 12:37:58,882] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1121869: learning rate 0.0005
[2019-04-23 12:37:59,476] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1121986: loss 1.6744
[2019-04-23 12:37:59,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1121986: learning rate 0.0005
[2019-04-23 12:38:06,355] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1123885: loss -108.1013
[2019-04-23 12:38:06,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1123885: learning rate 0.0005
[2019-04-23 12:38:11,536] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-23 12:38:11,539] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:38:11,539] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:38:11,540] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run46
[2019-04-23 12:38:11,697] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:38:11,698] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:38:11,699] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run46
[2019-04-23 12:38:11,852] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:38:11,852] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:38:11,854] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run46
[2019-04-23 12:38:11,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:38:11,999] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:38:12,001] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run46
[2019-04-23 12:38:12,005] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:38:12,167] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:38:12,168] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run46
[2019-04-23 12:39:26,084] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:39:26,087] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.2, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.568915891364002, 6.9112, 168.9092309801655, 2751271.982430073, 2284675.983692417, 474654.6999214517]
[2019-04-23 12:39:26,100] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:39:26,103] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6077255727543378
[2019-04-23 12:39:26,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2751271.982430073 W.
[2019-04-23 12:39:33,508] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:39:33,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.10765590833333, 93.05499270000001, 1.0, 2.0, 0.8535099658104258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1192925.000184227, 1192925.000184228, 257468.6012459154]
[2019-04-23 12:39:33,516] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:39:33,521] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2917050932385309
[2019-04-23 12:39:47,393] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:39:47,394] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.46666666666667, 56.00000000000001, 1.0, 2.0, 0.8835881030216592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1234988.749792884, 1234988.749792884, 265466.9229140639]
[2019-04-23 12:39:47,395] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:39:47,398] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.36718123458466034
[2019-04-23 12:39:51,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:39:51,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.83333333333334, 65.0, 1.0, 2.0, 0.5680852977509034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793846.4920598922, 793846.4920598922, 194950.6219578835]
[2019-04-23 12:39:51,330] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:39:51,331] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8113551291895419
[2019-04-23 12:40:32,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:40:32,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.7, 68.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.115754154526629, 6.9112, 168.9116744107524, 1598970.967720096, 1453854.311775268, 311356.2001699038]
[2019-04-23 12:40:32,228] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:40:32,242] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3825053914871729
[2019-04-23 12:40:55,116] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:40:55,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.21571096333334, 67.55631962333334, 1.0, 2.0, 0.5525274389959474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772097.9264263497, 772097.9264263503, 192235.8047807237]
[2019-04-23 12:40:55,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:40:55,136] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37856376903629296
[2019-04-23 12:41:06,816] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:41:06,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.0, 80.0, 1.0, 2.0, 0.5654421426939863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 790151.5543007232, 790151.5543007238, 194483.9740902573]
[2019-04-23 12:41:06,849] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:41:06,853] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7672808089879845
[2019-04-23 12:41:17,823] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.38966924]
[2019-04-23 12:41:17,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.33333333333334, 67.0, 1.0, 2.0, 0.4630501965038196, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564983627, 647024.9824617755, 647024.9824617755, 178010.7377809148]
[2019-04-23 12:41:17,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:41:17,835] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6868559330742663
[2019-04-23 12:41:50,865] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:41:51,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:41:51,166] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:41:51,222] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:41:51,436] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:41:52,451] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1125000, evaluation results [1125000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:41:54,815] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1126538: loss 69.4566
[2019-04-23 12:41:54,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1126539: learning rate 0.0005
[2019-04-23 12:41:55,367] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1126892: loss -28.0136
[2019-04-23 12:41:55,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1126892: learning rate 0.0005
[2019-04-23 12:41:55,386] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1126904: loss -93.3868
[2019-04-23 12:41:55,387] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1126904: learning rate 0.0005
[2019-04-23 12:41:55,910] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1127252: loss 0.7607
[2019-04-23 12:41:55,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1127252: learning rate 0.0005
[2019-04-23 12:41:55,933] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1127265: loss 8.5381
[2019-04-23 12:41:55,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1127265: learning rate 0.0005
[2019-04-23 12:41:56,244] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1127469: loss -186.7655
[2019-04-23 12:41:56,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1127471: learning rate 0.0005
[2019-04-23 12:41:57,041] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1127988: loss -22.0410
[2019-04-23 12:41:57,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1127988: learning rate 0.0005
[2019-04-23 12:41:57,088] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1128015: loss -69.7595
[2019-04-23 12:41:57,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1128017: learning rate 0.0005
[2019-04-23 12:41:58,385] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1128852: loss 47.0297
[2019-04-23 12:41:58,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1128852: learning rate 0.0005
[2019-04-23 12:41:58,520] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1128934: loss -109.9683
[2019-04-23 12:41:58,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1128935: learning rate 0.0005
[2019-04-23 12:41:58,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:41:58,892] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4815
[2019-04-23 12:41:58,896] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.78333333333333, 92.83333333333333, 1.0, 2.0, 0.4811914095476058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672381.9153298624, 672381.9153298624, 180701.0240746483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2595000.0000, 
sim time next is 2595600.0000, 
raw observation next is [24.7, 93.0, 1.0, 2.0, 0.4776870419611109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668719.214149378, 668719.214149378, 180333.1098262739], 
processed observation next is [0.0, 0.043478260869565216, 0.3696682464454976, 0.93, 1.0, 1.0, 0.3707072794712179, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18575533726371612, 0.18575533726371612, 0.26915389526309536], 
reward next is 0.7308, 
noisyNet noise sample is [array([0.3049862], dtype=float32), -0.16924328]. 
=============================================
[2019-04-23 12:41:58,905] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1129156: loss -71.7838
[2019-04-23 12:41:58,906] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1129156: learning rate 0.0005
[2019-04-23 12:41:59,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:41:59,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3930
[2019-04-23 12:41:59,105] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 78.0, 1.0, 2.0, 0.510337436713933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713122.1263212574, 713122.1263212574, 185233.8617594694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2569200.0000, 
sim time next is 2569800.0000, 
raw observation next is [28.76666666666667, 78.5, 1.0, 2.0, 0.5164667385774339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721689.841065754, 721689.8410657546, 186218.1996150363], 
processed observation next is [1.0, 0.7391304347826086, 0.5624012638230649, 0.785, 1.0, 1.0, 0.41742980551498055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20046940029604277, 0.20046940029604296, 0.2779376113657258], 
reward next is 0.7221, 
noisyNet noise sample is [array([-1.1612908], dtype=float32), -1.7538794]. 
=============================================
[2019-04-23 12:41:59,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:41:59,251] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5265
[2019-04-23 12:41:59,258] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3042721.615819497 W.
[2019-04-23 12:41:59,262] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.26666666666667, 69.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.961702091750879, 6.9112, 168.9070677983117, 3042721.615819497, 2297485.258078235, 474276.6124491457], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2554800.0000, 
sim time next is 2555400.0000, 
raw observation next is [30.38333333333333, 68.5, 1.0, 2.0, 0.9626626279133689, 1.0, 1.0, 0.9626626279133689, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2692750.521537017, 2692750.521537017, 506824.1608115105], 
processed observation next is [1.0, 0.5652173913043478, 0.6390205371248023, 0.685, 1.0, 1.0, 0.9550152143534565, 1.0, 0.5, 0.9550152143534565, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7479862559825048, 0.7479862559825048, 0.7564539713604634], 
reward next is 0.2435, 
noisyNet noise sample is [array([0.8240641], dtype=float32), -1.3638009]. 
=============================================
[2019-04-23 12:41:59,294] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1129387: loss -25.2209
[2019-04-23 12:41:59,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1129388: learning rate 0.0005
[2019-04-23 12:42:00,144] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1129886: loss 22.4342
[2019-04-23 12:42:00,147] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1129887: learning rate 0.0005
[2019-04-23 12:42:00,294] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1129976: loss -14.0074
[2019-04-23 12:42:00,296] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1129976: learning rate 0.0005
[2019-04-23 12:42:00,414] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1130042: loss -77.8998
[2019-04-23 12:42:00,416] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1130046: learning rate 0.0005
[2019-04-23 12:42:02,923] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1131554: loss 0.1700
[2019-04-23 12:42:02,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1131556: learning rate 0.0005
[2019-04-23 12:42:07,408] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1134338: loss 0.1654
[2019-04-23 12:42:07,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1134339: learning rate 0.0005
[2019-04-23 12:42:07,732] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:07,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6208
[2019-04-23 12:42:07,745] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3843101493343574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 578844.3630699612, 578844.3630699606, 172771.7823251573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2724600.0000, 
sim time next is 2725200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3841903961835303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578664.1240266134, 578664.1240266134, 172755.6668044197], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25806071829341, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16074003445183707, 0.16074003445183707, 0.2578442788125667], 
reward next is 0.7422, 
noisyNet noise sample is [array([-1.8526467], dtype=float32), -1.2332364]. 
=============================================
[2019-04-23 12:42:08,097] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1134765: loss 0.2645
[2019-04-23 12:42:08,099] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1134765: learning rate 0.0005
[2019-04-23 12:42:08,226] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1134845: loss 0.1772
[2019-04-23 12:42:08,229] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1134845: learning rate 0.0005
[2019-04-23 12:42:08,646] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:08,662] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3033
[2019-04-23 12:42:08,666] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3941833561731181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588179.7242589301, 588179.7242589295, 173450.8428641741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2745000.0000, 
sim time next is 2745600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3938673371689151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587708.875068235, 587708.8750682343, 173407.8066364152], 
processed observation next is [0.0, 0.782608695652174, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2697196833360423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16325246529673193, 0.16325246529673174, 0.2588176218453958], 
reward next is 0.7412, 
noisyNet noise sample is [array([0.02384603], dtype=float32), -0.19915675]. 
=============================================
[2019-04-23 12:42:08,913] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1135250: loss 0.1446
[2019-04-23 12:42:08,916] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1135251: learning rate 0.0005
[2019-04-23 12:42:08,969] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1135281: loss 0.1122
[2019-04-23 12:42:08,971] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1135283: learning rate 0.0005
[2019-04-23 12:42:09,196] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1135422: loss 0.2075
[2019-04-23 12:42:09,200] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1135425: learning rate 0.0005
[2019-04-23 12:42:10,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:10,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8477
[2019-04-23 12:42:10,026] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3496208299396052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538583.736769365, 538583.7367693643, 169700.2904138055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3482396457427301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536454.1334988913, 536454.1334988913, 169525.7583088381], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21474656113581939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14901503708302535, 0.14901503708302535, 0.25302351986393745], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.09072892], dtype=float32), -1.1082362]. 
=============================================
[2019-04-23 12:42:10,104] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1135995: loss 0.2776
[2019-04-23 12:42:10,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1135995: learning rate 0.0005
[2019-04-23 12:42:10,227] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1136074: loss 0.2270
[2019-04-23 12:42:10,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1136074: learning rate 0.0005
[2019-04-23 12:42:11,400] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1136800: loss 0.3366
[2019-04-23 12:42:11,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1136801: learning rate 0.0005
[2019-04-23 12:42:11,671] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1136970: loss 0.4120
[2019-04-23 12:42:11,674] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1136971: learning rate 0.0005
[2019-04-23 12:42:11,882] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1137088: loss 0.1762
[2019-04-23 12:42:11,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1137088: learning rate 0.0005
[2019-04-23 12:42:12,468] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1137448: loss 0.3461
[2019-04-23 12:42:12,469] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1137448: learning rate 0.0005
[2019-04-23 12:42:13,280] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1137939: loss 0.1945
[2019-04-23 12:42:13,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1137939: learning rate 0.0005
[2019-04-23 12:42:13,388] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1137999: loss 0.2055
[2019-04-23 12:42:13,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1137999: learning rate 0.0005
[2019-04-23 12:42:13,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:13,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3561
[2019-04-23 12:42:13,439] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.00000000000001, 1.0, 2.0, 0.532402841094277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836546.0507181135, 836546.0507181135, 199571.6985395206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2977800.0000, 
sim time next is 2978400.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.5230612596804076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821864.6246631006, 821864.6246631006, 197837.1363894793], 
processed observation next is [1.0, 0.4782608695652174, 0.2417061611374408, 0.88, 1.0, 1.0, 0.4253750116631417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2282957290730835, 0.2282957290730835, 0.29527930804399893], 
reward next is 0.7047, 
noisyNet noise sample is [array([-1.7014418], dtype=float32), 2.0640652]. 
=============================================
[2019-04-23 12:42:13,469] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1138046: loss 0.3018
[2019-04-23 12:42:13,470] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1138047: learning rate 0.0005
[2019-04-23 12:42:15,974] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1139544: loss 3.5857
[2019-04-23 12:42:15,979] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1139546: learning rate 0.0005
[2019-04-23 12:42:18,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:18,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7374
[2019-04-23 12:42:18,408] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4579285342637497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648162.2974202301, 648162.2974202301, 178337.4848020014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3211200.0000, 
sim time next is 3211800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4577248384834284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648075.29979201, 648075.2997920093, 178333.5326944034], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.89, 1.0, 1.0, 0.346656431907745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18002091660889166, 0.18002091660889147, 0.26616945178269164], 
reward next is 0.7338, 
noisyNet noise sample is [array([0.42781925], dtype=float32), 2.695995]. 
=============================================
[2019-04-23 12:42:18,625] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:18,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1324
[2019-04-23 12:42:18,639] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 99.00000000000001, 1.0, 2.0, 0.4264394184050594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621010.880074416, 621010.8800744154, 176098.6453409253], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3093000.0000, 
sim time next is 3093600.0000, 
raw observation next is [23.0, 98.0, 1.0, 2.0, 0.4229290631167505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618778.816211871, 618778.816211871, 175963.6500937533], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.98, 1.0, 1.0, 0.3047338109840367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1718830045032975, 0.1718830045032975, 0.2626323135727661], 
reward next is 0.7374, 
noisyNet noise sample is [array([0.8793301], dtype=float32), -0.3669733]. 
=============================================
[2019-04-23 12:42:20,386] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1142239: loss 3.7499
[2019-04-23 12:42:20,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1142240: learning rate 0.0005
[2019-04-23 12:42:20,485] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:20,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0207
[2019-04-23 12:42:20,498] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 97.0, 1.0, 2.0, 0.4176422193116753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613899.0158457832, 613899.0158457839, 175577.3353345396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3094200.0000, 
sim time next is 3094800.0000, 
raw observation next is [23.0, 96.0, 1.0, 2.0, 0.4113425000650328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607420.662355208, 607420.662355208, 175045.2776978148], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.96, 1.0, 1.0, 0.29077409646389496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16872796176533555, 0.16872796176533555, 0.2612616085042012], 
reward next is 0.7387, 
noisyNet noise sample is [array([-0.41595253], dtype=float32), 0.00057367265]. 
=============================================
[2019-04-23 12:42:21,360] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1142833: loss 3.8301
[2019-04-23 12:42:21,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1142833: learning rate 0.0005
[2019-04-23 12:42:21,362] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1142834: loss 3.8447
[2019-04-23 12:42:21,366] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1142834: learning rate 0.0005
[2019-04-23 12:42:22,051] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1143246: loss 3.8772
[2019-04-23 12:42:22,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1143247: learning rate 0.0005
[2019-04-23 12:42:22,129] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1143298: loss 3.8336
[2019-04-23 12:42:22,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1143298: learning rate 0.0005
[2019-04-23 12:42:22,369] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1143442: loss 3.8843
[2019-04-23 12:42:22,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1143443: learning rate 0.0005
[2019-04-23 12:42:23,183] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1143941: loss 4.2179
[2019-04-23 12:42:23,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1143941: learning rate 0.0005
[2019-04-23 12:42:23,446] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1144099: loss 4.0134
[2019-04-23 12:42:23,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1144100: learning rate 0.0005
[2019-04-23 12:42:23,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:23,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3315
[2019-04-23 12:42:23,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1659931.816500618 W.
[2019-04-23 12:42:23,759] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.201625928208909, 6.9112, 168.9110128617343, 1659931.816500618, 1453896.038181299, 311348.5782952139], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3168000.0000, 
sim time next is 3168600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5555651668740379, 1.0, 1.0, 0.5555651668740379, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1553245.634710679, 1553245.634710679, 318414.2135265229], 
processed observation next is [1.0, 0.6956521739130435, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4645363456313709, 1.0, 0.5, 0.4645363456313709, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4314571207529664, 0.4314571207529664, 0.47524509481570587], 
reward next is 0.5248, 
noisyNet noise sample is [array([-0.5232933], dtype=float32), 0.6355746]. 
=============================================
[2019-04-23 12:42:24,655] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1144836: loss 4.2540
[2019-04-23 12:42:24,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1144837: learning rate 0.0005
[2019-04-23 12:42:24,822] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1144936: loss 4.3033
[2019-04-23 12:42:24,824] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1144936: learning rate 0.0005
[2019-04-23 12:42:25,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:25,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9868
[2019-04-23 12:42:25,150] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.3742363454517803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571630.6657486432, 571630.6657486426, 172365.7004848244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3123600.0000, 
sim time next is 3124200.0000, 
raw observation next is [22.0, 95.0, 1.0, 2.0, 0.3673401473100638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563160.8548989792, 563160.8548989792, 171688.6325612299], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.95, 1.0, 1.0, 0.2377592136265829, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.156433570805272, 0.156433570805272, 0.25625169038989537], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.4367309], dtype=float32), -2.027337]. 
=============================================
[2019-04-23 12:42:25,223] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1145184: loss 4.0730
[2019-04-23 12:42:25,225] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1145185: learning rate 0.0005
[2019-04-23 12:42:25,779] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1145516: loss 4.2683
[2019-04-23 12:42:25,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1145516: learning rate 0.0005
[2019-04-23 12:42:26,323] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1145846: loss 4.2836
[2019-04-23 12:42:26,324] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1145847: learning rate 0.0005
[2019-04-23 12:42:26,507] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1145958: loss 4.0203
[2019-04-23 12:42:26,510] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1145959: learning rate 0.0005
[2019-04-23 12:42:26,554] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1145988: loss 4.1919
[2019-04-23 12:42:26,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1145988: learning rate 0.0005
[2019-04-23 12:42:29,326] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1147648: loss 6.3737
[2019-04-23 12:42:29,328] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1147649: learning rate 0.0005
[2019-04-23 12:42:29,720] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:42:29,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7896
[2019-04-23 12:42:29,732] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4210350578213153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616474.9472071078, 616474.9472071084, 175755.7255499691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3303600.0000, 
sim time next is 3304200.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4209428935094727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616340.1499663205, 616340.1499663205, 175742.8214674003], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 1.0, 1.0, 0.30234083555358165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1712055972128668, 0.1712055972128668, 0.26230271860806015], 
reward next is 0.7377, 
noisyNet noise sample is [array([1.0344623], dtype=float32), 1.4318874]. 
=============================================
[2019-04-23 12:42:33,151] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-23 12:42:33,153] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:42:33,154] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:42:33,154] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:42:33,155] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:42:33,155] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:42:33,156] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:42:33,157] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:42:33,159] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:42:33,159] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:42:33,154] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:42:33,170] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run47
[2019-04-23 12:42:33,170] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run47
[2019-04-23 12:42:33,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run47
[2019-04-23 12:42:33,207] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run47
[2019-04-23 12:42:33,258] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run47
[2019-04-23 12:42:49,346] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:42:49,354] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.63333333333334, 63.0, 1.0, 2.0, 0.2852090371693841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 458069.2428035328, 458069.2428035334, 164137.1022577812]
[2019-04-23 12:42:49,354] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:42:49,356] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9963141823645854
[2019-04-23 12:43:12,440] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:43:12,449] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.53076131, 98.11043020666668, 1.0, 2.0, 0.4418492077848136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 637481.1038180131, 637481.1038180138, 177563.8435996741]
[2019-04-23 12:43:12,454] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:43:12,457] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2543818803194774
[2019-04-23 12:43:24,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:43:24,074] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.88333333333333, 93.16666666666667, 1.0, 2.0, 0.5519965502658011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771355.7969124786, 771355.7969124786, 192143.8699131767]
[2019-04-23 12:43:24,075] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:43:24,076] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6825327663361757
[2019-04-23 12:43:58,561] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:43:58,565] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.75, 71.5, 1.0, 2.0, 0.5751627885714258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803740.3706973876, 803740.3706973876, 196209.5948750798]
[2019-04-23 12:43:58,568] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:43:58,573] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.49197177455715957
[2019-04-23 12:44:17,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:44:17,692] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.639401755, 69.12760816, 1.0, 2.0, 0.9808740992367386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1371052.930099728, 1371052.930099728, 293158.8065416522]
[2019-04-23 12:44:17,699] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:44:17,708] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9588087424885773
[2019-04-23 12:44:45,547] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:44:45,548] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.26666666666667, 93.16666666666667, 1.0, 2.0, 0.6160829179144384, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.952285583936567, 6.9112, 168.9126131498817, 1722590.246687058, 1693442.778635897, 367740.9474528373]
[2019-04-23 12:44:45,549] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:44:45,550] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09372011316747919
[2019-04-23 12:44:45,557] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1722590.246687058 W.
[2019-04-23 12:45:11,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:45:11,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.81569834666666, 55.7386873, 1.0, 2.0, 0.5395738356448262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753990.2284657883, 753990.2284657877, 190028.2870381636]
[2019-04-23 12:45:11,798] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:45:11,804] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9240060843657347
[2019-04-23 12:45:53,074] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.39686942]
[2019-04-23 12:45:53,075] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.7, 82.0, 1.0, 2.0, 0.7750000565356212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083138.113945953, 1083138.113945952, 237866.0188770856]
[2019-04-23 12:45:53,078] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:45:53,081] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07959726653302046
[2019-04-23 12:45:55,526] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:45:56,133] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:45:56,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:45:56,314] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:45:56,467] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:45:57,482] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1150000, evaluation results [1150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:45:58,222] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1150283: loss 7.2764
[2019-04-23 12:45:58,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1150286: learning rate 0.0005
[2019-04-23 12:45:58,594] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:45:58,612] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2660
[2019-04-23 12:45:58,627] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8604299167453348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1202602.283125204, 1202602.283125204, 259287.6254139024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3395400.0000, 
sim time next is 3396000.0000, 
raw observation next is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.8712014140019531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217665.979690903, 1217665.979690903, 262141.6740370762], 
processed observation next is [1.0, 0.30434782608695654, 0.5418641390205374, 0.8733333333333334, 1.0, 1.0, 0.8448209807252446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33824054991413977, 0.33824054991413977, 0.3912562299060839], 
reward next is 0.6087, 
noisyNet noise sample is [array([1.6217867], dtype=float32), 0.9316177]. 
=============================================
[2019-04-23 12:45:58,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[65.40377 ]
 [65.42318 ]
 [65.57122 ]
 [65.753174]
 [66.15534 ]], R is [[65.12508392]
 [65.08683777]
 [65.0490799 ]
 [65.01941681]
 [65.00006104]].
[2019-04-23 12:46:00,234] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1150925: loss 7.7132
[2019-04-23 12:46:00,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1150925: learning rate 0.0005
[2019-04-23 12:46:00,390] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1150937: loss 7.8676
[2019-04-23 12:46:00,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1150937: learning rate 0.0005
[2019-04-23 12:46:02,122] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1151314: loss 6.6144
[2019-04-23 12:46:02,174] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1151321: loss 6.4443
[2019-04-23 12:46:02,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1151314: learning rate 0.0005
[2019-04-23 12:46:02,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1151322: learning rate 0.0005
[2019-04-23 12:46:02,590] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1151414: loss 6.0767
[2019-04-23 12:46:02,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1151417: learning rate 0.0005
[2019-04-23 12:46:04,338] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1151887: loss 4.4228
[2019-04-23 12:46:04,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1151887: learning rate 0.0005
[2019-04-23 12:46:04,899] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1152044: loss 4.7311
[2019-04-23 12:46:04,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1152044: learning rate 0.0005
[2019-04-23 12:46:04,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:04,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0945
[2019-04-23 12:46:04,989] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7356349309633216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1028094.881958236, 1028094.881958236, 228724.3100700208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3387000.0000, 
sim time next is 3387600.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7310351953082055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1021663.377300856, 1021663.377300855, 227686.0590640607], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.94, 1.0, 1.0, 0.675946018443621, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2837953825835711, 0.28379538258357084, 0.33982993890158314], 
reward next is 0.6602, 
noisyNet noise sample is [array([0.11495274], dtype=float32), -1.4710448]. 
=============================================
[2019-04-23 12:46:08,031] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1152847: loss 4.0071
[2019-04-23 12:46:08,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1152848: learning rate 0.0005
[2019-04-23 12:46:08,280] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1152907: loss 4.2381
[2019-04-23 12:46:08,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1152907: learning rate 0.0005
[2019-04-23 12:46:10,284] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1153303: loss 4.8728
[2019-04-23 12:46:10,298] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1153303: learning rate 0.0005
[2019-04-23 12:46:10,351] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1153329: loss 5.4291
[2019-04-23 12:46:10,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1153329: learning rate 0.0005
[2019-04-23 12:46:11,294] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1153715: loss 4.6083
[2019-04-23 12:46:11,296] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1153715: learning rate 0.0005
[2019-04-23 12:46:11,552] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1153776: loss 3.7853
[2019-04-23 12:46:11,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1153779: learning rate 0.0005
[2019-04-23 12:46:11,647] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1153819: loss 5.6997
[2019-04-23 12:46:11,659] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1153821: learning rate 0.0005
[2019-04-23 12:46:12,166] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:12,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5925
[2019-04-23 12:46:12,241] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6299421513822115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 880321.5269919345, 880321.5269919339, 206446.722956264], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3477600.0000, 
sim time next is 3478200.0000, 
raw observation next is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.6556928344327676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916322.7052083366, 916322.705208336, 211569.2633407649], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.7900000000000001, 1.0, 1.0, 0.5851720896780332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2545340847800935, 0.2545340847800933, 0.3157750199115894], 
reward next is 0.6842, 
noisyNet noise sample is [array([0.59560025], dtype=float32), -0.41287896]. 
=============================================
[2019-04-23 12:46:18,197] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1155766: loss -66.9279
[2019-04-23 12:46:18,207] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1155767: learning rate 0.0005
[2019-04-23 12:46:19,385] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:19,395] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8149
[2019-04-23 12:46:19,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2167768.934403481 W.
[2019-04-23 12:46:19,408] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.5167638925101332, 1.0, 2.0, 0.5167638925101332, 1.0, 1.0, 0.8924131921828045, 6.9112, 6.9112, 170.5573041426782, 2167768.934403481, 2167768.934403481, 425988.6580929717], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3576000.0000, 
sim time next is 3576600.0000, 
raw observation next is [30.5, 68.0, 1.0, 2.0, 0.7752679265957402, 1.0, 2.0, 0.7752679265957402, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2168110.710158044, 2168110.710158044, 408007.9318151239], 
processed observation next is [1.0, 0.391304347826087, 0.6445497630331753, 0.68, 1.0, 1.0, 0.7292384657780002, 1.0, 1.0, 0.7292384657780002, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6022529750439011, 0.6022529750439011, 0.6089670624106327], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37338793], dtype=float32), 0.18894501]. 
=============================================
[2019-04-23 12:46:20,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:20,395] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2971
[2019-04-23 12:46:20,400] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7736879900000767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1081303.439445454, 1081303.439445454, 237551.2278678445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3641400.0000, 
sim time next is 3642000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7243636465149337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1012335.063248648, 1012335.063248648, 226188.5617082594], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6679080078493177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28120418423573557, 0.28120418423573557, 0.3375948682212827], 
reward next is 0.6624, 
noisyNet noise sample is [array([-2.123899], dtype=float32), -1.008929]. 
=============================================
[2019-04-23 12:46:20,426] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[61.15364 ]
 [61.307323]
 [61.30811 ]
 [60.984413]
 [60.52231 ]], R is [[61.40023804]
 [61.43168259]
 [61.46767426]
 [61.4985199 ]
 [61.51705551]].
[2019-04-23 12:46:28,169] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1158434: loss -88.7591
[2019-04-23 12:46:28,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1158434: learning rate 0.0005
[2019-04-23 12:46:30,002] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1158821: loss -38.0495
[2019-04-23 12:46:30,005] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1158821: learning rate 0.0005
[2019-04-23 12:46:30,259] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1158925: loss -40.7309
[2019-04-23 12:46:30,261] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1158926: learning rate 0.0005
[2019-04-23 12:46:31,492] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1159329: loss -147.0911
[2019-04-23 12:46:31,493] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1159329: learning rate 0.0005
[2019-04-23 12:46:31,782] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1159402: loss 39.4390
[2019-04-23 12:46:31,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1159402: learning rate 0.0005
[2019-04-23 12:46:32,241] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1159564: loss -95.1944
[2019-04-23 12:46:32,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1159566: learning rate 0.0005
[2019-04-23 12:46:33,150] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1159836: loss -55.1271
[2019-04-23 12:46:33,150] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1159836: learning rate 0.0005
[2019-04-23 12:46:33,606] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1159949: loss -142.2292
[2019-04-23 12:46:33,653] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1159956: learning rate 0.0005
[2019-04-23 12:46:35,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:35,773] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6334
[2019-04-23 12:46:35,777] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 74.83333333333334, 1.0, 2.0, 0.6414056247883516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 917541.7052253456, 917541.7052253456, 211475.3498743518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3730200.0000, 
sim time next is 3730800.0000, 
raw observation next is [26.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6117656248956994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876283.5734749376, 876283.5734749376, 205718.1153941139], 
processed observation next is [1.0, 0.17391304347826086, 0.4628751974723541, 0.7566666666666667, 1.0, 1.0, 0.532247740838192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2434121037430382, 0.2434121037430382, 0.3070419632747969], 
reward next is 0.6930, 
noisyNet noise sample is [array([-0.42272732], dtype=float32), 2.3601658]. 
=============================================
[2019-04-23 12:46:36,478] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1160719: loss -139.0573
[2019-04-23 12:46:36,480] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1160719: learning rate 0.0005
[2019-04-23 12:46:37,278] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1160941: loss -150.5990
[2019-04-23 12:46:37,280] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1160941: learning rate 0.0005
[2019-04-23 12:46:37,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1161100: loss -53.0255
[2019-04-23 12:46:37,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1161102: learning rate 0.0005
[2019-04-23 12:46:38,925] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1161528: loss -119.4510
[2019-04-23 12:46:38,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1161528: learning rate 0.0005
[2019-04-23 12:46:39,862] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1161828: loss -13.3704
[2019-04-23 12:46:39,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1161828: learning rate 0.0005
[2019-04-23 12:46:39,877] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1161832: loss -42.7943
[2019-04-23 12:46:39,879] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1161832: learning rate 0.0005
[2019-04-23 12:46:40,098] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1161897: loss -103.3828
[2019-04-23 12:46:40,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1161897: learning rate 0.0005
[2019-04-23 12:46:43,204] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:43,204] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1839
[2019-04-23 12:46:43,245] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.5566760394165543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777897.274046499, 777897.274046499, 192952.8135621715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3787800.0000, 
sim time next is 3788400.0000, 
raw observation next is [29.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5575228775608567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779081.0768769619, 779081.0768769624, 193099.6265252201], 
processed observation next is [1.0, 0.8695652173913043, 0.6050552922590839, 0.7633333333333334, 1.0, 1.0, 0.46689503320585146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21641141024360053, 0.21641141024360067, 0.288208397798836], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.7604132], dtype=float32), -1.4591695]. 
=============================================
[2019-04-23 12:46:47,130] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1163781: loss 0.0210
[2019-04-23 12:46:47,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1163788: learning rate 0.0005
[2019-04-23 12:46:52,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:52,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8577
[2019-04-23 12:46:52,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3037486.705958961 W.
[2019-04-23 12:46:52,341] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.955910470166202, 6.9112, 168.9071247872629, 3037486.705958961, 2296358.730605084, 474239.4368346689], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4012200.0000, 
sim time next is 4012800.0000, 
raw observation next is [30.33333333333333, 68.66666666666667, 1.0, 2.0, 0.6621690939052338, 1.0, 1.0, 0.6516745864668795, 1.0, 2.0, 1.03, 7.000260090262944, 6.9112, 170.5573041426782, 2734332.158772592, 2670534.793384397, 509847.3042594253], 
processed observation next is [1.0, 0.43478260869565216, 0.6366508688783569, 0.6866666666666668, 1.0, 1.0, 0.5929748119340166, 1.0, 0.5, 0.5803308270685295, 1.0, 1.0, 1.0365853658536586, 0.00890600902629437, 0.0, 0.8375144448122397, 0.7595367107701644, 0.7418152203845547, 0.7609661257603363], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1908782], dtype=float32), 0.85540295]. 
=============================================
[2019-04-23 12:46:53,360] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1166541: loss 0.2235
[2019-04-23 12:46:53,366] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1166542: learning rate 0.0005
[2019-04-23 12:46:53,848] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1166841: loss 0.1793
[2019-04-23 12:46:53,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1166841: learning rate 0.0005
[2019-04-23 12:46:54,203] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1167066: loss 0.2232
[2019-04-23 12:46:54,205] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1167067: learning rate 0.0005
[2019-04-23 12:46:54,835] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1167468: loss 0.3968
[2019-04-23 12:46:54,836] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1167469: learning rate 0.0005
[2019-04-23 12:46:54,914] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1167522: loss 0.5071
[2019-04-23 12:46:54,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1167523: learning rate 0.0005
[2019-04-23 12:46:54,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:46:54,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6040
[2019-04-23 12:46:54,936] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2495314.293780281 W.
[2019-04-23 12:46:54,940] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 71.33333333333334, 1.0, 2.0, 0.5947661824945994, 1.0, 2.0, 0.5947661824945994, 1.0, 1.0, 1.026003938633268, 6.911200000000001, 6.9112, 170.5573041426782, 2495314.293780281, 2495314.29378028, 485348.8216389989], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4011600.0000, 
sim time next is 4012200.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5967955810162933, 1.0, 2.0, 0.5967955810162933, 1.0, 2.0, 1.03, 6.913064723669118, 6.9112, 170.5573041426782, 2503837.07299934, 2502501.295583042, 487042.9817689853], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.7, 1.0, 1.0, 0.5142115433931244, 1.0, 1.0, 0.5142115433931244, 1.0, 1.0, 1.0365853658536586, 0.00018647236691178294, 0.0, 0.8375144448122397, 0.6955102980553722, 0.6951392487730672, 0.7269298235357989], 
reward next is 0.2637, 
noisyNet noise sample is [array([0.51062435], dtype=float32), 1.804973]. 
=============================================
[2019-04-23 12:46:55,251] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1167730: loss 0.2858
[2019-04-23 12:46:55,253] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1167732: learning rate 0.0005
[2019-04-23 12:46:55,512] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1167902: loss 0.4626
[2019-04-23 12:46:55,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1167904: learning rate 0.0005
[2019-04-23 12:46:55,520] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1167908: loss 0.2679
[2019-04-23 12:46:55,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1167909: learning rate 0.0005
[2019-04-23 12:46:56,740] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1168696: loss 0.1004
[2019-04-23 12:46:56,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1168697: learning rate 0.0005
[2019-04-23 12:46:56,977] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1168844: loss 0.0621
[2019-04-23 12:46:56,979] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1168845: learning rate 0.0005
[2019-04-23 12:46:57,260] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1169030: loss 0.1174
[2019-04-23 12:46:57,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1169030: learning rate 0.0005
[2019-04-23 12:46:57,914] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1169429: loss 0.0373
[2019-04-23 12:46:57,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1169429: learning rate 0.0005
[2019-04-23 12:46:58,111] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1169551: loss 0.0383
[2019-04-23 12:46:58,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1169555: learning rate 0.0005
[2019-04-23 12:46:58,261] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1169643: loss 0.0034
[2019-04-23 12:46:58,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1169643: learning rate 0.0005
[2019-04-23 12:46:58,482] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1169774: loss 0.0102
[2019-04-23 12:46:58,484] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1169776: learning rate 0.0005
[2019-04-23 12:47:02,249] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1171968: loss 46.3432
[2019-04-23 12:47:02,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1171968: learning rate 0.0005
[2019-04-23 12:47:06,122] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:47:06,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7236
[2019-04-23 12:47:06,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1731677.398874911 W.
[2019-04-23 12:47:06,133] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.33333333333334, 82.5, 1.0, 2.0, 0.412890099511882, 1.0, 2.0, 0.412890099511882, 1.0, 1.0, 0.7170535580097029, 6.911199999999999, 6.9112, 170.5573041426782, 1731677.398874911, 1731677.398874911, 360141.5313155904], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4263000.0000, 
sim time next is 4263600.0000, 
raw observation next is [31.66666666666667, 81.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.873804031227383, 6.9112, 168.9078754756278, 2967191.734608087, 2284307.84688474, 473906.6599421656], 
processed observation next is [1.0, 0.34782608695652173, 0.6998420221169038, 0.81, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.09626040312273831, 0.0, 0.8294149949453112, 0.8242199262800243, 0.6345299574679834, 0.7073233730480084], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8478003], dtype=float32), -0.97167945]. 
=============================================
[2019-04-23 12:47:06,589] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1174611: loss -78.1071
[2019-04-23 12:47:06,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1174611: learning rate 0.0005
[2019-04-23 12:47:06,900] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1174815: loss -32.1652
[2019-04-23 12:47:06,903] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1174819: learning rate 0.0005
[2019-04-23 12:47:07,173] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 12:47:07,176] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:47:07,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:47:07,177] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:47:07,181] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:47:07,182] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:47:07,183] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:47:07,184] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:47:07,184] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:47:07,185] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:47:07,184] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:47:07,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run48
[2019-04-23 12:47:07,222] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run48
[2019-04-23 12:47:07,248] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run48
[2019-04-23 12:47:07,265] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run48
[2019-04-23 12:47:07,266] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run48
[2019-04-23 12:47:15,918] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:47:15,920] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.6, 88.0, 1.0, 2.0, 0.2565693797570703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421484.6011611307, 421484.6011611313, 161564.6054326014]
[2019-04-23 12:47:15,920] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:47:15,922] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2726820070446887
[2019-04-23 12:47:22,066] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:47:22,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.1, 94.5, 1.0, 2.0, 0.324238512798485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510714.784000715, 510714.7840007156, 167796.0366305613]
[2019-04-23 12:47:22,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:47:22,069] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1422622226222483
[2019-04-23 12:47:36,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:47:36,431] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.45, 83.5, 1.0, 2.0, 0.5672289583428313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792649.3906920834, 792649.390692084, 194798.9276302756]
[2019-04-23 12:47:36,432] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:47:36,435] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23578557717746906
[2019-04-23 12:47:56,018] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:47:56,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.45, 57.5, 1.0, 2.0, 0.5681647088000756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793957.5031067089, 793957.5031067089, 194964.7614735246]
[2019-04-23 12:47:56,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:47:56,027] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.683461726695431
[2019-04-23 12:48:08,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:48:08,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5820091056976335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813311.1710184453, 813311.1710184453, 197442.0592235731]
[2019-04-23 12:48:08,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:48:08,524] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09506155031939745
[2019-04-23 12:48:19,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:48:19,008] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.1, 80.33333333333333, 1.0, 2.0, 0.8673523244381226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1212283.089996613, 1212283.089996613, 261117.0799125838]
[2019-04-23 12:48:19,010] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:48:19,017] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8329916422858974
[2019-04-23 12:48:29,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:48:29,250] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 75.0, 1.0, 2.0, 0.5820243123782255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 813332.4292742313, 813332.4292742319, 197444.0993763695]
[2019-04-23 12:48:29,252] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:48:29,293] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0810916983659008
[2019-04-23 12:49:06,818] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:49:06,822] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.98348292775195, 6.9112, 168.9121449850956, 1505069.797452295, 1453790.047088279, 311350.8999857722]
[2019-04-23 12:49:06,825] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:49:06,844] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7092442080030416
[2019-04-23 12:49:10,971] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:49:10,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.2376787, 78.18495268000001, 1.0, 2.0, 0.3273292281174366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515442.4578000918, 515442.4578000924, 168156.8064156722]
[2019-04-23 12:49:10,982] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:49:10,986] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0012828390894830122
[2019-04-23 12:49:34,396] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.35934618]
[2019-04-23 12:49:34,401] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.36666666666667, 73.5, 1.0, 2.0, 0.5072462378621744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708801.1857152755, 708801.1857152755, 184738.7566282205]
[2019-04-23 12:49:34,415] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:49:34,418] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.543109127788538
[2019-04-23 12:49:46,543] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:49:47,776] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:49:48,111] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:49:48,232] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:49:48,417] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:49:49,439] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1175000, evaluation results [1175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:49:49,976] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1175116: loss 27.6186
[2019-04-23 12:49:49,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1175116: learning rate 0.0005
[2019-04-23 12:49:52,657] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1175443: loss 19.4896
[2019-04-23 12:49:52,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1175443: learning rate 0.0005
[2019-04-23 12:49:54,254] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1175652: loss 20.9096
[2019-04-23 12:49:54,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1175652: learning rate 0.0005
[2019-04-23 12:49:54,794] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1175711: loss 34.1239
[2019-04-23 12:49:54,847] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1175711: learning rate 0.0005
[2019-04-23 12:49:56,109] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1175888: loss 48.9562
[2019-04-23 12:49:56,135] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1175888: learning rate 0.0005
[2019-04-23 12:49:56,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:49:56,242] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8255
[2019-04-23 12:49:56,242] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2646818.221882805 W.
[2019-04-23 12:49:56,280] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.9462591330703524, 1.0, 2.0, 0.9462591330703524, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2646818.221882805, 2646818.221882805, 497372.5042353528], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.9135752042644976, 1.0, 2.0, 0.9135752042644976, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2555303.245925576, 2555303.245925576, 478967.782688645], 
processed observation next is [1.0, 0.5652173913043478, 0.95260663507109, 0.54, 1.0, 1.0, 0.8958737400777079, 1.0, 1.0, 0.8958737400777079, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7098064572015489, 0.7098064572015489, 0.7148772875949926], 
reward next is 0.2851, 
noisyNet noise sample is [array([0.45743683], dtype=float32), -0.35481822]. 
=============================================
[2019-04-23 12:49:56,537] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1175935: loss -8.1490
[2019-04-23 12:49:56,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1175935: learning rate 0.0005
[2019-04-23 12:49:58,400] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:49:58,402] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5095
[2019-04-23 12:49:58,425] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1962865.699229416 W.
[2019-04-23 12:49:58,436] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.4679627164461382, 1.0, 1.0, 0.4679627164461382, 1.0, 2.0, 0.8126964808317781, 6.911199999999999, 6.9112, 170.5573041426782, 1962865.699229416, 1962865.699229417, 393700.1526323257], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4330200.0000, 
sim time next is 4330800.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.7685769442927937, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005979522415934, 6.9112, 168.9123931930542, 1971096.553113769, 1903856.922094495, 400233.9201209308], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.84, 1.0, 1.0, 0.7211770413166189, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009477952241593358, 0.0, 0.8294371790060948, 0.5475268203093803, 0.5288491450262486, 0.5973640598819863], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.31695074], dtype=float32), 1.8759881]. 
=============================================
[2019-04-23 12:50:01,920] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1176783: loss 43.1812
[2019-04-23 12:50:01,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1176784: learning rate 0.0005
[2019-04-23 12:50:02,251] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1176845: loss 50.9649
[2019-04-23 12:50:02,257] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1176845: learning rate 0.0005
[2019-04-23 12:50:03,281] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1177085: loss 1.3545
[2019-04-23 12:50:03,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1177085: learning rate 0.0005
[2019-04-23 12:50:04,516] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1177315: loss -50.6299
[2019-04-23 12:50:04,551] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1177323: learning rate 0.0005
[2019-04-23 12:50:05,407] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1177486: loss -78.9432
[2019-04-23 12:50:05,409] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1177486: learning rate 0.0005
[2019-04-23 12:50:06,026] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1177586: loss -88.3539
[2019-04-23 12:50:06,051] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1177586: learning rate 0.0005
[2019-04-23 12:50:07,244] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1177795: loss 1.0548
[2019-04-23 12:50:07,262] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1177795: learning rate 0.0005
[2019-04-23 12:50:13,315] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:13,316] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8277
[2019-04-23 12:50:13,327] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6172120494033209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862524.4432381731, 862524.4432381738, 203997.1329587804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4396800.0000, 
sim time next is 4397400.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.616364673260863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861339.7952720759, 861339.7952720753, 203835.1097823063], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5377887629648951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2392610542422433, 0.23926105424224314, 0.3042315071377706], 
reward next is 0.6958, 
noisyNet noise sample is [array([-1.1072204], dtype=float32), -0.09029244]. 
=============================================
[2019-04-23 12:50:15,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:15,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9971
[2019-04-23 12:50:15,452] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 84.0, 1.0, 2.0, 0.5266037821918546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735859.8443584514, 735859.844358452, 187868.5678555646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4488600.0000, 
sim time next is 4489200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.521256030414582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728384.499280889, 728384.4992808898, 186992.7269796804], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4232000366440747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20232902757802473, 0.20232902757802493, 0.27909362235773194], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.80354744], dtype=float32), 0.033621162]. 
=============================================
[2019-04-23 12:50:18,385] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1179722: loss 2.1582
[2019-04-23 12:50:18,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1179723: learning rate 0.0005
[2019-04-23 12:50:20,915] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:20,916] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4565
[2019-04-23 12:50:20,932] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 77.33333333333333, 1.0, 2.0, 0.5069479643517638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708384.2538676553, 708384.2538676547, 184691.9683344368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4527600.0000, 
sim time next is 4528200.0000, 
raw observation next is [28.0, 78.16666666666667, 1.0, 2.0, 0.5108555554023693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713846.3648783503, 713846.3648783503, 185314.5741124338], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7816666666666667, 1.0, 1.0, 0.4106693438582762, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19829065691065287, 0.19829065691065287, 0.2765889165857221], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.12511839], dtype=float32), 0.91089106]. 
=============================================
[2019-04-23 12:50:32,444] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:32,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5173
[2019-04-23 12:50:32,514] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 60.83333333333333, 1.0, 2.0, 0.5629686355993461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786693.7836470098, 786693.7836470098, 194049.0831625936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4540200.0000, 
sim time next is 4540800.0000, 
raw observation next is [32.66666666666667, 58.66666666666667, 1.0, 2.0, 0.5479223699667236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765660.516317923, 765660.516317923, 191445.165749903], 
processed observation next is [0.0, 0.5652173913043478, 0.7472353870458138, 0.5866666666666667, 1.0, 1.0, 0.45532815658641396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2126834767549786, 0.2126834767549786, 0.2857390533580642], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.31494376], dtype=float32), 0.97706735]. 
=============================================
[2019-04-23 12:50:32,957] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1182763: loss 1.4518
[2019-04-23 12:50:33,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1182767: learning rate 0.0005
[2019-04-23 12:50:33,181] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1182797: loss 0.3303
[2019-04-23 12:50:33,191] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1182802: learning rate 0.0005
[2019-04-23 12:50:33,464] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:33,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1863
[2019-04-23 12:50:33,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2821787.170136707 W.
[2019-04-23 12:50:33,572] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.33333333333334, 62.00000000000001, 1.0, 2.0, 1.008741395990606, 1.0, 2.0, 1.008741395990606, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2821787.170136707, 2821787.170136706, 534298.1810040498], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4627200.0000, 
sim time next is 4627800.0000, 
raw observation next is [34.5, 61.5, 1.0, 2.0, 0.7718644704671703, 1.0, 2.0, 0.7065222747478478, 1.0, 1.0, 1.03, 7.005103399286969, 6.9112, 170.5573041426782, 2964738.208831936, 2897471.384176616, 544988.823210829], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.615, 1.0, 1.0, 0.7251379162255064, 1.0, 1.0, 0.6464123792142744, 1.0, 0.5, 1.0365853658536586, 0.00939033992869689, 0.0, 0.8375144448122397, 0.8235383913422045, 0.8048531622712822, 0.8134161540460135], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8777639], dtype=float32), -1.3575284]. 
=============================================
[2019-04-23 12:50:35,174] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1183122: loss 0.0488
[2019-04-23 12:50:35,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1183122: learning rate 0.0005
[2019-04-23 12:50:37,209] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1183562: loss 0.0553
[2019-04-23 12:50:37,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1183562: learning rate 0.0005
[2019-04-23 12:50:37,668] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1183651: loss 0.1680
[2019-04-23 12:50:37,710] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1183651: learning rate 0.0005
[2019-04-23 12:50:37,892] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1183705: loss 0.2480
[2019-04-23 12:50:37,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1183705: learning rate 0.0005
[2019-04-23 12:50:38,295] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1183792: loss 0.1250
[2019-04-23 12:50:38,297] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1183792: learning rate 0.0005
[2019-04-23 12:50:38,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:38,692] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2124
[2019-04-23 12:50:38,752] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7433204213897285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1038841.079339711, 1038841.079339712, 230473.8937004936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6741954722451413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 942191.3845995613, 942191.3845995606, 215374.1392244629], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.6074644243917365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26171982905543373, 0.2617198290554335, 0.3214539391409894], 
reward next is 0.6785, 
noisyNet noise sample is [array([1.3244483], dtype=float32), -0.1202158]. 
=============================================
[2019-04-23 12:50:38,824] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1183888: loss 1.0169
[2019-04-23 12:50:38,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1183888: learning rate 0.0005
[2019-04-23 12:50:39,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:39,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8927
[2019-04-23 12:50:39,333] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6436963845000729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 899550.7135327054, 899550.7135327049, 209161.7714681792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4686000.0000, 
sim time next is 4686600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6461895460635244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903036.3300000359, 903036.3300000359, 209659.2655917867], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.5737223446548486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25084342500001, 0.25084342500001, 0.31292427700266673], 
reward next is 0.6871, 
noisyNet noise sample is [array([1.5941801], dtype=float32), -0.9303163]. 
=============================================
[2019-04-23 12:50:41,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:41,857] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2039
[2019-04-23 12:50:41,885] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4927319954189953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688513.1175253581, 688513.1175253581, 182464.9332078178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4826400.0000, 
sim time next is 4827000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.493551619166988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689658.780671726, 689658.7806717253, 182591.6383510742], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38982122791203366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19157188351992388, 0.19157188351992369, 0.2725248333598122], 
reward next is 0.7275, 
noisyNet noise sample is [array([-1.719075], dtype=float32), -1.0142487]. 
=============================================
[2019-04-23 12:50:41,908] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[59.733643]
 [59.857597]
 [60.004135]
 [59.58945 ]
 [59.14914 ]], R is [[59.90203857]
 [60.03068542]
 [60.15818405]
 [60.28448486]
 [60.40955353]].
[2019-04-23 12:50:43,767] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1184779: loss 0.4401
[2019-04-23 12:50:43,785] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1184779: learning rate 0.0005
[2019-04-23 12:50:44,214] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1184831: loss 0.1081
[2019-04-23 12:50:44,215] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1184833: learning rate 0.0005
[2019-04-23 12:50:45,586] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1185134: loss 0.0295
[2019-04-23 12:50:45,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1185135: learning rate 0.0005
[2019-04-23 12:50:46,289] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1185252: loss 0.4264
[2019-04-23 12:50:46,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1185252: learning rate 0.0005
[2019-04-23 12:50:46,810] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1185344: loss 0.0791
[2019-04-23 12:50:46,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1185344: learning rate 0.0005
[2019-04-23 12:50:48,731] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1185653: loss 0.3682
[2019-04-23 12:50:48,732] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1185653: learning rate 0.0005
[2019-04-23 12:50:48,991] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1185689: loss 0.1244
[2019-04-23 12:50:49,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1185689: learning rate 0.0005
[2019-04-23 12:50:54,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:54,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8195
[2019-04-23 12:50:54,344] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4879558388627915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681837.0708440433, 681837.070844044, 181730.1153975545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4755600.0000, 
sim time next is 4756200.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.4867457013041316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680145.5636836846, 680145.5636836846, 181545.1160867654], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.3816213268724477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18892932324546793, 0.18892932324546793, 0.2709628598309931], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.83184123], dtype=float32), 0.33426124]. 
=============================================
[2019-04-23 12:50:56,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:50:56,275] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9345
[2019-04-23 12:50:56,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2375078.534215856 W.
[2019-04-23 12:50:56,402] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8492022188328293, 1.0, 2.0, 0.8492022188328293, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2375078.534215856, 2375078.534215856, 444532.1774962368], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4715400.0000, 
sim time next is 4716000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.761799110915207, 6.9112, 168.9072415479739, 2897507.391878514, 2294083.524029558, 474650.9836698716], 
processed observation next is [1.0, 0.6086956521739131, 0.6682464454976303, 0.66, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.08505991109152067, 0.0, 0.8294118820703172, 0.8048631644106984, 0.6372454233415439, 0.7084343039848829], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.275119], dtype=float32), -0.045143493]. 
=============================================
[2019-04-23 12:50:56,537] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[22.685246]
 [22.723934]
 [23.215202]
 [23.518705]
 [24.109127]], R is [[21.66863632]
 [21.78846931]
 [21.89326286]
 [21.67432976]
 [21.45758629]].
[2019-04-23 12:51:01,863] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1187710: loss 236.2305
[2019-04-23 12:51:01,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1187717: learning rate 0.0005
[2019-04-23 12:51:03,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:03,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0680
[2019-04-23 12:51:03,424] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7863176796647563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1098963.780301931, 1098963.780301931, 240576.5413430417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4848000.0000, 
sim time next is 4848600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7634146406370645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1066938.231714663, 1066938.231714663, 235125.1919017656], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7149573983579091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2963717310318509, 0.2963717310318509, 0.3509331222414412], 
reward next is 0.6491, 
noisyNet noise sample is [array([0.5423821], dtype=float32), -1.0061778]. 
=============================================
[2019-04-23 12:51:08,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:08,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0300
[2019-04-23 12:51:08,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2141610.97448659 W.
[2019-04-23 12:51:08,594] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.7658016765805711, 1.0, 1.0, 0.7658016765805711, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2141610.97448659, 2141610.97448659, 403562.1369874596], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4873200.0000, 
sim time next is 4873800.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5078036112100386, 1.0, 2.0, 0.5078036112100386, 1.0, 1.0, 0.8752066877749529, 6.911200000000001, 6.9112, 170.5573041426782, 2130144.144733581, 2130144.14473358, 419354.7635540083], 
processed observation next is [1.0, 0.391304347826087, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.40699230266269704, 1.0, 1.0, 0.40699230266269704, 1.0, 0.5, 0.8478130338718938, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5917067068704392, 0.5917067068704388, 0.6259026321701617], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9083545], dtype=float32), -1.1312109]. 
=============================================
[2019-04-23 12:51:12,240] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:12,243] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1078
[2019-04-23 12:51:12,271] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2640495.157301054 W.
[2019-04-23 12:51:12,339] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333334, 63.5, 1.0, 2.0, 0.6293339813998072, 1.0, 1.0, 0.6293339813998072, 1.0, 2.0, 1.03, 6.98196319756561, 6.9112, 170.5573041426782, 2640495.157301054, 2589804.600421967, 498859.0076745449], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [31.66666666666667, 64.0, 1.0, 2.0, 0.8942972984891335, 1.0, 2.0, 0.8942972984891335, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2501328.266920052, 2501328.266920053, 468387.0321534466], 
processed observation next is [1.0, 0.6956521739130435, 0.6998420221169038, 0.64, 1.0, 1.0, 0.8726473475772694, 1.0, 1.0, 0.8726473475772694, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6948134074777922, 0.6948134074777925, 0.6990851226170844], 
reward next is 0.3009, 
noisyNet noise sample is [array([-1.1706244], dtype=float32), -0.022930153]. 
=============================================
[2019-04-23 12:51:16,033] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1190958: loss 266.8938
[2019-04-23 12:51:16,056] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1190958: learning rate 0.0005
[2019-04-23 12:51:16,107] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1190976: loss 328.8483
[2019-04-23 12:51:16,116] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1190976: learning rate 0.0005
[2019-04-23 12:51:17,766] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1191305: loss 240.6376
[2019-04-23 12:51:17,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1191311: learning rate 0.0005
[2019-04-23 12:51:18,259] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1191404: loss 261.5747
[2019-04-23 12:51:18,308] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1191404: learning rate 0.0005
[2019-04-23 12:51:18,939] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1191570: loss 233.8670
[2019-04-23 12:51:18,939] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1191570: learning rate 0.0005
[2019-04-23 12:51:19,156] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1191632: loss 221.0721
[2019-04-23 12:51:19,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1191632: learning rate 0.0005
[2019-04-23 12:51:19,308] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1191654: loss 230.6858
[2019-04-23 12:51:19,308] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1191654: learning rate 0.0005
[2019-04-23 12:51:19,815] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1191763: loss 236.6133
[2019-04-23 12:51:19,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1191765: learning rate 0.0005
[2019-04-23 12:51:22,329] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:22,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0539
[2019-04-23 12:51:22,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2232075.189735821 W.
[2019-04-23 12:51:22,438] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666667, 63.66666666666667, 1.0, 2.0, 0.5320782811347433, 1.0, 1.0, 0.5320782811347433, 1.0, 2.0, 0.9079735702443185, 6.9112, 6.9112, 170.5573041426782, 2232075.189735821, 2232075.189735821, 434896.0574845737], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4976400.0000, 
sim time next is 4977000.0000, 
raw observation next is [30.7, 63.5, 1.0, 2.0, 0.9773558363977483, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.980916551606541, 6.9112, 168.9124794356833, 2263307.810887493, 2213848.629411314, 457307.1881133717], 
processed observation next is [1.0, 0.6086956521739131, 0.6540284360189573, 0.635, 1.0, 1.0, 0.9727178751780099, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006971655160654055, 0.0, 0.8294376024968858, 0.6286966141354148, 0.6149579526142539, 0.6825480419602563], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0933408], dtype=float32), 0.538301]. 
=============================================
[2019-04-23 12:51:22,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[27.291798]
 [27.992222]
 [27.863472]
 [28.25726 ]
 [28.158792]], R is [[27.00305557]
 [27.08392525]
 [26.81646729]
 [26.5567112 ]
 [26.71219635]].
[2019-04-23 12:51:24,113] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1192760: loss 335.5338
[2019-04-23 12:51:24,115] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1192760: learning rate 0.0005
[2019-04-23 12:51:24,810] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1192939: loss 345.7542
[2019-04-23 12:51:24,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1192941: learning rate 0.0005
[2019-04-23 12:51:25,510] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1193112: loss 346.0798
[2019-04-23 12:51:25,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1193115: learning rate 0.0005
[2019-04-23 12:51:25,978] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1193211: loss 313.4559
[2019-04-23 12:51:25,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1193212: learning rate 0.0005
[2019-04-23 12:51:26,022] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1193218: loss 344.3762
[2019-04-23 12:51:26,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1193218: learning rate 0.0005
[2019-04-23 12:51:28,232] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1193762: loss 285.8349
[2019-04-23 12:51:28,245] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1193766: learning rate 0.0005
[2019-04-23 12:51:28,461] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1193830: loss 354.3688
[2019-04-23 12:51:28,469] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1193830: learning rate 0.0005
[2019-04-23 12:51:28,606] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:28,619] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1195
[2019-04-23 12:51:28,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2155202.217182725 W.
[2019-04-23 12:51:28,660] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.96666666666667, 63.0, 1.0, 2.0, 0.5137711846372645, 1.0, 1.0, 0.5137711846372645, 1.0, 2.0, 0.8814337997242571, 6.9112, 6.9112, 170.5573041426782, 2155202.217182725, 2155202.217182725, 422775.0759358465], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4981800.0000, 
sim time next is 4982400.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.9264216467576935, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98365130520964, 6.9112, 168.9125251101341, 2192020.031282102, 2140620.713058064, 442293.9648489524], 
processed observation next is [1.0, 0.6956521739130435, 0.6682464454976303, 0.63, 1.0, 1.0, 0.9113513816357752, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007245130520963983, 0.0, 0.8294378267793491, 0.6088944531339173, 0.5946168647383512, 0.6601402460432125], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3967984], dtype=float32), 0.63416904]. 
=============================================
[2019-04-23 12:51:36,233] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1195682: loss 2.0272
[2019-04-23 12:51:36,267] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1195682: learning rate 0.0005
[2019-04-23 12:51:47,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:47,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4805
[2019-04-23 12:51:47,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2170232.195874028 W.
[2019-04-23 12:51:47,509] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.7760257547544553, 1.0, 2.0, 0.7760257547544553, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2170232.195874028, 2170232.195874028, 408374.526915853], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5230800.0000, 
sim time next is 5231400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.7362214130057269, 1.0, 2.0, 0.7362214130057269, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2058808.473685183, 2058808.473685184, 390023.0454895264], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.6821944735008757, 1.0, 1.0, 0.6821944735008757, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5718912426903286, 0.5718912426903289, 0.5821239484918305], 
reward next is 0.4179, 
noisyNet noise sample is [array([0.07535672], dtype=float32), 0.56293154]. 
=============================================
[2019-04-23 12:51:48,344] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:51:48,381] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0226
[2019-04-23 12:51:48,387] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2416140.157005845 W.
[2019-04-23 12:51:48,389] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8638694995353429, 1.0, 2.0, 0.8638694995353429, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2416140.157005845, 2416140.157005845, 452167.058053899], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5241600.0000, 
sim time next is 5242200.0000, 
raw observation next is [31.83333333333334, 67.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.69330205177361, 6.9112, 168.9027887189632, 3549053.127749383, 2284844.701178718, 471454.756539808], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.675, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.17821020517736103, 0.0, 0.8293900166413607, 0.9858480910414953, 0.634679083660755, 0.7036638157310567], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.23686676], dtype=float32), 0.16493459]. 
=============================================
[2019-04-23 12:51:50,060] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1198958: loss 0.0910
[2019-04-23 12:51:50,084] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1198958: learning rate 0.0005
[2019-04-23 12:51:50,758] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1199114: loss 0.6356
[2019-04-23 12:51:50,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1199114: learning rate 0.0005
[2019-04-23 12:51:51,470] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1199281: loss 0.0275
[2019-04-23 12:51:51,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1199281: learning rate 0.0005
[2019-04-23 12:51:52,136] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1199454: loss 0.1509
[2019-04-23 12:51:52,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1199454: learning rate 0.0005
[2019-04-23 12:51:52,154] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1199454: loss 0.9210
[2019-04-23 12:51:52,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1199455: learning rate 0.0005
[2019-04-23 12:51:52,495] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1199543: loss 0.3284
[2019-04-23 12:51:52,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1199544: learning rate 0.0005
[2019-04-23 12:51:52,768] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1199633: loss 0.1033
[2019-04-23 12:51:52,769] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1199634: learning rate 0.0005
[2019-04-23 12:51:53,959] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-23 12:51:53,959] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:51:53,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:51:53,983] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:51:53,985] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:51:53,985] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:51:53,983] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:51:53,986] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:51:53,992] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:51:53,992] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:51:53,998] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:51:53,985] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run49
[2019-04-23 12:51:53,997] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run49
[2019-04-23 12:51:53,999] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run49
[2019-04-23 12:51:54,126] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run49
[2019-04-23 12:51:54,188] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run49
[2019-04-23 12:52:07,290] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33683497]
[2019-04-23 12:52:07,292] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.66666666666666, 53.33333333333334, 1.0, 2.0, 0.5974845082175666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 982588.4422644093, 982588.4422644093, 215072.6719184397]
[2019-04-23 12:52:07,292] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:52:07,294] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5785100898565686
[2019-04-23 12:52:35,838] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33683497]
[2019-04-23 12:52:35,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.86062636333334, 62.70960137333333, 1.0, 2.0, 0.7067247398424154, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975699026741, 6.9112, 168.9123237531409, 1884539.129355487, 1817302.238412159, 386105.2974354481]
[2019-04-23 12:52:35,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:52:35,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15141727876068234
[2019-04-23 12:52:35,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1884539.129355487 W.
[2019-04-23 12:52:36,167] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33683497]
[2019-04-23 12:52:36,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5019401521394582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701384.2721856466, 701384.2721856473, 183900.7382228879]
[2019-04-23 12:52:36,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:52:36,170] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.01816424529480154
[2019-04-23 12:53:03,731] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33683497]
[2019-04-23 12:53:03,733] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666667, 84.33333333333334, 1.0, 2.0, 0.8174522797683645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1142501.152132035, 1142501.152132035, 248238.4832001137]
[2019-04-23 12:53:03,735] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:53:03,736] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9865132243684686
[2019-04-23 12:53:11,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.33683497]
[2019-04-23 12:53:11,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 83.0, 1.0, 2.0, 0.4736776459209657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663084.7484409822, 663084.7484409829, 179730.2314350751]
[2019-04-23 12:53:11,351] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:53:11,352] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9495444288510516
[2019-04-23 12:53:24,938] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:53:25,517] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:53:26,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:53:26,294] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:53:26,299] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:53:27,315] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1200000, evaluation results [1200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:53:27,350] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1200026: loss 1.3138
[2019-04-23 12:53:27,351] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1200026: learning rate 0.0005
[2019-04-23 12:53:30,394] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1200708: loss 0.1039
[2019-04-23 12:53:30,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1200708: learning rate 0.0005
[2019-04-23 12:53:31,556] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1201046: loss 0.2268
[2019-04-23 12:53:31,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1201046: learning rate 0.0005
[2019-04-23 12:53:31,747] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1201078: loss 0.1180
[2019-04-23 12:53:31,748] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1201078: learning rate 0.0005
[2019-04-23 12:53:31,902] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1201100: loss 0.1465
[2019-04-23 12:53:31,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1201100: learning rate 0.0005
[2019-04-23 12:53:32,889] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1201322: loss 0.4319
[2019-04-23 12:53:32,890] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1201322: learning rate 0.0005
[2019-04-23 12:53:32,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:53:32,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8795
[2019-04-23 12:53:32,991] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3268602.293441052 W.
[2019-04-23 12:53:33,064] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.95, 53.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.412155153465977, 6.9112, 170.5573041426782, 3268602.293441052, 2909747.73946059, 550947.3918123795], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5329800.0000, 
sim time next is 5330400.0000, 
raw observation next is [35.93333333333333, 53.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.789466846491671, 6.9112, 170.5573041426782, 3539200.894553869, 2910062.626800724, 548665.5591511764], 
processed observation next is [1.0, 0.6956521739130435, 0.9020537124802526, 0.5366666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.08782668464916714, 0.0, 0.8375144448122397, 0.983111359598297, 0.8083507296668677, 0.8189038196286215], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2020024], dtype=float32), -0.97684133]. 
=============================================
[2019-04-23 12:53:34,695] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1201677: loss 0.4461
[2019-04-23 12:53:34,729] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1201681: learning rate 0.0005
[2019-04-23 12:53:34,976] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1201724: loss 1.9536
[2019-04-23 12:53:35,016] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1201724: learning rate 0.0005
[2019-04-23 12:53:43,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:53:43,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9854
[2019-04-23 12:53:43,590] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.25, 78.5, 1.0, 2.0, 0.6276846513960663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 877165.4491132319, 877165.4491132324, 206017.5566451002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5344200.0000, 
sim time next is 5344800.0000, 
raw observation next is [31.16666666666666, 78.66666666666667, 1.0, 2.0, 0.6261678895408834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875044.9588910389, 875044.9588910383, 205722.8452377084], 
processed observation next is [1.0, 0.8695652173913043, 0.6761453396524484, 0.7866666666666667, 1.0, 1.0, 0.549599866916727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2430680441363997, 0.24306804413639954, 0.30704902274284834], 
reward next is 0.6930, 
noisyNet noise sample is [array([1.0193905], dtype=float32), 2.5316885]. 
=============================================
[2019-04-23 12:53:43,679] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1203744: loss 0.3430
[2019-04-23 12:53:43,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1203744: learning rate 0.0005
[2019-04-23 12:53:46,999] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:53:47,000] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9608
[2019-04-23 12:53:47,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.78333333333334, 91.0, 1.0, 2.0, 0.9952744216933831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1391194.700776381, 1391194.700776381, 297492.3228865843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5461800.0000, 
sim time next is 5462400.0000, 
raw observation next is [27.96666666666667, 90.0, 1.0, 2.0, 0.8377966243943195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1170950.829298204, 1170950.829298205, 253403.0225037101], 
processed observation next is [1.0, 0.21739130434782608, 0.524486571879937, 0.9, 1.0, 1.0, 0.8045742462582162, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3252641192495011, 0.3252641192495014, 0.3782134664234479], 
reward next is 0.6218, 
noisyNet noise sample is [array([0.21030606], dtype=float32), -0.28184956]. 
=============================================
[2019-04-23 12:53:49,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:53:49,683] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9181
[2019-04-23 12:53:49,685] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3106451.397303894 W.
[2019-04-23 12:53:49,723] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.5, 53.5, 1.0, 2.0, 0.8393234896479198, 1.0, 2.0, 0.7402517843382226, 1.0, 1.0, 1.03, 7.005108720256136, 6.9112, 170.5573041426782, 3106451.397303894, 3039180.761021917, 568946.8359476924], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5412600.0000, 
sim time next is 5413200.0000, 
raw observation next is [36.0, 53.33333333333333, 1.0, 2.0, 0.9859595419840571, 1.0, 2.0, 0.9859595419840571, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2757988.354070666, 2757988.354070665, 520565.2749461232], 
processed observation next is [1.0, 0.6521739130434783, 0.9052132701421801, 0.5333333333333333, 1.0, 1.0, 0.9830837855229604, 1.0, 1.0, 0.9830837855229604, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7661078761307405, 0.7661078761307403, 0.7769630969345123], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.30920702], dtype=float32), 0.8419776]. 
=============================================
[2019-04-23 12:53:55,992] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:53:56,114] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7276
[2019-04-23 12:53:56,307] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 83.66666666666667, 1.0, 2.0, 0.587162889794152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820515.9538384827, 820515.9538384827, 198378.926590865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5442000.0000, 
sim time next is 5442600.0000, 
raw observation next is [29.18333333333334, 84.33333333333334, 1.0, 2.0, 0.5873787972320692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820817.7848168855, 820817.7848168849, 198418.3753768629], 
processed observation next is [1.0, 1.0, 0.5821484992101109, 0.8433333333333334, 1.0, 1.0, 0.5028660207615291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22800494022691264, 0.22800494022691248, 0.2961468289206909], 
reward next is 0.7039, 
noisyNet noise sample is [array([-2.113391], dtype=float32), 0.905901]. 
=============================================
[2019-04-23 12:53:59,834] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1207004: loss 0.4287
[2019-04-23 12:53:59,875] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1207006: learning rate 0.0005
[2019-04-23 12:54:00,716] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1207109: loss 0.2350
[2019-04-23 12:54:00,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1207109: learning rate 0.0005
[2019-04-23 12:54:01,577] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1207278: loss 0.1942
[2019-04-23 12:54:01,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1207278: learning rate 0.0005
[2019-04-23 12:54:01,980] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1207374: loss 0.4088
[2019-04-23 12:54:01,981] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1207374: learning rate 0.0005
[2019-04-23 12:54:02,082] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1207389: loss 0.5812
[2019-04-23 12:54:02,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1207389: learning rate 0.0005
[2019-04-23 12:54:02,369] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1207473: loss 0.3916
[2019-04-23 12:54:02,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1207475: learning rate 0.0005
[2019-04-23 12:54:03,173] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1207649: loss 0.2315
[2019-04-23 12:54:03,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1207650: learning rate 0.0005
[2019-04-23 12:54:05,279] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1208079: loss 0.1080
[2019-04-23 12:54:05,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1208081: learning rate 0.0005
[2019-04-23 12:54:06,731] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:54:06,755] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7002
[2019-04-23 12:54:06,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2011510.098922495 W.
[2019-04-23 12:54:06,820] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.26666666666667, 95.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.693277045166149, 6.9112, 169.6884640040826, 2011510.098922495, 1454130.398308694, 311494.5481175783], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5537400.0000, 
sim time next is 5538000.0000, 
raw observation next is [26.23333333333333, 95.0, 1.0, 2.0, 0.6074793544462477, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.920771471725332, 6.9112, 168.9122488835891, 1698515.122769473, 1691724.819749616, 366660.4525854584], 
processed observation next is [1.0, 0.08695652173913043, 0.44233807266982617, 0.95, 1.0, 1.0, 0.5270835595737924, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0009571471725331904, 0.0, 0.8294364703805559, 0.47180975632485356, 0.46992356104156, 0.5472544068439678], 
reward next is 0.4049, 
noisyNet noise sample is [array([-0.12779889], dtype=float32), 0.88883793]. 
=============================================
[2019-04-23 12:54:06,898] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.427536]
 [56.122086]
 [56.03187 ]
 [56.46535 ]
 [56.72531 ]], R is [[37.49536896]
 [37.12041473]
 [37.46452713]
 [37.80518341]
 [38.14253998]].
[2019-04-23 12:54:08,140] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1208690: loss 1.0961
[2019-04-23 12:54:08,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1208691: learning rate 0.0005
[2019-04-23 12:54:09,422] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1208940: loss 2.3911
[2019-04-23 12:54:09,470] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1208940: learning rate 0.0005
[2019-04-23 12:54:10,088] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1209053: loss 0.9396
[2019-04-23 12:54:10,089] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1209053: learning rate 0.0005
[2019-04-23 12:54:10,276] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1209089: loss 0.5077
[2019-04-23 12:54:10,296] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1209091: learning rate 0.0005
[2019-04-23 12:54:13,059] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1209501: loss 1.9561
[2019-04-23 12:54:13,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1209501: learning rate 0.0005
[2019-04-23 12:54:14,722] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1209730: loss 0.5599
[2019-04-23 12:54:14,766] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1209731: learning rate 0.0005
[2019-04-23 12:54:15,171] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1209809: loss 1.0251
[2019-04-23 12:54:15,178] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1209809: learning rate 0.0005
[2019-04-23 12:54:24,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:54:24,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1127
[2019-04-23 12:54:24,639] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.43333333333333, 72.83333333333334, 1.0, 2.0, 0.5322740261263339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743786.0419720585, 743786.0419720579, 188807.3144585471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5683800.0000, 
sim time next is 5684400.0000, 
raw observation next is [29.2, 74.0, 1.0, 2.0, 0.5315173219168408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742728.2732834636, 742728.2732834629, 188681.5681455161], 
processed observation next is [0.0, 0.8260869565217391, 0.5829383886255924, 0.74, 1.0, 1.0, 0.435563038454025, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20631340924540653, 0.20631340924540634, 0.28161428081420314], 
reward next is 0.7184, 
noisyNet noise sample is [array([-0.968613], dtype=float32), -0.43025756]. 
=============================================
[2019-04-23 12:54:25,835] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1211759: loss 0.2613
[2019-04-23 12:54:25,866] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1211759: learning rate 0.0005
[2019-04-23 12:54:42,776] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1214845: loss 0.5841
[2019-04-23 12:54:42,821] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1214845: learning rate 0.0005
[2019-04-23 12:54:43,421] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1214936: loss 2.2971
[2019-04-23 12:54:43,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1214942: learning rate 0.0005
[2019-04-23 12:54:44,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:54:44,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9018
[2019-04-23 12:54:44,016] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2774794.661479514 W.
[2019-04-23 12:54:44,082] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.05, 64.66666666666667, 1.0, 2.0, 0.6814345902948775, 1.0, 2.0, 0.6613073346617012, 1.0, 1.0, 1.03, 7.005096268336368, 6.9112, 170.5573041426782, 2774794.661479514, 2707532.945014178, 515528.5003258703], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5849400.0000, 
sim time next is 5850000.0000, 
raw observation next is [32.0, 65.0, 1.0, 2.0, 0.7040784414449773, 1.0, 2.0, 0.6726292602367513, 1.0, 2.0, 1.03, 7.005098053739491, 6.9112, 170.5573041426782, 2822354.254516319, 2755091.259094101, 522619.7306306795], 
processed observation next is [1.0, 0.7391304347826086, 0.7156398104265403, 0.65, 1.0, 1.0, 0.6434680017409365, 1.0, 1.0, 0.6055774219719895, 1.0, 1.0, 1.0365853658536586, 0.009389805373949133, 0.0, 0.8375144448122397, 0.7839872929211997, 0.7653031275261392, 0.7800294487025068], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7485897], dtype=float32), 2.329779]. 
=============================================
[2019-04-23 12:54:44,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[20.145235]
 [20.035805]
 [19.579617]
 [19.459497]
 [19.232285]], R is [[19.80094528]
 [19.60293579]
 [19.63469696]
 [19.64922905]
 [19.45273781]].
[2019-04-23 12:54:44,563] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1215150: loss 0.1382
[2019-04-23 12:54:44,565] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1215150: learning rate 0.0005
[2019-04-23 12:54:45,345] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1215271: loss 2.3351
[2019-04-23 12:54:45,345] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1215271: learning rate 0.0005
[2019-04-23 12:54:45,500] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1215292: loss 0.2716
[2019-04-23 12:54:45,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1215292: learning rate 0.0005
[2019-04-23 12:54:45,765] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1215330: loss 0.9753
[2019-04-23 12:54:45,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1215330: learning rate 0.0005
[2019-04-23 12:54:46,162] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1215432: loss 0.2003
[2019-04-23 12:54:46,182] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1215432: learning rate 0.0005
[2019-04-23 12:54:50,798] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1216383: loss 3.2285
[2019-04-23 12:54:50,799] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1216383: learning rate 0.0005
[2019-04-23 12:54:53,036] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:54:53,036] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2345
[2019-04-23 12:54:53,036] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2623125.001809053 W.
[2019-04-23 12:54:53,116] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.26666666666667, 64.66666666666667, 1.0, 2.0, 0.6251983307609483, 1.0, 2.0, 0.6251983307609483, 1.0, 1.0, 1.03, 6.973888419712202, 6.9112, 170.5573041426782, 2623125.001809053, 2578218.736767326, 497295.9698436047], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5832600.0000, 
sim time next is 5833200.0000, 
raw observation next is [32.33333333333334, 64.33333333333334, 1.0, 2.0, 0.75968365947459, 1.0, 2.0, 0.75968365947459, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2124484.600851214, 2124484.600851214, 400726.4428135869], 
processed observation next is [1.0, 0.5217391304347826, 0.7314375987361774, 0.6433333333333334, 1.0, 1.0, 0.7104622403308314, 1.0, 1.0, 0.7104622403308314, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5901346113475594, 0.5901346113475594, 0.598099168378488], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41510153], dtype=float32), 1.1582657]. 
=============================================
[2019-04-23 12:54:53,314] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1216889: loss 1.3211
[2019-04-23 12:54:53,364] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1216889: learning rate 0.0005
[2019-04-23 12:54:54,039] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1216987: loss 0.3464
[2019-04-23 12:54:54,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1216987: learning rate 0.0005
[2019-04-23 12:54:55,297] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1217184: loss 0.5337
[2019-04-23 12:54:55,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1217184: learning rate 0.0005
[2019-04-23 12:54:55,561] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1217254: loss 2.2815
[2019-04-23 12:54:55,562] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1217254: learning rate 0.0005
[2019-04-23 12:54:56,646] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1217460: loss 0.0323
[2019-04-23 12:54:56,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1217460: learning rate 0.0005
[2019-04-23 12:54:57,930] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1217803: loss 6.4293
[2019-04-23 12:54:58,021] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1217818: learning rate 0.0005
[2019-04-23 12:54:58,301] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1217866: loss 4.3132
[2019-04-23 12:54:58,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1217866: learning rate 0.0005
[2019-04-23 12:55:08,714] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1219872: loss 0.0454
[2019-04-23 12:55:08,716] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1219872: learning rate 0.0005
[2019-04-23 12:55:14,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:55:14,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8304
[2019-04-23 12:55:14,629] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 84.5, 1.0, 2.0, 0.750790183519081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049285.739953067, 1049285.739953067, 232192.690745666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5989800.0000, 
sim time next is 5990400.0000, 
raw observation next is [28.7, 84.0, 1.0, 2.0, 0.751059915969747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049662.897646974, 1049662.897646974, 232255.1746011922], 
processed observation next is [1.0, 0.34782608695652173, 0.5592417061611374, 0.84, 1.0, 1.0, 0.7000721879153577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29157302712415945, 0.29157302712415945, 0.34664951433013763], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.04857592], dtype=float32), -0.4621144]. 
=============================================
[2019-04-23 12:55:26,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:55:26,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1691
[2019-04-23 12:55:26,357] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.0, 1.0, 2.0, 0.7153428128753893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 999722.0550100794, 999722.0550100801, 224192.024811249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6147600.0000, 
sim time next is 6148200.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.7062129601582489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 986956.7678765169, 986956.7678765175, 222192.2251174253], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.6460397110340348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2741546577434769, 0.2741546577434771, 0.3316301867424258], 
reward next is 0.6684, 
noisyNet noise sample is [array([-0.18338953], dtype=float32), -0.4078789]. 
=============================================
[2019-04-23 12:55:27,490] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1222994: loss 0.3561
[2019-04-23 12:55:27,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1222994: learning rate 0.0005
[2019-04-23 12:55:27,873] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1223067: loss 0.4261
[2019-04-23 12:55:27,937] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1223067: learning rate 0.0005
[2019-04-23 12:55:28,725] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1223190: loss 0.2686
[2019-04-23 12:55:28,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1223190: learning rate 0.0005
[2019-04-23 12:55:28,851] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1223224: loss 0.2614
[2019-04-23 12:55:28,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1223224: learning rate 0.0005
[2019-04-23 12:55:28,969] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1223254: loss 0.2801
[2019-04-23 12:55:28,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1223254: learning rate 0.0005
[2019-04-23 12:55:29,906] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1223437: loss 0.3615
[2019-04-23 12:55:29,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1223437: learning rate 0.0005
[2019-04-23 12:55:30,210] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1223520: loss 0.3431
[2019-04-23 12:55:30,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1223520: learning rate 0.0005
[2019-04-23 12:55:33,692] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1224213: loss 0.1506
[2019-04-23 12:55:33,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1224216: learning rate 0.0005
[2019-04-23 12:55:37,507] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1224750: loss 0.2969
[2019-04-23 12:55:37,595] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1224750: learning rate 0.0005
[2019-04-23 12:55:38,570] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1224946: loss 0.0842
[2019-04-23 12:55:38,598] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1224948: learning rate 0.0005
[2019-04-23 12:55:38,905] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-23 12:55:38,917] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:55:38,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:55:38,929] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:55:38,929] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:55:38,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run50
[2019-04-23 12:55:38,936] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:55:38,937] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1225000: loss 0.1196
[2019-04-23 12:55:38,938] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:55:39,054] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:55:38,939] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run50
[2019-04-23 12:55:39,052] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:55:39,057] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1225000: learning rate 0.0005
[2019-04-23 12:55:39,041] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:55:39,083] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:55:39,060] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run50
[2019-04-23 12:55:39,085] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run50
[2019-04-23 12:55:39,061] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run50
[2019-04-23 12:56:06,266] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3010518]
[2019-04-23 12:56:06,295] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 96.0, 1.0, 2.0, 0.3964882551488776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 592925.4866486357, 592925.4866486357, 173925.084588858]
[2019-04-23 12:56:06,297] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:56:06,304] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.30273366111058597
[2019-04-23 12:57:38,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3010518]
[2019-04-23 12:57:38,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.89007402, 74.3528462, 1.0, 2.0, 0.6936460454115287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 969386.059030415, 969386.059030415, 219478.8938943213]
[2019-04-23 12:57:38,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 12:57:38,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6999901688026544
[2019-04-23 12:57:41,185] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3010518]
[2019-04-23 12:57:41,185] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [36.881285435, 59.686914585, 1.0, 2.0, 0.6840949065920131, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974290380792, 6.9112, 168.9123160480436, 1852872.174636536, 1785636.286098412, 381289.312366611]
[2019-04-23 12:57:41,186] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:57:41,188] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19485914080420774
[2019-04-23 12:57:41,189] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1852872.174636536 W.
[2019-04-23 12:57:52,594] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3010518]
[2019-04-23 12:57:52,595] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.3, 68.5, 1.0, 2.0, 0.7333297393180408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024871.680820144, 1024871.680820144, 228211.5294180566]
[2019-04-23 12:57:52,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 12:57:52,599] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46379297637367356
[2019-04-23 12:58:08,314] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3010518]
[2019-04-23 12:58:08,315] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.91436378, 88.6870476, 1.0, 2.0, 0.3293026204025092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524954.3860988978, 524954.3860988984, 168980.7675023871]
[2019-04-23 12:58:08,316] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:58:08,318] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5522312482273357
[2019-04-23 12:58:10,449] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.3010518]
[2019-04-23 12:58:10,451] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 82.83333333333334, 1.0, 2.0, 0.6212613923378563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868185.5199320214, 868185.5199320214, 204773.446115225]
[2019-04-23 12:58:10,452] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 12:58:10,454] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.00046308762499291944
[2019-04-23 12:58:16,282] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 12:58:16,441] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 12:58:16,449] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 12:58:16,510] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 12:58:16,585] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 12:58:17,601] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1225000, evaluation results [1225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 12:58:17,640] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1225031: loss 0.1323
[2019-04-23 12:58:17,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1225032: learning rate 0.0005
[2019-04-23 12:58:17,885] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1225199: loss 0.1179
[2019-04-23 12:58:17,888] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1225199: learning rate 0.0005
[2019-04-23 12:58:18,068] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:18,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0135
[2019-04-23 12:58:18,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2068629.743435921 W.
[2019-04-23 12:58:18,086] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.3, 84.0, 1.0, 2.0, 0.4931533805685917, 1.0, 1.0, 0.4931533805685917, 1.0, 2.0, 0.8564443337325542, 6.9112, 6.9112, 170.5573041426782, 2068629.743435921, 2068629.743435921, 410443.307355101], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6166800.0000, 
sim time next is 6167400.0000, 
raw observation next is [28.38333333333334, 83.33333333333333, 1.0, 2.0, 0.4615419619859759, 1.0, 2.0, 0.4615419619859759, 1.0, 2.0, 0.8015457537104231, 6.911199999999999, 6.9112, 170.5573041426782, 1935909.566079393, 1935909.566079393, 389570.1218878131], 
processed observation next is [1.0, 0.391304347826087, 0.544233807266983, 0.8333333333333333, 1.0, 1.0, 0.351255375886718, 1.0, 1.0, 0.351255375886718, 1.0, 1.0, 0.7579826264761258, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5377526572442758, 0.5377526572442758, 0.5814479431161389], 
reward next is 0.4186, 
noisyNet noise sample is [array([-0.7064166], dtype=float32), -0.95735925]. 
=============================================
[2019-04-23 12:58:18,109] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:18,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1424
[2019-04-23 12:58:18,120] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 87.66666666666667, 1.0, 2.0, 0.535146327582743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747801.1350051593, 747801.1350051587, 189286.2240246458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6132000.0000, 
sim time next is 6132600.0000, 
raw observation next is [27.2, 88.0, 1.0, 2.0, 0.5367055306939107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749980.6986402537, 749980.6986402532, 189547.0545906634], 
processed observation next is [1.0, 1.0, 0.4881516587677725, 0.88, 1.0, 1.0, 0.441813892402302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20832797184451493, 0.20832797184451476, 0.2829060516278558], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.19545658], dtype=float32), -1.3524894]. 
=============================================
[2019-04-23 12:58:19,083] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1225939: loss 0.1446
[2019-04-23 12:58:19,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1225939: learning rate 0.0005
[2019-04-23 12:58:19,208] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1226018: loss 0.0813
[2019-04-23 12:58:19,209] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1226018: learning rate 0.0005
[2019-04-23 12:58:19,762] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:19,766] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2521
[2019-04-23 12:58:19,770] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.91666666666667, 76.5, 1.0, 2.0, 0.3600042623272432, 1.0, 2.0, 0.3600042623272432, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1006241.079097243, 1006241.079097243, 262522.9666099878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6196200.0000, 
sim time next is 6196800.0000, 
raw observation next is [28.83333333333334, 77.0, 1.0, 2.0, 0.493673005053882, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689828.4529433534, 689828.4529433529, 182613.5669882463], 
processed observation next is [1.0, 0.7391304347826086, 0.5655608214849924, 0.77, 1.0, 1.0, 0.3899674759685325, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19161901470648707, 0.1916190147064869, 0.27255756266902437], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.10987027], dtype=float32), 0.09029303]. 
=============================================
[2019-04-23 12:58:20,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:20,968] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3799
[2019-04-23 12:58:20,971] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 91.0, 1.0, 2.0, 0.5235740968312212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731624.7977381628, 731624.7977381635, 187372.411295583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6231000.0000, 
sim time next is 6231600.0000, 
raw observation next is [26.5, 91.0, 1.0, 2.0, 0.5245850156945736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733037.909107554, 733037.9091075534, 187537.9955067001], 
processed observation next is [0.0, 0.13043478260869565, 0.4549763033175356, 0.91, 1.0, 1.0, 0.4272108622826188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20362164141876501, 0.20362164141876485, 0.2799074559801494], 
reward next is 0.7201, 
noisyNet noise sample is [array([-1.5901603], dtype=float32), -1.0481961]. 
=============================================
[2019-04-23 12:58:21,984] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1227798: loss 2010.5933
[2019-04-23 12:58:21,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1227798: learning rate 0.0005
[2019-04-23 12:58:22,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:22,028] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1746
[2019-04-23 12:58:22,033] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.33333333333334, 1.0, 2.0, 0.5419008612283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757243.1288852462, 757243.1288852456, 190422.0233671023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6258000.0000, 
sim time next is 6258600.0000, 
raw observation next is [30.15, 71.5, 1.0, 2.0, 0.5420248353274145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757416.4299662584, 757416.4299662578, 190442.9864727354], 
processed observation next is [0.0, 0.43478260869565216, 0.6279620853080569, 0.715, 1.0, 1.0, 0.4482226931655596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2103934527684051, 0.21039345276840493, 0.2842432633921424], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.6613552], dtype=float32), 0.66327554]. 
=============================================
[2019-04-23 12:58:25,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:25,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6100
[2019-04-23 12:58:25,446] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.5124905265649867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716131.769476908, 716131.769476908, 185575.9632581578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6394800.0000, 
sim time next is 6395400.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 0.5123525211006227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715938.8617114736, 715938.8617114729, 185553.8411422499], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.83, 1.0, 1.0, 0.412472916988702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19887190603096488, 0.1988719060309647, 0.2769460315555969], 
reward next is 0.7231, 
noisyNet noise sample is [array([1.244564], dtype=float32), 0.23903657]. 
=============================================
[2019-04-23 12:58:26,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:26,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4257
[2019-04-23 12:58:26,410] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2317753.357161385 W.
[2019-04-23 12:58:26,414] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.2, 78.5, 1.0, 2.0, 0.5524831754024473, 1.0, 1.0, 0.5524831754024473, 1.0, 1.0, 0.9513804314551294, 6.9112, 6.9112, 170.5573041426782, 2317753.357161385, 2317753.357161385, 451697.781156348], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6424200.0000, 
sim time next is 6424800.0000, 
raw observation next is [28.3, 78.0, 1.0, 2.0, 0.4897526006744464, 1.0, 2.0, 0.4897526006744464, 1.0, 2.0, 0.8413578776798958, 6.9112, 6.9112, 170.5573041426782, 2054350.818844136, 2054350.818844136, 406505.1439208342], 
processed observation next is [1.0, 0.34782608695652173, 0.5402843601895735, 0.78, 1.0, 1.0, 0.38524409719812824, 1.0, 1.0, 0.38524409719812824, 1.0, 1.0, 0.8065339971706046, 0.0, 0.0, 0.8375144448122397, 0.5706530052344823, 0.5706530052344823, 0.6067240954042301], 
reward next is 0.3933, 
noisyNet noise sample is [array([0.7407459], dtype=float32), -2.4833906]. 
=============================================
[2019-04-23 12:58:27,219] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1230939: loss 1660.7417
[2019-04-23 12:58:27,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1230939: learning rate 0.0005
[2019-04-23 12:58:27,682] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1231214: loss 2596.8706
[2019-04-23 12:58:27,686] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1231215: learning rate 0.0005
[2019-04-23 12:58:27,722] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1231233: loss 1509.5720
[2019-04-23 12:58:27,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1231234: learning rate 0.0005
[2019-04-23 12:58:27,761] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1231257: loss 1255.7510
[2019-04-23 12:58:27,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1231257: learning rate 0.0005
[2019-04-23 12:58:27,978] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1231389: loss 2808.8672
[2019-04-23 12:58:27,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1231389: learning rate 0.0005
[2019-04-23 12:58:27,994] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1231399: loss 2225.5027
[2019-04-23 12:58:27,996] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1231399: learning rate 0.0005
[2019-04-23 12:58:28,256] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1231560: loss 2602.1387
[2019-04-23 12:58:28,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1231561: learning rate 0.0005
[2019-04-23 12:58:29,593] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1232316: loss 1712.8799
[2019-04-23 12:58:29,595] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1232318: learning rate 0.0005
[2019-04-23 12:58:29,881] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:29,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-23 12:58:29,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1705302.974156371 W.
[2019-04-23 12:58:29,894] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.98333333333333, 83.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.265501844356185, 6.9112, 168.9277914762471, 1705302.974156371, 1453926.976207555, 311351.4192143306], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6401400.0000, 
sim time next is 6402000.0000, 
raw observation next is [26.96666666666667, 83.66666666666667, 1.0, 2.0, 0.39459431029477, 1.0, 1.0, 0.39459431029477, 1.0, 1.0, 0.6712390800234933, 6.9112, 6.9112, 170.5573041426782, 1654884.886802248, 1654884.886802248, 347958.5346783801], 
processed observation next is [1.0, 0.08695652173913043, 0.47709320695102697, 0.8366666666666667, 1.0, 1.0, 0.270595554572012, 1.0, 0.5, 0.270595554572012, 1.0, 0.5, 0.5990720488091381, 0.0, 0.0, 0.8375144448122397, 0.4596902463339578, 0.4596902463339578, 0.5193410965348957], 
reward next is 0.4807, 
noisyNet noise sample is [array([1.8042952], dtype=float32), 1.0003983]. 
=============================================
[2019-04-23 12:58:29,925] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[49.865017]
 [54.462807]
 [54.12936 ]
 [54.655865]
 [55.337433]], R is [[30.24593544]
 [29.94347572]
 [30.36782646]
 [30.7877388 ]
 [31.2032299 ]].
[2019-04-23 12:58:30,494] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1232879: loss 1444.5657
[2019-04-23 12:58:30,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1232880: learning rate 0.0005
[2019-04-23 12:58:30,647] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1232980: loss 1056.4917
[2019-04-23 12:58:30,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1232981: learning rate 0.0005
[2019-04-23 12:58:30,779] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1233066: loss 1305.2043
[2019-04-23 12:58:30,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1233066: learning rate 0.0005
[2019-04-23 12:58:30,821] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1233089: loss 1242.9807
[2019-04-23 12:58:30,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1233090: learning rate 0.0005
[2019-04-23 12:58:30,973] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1233188: loss 1447.1948
[2019-04-23 12:58:30,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1233188: learning rate 0.0005
[2019-04-23 12:58:32,198] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1233932: loss 2329.9434
[2019-04-23 12:58:32,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1233932: learning rate 0.0005
[2019-04-23 12:58:32,250] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1233963: loss 2194.6685
[2019-04-23 12:58:32,251] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1233963: learning rate 0.0005
[2019-04-23 12:58:33,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:33,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2393
[2019-04-23 12:58:33,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2011392.907671436 W.
[2019-04-23 12:58:33,548] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.18333333333334, 78.16666666666667, 1.0, 2.0, 0.4795211366407971, 1.0, 2.0, 0.4795211366407971, 1.0, 2.0, 0.8217923084680766, 6.9112, 6.9112, 170.5573041426782, 2011392.907671436, 2011392.907671436, 399369.8813864338], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6599400.0000, 
sim time next is 6600000.0000, 
raw observation next is [28.36666666666667, 77.33333333333334, 1.0, 2.0, 0.6946227724744906, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.981987810784306, 6.9112, 168.9125341913649, 1867604.176454297, 1817384.992714667, 384349.276555431], 
processed observation next is [1.0, 0.391304347826087, 0.543443917851501, 0.7733333333333334, 1.0, 1.0, 0.632075629487338, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0070787810784305625, 0.0, 0.8294378713723496, 0.5187789379039714, 0.504829164642963, 0.573655636649897], 
reward next is 0.0724, 
noisyNet noise sample is [array([0.7605514], dtype=float32), -1.3217876]. 
=============================================
[2019-04-23 12:58:33,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[28.267805]
 [28.808022]
 [28.798008]
 [29.068214]
 [29.83392 ]], R is [[28.3779583 ]
 [28.4981041 ]
 [28.65789413]
 [28.83337784]
 [28.54504395]].
[2019-04-23 12:58:35,298] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1235784: loss 0.0208
[2019-04-23 12:58:35,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1235784: learning rate 0.0005
[2019-04-23 12:58:40,551] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1238953: loss 0.3810
[2019-04-23 12:58:40,552] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1238953: learning rate 0.0005
[2019-04-23 12:58:40,938] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1239188: loss 0.5037
[2019-04-23 12:58:40,942] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1239189: learning rate 0.0005
[2019-04-23 12:58:41,030] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1239245: loss 0.3196
[2019-04-23 12:58:41,032] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1239245: learning rate 0.0005
[2019-04-23 12:58:41,035] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1239247: loss 0.4568
[2019-04-23 12:58:41,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1239249: learning rate 0.0005
[2019-04-23 12:58:41,085] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1239273: loss 0.2729
[2019-04-23 12:58:41,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1239276: learning rate 0.0005
[2019-04-23 12:58:41,299] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1239408: loss 0.1986
[2019-04-23 12:58:41,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1239408: learning rate 0.0005
[2019-04-23 12:58:41,634] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1239611: loss 0.3624
[2019-04-23 12:58:41,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1239611: learning rate 0.0005
[2019-04-23 12:58:42,246] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:42,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5918
[2019-04-23 12:58:42,256] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 88.66666666666667, 1.0, 2.0, 0.4221723106715592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619006.1099668499, 619006.1099668499, 176022.4753719938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6924000.0000, 
sim time next is 6924600.0000, 
raw observation next is [24.1, 89.0, 1.0, 2.0, 0.4201001189778401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616474.4550568531, 616474.4550568538, 175793.987232664], 
processed observation next is [0.0, 0.13043478260869565, 0.3412322274881518, 0.89, 1.0, 1.0, 0.30132544455161464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17124290418245922, 0.17124290418245938, 0.26237908542188654], 
reward next is 0.7376, 
noisyNet noise sample is [array([1.4802608], dtype=float32), -0.22847833]. 
=============================================
[2019-04-23 12:58:42,858] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1240365: loss 0.1184
[2019-04-23 12:58:42,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1240365: learning rate 0.0005
[2019-04-23 12:58:43,644] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1240861: loss 0.6799
[2019-04-23 12:58:43,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1240863: learning rate 0.0005
[2019-04-23 12:58:43,925] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1241038: loss 0.3829
[2019-04-23 12:58:43,926] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1241038: learning rate 0.0005
[2019-04-23 12:58:44,006] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1241068: loss 0.2854
[2019-04-23 12:58:44,007] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1241068: learning rate 0.0005
[2019-04-23 12:58:44,163] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1241178: loss 0.1947
[2019-04-23 12:58:44,167] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1241178: loss 0.3240
[2019-04-23 12:58:44,167] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1241178: learning rate 0.0005
[2019-04-23 12:58:44,168] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1241178: learning rate 0.0005
[2019-04-23 12:58:44,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:44,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4427
[2019-04-23 12:58:44,348] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 79.0, 1.0, 2.0, 0.3478090090862956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549331.0909467237, 549331.0909467237, 170876.5057150823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6764400.0000, 
sim time next is 6765000.0000, 
raw observation next is [23.13333333333333, 78.50000000000001, 1.0, 2.0, 0.3724173087015109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587538.6706923113, 587538.6706923113, 174095.6664532488], 
processed observation next is [1.0, 0.30434782608695654, 0.29541864139020524, 0.7850000000000001, 1.0, 1.0, 0.24387627554398905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1632051863034198, 0.1632051863034198, 0.25984427828843104], 
reward next is 0.7402, 
noisyNet noise sample is [array([1.3109059], dtype=float32), 0.99050695]. 
=============================================
[2019-04-23 12:58:44,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.694244]
 [72.32139 ]
 [72.33616 ]
 [72.385826]
 [72.26193 ]], R is [[72.56859589]
 [72.58786774]
 [72.60220337]
 [72.61885071]
 [72.63832855]].
[2019-04-23 12:58:44,841] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:44,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8505
[2019-04-23 12:58:44,852] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 75.33333333333333, 1.0, 2.0, 0.3452486550663872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535628.2394388029, 535628.2394388035, 169567.1435543828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6738600.0000, 
sim time next is 6739200.0000, 
raw observation next is [24.1, 76.0, 1.0, 2.0, 0.3432735805155303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 533249.7136302579, 533249.7136302573, 169393.0204603326], 
processed observation next is [1.0, 0.0, 0.3412322274881518, 0.76, 1.0, 1.0, 0.2087633500187112, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14812492045284942, 0.14812492045284925, 0.2528254036721382], 
reward next is 0.7472, 
noisyNet noise sample is [array([0.79345214], dtype=float32), -1.076614]. 
=============================================
[2019-04-23 12:58:45,591] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1242060: loss 0.1114
[2019-04-23 12:58:45,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1242060: learning rate 0.0005
[2019-04-23 12:58:45,649] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1242098: loss 0.0753
[2019-04-23 12:58:45,651] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1242098: learning rate 0.0005
[2019-04-23 12:58:46,202] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:46,208] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4178
[2019-04-23 12:58:46,215] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 39.0, 1.0, 2.0, 0.2794126779231882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 449936.831319606, 449936.831319606, 163595.2500997748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6880800.0000, 
sim time next is 6881400.0000, 
raw observation next is [29.55, 40.0, 1.0, 2.0, 0.2864170410564127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459727.3100937979, 459727.3100937979, 164248.2140410685], 
processed observation next is [0.0, 0.6521739130434783, 0.5995260663507109, 0.4, 1.0, 1.0, 0.14026149524868997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12770203058161053, 0.12770203058161053, 0.24514658812099777], 
reward next is 0.7549, 
noisyNet noise sample is [array([0.56805855], dtype=float32), -0.5111568]. 
=============================================
[2019-04-23 12:58:46,600] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:46,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3739
[2019-04-23 12:58:46,613] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 73.0, 1.0, 2.0, 0.3322398207757742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520035.16532814, 520035.16532814, 168444.0701858236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6823200.0000, 
sim time next is 6823800.0000, 
raw observation next is [24.2, 73.5, 1.0, 2.0, 0.3324294445221038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520206.5995089056, 520206.599508905, 168454.4481746392], 
processed observation next is [1.0, 1.0, 0.3459715639810427, 0.735, 1.0, 1.0, 0.19569812593024558, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14450183319691823, 0.14450183319691806, 0.25142454951438686], 
reward next is 0.7486, 
noisyNet noise sample is [array([1.3271998], dtype=float32), 1.4631411]. 
=============================================
[2019-04-23 12:58:48,147] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1243614: loss 3.4350
[2019-04-23 12:58:48,149] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1243614: learning rate 0.0005
[2019-04-23 12:58:48,858] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:48,865] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7858
[2019-04-23 12:58:48,870] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 89.0, 1.0, 2.0, 0.4201001189778401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616474.4550568531, 616474.4550568538, 175793.987232664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6924600.0000, 
sim time next is 6925200.0000, 
raw observation next is [24.03333333333333, 89.33333333333334, 1.0, 2.0, 0.4188181688535201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 615101.9956789002, 615101.9956789007, 175677.209279143], 
processed observation next is [0.0, 0.13043478260869565, 0.3380726698262243, 0.8933333333333334, 1.0, 1.0, 0.2997809263295423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17086166546636117, 0.1708616654663613, 0.26220478996887014], 
reward next is 0.7378, 
noisyNet noise sample is [array([0.8516193], dtype=float32), 0.97549427]. 
=============================================
[2019-04-23 12:58:51,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:51,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5109
[2019-04-23 12:58:51,551] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 57.33333333333334, 1.0, 2.0, 0.3851901583559446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580746.5785393581, 580746.5785393575, 172959.3371488834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6985200.0000, 
sim time next is 6985800.0000, 
raw observation next is [28.35, 58.5, 1.0, 2.0, 0.3891835891993862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584786.8259990885, 584786.8259990878, 173265.0750511373], 
processed observation next is [0.0, 0.8695652173913043, 0.5426540284360191, 0.585, 1.0, 1.0, 0.2640766134932364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1624407849997468, 0.1624407849997466, 0.25860458962856314], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.31637242], dtype=float32), -1.3729742]. 
=============================================
[2019-04-23 12:58:53,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:53,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4972
[2019-04-23 12:58:53,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1703754.744240589 W.
[2019-04-23 12:58:53,212] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.45, 59.66666666666667, 1.0, 2.0, 0.6093565327123311, 1.0, 2.0, 0.6093565327123311, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1703754.744240589, 1703754.744240589, 337717.7305775066], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7033800.0000, 
sim time next is 7034400.0000, 
raw observation next is [29.6, 59.0, 1.0, 2.0, 0.6552204350424761, 1.0, 2.0, 0.6552204350424761, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1832515.253738282, 1832515.253738282, 355579.3182941868], 
processed observation next is [1.0, 0.43478260869565216, 0.6018957345971565, 0.59, 1.0, 1.0, 0.5846029337861157, 1.0, 1.0, 0.5846029337861157, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5090320149273005, 0.5090320149273005, 0.5307154004390847], 
reward next is 0.4693, 
noisyNet noise sample is [array([0.64365315], dtype=float32), -0.3004972]. 
=============================================
[2019-04-23 12:58:53,815] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1246969: loss -9.2480
[2019-04-23 12:58:53,821] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1246969: learning rate 0.0005
[2019-04-23 12:58:54,450] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1247150: loss -7.1839
[2019-04-23 12:58:54,467] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1247151: learning rate 0.0005
[2019-04-23 12:58:54,626] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1247208: loss -11.0121
[2019-04-23 12:58:54,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1247216: learning rate 0.0005
[2019-04-23 12:58:54,839] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1247258: loss 14.1407
[2019-04-23 12:58:54,868] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1247258: learning rate 0.0005
[2019-04-23 12:58:54,998] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1247314: loss -101.4636
[2019-04-23 12:58:55,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1247314: learning rate 0.0005
[2019-04-23 12:58:55,261] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1247380: loss -74.1191
[2019-04-23 12:58:55,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1247391: learning rate 0.0005
[2019-04-23 12:58:55,864] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1247576: loss -84.0622
[2019-04-23 12:58:55,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1247580: learning rate 0.0005
[2019-04-23 12:58:57,367] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 12:58:57,389] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4634
[2019-04-23 12:58:57,403] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 58.0, 1.0, 2.0, 0.4388025248677382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633761.5362040659, 633761.5362040659, 177209.9897885411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6976800.0000, 
sim time next is 6977400.0000, 
raw observation next is [29.51666666666667, 57.83333333333334, 1.0, 2.0, 0.4310396285639245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625009.0755310854, 625009.075531086, 176413.4769398913], 
processed observation next is [0.0, 0.782608695652174, 0.5979462875197474, 0.5783333333333335, 1.0, 1.0, 0.31450557658304157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17361363209196815, 0.1736136320919683, 0.2633036969252109], 
reward next is 0.7367, 
noisyNet noise sample is [array([-1.2436391], dtype=float32), 0.98852235]. 
=============================================
[2019-04-23 12:58:58,499] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1248300: loss 16.6941
[2019-04-23 12:58:58,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1248300: learning rate 0.0005
[2019-04-23 12:59:00,499] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1248878: loss -87.1317
[2019-04-23 12:59:00,506] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1248882: learning rate 0.0005
[2019-04-23 12:59:01,211] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1249023: loss 5.7225
[2019-04-23 12:59:01,212] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1249023: loss -14.0325
[2019-04-23 12:59:01,230] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1249023: learning rate 0.0005
[2019-04-23 12:59:01,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1249023: learning rate 0.0005
[2019-04-23 12:59:01,311] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1249052: loss 22.5016
[2019-04-23 12:59:01,342] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1249052: learning rate 0.0005
[2019-04-23 12:59:01,852] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1249176: loss 23.2475
[2019-04-23 12:59:01,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1249176: learning rate 0.0005
[2019-04-23 12:59:04,977] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-23 12:59:04,996] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 12:59:04,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:59:04,998] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 12:59:04,999] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 12:59:04,999] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:59:05,000] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:59:05,003] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run51
[2019-04-23 12:59:05,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run51
[2019-04-23 12:59:05,026] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 12:59:05,141] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:59:05,148] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run51
[2019-04-23 12:59:05,056] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run51
[2019-04-23 12:59:05,134] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 12:59:05,241] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 12:59:05,282] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run51
[2019-04-23 12:59:19,011] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 12:59:19,015] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.9, 92.0, 1.0, 2.0, 0.2224132996837782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 369961.1697761447, 369961.1697761447, 157944.6875884107]
[2019-04-23 12:59:19,015] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 12:59:19,027] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6734769617747882
[2019-04-23 12:59:33,396] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 12:59:33,399] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.21245326, 78.73691624, 1.0, 2.0, 0.5729155650129496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817261.8680248938, 817261.8680248938, 197945.3323989038]
[2019-04-23 12:59:33,401] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 12:59:33,412] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4936695812369202
[2019-04-23 13:00:40,464] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 13:00:40,470] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.63202827, 63.82914563, 1.0, 2.0, 0.5361772472884967, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9311625620148852, 6.911199999999999, 6.9112, 168.9127072894647, 1499013.3106147, 1499013.310614701, 328482.4041612113]
[2019-04-23 13:00:40,472] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:00:40,475] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9884812227577465
[2019-04-23 13:01:20,261] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 13:01:20,276] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 70.0, 1.0, 2.0, 0.5176365907795337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723325.102033454, 723325.1020334547, 186406.7668424725]
[2019-04-23 13:01:20,286] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:01:20,289] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4705054225465277
[2019-04-23 13:01:31,920] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 13:01:31,925] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.62594567333333, 80.72025701666666, 1.0, 2.0, 0.5057995190432399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706778.9387729893, 706778.9387729886, 184510.2859581395]
[2019-04-23 13:01:31,927] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:01:31,932] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.71231323683039
[2019-04-23 13:02:23,287] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 13:02:23,290] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.85688615333333, 57.61752304000001, 1.0, 2.0, 0.5071844596287476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708714.8310236437, 708714.8310236444, 184727.8848414927]
[2019-04-23 13:02:23,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:02:23,312] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3678993722901538
[2019-04-23 13:02:25,977] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.24798468]
[2019-04-23 13:02:25,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.66004857333334, 88.52559513666667, 1.0, 2.0, 0.4924196773333741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688076.5622705304, 688076.5622705304, 182416.4084086744]
[2019-04-23 13:02:25,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:02:25,985] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10463404031504653
[2019-04-23 13:02:45,662] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 13:02:45,948] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 13:02:46,108] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 13:02:46,122] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 13:02:46,313] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 13:02:47,334] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1250000, evaluation results [1250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 13:02:47,430] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1250053: loss -114.6970
[2019-04-23 13:02:47,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1250053: learning rate 0.0005
[2019-04-23 13:02:47,512] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1250065: loss -63.3944
[2019-04-23 13:02:47,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1250066: learning rate 0.0005
[2019-04-23 13:02:48,180] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:02:48,182] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9598
[2019-04-23 13:02:48,196] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 82.16666666666666, 1.0, 2.0, 0.5359684677292955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763047.9393231751, 763047.9393231751, 191225.3700392197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7109400.0000, 
sim time next is 7110000.0000, 
raw observation next is [26.0, 81.0, 1.0, 2.0, 0.537309990994946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765116.9800715566, 765116.9800715559, 191474.8870578634], 
processed observation next is [1.0, 0.30434782608695654, 0.4312796208530806, 0.81, 1.0, 1.0, 0.44254215782523615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2125324944643213, 0.2125324944643211, 0.2857834135191991], 
reward next is 0.7142, 
noisyNet noise sample is [array([2.0388174], dtype=float32), -0.067775674]. 
=============================================
[2019-04-23 13:02:48,304] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.69494]
 [72.66858]
 [72.76494]
 [72.63656]
 [72.68142]], R is [[72.77635956]
 [72.76319122]
 [72.75251007]
 [72.74721527]
 [72.74163818]].
[2019-04-23 13:02:54,443] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1251693: loss 0.0143
[2019-04-23 13:02:54,449] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1251693: learning rate 0.0005
[2019-04-23 13:03:07,524] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1254912: loss 0.0613
[2019-04-23 13:03:07,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1254914: learning rate 0.0005
[2019-04-23 13:03:07,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:07,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7239
[2019-04-23 13:03:07,677] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 74.66666666666666, 1.0, 2.0, 0.3644191952882042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557667.0587665866, 557667.0587665872, 171189.7534645393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7346400.0000, 
sim time next is 7347000.0000, 
raw observation next is [24.88333333333333, 74.33333333333334, 1.0, 2.0, 0.3633204817535319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 556529.5601895028, 556529.5601895028, 171108.6675328962], 
processed observation next is [1.0, 0.0, 0.3783570300157976, 0.7433333333333334, 1.0, 1.0, 0.23291624307654443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1545915444970841, 0.1545915444970841, 0.2553860709446212], 
reward next is 0.7446, 
noisyNet noise sample is [array([-0.6144231], dtype=float32), 0.16680098]. 
=============================================
[2019-04-23 13:03:07,700] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.12309 ]
 [72.14087 ]
 [72.308685]
 [72.438644]
 [73.40849 ]], R is [[72.23620605]
 [72.25833893]
 [72.28031158]
 [72.30201721]
 [72.32328796]].
[2019-04-23 13:03:07,847] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1255102: loss 0.0191
[2019-04-23 13:03:07,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1255106: learning rate 0.0005
[2019-04-23 13:03:08,061] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1255233: loss 0.0516
[2019-04-23 13:03:08,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1255234: learning rate 0.0005
[2019-04-23 13:03:08,097] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1255251: loss 0.0384
[2019-04-23 13:03:08,101] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1255251: learning rate 0.0005
[2019-04-23 13:03:08,127] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1255271: loss 0.0295
[2019-04-23 13:03:08,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1255271: learning rate 0.0005
[2019-04-23 13:03:08,274] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1255365: loss 0.0115
[2019-04-23 13:03:08,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1255367: learning rate 0.0005
[2019-04-23 13:03:08,843] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1255744: loss 0.1454
[2019-04-23 13:03:08,845] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1255747: learning rate 0.0005
[2019-04-23 13:03:09,686] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1256311: loss 0.2757
[2019-04-23 13:03:09,687] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1256311: learning rate 0.0005
[2019-04-23 13:03:10,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:10,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1795
[2019-04-23 13:03:10,522] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 75.33333333333333, 1.0, 2.0, 0.4143900331792998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636312.9102827344, 636312.9102827339, 178298.6179107785], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7353600.0000, 
sim time next is 7354200.0000, 
raw observation next is [24.58333333333334, 76.16666666666667, 1.0, 2.0, 0.4114681303065342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 631072.2264921246, 631072.2264921246, 177794.5795916351], 
processed observation next is [1.0, 0.08695652173913043, 0.3641390205371251, 0.7616666666666667, 1.0, 1.0, 0.2909254582006436, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17529784069225685, 0.17529784069225685, 0.26536504416661955], 
reward next is 0.7346, 
noisyNet noise sample is [array([-1.9606202], dtype=float32), -1.8183681]. 
=============================================
[2019-04-23 13:03:10,586] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1256908: loss 0.1186
[2019-04-23 13:03:10,588] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1256908: learning rate 0.0005
[2019-04-23 13:03:10,594] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1256910: loss 0.1421
[2019-04-23 13:03:10,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1256910: learning rate 0.0005
[2019-04-23 13:03:10,634] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1256935: loss 0.1768
[2019-04-23 13:03:10,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1256935: learning rate 0.0005
[2019-04-23 13:03:10,672] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1256956: loss 0.2910
[2019-04-23 13:03:10,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1256958: learning rate 0.0005
[2019-04-23 13:03:10,939] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1257118: loss 0.1006
[2019-04-23 13:03:10,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1257119: learning rate 0.0005
[2019-04-23 13:03:11,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:11,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5613
[2019-04-23 13:03:11,847] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.5520598120229266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887120.686063668, 887120.686063668, 204900.8962486372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7376400.0000, 
sim time next is 7377000.0000, 
raw observation next is [20.56666666666667, 92.16666666666667, 1.0, 2.0, 0.5690211435355871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912923.0050732447, 912923.0050732447, 208190.4419254536], 
processed observation next is [1.0, 0.391304347826087, 0.17377567140600336, 0.9216666666666667, 1.0, 1.0, 0.4807483657055266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2535897236314569, 0.2535897236314569, 0.31073200287381136], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.6502669], dtype=float32), 0.9968937]. 
=============================================
[2019-04-23 13:03:11,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.14108 ]
 [69.38417 ]
 [69.789894]
 [70.347855]
 [70.96585 ]], R is [[68.72789764]
 [68.73479462]
 [68.74543762]
 [68.7598877 ]
 [68.78014374]].
[2019-04-23 13:03:12,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:12,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0137
[2019-04-23 13:03:12,341] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 93.33333333333333, 1.0, 2.0, 0.3469143773739593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534445.9834828508, 534445.9834828508, 169362.8886042231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7462200.0000, 
sim time next is 7462800.0000, 
raw observation next is [22.2, 93.0, 1.0, 2.0, 0.3488492505742917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536509.7112452202, 536509.7112452195, 169503.3445936645], 
processed observation next is [0.0, 0.391304347826087, 0.2511848341232228, 0.93, 1.0, 1.0, 0.21548102478830328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14903047534589448, 0.14903047534589428, 0.25299006655770817], 
reward next is 0.7470, 
noisyNet noise sample is [array([1.8135415], dtype=float32), -2.3448446]. 
=============================================
[2019-04-23 13:03:12,389] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1258011: loss 0.0689
[2019-04-23 13:03:12,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1258014: learning rate 0.0005
[2019-04-23 13:03:12,451] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1258048: loss 0.0696
[2019-04-23 13:03:12,453] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1258048: learning rate 0.0005
[2019-04-23 13:03:14,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:15,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2576
[2019-04-23 13:03:15,005] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 95.0, 1.0, 2.0, 0.3323674915720851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517606.0639663989, 517606.0639663996, 168184.8189318181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7455600.0000, 
sim time next is 7456200.0000, 
raw observation next is [21.51666666666667, 95.0, 1.0, 2.0, 0.3335658185895803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519165.1105835736, 519165.1105835736, 168298.5546337494], 
processed observation next is [0.0, 0.30434782608695654, 0.21879936808846778, 0.95, 1.0, 1.0, 0.19706725131274733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14421253071765933, 0.14421253071765933, 0.25119187258768566], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.8441362], dtype=float32), -0.01905653]. 
=============================================
[2019-04-23 13:03:15,258] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1259758: loss 125.3234
[2019-04-23 13:03:15,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1259759: learning rate 0.0005
[2019-04-23 13:03:18,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:18,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0931
[2019-04-23 13:03:18,849] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 93.0, 1.0, 2.0, 0.467693334089243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 657636.0919025288, 657636.0919025281, 179221.3265298578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7603200.0000, 
sim time next is 7603800.0000, 
raw observation next is [24.55, 93.16666666666667, 1.0, 2.0, 0.4653841024394188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655571.3896151873, 655571.3896151873, 179032.4917273292], 
processed observation next is [1.0, 0.0, 0.3625592417061612, 0.9316666666666668, 1.0, 1.0, 0.35588446077038416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18210316378199648, 0.18210316378199648, 0.26721267421989436], 
reward next is 0.7328, 
noisyNet noise sample is [array([0.2605458], dtype=float32), -1.5233946]. 
=============================================
[2019-04-23 13:03:20,431] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1262913: loss -48.6088
[2019-04-23 13:03:20,434] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1262914: learning rate 0.0005
[2019-04-23 13:03:20,837] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1263161: loss -63.7373
[2019-04-23 13:03:20,841] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1263162: learning rate 0.0005
[2019-04-23 13:03:20,963] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1263233: loss 5.5056
[2019-04-23 13:03:20,964] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1263234: learning rate 0.0005
[2019-04-23 13:03:20,986] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1263247: loss -73.3093
[2019-04-23 13:03:20,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1263247: learning rate 0.0005
[2019-04-23 13:03:21,092] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1263307: loss -115.4756
[2019-04-23 13:03:21,094] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1263308: learning rate 0.0005
[2019-04-23 13:03:21,171] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1263353: loss -13.4038
[2019-04-23 13:03:21,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1263353: learning rate 0.0005
[2019-04-23 13:03:21,743] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1263706: loss -114.8537
[2019-04-23 13:03:21,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1263706: learning rate 0.0005
[2019-04-23 13:03:21,973] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:21,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6672
[2019-04-23 13:03:21,984] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 88.5, 1.0, 2.0, 0.5002615607298454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699037.9267299458, 699037.9267299464, 183636.8798914541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7677000.0000, 
sim time next is 7677600.0000, 
raw observation next is [25.96666666666667, 88.33333333333334, 1.0, 2.0, 0.4975514163924357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695249.6811494034, 695249.6811494041, 183213.1641964851], 
processed observation next is [1.0, 0.8695652173913043, 0.42969984202211703, 0.8833333333333334, 1.0, 1.0, 0.39464026071377795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19312491143038985, 0.19312491143039004, 0.27345248387535087], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.25652444], dtype=float32), -0.85232556]. 
=============================================
[2019-04-23 13:03:22,843] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1264375: loss -20.8956
[2019-04-23 13:03:22,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1264375: learning rate 0.0005
[2019-04-23 13:03:23,588] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1264829: loss -84.7760
[2019-04-23 13:03:23,590] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1264829: learning rate 0.0005
[2019-04-23 13:03:23,683] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1264884: loss -94.9554
[2019-04-23 13:03:23,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1264884: learning rate 0.0005
[2019-04-23 13:03:23,754] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1264925: loss -20.0068
[2019-04-23 13:03:23,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1264925: learning rate 0.0005
[2019-04-23 13:03:23,790] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1264944: loss 3.9841
[2019-04-23 13:03:23,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1264945: learning rate 0.0005
[2019-04-23 13:03:24,047] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1265099: loss -63.7990
[2019-04-23 13:03:24,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1265100: learning rate 0.0005
[2019-04-23 13:03:25,003] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:25,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7552
[2019-04-23 13:03:25,010] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1798428.376194603 W.
[2019-04-23 13:03:25,014] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 62.0, 1.0, 2.0, 0.6451867130160831, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.964961079596719, 6.9112, 168.9126365211864, 1798428.376194603, 1760288.488267592, 374544.0353675185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7646400.0000, 
sim time next is 7647000.0000, 
raw observation next is [30.16666666666667, 61.66666666666667, 1.0, 2.0, 0.6358519258206387, 1.0, 1.0, 0.6358519258206387, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1777897.052668389, 1777897.052668389, 347861.7088768844], 
processed observation next is [1.0, 0.5217391304347826, 0.6287519747235389, 0.6166666666666667, 1.0, 1.0, 0.5612673805067936, 1.0, 0.5, 0.5612673805067936, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4938602924078858, 0.4938602924078858, 0.5191965804132603], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49870443], dtype=float32), -1.0470904]. 
=============================================
[2019-04-23 13:03:25,039] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[43.413227]
 [43.321888]
 [42.660736]
 [42.20642 ]
 [41.72741 ]], R is [[42.59952927]
 [42.34570694]
 [42.4241066 ]
 [41.99986649]
 [42.04305649]].
[2019-04-23 13:03:25,395] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1265919: loss -31.3122
[2019-04-23 13:03:25,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1265919: learning rate 0.0005
[2019-04-23 13:03:25,461] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1265950: loss -26.5902
[2019-04-23 13:03:25,463] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1265950: learning rate 0.0005
[2019-04-23 13:03:27,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:27,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2199
[2019-04-23 13:03:27,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2030969.146128536 W.
[2019-04-23 13:03:27,989] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.3, 73.5, 1.0, 2.0, 0.4841837415024421, 1.0, 2.0, 0.4841837415024421, 1.0, 1.0, 0.8314703351433356, 6.9112, 6.9112, 170.5573041426782, 2030969.146128536, 2030969.146128536, 402736.9134216547], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7813800.0000, 
sim time next is 7814400.0000, 
raw observation next is [29.4, 73.0, 1.0, 2.0, 0.8641877418473689, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.987824012404884, 6.9112, 168.9124430735596, 2104910.439491148, 2050550.893130836, 424986.3956210082], 
processed observation next is [1.0, 0.43478260869565216, 0.5924170616113744, 0.73, 1.0, 1.0, 0.836370773310083, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007662401240488404, 0.0, 0.8294374239422116, 0.5846973443030967, 0.5695974703141211, 0.6343080531656838], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4136654], dtype=float32), 2.0705068]. 
=============================================
[2019-04-23 13:03:29,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:29,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:29,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-04-23 13:03:30,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:30,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3313
[2019-04-23 13:03:30,172] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2418050.347225101 W.
[2019-04-23 13:03:30,176] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 80.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.269608184639154, 6.9112, 168.9052210898378, 2418050.347225101, 1454392.961246871, 310956.4473081857], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7806600.0000, 
sim time next is 7807200.0000, 
raw observation next is [28.1, 80.0, 1.0, 2.0, 0.6957176480308327, 1.0, 1.0, 0.6957176480308327, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1945438.761530149, 1945438.761530149, 372293.6027864647], 
processed observation next is [1.0, 0.34782608695652173, 0.5308056872037916, 0.8, 1.0, 1.0, 0.6333947566636539, 1.0, 0.5, 0.6333947566636539, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5403996559805969, 0.5403996559805969, 0.5556620937111413], 
reward next is 0.4443, 
noisyNet noise sample is [array([0.9109679], dtype=float32), -1.5491694]. 
=============================================
[2019-04-23 13:03:32,215] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:32,217] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1692
[2019-04-23 13:03:32,219] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2219021.302160638 W.
[2019-04-23 13:03:32,224] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.53333333333333, 70.66666666666667, 1.0, 2.0, 0.9457140470523017, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.996699980279674, 6.9112, 168.9123809290254, 2219021.302160638, 2158364.880159256, 447403.6184901359], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7818000.0000, 
sim time next is 7818600.0000, 
raw observation next is [30.76666666666667, 70.33333333333333, 1.0, 2.0, 0.7869389088674783, 1.0, 1.0, 0.7869389088674783, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2200783.256037083, 2200783.256037083, 413564.7687123195], 
processed observation next is [1.0, 0.4782608695652174, 0.6571879936808849, 0.7033333333333333, 1.0, 1.0, 0.7432998902017811, 1.0, 0.5, 0.7432998902017811, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.611328682232523, 0.611328682232523, 0.6172608488243575], 
reward next is 0.3827, 
noisyNet noise sample is [array([-0.69477123], dtype=float32), -1.6610976]. 
=============================================
[2019-04-23 13:03:32,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:32,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1276
[2019-04-23 13:03:32,803] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 91.66666666666667, 1.0, 2.0, 0.7750309242955185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1083181.276651891, 1083181.276651891, 237872.7383336706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7872000.0000, 
sim time next is 7872600.0000, 
raw observation next is [26.08333333333334, 91.33333333333334, 1.0, 2.0, 0.7600381123005231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1062216.880056028, 1062216.880056028, 234336.6552992772], 
processed observation next is [1.0, 0.08695652173913043, 0.43522906793049004, 0.9133333333333334, 1.0, 1.0, 0.710889291928341, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2950602444600078, 0.2950602444600078, 0.3497562019392197], 
reward next is 0.6502, 
noisyNet noise sample is [array([-0.32283488], dtype=float32), 0.88651794]. 
=============================================
[2019-04-23 13:03:33,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:33,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1843
[2019-04-23 13:03:33,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1930840.478963529 W.
[2019-04-23 13:03:33,735] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.21666666666667, 78.50000000000001, 1.0, 2.0, 0.6905017864985799, 1.0, 2.0, 0.6905017864985799, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1930840.478963529, 1930840.478963528, 370080.8947752611], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7895400.0000, 
sim time next is 7896000.0000, 
raw observation next is [28.33333333333334, 78.0, 1.0, 2.0, 0.45868152972995, 1.0, 2.0, 0.45868152972995, 1.0, 1.0, 0.787090097996072, 6.9112, 6.9112, 170.5573041426782, 1923900.88530602, 1923900.88530602, 386187.4510447478], 
processed observation next is [1.0, 0.391304347826087, 0.5418641390205374, 0.78, 1.0, 1.0, 0.3478090719637952, 1.0, 1.0, 0.3478090719637952, 1.0, 0.5, 0.7403537780439903, 0.0, 0.0, 0.8375144448122397, 0.5344169125850056, 0.5344169125850056, 0.5763991806638027], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.64830285], dtype=float32), -0.16836433]. 
=============================================
[2019-04-23 13:03:33,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[39.198215]
 [39.26891 ]
 [38.88094 ]
 [40.031193]
 [44.30468 ]], R is [[39.11388397]
 [38.72274399]
 [38.33551788]
 [38.39883041]
 [38.08917618]].
[2019-04-23 13:03:34,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:34,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:34,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-04-23 13:03:34,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:34,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:34,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-04-23 13:03:34,546] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:34,546] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:34,554] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-04-23 13:03:34,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:34,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:34,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-04-23 13:03:34,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:34,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:34,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-04-23 13:03:34,797] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:34,797] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:34,800] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-04-23 13:03:35,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:35,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:35,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-04-23 13:03:36,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:36,172] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:36,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-04-23 13:03:36,513] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:36,517] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8630
[2019-04-23 13:03:36,525] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 85.0, 1.0, 2.0, 0.3098742515674188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499538.4435189567, 499538.4435189574, 167065.2828317584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 14400.0000, 
sim time next is 15000.0000, 
raw observation next is [21.23333333333333, 85.16666666666667, 1.0, 2.0, 0.2999811684528647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483166.3040385763, 483166.3040385763, 165885.2858995506], 
processed observation next is [1.0, 0.17391304347826086, 0.2053712480252764, 0.8516666666666667, 1.0, 1.0, 0.15660381741309, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13421286223293785, 0.13421286223293785, 0.24758997895455312], 
reward next is 0.7524, 
noisyNet noise sample is [array([-0.04744366], dtype=float32), 0.34680498]. 
=============================================
[2019-04-23 13:03:36,538] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[69.94276 ]
 [69.58892 ]
 [69.32448 ]
 [68.860374]
 [68.58094 ]], R is [[70.19161224]
 [70.24034882]
 [70.28840637]
 [70.33544159]
 [70.38076019]].
[2019-04-23 13:03:36,730] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:36,731] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:36,765] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-04-23 13:03:36,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:36,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:36,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-04-23 13:03:37,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:37,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:37,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-04-23 13:03:37,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:37,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:37,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-04-23 13:03:37,269] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:37,269] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:37,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-04-23 13:03:38,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:38,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:38,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run7
[2019-04-23 13:03:38,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 13:03:38,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:38,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-04-23 13:03:39,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:39,242] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4491
[2019-04-23 13:03:39,245] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 86.33333333333334, 1.0, 2.0, 0.3523763629921491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 544875.285667902, 544875.2856679014, 170276.4894332824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 78000.0000, 
sim time next is 78600.0000, 
raw observation next is [22.76666666666667, 86.66666666666666, 1.0, 2.0, 0.3517301414491896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544213.7421216262, 544213.7421216267, 170230.8915424977], 
processed observation next is [1.0, 0.9130434782608695, 0.2780410742496052, 0.8666666666666666, 1.0, 1.0, 0.21895197764962604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15117048392267393, 0.1511704839226741, 0.25407595752611595], 
reward next is 0.7459, 
noisyNet noise sample is [array([-0.9072797], dtype=float32), 0.2588713]. 
=============================================
[2019-04-23 13:03:40,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:03:40,365] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6612
[2019-04-23 13:03:40,369] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.58333333333334, 90.83333333333334, 1.0, 2.0, 0.3727829568078108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571299.4506704927, 571299.4506704921, 172387.3284338278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [22.6, 91.0, 1.0, 2.0, 0.3666308478678393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561238.6564266729, 561238.6564266729, 171500.5371181019], 
processed observation next is [1.0, 0.21739130434782608, 0.27014218009478685, 0.91, 1.0, 1.0, 0.23690463598534853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15589962678518693, 0.15589962678518693, 0.25597095092254013], 
reward next is 0.7440, 
noisyNet noise sample is [array([-0.16139874], dtype=float32), -0.79591936]. 
=============================================
[2019-04-23 13:03:41,853] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-23 13:03:41,862] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 13:03:41,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:41,866] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 13:03:41,867] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 13:03:41,868] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 13:03:41,868] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 13:03:41,868] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:41,869] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:41,870] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:41,870] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:03:41,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run52
[2019-04-23 13:03:41,894] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run52
[2019-04-23 13:03:41,909] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run52
[2019-04-23 13:03:41,927] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run52
[2019-04-23 13:03:41,953] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run52
[2019-04-23 13:03:44,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.21483304]
[2019-04-23 13:03:44,869] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.22887276666667, 95.01452770666667, 1.0, 2.0, 0.3991407738487954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601324.0459394564, 601324.0459394564, 174813.5146888203]
[2019-04-23 13:03:44,869] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:03:44,871] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5398452500553073
[2019-04-23 13:04:09,912] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.21483304]
[2019-04-23 13:04:09,913] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 88.5, 1.0, 2.0, 0.5302109986076802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740902.2149629525, 740902.2149629525, 188464.9146359523]
[2019-04-23 13:04:09,915] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 13:04:09,917] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7011257737659291
[2019-04-23 13:04:19,495] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.21483304]
[2019-04-23 13:04:19,497] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.4, 59.0, 1.0, 2.0, 0.5150039481535555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719645.1041867231, 719645.1041867237, 185979.9508038234]
[2019-04-23 13:04:19,509] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 13:04:19,511] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2646918816459646
[2019-04-23 13:04:22,597] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.21483304]
[2019-04-23 13:04:22,601] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.71608857, 78.74639636, 1.0, 2.0, 0.5158855596107621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720877.4493954248, 720877.4493954248, 186121.8658280396]
[2019-04-23 13:04:22,618] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 13:04:22,621] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0019245650123044244
[2019-04-23 13:04:34,994] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.21483304]
[2019-04-23 13:04:34,994] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.43333333333334, 71.33333333333334, 1.0, 2.0, 0.837247603578221, 1.0, 1.0, 0.837247603578221, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2341631.809740596, 2341631.809740596, 438124.5048235145]
[2019-04-23 13:04:34,995] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 13:04:34,999] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5274998663274441
[2019-04-23 13:04:35,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2341631.809740596 W.
[2019-04-23 13:04:38,113] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.21483304]
[2019-04-23 13:04:38,145] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.734030244170523, 6.9112, 168.908379517061, 2037883.063692408, 1454154.786418432, 311359.7392764043]
[2019-04-23 13:04:38,150] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:04:38,151] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0859021428998139
[2019-04-23 13:04:38,153] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2037883.063692408 W.
[2019-04-23 13:06:30,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 13:06:31,341] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 13:06:31,742] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 13:06:31,935] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 13:06:32,242] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 13:06:33,257] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1275000, evaluation results [1275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 13:06:58,753] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:06:58,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8160
[2019-04-23 13:06:58,853] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 91.0, 1.0, 2.0, 0.2845031306979766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457763.0814410559, 457763.0814410559, 164120.3348613558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 255000.0000, 
sim time next is 255600.0000, 
raw observation next is [20.5, 91.0, 1.0, 2.0, 0.2833729599606403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456234.4313794903, 456234.4313794903, 164017.6541141958], 
processed observation next is [0.0, 1.0, 0.1706161137440759, 0.91, 1.0, 1.0, 0.13659392766342207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12673178649430286, 0.12673178649430286, 0.2448024688271579], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.13138863], dtype=float32), 0.080786265]. 
=============================================
[2019-04-23 13:07:08,182] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:07:08,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2423
[2019-04-23 13:07:08,232] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [20.15, 88.5, 1.0, 2.0, 0.2553416395063423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415827.6439295192, 415827.6439295192, 161372.9221402556], 
processed observation next is [1.0, 0.21739130434782608, 0.15402843601895733, 0.885, 1.0, 1.0, 0.10282125241727988, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11550767886931089, 0.11550767886931089, 0.24085510767202328], 
reward next is 0.7591, 
noisyNet noise sample is [array([1.9159367], dtype=float32), 2.0837247]. 
=============================================
[2019-04-23 13:07:17,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:07:17,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0216
[2019-04-23 13:07:17,154] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 86.0, 1.0, 2.0, 0.245100758040299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403372.8761155003, 403372.8761155003, 160422.9433864815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 430200.0000, 
sim time next is 430800.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.244680711747906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 402682.2676706034, 402682.2676706034, 160382.3105867617], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.08997676114205543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1118561854640565, 0.1118561854640565, 0.239376582965316], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.87401396], dtype=float32), -0.017544424]. 
=============================================
[2019-04-23 13:07:19,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:07:19,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7593
[2019-04-23 13:07:19,898] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.95, 81.83333333333334, 1.0, 2.0, 0.2303358724793361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 380295.462992765, 380295.4629927656, 158986.5359185053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 454200.0000, 
sim time next is 454800.0000, 
raw observation next is [20.0, 81.66666666666667, 1.0, 2.0, 0.2308156578175492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380968.9817660752, 380968.9817660752, 159036.7521455919], 
processed observation next is [1.0, 0.2608695652173913, 0.1469194312796209, 0.8166666666666668, 1.0, 1.0, 0.07327187688861349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1058247171572431, 0.1058247171572431, 0.2373682867844655], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.9829923], dtype=float32), -1.5153867]. 
=============================================
[2019-04-23 13:07:55,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:07:55,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2894
[2019-04-23 13:07:55,233] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 73.83333333333333, 1.0, 2.0, 0.2422879635159711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399902.8632683962, 399902.8632683962, 160113.5336271545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 676200.0000, 
sim time next is 676800.0000, 
raw observation next is [20.8, 75.0, 1.0, 2.0, 0.2413351189298515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398553.193688629, 398553.1936886283, 160011.0346077084], 
processed observation next is [1.0, 0.8695652173913043, 0.1848341232227489, 0.75, 1.0, 1.0, 0.0859459264215078, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11070922046906362, 0.11070922046906342, 0.2388224397129976], 
reward next is 0.7612, 
noisyNet noise sample is [array([-0.90136904], dtype=float32), -0.6759899]. 
=============================================
[2019-04-23 13:07:56,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:07:56,475] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8756
[2019-04-23 13:07:56,528] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 80.0, 1.0, 2.0, 0.287250080838597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460179.4505003898, 460179.4505003898, 164271.1997131335], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [22.3, 79.66666666666667, 1.0, 2.0, 0.2894266845643926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463172.4831846419, 463172.4831846413, 164471.3739529768], 
processed observation next is [0.0, 0.30434782608695654, 0.25592417061611383, 0.7966666666666667, 1.0, 1.0, 0.1438875717643284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.128659023106845, 0.1286590231068448, 0.2454796626163833], 
reward next is 0.7545, 
noisyNet noise sample is [array([1.9815286], dtype=float32), 0.92557466]. 
=============================================
[2019-04-23 13:08:08,396] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:08:08,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5031
[2019-04-23 13:08:08,403] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 82.66666666666667, 1.0, 2.0, 0.2808857387208815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 452917.6797387828, 452917.6797387835, 163795.2069979346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 804000.0000, 
sim time next is 804600.0000, 
raw observation next is [21.65, 82.0, 1.0, 2.0, 0.2824387353538764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 454906.8589584904, 454906.8589584911, 163928.4994987001], 
processed observation next is [0.0, 0.30434782608695654, 0.22511848341232227, 0.82, 1.0, 1.0, 0.13546835584804387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12636301637735844, 0.12636301637735864, 0.24466940223686584], 
reward next is 0.7553, 
noisyNet noise sample is [array([0.487906], dtype=float32), 0.03782399]. 
=============================================
[2019-04-23 13:08:14,586] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:08:14,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3906
[2019-04-23 13:08:14,598] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.65, 82.5, 1.0, 2.0, 0.2830539234658758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455198.6208543547, 455198.6208543541, 163945.9648696177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 887400.0000, 
sim time next is 888000.0000, 
raw observation next is [21.73333333333333, 82.0, 1.0, 2.0, 0.282858354072015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454739.1531429821, 454739.1531429821, 163914.2444039217], 
processed observation next is [0.0, 0.2608695652173913, 0.22906793048973137, 0.82, 1.0, 1.0, 0.13597392056869273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12631643142860613, 0.12631643142860613, 0.24464812597600255], 
reward next is 0.7554, 
noisyNet noise sample is [array([-0.25065792], dtype=float32), 0.96246594]. 
=============================================
[2019-04-23 13:08:14,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.910965]
 [78.91043 ]
 [78.91267 ]
 [78.96595 ]
 [78.88856 ]], R is [[78.88274384]
 [78.84922028]
 [78.8159256 ]
 [78.78286743]
 [78.75024414]].
[2019-04-23 13:08:18,670] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-23 13:08:18,671] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 13:08:18,672] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 13:08:18,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:08:18,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 13:08:18,673] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:08:18,674] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 13:08:18,673] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 13:08:18,674] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:08:18,677] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:08:18,677] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:08:18,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run53
[2019-04-23 13:08:18,712] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run53
[2019-04-23 13:08:18,736] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run53
[2019-04-23 13:08:18,763] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run53
[2019-04-23 13:08:18,786] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run53
[2019-04-23 13:08:29,418] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:08:29,420] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.55, 80.5, 1.0, 2.0, 0.3178003351444019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504092.3839376875, 504092.3839376881, 167359.2788555816]
[2019-04-23 13:08:29,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 13:08:29,422] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22109625499799546
[2019-04-23 13:08:34,905] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:08:34,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.33333333333333, 98.66666666666667, 1.0, 2.0, 0.3104452244773311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492129.4430516197, 492129.4430516197, 166464.6233860319]
[2019-04-23 13:08:34,907] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:08:34,909] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3509262548402263
[2019-04-23 13:08:38,837] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:08:38,840] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.1, 88.0, 1.0, 2.0, 0.4644061241375815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658324.6432128618, 658324.6432128613, 179418.0188848631]
[2019-04-23 13:08:38,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:08:38,843] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8648338979907269
[2019-04-23 13:09:07,575] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:09:07,576] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.08970121, 88.62894560000001, 1.0, 2.0, 0.5995594881719084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837846.0997881646, 837846.0997881652, 200664.7629977498]
[2019-04-23 13:09:07,576] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:09:07,579] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4512659197041283
[2019-04-23 13:09:08,274] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:09:08,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.69261707, 70.25925808333334, 1.0, 2.0, 0.6553733142406275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 915875.987119186, 915875.987119186, 211511.9906836282]
[2019-04-23 13:09:08,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:09:08,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44496568414300064
[2019-04-23 13:09:10,598] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:09:10,600] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.66666666666667, 70.33333333333334, 1.0, 2.0, 0.6366983093544176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889766.9698932037, 889766.9698932037, 207781.9870369314]
[2019-04-23 13:09:10,600] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 13:09:10,602] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.195076208634959
[2019-04-23 13:09:25,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:09:25,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.33333333333334, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.398950685628835, 6.9112, 168.9101274769019, 1800013.25467753, 1453991.926634177, 311351.4231580564]
[2019-04-23 13:09:25,323] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 13:09:25,324] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6923566928353115
[2019-04-23 13:09:25,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1800013.25467753 W.
[2019-04-23 13:09:26,101] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20845982]
[2019-04-23 13:09:26,102] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.73333333333333, 75.66666666666666, 1.0, 2.0, 0.7818989314659484, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980946797585654, 6.9112, 168.9124858902317, 1989740.577423176, 1940259.936578258, 404311.8147855383]
[2019-04-23 13:09:26,102] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:09:26,106] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19083135637376858
[2019-04-23 13:09:26,108] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1989740.577423176 W.
[2019-04-23 13:09:43,689] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 13:09:44,187] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 13:09:44,347] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 13:09:44,426] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 13:09:44,486] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 13:09:45,502] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1300000, evaluation results [1300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 13:09:47,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:09:47,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2535
[2019-04-23 13:09:47,852] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 96.16666666666666, 1.0, 2.0, 0.3564194821925666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546522.4986521256, 546522.4986521256, 170282.1450057607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1026600.0000, 
sim time next is 1027200.0000, 
raw observation next is [21.9, 96.33333333333333, 1.0, 2.0, 0.3566509502024217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546549.2769441827, 546549.2769441821, 170274.4045019448], 
processed observation next is [1.0, 0.9130434782608695, 0.23696682464454974, 0.9633333333333333, 1.0, 1.0, 0.224880662894484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1518192435956063, 0.15181924359560614, 0.25414090224170865], 
reward next is 0.7459, 
noisyNet noise sample is [array([-0.5000788], dtype=float32), -0.6588105]. 
=============================================
[2019-04-23 13:10:09,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:10:09,032] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3456
[2019-04-23 13:10:09,128] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 78.0, 1.0, 2.0, 0.6821492543553236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1071185.225632032, 1071185.225632032, 231280.6963054911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1155600.0000, 
sim time next is 1156200.0000, 
raw observation next is [23.58333333333333, 77.16666666666667, 1.0, 2.0, 0.6333834938920408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 992957.2007269862, 992957.2007269862, 219962.0367019851], 
processed observation next is [1.0, 0.391304347826087, 0.31674565560821466, 0.7716666666666667, 1.0, 1.0, 0.5582933661349889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27582144464638503, 0.27582144464638503, 0.3283015473163957], 
reward next is 0.6717, 
noisyNet noise sample is [array([0.29564014], dtype=float32), 0.33032694]. 
=============================================
[2019-04-23 13:10:16,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:10:16,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6517
[2019-04-23 13:10:16,871] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 94.0, 1.0, 2.0, 0.4646238933654218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655924.6479070106, 655924.6479070106, 179103.355951271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1292400.0000, 
sim time next is 1293000.0000, 
raw observation next is [24.38333333333333, 94.00000000000001, 1.0, 2.0, 0.4636476081755364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654985.2056787602, 654985.2056787602, 179015.5343339711], 
processed observation next is [1.0, 1.0, 0.3546603475513427, 0.9400000000000002, 1.0, 1.0, 0.35379229900667036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18194033491076672, 0.18194033491076672, 0.2671873646775688], 
reward next is 0.7328, 
noisyNet noise sample is [array([-0.94807], dtype=float32), -0.5998653]. 
=============================================
[2019-04-23 13:10:16,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.251625]
 [74.17577 ]
 [74.188545]
 [74.20071 ]
 [74.22183 ]], R is [[74.18700409]
 [74.1778183 ]
 [74.16864014]
 [74.15940857]
 [74.1500473 ]].
[2019-04-23 13:10:20,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:10:20,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6127
[2019-04-23 13:10:20,361] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1773119.99075984 W.
[2019-04-23 13:10:20,413] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.15, 71.5, 1.0, 2.0, 0.422763237877834, 1.0, 1.0, 0.422763237877834, 1.0, 1.0, 0.7151079315003558, 6.9112, 6.9112, 170.5573041426782, 1773119.99075984, 1773119.99075984, 362943.4841148028], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [28.16666666666666, 71.66666666666667, 1.0, 2.0, 0.6049978857445968, 0.0, 1.0, 0.0, 1.0, 2.0, 1.014116900509415, 6.9112, 6.9112, 168.9129562807476, 1691571.389548041, 1691571.389548041, 362629.2230127604], 
processed observation next is [1.0, 0.4782608695652174, 0.5339652448657185, 0.7166666666666667, 1.0, 1.0, 0.5240938382465021, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.017215732328555, 0.0, 0.0, 0.8294399440244521, 0.4698809415411225, 0.4698809415411225, 0.5412376462877021], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.13999647], dtype=float32), -1.0389179]. 
=============================================
[2019-04-23 13:10:34,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:10:34,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7752
[2019-04-23 13:10:34,605] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 87.5, 1.0, 2.0, 0.3711971764614211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561635.2414356708, 561635.2414356708, 171336.9893165899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1416600.0000, 
sim time next is 1417200.0000, 
raw observation next is [23.66666666666667, 88.0, 1.0, 2.0, 0.379955787005837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570553.6670177527, 570553.6670177534, 171980.2106871535], 
processed observation next is [0.0, 0.391304347826087, 0.3206951026856243, 0.88, 1.0, 1.0, 0.2529587795251048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1584871297271535, 0.1584871297271537, 0.25668688162261716], 
reward next is 0.7433, 
noisyNet noise sample is [array([0.15730426], dtype=float32), -0.9445609]. 
=============================================
[2019-04-23 13:11:02,845] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:02,884] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6399
[2019-04-23 13:11:02,952] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3271180828983243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512930.2020848742, 512930.2020848735, 167913.7722458612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1558800.0000, 
sim time next is 1559400.0000, 
raw observation next is [21.7, 91.00000000000001, 1.0, 2.0, 0.3264188989787366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511970.4120501378, 511970.4120501378, 167843.0780645584], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.9100000000000001, 1.0, 1.0, 0.18845650479365855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1422140033472605, 0.1422140033472605, 0.2505120568127737], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.31058872], dtype=float32), 0.96987355]. 
=============================================
[2019-04-23 13:11:06,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:06,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8356
[2019-04-23 13:11:06,301] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.35, 84.5, 1.0, 2.0, 0.3581996044590471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549699.2639673849, 549699.2639673844, 170561.4200263185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [23.26666666666667, 85.0, 1.0, 2.0, 0.3581687164955957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549893.545817771, 549893.5458177704, 170584.802836403], 
processed observation next is [0.0, 0.8695652173913043, 0.3017377567140602, 0.85, 1.0, 1.0, 0.2267092969826454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15274820717160306, 0.1527482071716029, 0.2546041833379149], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.0047878], dtype=float32), -0.300561]. 
=============================================
[2019-04-23 13:11:17,569] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:17,592] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2456
[2019-04-23 13:11:17,769] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 99.00000000000001, 1.0, 2.0, 0.4387657657414944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632192.7737544909, 632192.7737544916, 177013.8418019125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.4301957118243803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619853.3233610058, 619853.3233610058, 175798.5548519476], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 1.0, 1.0, 0.3134888094269642, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17218147871139047, 0.17218147871139047, 0.2623859027641009], 
reward next is 0.7376, 
noisyNet noise sample is [array([0.30849725], dtype=float32), -0.10267127]. 
=============================================
[2019-04-23 13:11:19,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:19,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5491
[2019-04-23 13:11:19,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 92.5, 1.0, 2.0, 0.844772704658518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1182885.458558733, 1182885.458558734, 255509.3238640414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [24.93333333333333, 92.0, 1.0, 2.0, 0.8680501965999639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1213259.050937398, 1213259.050937398, 261296.2513159847], 
processed observation next is [1.0, 0.391304347826087, 0.38072669826224315, 0.92, 1.0, 1.0, 0.8410243332529684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33701640303816616, 0.33701640303816616, 0.3899944049492309], 
reward next is 0.6100, 
noisyNet noise sample is [array([-1.1608156], dtype=float32), -1.3199003]. 
=============================================
[2019-04-23 13:11:21,109] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:21,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7453
[2019-04-23 13:11:21,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1901673.675349574 W.
[2019-04-23 13:11:21,166] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.63333333333334, 76.0, 1.0, 2.0, 0.4533869894424082, 1.0, 2.0, 0.4533869894424082, 1.0, 2.0, 0.7781101516628173, 6.9112, 6.9112, 170.5573041426782, 1901673.675349574, 1901673.675349574, 382902.7245329102], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1700400.0000, 
sim time next is 1701000.0000, 
raw observation next is [28.7, 75.5, 1.0, 2.0, 0.4646294954708465, 1.0, 2.0, 0.4646294954708465, 1.0, 2.0, 0.7972613590084087, 6.9112, 6.9112, 170.5573041426782, 1948871.815405815, 1948871.815405815, 389936.4894179892], 
processed observation next is [1.0, 0.6956521739130435, 0.5592417061611374, 0.755, 1.0, 1.0, 0.35497529574800785, 1.0, 1.0, 0.35497529574800785, 1.0, 1.0, 0.7527577548883031, 0.0, 0.0, 0.8375144448122397, 0.5413532820571708, 0.5413532820571708, 0.581994760325357], 
reward next is 0.4180, 
noisyNet noise sample is [array([-1.3815126], dtype=float32), 2.0264142]. 
=============================================
[2019-04-23 13:11:21,243] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[54.917263]
 [55.094044]
 [57.300137]
 [54.83032 ]
 [57.12273 ]], R is [[54.67104721]
 [54.55284119]
 [54.00731277]
 [53.51371002]
 [52.97857285]].
[2019-04-23 13:11:26,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:26,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9907
[2019-04-23 13:11:26,466] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.36666666666667, 99.0, 1.0, 2.0, 0.4450125520757766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639576.0243611212, 639576.0243611212, 177710.6118858715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1658400.0000, 
sim time next is 1659000.0000, 
raw observation next is [23.38333333333333, 99.0, 1.0, 2.0, 0.4489660344023746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644802.908381624, 644802.9083816233, 178227.8249370781], 
processed observation next is [1.0, 0.17391304347826086, 0.30726698262243274, 0.99, 1.0, 1.0, 0.3361036559064754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17911191899489556, 0.17911191899489537, 0.26601167901056433], 
reward next is 0.7340, 
noisyNet noise sample is [array([-1.1216435], dtype=float32), -0.39188576]. 
=============================================
[2019-04-23 13:11:26,575] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.802734]
 [74.93507 ]
 [74.9845  ]
 [75.05217 ]
 [75.12869 ]], R is [[74.734375  ]
 [74.7217865 ]
 [74.71131897]
 [74.7019577 ]
 [74.68911743]].
[2019-04-23 13:11:27,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:27,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2758
[2019-04-23 13:11:27,213] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 92.33333333333334, 1.0, 2.0, 0.5182720004318683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731460.8390015797, 731460.839001579, 187437.0584605661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1750800.0000, 
sim time next is 1751400.0000, 
raw observation next is [24.7, 92.0, 1.0, 2.0, 0.575435989696104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811331.1134664734, 811331.1134664734, 197185.1258892216], 
processed observation next is [1.0, 0.2608695652173913, 0.3696682464454976, 0.92, 1.0, 1.0, 0.48847709601940237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22536975374068705, 0.22536975374068705, 0.2943061580436144], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.6366219], dtype=float32), -0.7980993]. 
=============================================
[2019-04-23 13:11:28,559] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:11:28,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5616
[2019-04-23 13:11:28,624] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1680000.0000, 
sim time next is 1680600.0000, 
raw observation next is [25.78333333333333, 88.5, 1.0, 2.0, 0.9213328693092347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287776.504167697, 1287776.504167696, 275870.1029602659], 
processed observation next is [1.0, 0.43478260869565216, 0.4210110584518167, 0.885, 1.0, 1.0, 0.9052203244689575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35771569560213806, 0.3577156956021378, 0.411746422328755], 
reward next is 0.5883, 
noisyNet noise sample is [array([2.1399834], dtype=float32), -0.86831385]. 
=============================================
[2019-04-23 13:12:03,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:12:03,629] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8685
[2019-04-23 13:12:03,629] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1761198.375544957 W.
[2019-04-23 13:12:03,754] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.73333333333333, 77.33333333333334, 1.0, 2.0, 0.6298796340738809, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.932789024708686, 6.9112, 168.9128065667745, 1761198.375544957, 1745882.391975706, 371738.656506504], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1943400.0000, 
sim time next is 1944000.0000, 
raw observation next is [26.8, 77.0, 1.0, 2.0, 0.6387597801656563, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.952783211129443, 6.9112, 168.912688243027, 1791330.182594041, 1761829.668287904, 373813.1530684388], 
processed observation next is [1.0, 0.5217391304347826, 0.4691943127962086, 0.77, 1.0, 1.0, 0.5647708194766943, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004158321112944297, 0.0, 0.8294386278365357, 0.4975917173872336, 0.4893971300799733, 0.5579300792066251], 
reward next is 0.2342, 
noisyNet noise sample is [array([-0.09044978], dtype=float32), 0.32447314]. 
=============================================
[2019-04-23 13:12:03,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[60.002827]
 [60.601425]
 [60.828197]
 [61.46748 ]
 [61.403446]], R is [[60.35292053]
 [59.74939346]
 [59.15190125]
 [58.56038284]
 [57.97478104]].
[2019-04-23 13:12:12,638] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-23 13:12:12,672] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 13:12:12,674] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:12:12,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run54
[2019-04-23 13:12:12,704] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 13:12:12,793] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:12:12,808] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run54
[2019-04-23 13:12:12,875] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 13:12:12,875] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:12:12,879] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run54
[2019-04-23 13:12:13,009] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 13:12:13,009] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:12:13,024] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run54
[2019-04-23 13:12:13,193] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 13:12:13,193] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:12:13,199] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run54
[2019-04-23 13:12:45,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.18510714]
[2019-04-23 13:12:45,490] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.4, 85.33333333333334, 1.0, 2.0, 0.2905908895111302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466285.5112007728, 466285.5112007728, 164697.7362576275]
[2019-04-23 13:12:45,492] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 13:12:45,495] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09227206646337272
[2019-04-23 13:13:03,688] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.18510714]
[2019-04-23 13:13:03,688] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.30551002333333, 91.27663651333333, 1.0, 2.0, 0.4536559246690051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633894.3497653095, 633894.3497653095, 176654.7649868357]
[2019-04-23 13:13:03,693] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 13:13:03,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8071281457719367
[2019-04-23 13:13:35,066] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.18510714]
[2019-04-23 13:13:35,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.00000000000001, 1.0, 2.0, 0.3875935231170022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597063.342476695, 597063.342476695, 174739.5927327472]
[2019-04-23 13:13:35,069] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:13:35,090] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9529280745192261
[2019-04-23 13:14:06,636] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.18510714]
[2019-04-23 13:14:06,637] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.925661275, 83.05831145833334, 1.0, 2.0, 0.6475381290183032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 904921.7492643262, 904921.7492643262, 209937.2233065879]
[2019-04-23 13:14:06,637] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 13:14:06,640] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9790919700750103
[2019-04-23 13:14:28,475] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.18510714]
[2019-04-23 13:14:28,476] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.6, 73.5, 1.0, 2.0, 0.5472840287919637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764768.1844707952, 764768.1844707945, 191335.7299789043]
[2019-04-23 13:14:28,477] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 13:14:28,479] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7462514096292048
[2019-04-23 13:14:31,988] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 13:14:32,089] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 13:14:32,109] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 13:14:32,139] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 13:14:32,203] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 13:14:33,215] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1325000, evaluation results [1325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 13:14:33,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:33,223] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7308
[2019-04-23 13:14:33,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 86.83333333333333, 1.0, 2.0, 0.4950296954803407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691724.8263710734, 691724.8263710728, 182820.9481054765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2051400.0000, 
sim time next is 2052000.0000, 
raw observation next is [26.1, 87.0, 1.0, 2.0, 0.4940154345735584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690307.0984736709, 690307.0984736709, 182663.6414778271], 
processed observation next is [0.0, 0.782608695652174, 0.4360189573459717, 0.87, 1.0, 1.0, 0.39038004165488965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1917519717982419, 0.1917519717982419, 0.2726323007131748], 
reward next is 0.7274, 
noisyNet noise sample is [array([-1.2988122], dtype=float32), -1.5489471]. 
=============================================
[2019-04-23 13:14:33,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.62457 ]
 [76.60635 ]
 [76.58028 ]
 [76.56033 ]
 [76.538956]], R is [[76.68668365]
 [76.64694977]
 [76.60739136]
 [76.56796265]
 [76.52864838]].
[2019-04-23 13:14:37,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:37,615] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5950
[2019-04-23 13:14:37,620] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 76.0, 1.0, 2.0, 0.5622836267046358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785736.1964479298, 785736.1964479298, 193930.428104229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2120400.0000, 
sim time next is 2121000.0000, 
raw observation next is [30.0, 76.16666666666667, 1.0, 2.0, 0.5627679389941809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786413.2258867561, 786413.2258867561, 194015.3221651925], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.7616666666666667, 1.0, 1.0, 0.47321438433033836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2184481183018767, 0.2184481183018767, 0.28957510770924255], 
reward next is 0.7104, 
noisyNet noise sample is [array([-0.34476012], dtype=float32), -0.44435573]. 
=============================================
[2019-04-23 13:14:37,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.51001 ]
 [74.41    ]
 [74.396194]
 [74.38478 ]
 [74.37644 ]], R is [[74.45198059]
 [74.4180069 ]
 [74.38446045]
 [74.3514328 ]
 [74.31912231]].
[2019-04-23 13:14:41,900] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:41,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7198
[2019-04-23 13:14:41,915] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 94.66666666666666, 1.0, 2.0, 0.8384846329673163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1171912.958737343, 1171912.958737343, 253573.8587463729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2169600.0000, 
sim time next is 2170200.0000, 
raw observation next is [25.05, 94.83333333333333, 1.0, 2.0, 0.7996795206832504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117648.222401079, 1117648.222401079, 243829.3396713943], 
processed observation next is [1.0, 0.08695652173913043, 0.3862559241706162, 0.9483333333333333, 1.0, 1.0, 0.7586500249195788, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31045783955585526, 0.31045783955585526, 0.3639243875692452], 
reward next is 0.6361, 
noisyNet noise sample is [array([-0.20266028], dtype=float32), 1.0561839]. 
=============================================
[2019-04-23 13:14:44,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:44,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7948
[2019-04-23 13:14:44,247] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 78.0, 1.0, 2.0, 0.6397260890291292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893999.9828443662, 893999.9828443662, 208370.7297452536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2271600.0000, 
sim time next is 2272200.0000, 
raw observation next is [27.76666666666667, 77.33333333333333, 1.0, 2.0, 0.6467653924060522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 903841.4056028577, 903841.4056028584, 209771.9709028536], 
processed observation next is [1.0, 0.30434782608695654, 0.515007898894155, 0.7733333333333333, 1.0, 1.0, 0.5744161354289786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2510670571119049, 0.25106705711190513, 0.3130924938848561], 
reward next is 0.6869, 
noisyNet noise sample is [array([-1.2103295], dtype=float32), 0.89444065]. 
=============================================
[2019-04-23 13:14:50,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:50,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-23 13:14:50,733] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.55, 82.00000000000001, 1.0, 2.0, 0.7674307300008373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1072553.899814714, 1072553.899814714, 236072.1416464229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2344200.0000, 
sim time next is 2344800.0000, 
raw observation next is [27.5, 82.0, 1.0, 2.0, 0.7319434201240673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1022933.284611925, 1022933.284611925, 227889.9815868014], 
processed observation next is [1.0, 0.13043478260869565, 0.5023696682464456, 0.82, 1.0, 1.0, 0.6770402652097196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2841481346144236, 0.2841481346144236, 0.340134300875823], 
reward next is 0.6599, 
noisyNet noise sample is [array([-1.3008138], dtype=float32), 0.96816367]. 
=============================================
[2019-04-23 13:14:51,404] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:51,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0425
[2019-04-23 13:14:51,420] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412600.0000, 
sim time next is 2413200.0000, 
raw observation next is [29.63333333333334, 78.66666666666667, 1.0, 2.0, 0.5723458124047743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799802.4074891193, 799802.4074891187, 195707.1499520162], 
processed observation next is [1.0, 0.9565217391304348, 0.6034755134281204, 0.7866666666666667, 1.0, 1.0, 0.4847539908491257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22216733541364425, 0.22216733541364408, 0.2921002238089794], 
reward next is 0.7079, 
noisyNet noise sample is [array([2.3492608], dtype=float32), -0.2584718]. 
=============================================
[2019-04-23 13:14:58,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:58,185] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-23 13:14:58,189] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 86.0, 1.0, 2.0, 0.5349974238189689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747592.9870983448, 747592.9870983455, 189261.0103431962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2579400.0000, 
sim time next is 2580000.0000, 
raw observation next is [27.26666666666667, 86.33333333333334, 1.0, 2.0, 0.5327858104983352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744501.4469203365, 744501.4469203365, 188892.3380741523], 
processed observation next is [1.0, 0.8695652173913043, 0.4913112164297, 0.8633333333333334, 1.0, 1.0, 0.4370913379498014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20680595747787123, 0.20680595747787123, 0.2819288627972422], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.4412058], dtype=float32), -1.0216706]. 
=============================================
[2019-04-23 13:14:58,203] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.07653 ]
 [72.48728 ]
 [72.69921 ]
 [72.691734]
 [72.91215 ]], R is [[72.03797913]
 [72.0351181 ]
 [72.03192139]
 [72.02870941]
 [72.02532959]].
[2019-04-23 13:14:58,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:14:58,265] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4826
[2019-04-23 13:14:58,268] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4757732070636331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664937.0254277509, 664937.0254277509, 179903.5648397444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4755530615936887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664629.3345159743, 664629.3345159743, 179870.6752010286], 
processed observation next is [0.0, 0.391304347826087, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36813621878757674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18461925958777065, 0.18461925958777065, 0.2684636943298935], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.5723777], dtype=float32), 1.2919513]. 
=============================================
[2019-04-23 13:15:09,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:15:09,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-23 13:15:09,033] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3502677172761182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539576.9196183477, 539576.9196183482, 169781.7801830027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2853000.0000, 
sim time next is 2853600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3502578939371979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539562.0788390585, 539562.0788390585, 169780.5686765022], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2171781854665035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1498783552330718, 0.1498783552330718, 0.2534038338455257], 
reward next is 0.7466, 
noisyNet noise sample is [array([1.0373456], dtype=float32), -1.113278]. 
=============================================
[2019-04-23 13:15:13,434] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 13:15:13,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5102
[2019-04-23 13:15:13,447] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3102961502405507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490680.4228398422, 490680.4228398422, 166333.9938178167], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2950200.0000, 
sim time next is 2950800.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3103253255364233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490725.9357552922, 490725.9357552929, 166337.3279105634], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16906665727279913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1363127599320256, 0.1363127599320258, 0.24826466852322898], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.14311975], dtype=float32), 1.2024691]. 
=============================================
[2019-04-23 13:15:14,242] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-23 13:15:14,246] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 13:15:14,247] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 13:15:14,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:15:14,248] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:15:14,248] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 13:15:14,249] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 13:15:14,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 13:15:14,255] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:15:14,257] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:15:14,255] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 13:15:14,269] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run55
[2019-04-23 13:15:14,290] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run55
[2019-04-23 13:15:14,290] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run55
[2019-04-23 13:15:14,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run55
[2019-04-23 13:15:14,366] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run55
[2019-04-23 13:15:15,440] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.18387288]
[2019-04-23 13:15:15,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.95, 88.16666666666667, 1.0, 2.0, 0.3358250999682243, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524627.6354652327, 524627.6354652327, 168780.1232804742]
[2019-04-23 13:15:15,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 13:15:15,442] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8765067103817079
