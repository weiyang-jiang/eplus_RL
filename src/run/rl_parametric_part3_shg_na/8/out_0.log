Using TensorFlow backend.
[2019-03-26 23:50:47,670] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0005, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-26 23:50:47,670] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-26 23:50:47.711990: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-26 23:51:06,512] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-26 23:51:06,512] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-26 23:51:06,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-26 23:51:06,527] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-26 23:51:06,532] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-26 23:51:06,537] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-26 23:51:06,539] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-26 23:51:06,540] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:06,540] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-26 23:51:06,599] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:06,599] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-26 23:51:07,541] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:07,543] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-26 23:51:07,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:07,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-26 23:51:07,859] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 23:51:07,860] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:51:07,860] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:51:07,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:07,861] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:51:07,861] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:51:07,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:51:07,861] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:07,862] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:07,862] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:07,863] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:07,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-26 23:51:07,867] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-26 23:51:07,867] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-26 23:51:07,868] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-26 23:51:07,868] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-26 23:51:08,544] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:08,545] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-26 23:51:08,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:08,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-26 23:51:09,546] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:09,550] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-26 23:51:09,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:09,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-26 23:51:10,549] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:10,550] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-26 23:51:10,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:10,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-26 23:51:11,551] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:11,558] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-26 23:51:11,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:11,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-26 23:51:12,556] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:12,561] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-26 23:51:12,623] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:12,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-26 23:51:13,561] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:13,566] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-26 23:51:13,623] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:13,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-26 23:51:14,566] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:14,570] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-26 23:51:14,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:14,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-26 23:51:15,571] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:15,578] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-26 23:51:15,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:15,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-26 23:51:16,581] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:16,583] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-26 23:51:16,644] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:16,644] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-26 23:51:17,584] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:17,589] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-26 23:51:17,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:17,667] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-26 23:51:18,043] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:51:18,045] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.95, 55.0, 1.0, 2.0, 0.5800125951446595, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 944388.9902299583, 944388.9902299589, 211197.6860893686]
[2019-03-26 23:51:18,046] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:18,049] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.24323353 0.22727494 0.17694154 0.1224781  0.2300719 ], sampled 0.35100492011938456
[2019-03-26 23:51:18,589] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:18,596] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-26 23:51:18,657] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:18,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-26 23:51:18,725] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:51:18,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.5, 76.0, 1.0, 2.0, 0.2206959540768868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367878.0751669627, 367878.0751669627, 157591.1789757932]
[2019-03-26 23:51:18,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:18,771] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.2513804  0.17827302 0.19518445 0.14681569 0.22834648], sampled 0.8084037352271841
[2019-03-26 23:51:19,594] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:19,601] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-26 23:51:19,675] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:19,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-26 23:51:20,599] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:20,600] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-26 23:51:20,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:20,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-26 23:51:21,602] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:51:21,606] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-26 23:51:21,668] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:51:21,669] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-26 23:51:45,744] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:51:45,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.4, 87.0, 1.0, 2.0, 0.1922183064820917, 0.0, 2.0, 0.0, 1.0, 1.0, 0.3377464823358917, 6.911200000000001, 6.9112, 168.912956510431, 581898.8558760498, 581898.8558760491, 198095.9216884372]
[2019-03-26 23:51:45,746] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:45,751] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.307278   0.16742587 0.16333683 0.11154361 0.25041568], sampled 0.08696198523819432
[2019-03-26 23:51:53,198] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:51:53,200] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.2077976321168372, 0.0, 2.0, 0.0, 1.0, 1.0, 0.3580094141036285, 6.9112, 6.9112, 168.912956510431, 610664.4564224188, 610664.4564224188, 199569.7345736934]
[2019-03-26 23:51:53,201] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:53,203] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.26511514 0.20404659 0.12384003 0.11436154 0.2926367 ], sampled 0.4756678435054763
[2019-03-26 23:51:56,895] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:51:56,896] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.10759169166667, 94.01927495166666, 1.0, 1.0, 0.7033336405406632, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 982930.9580156743, 982930.9580156748, 221563.6025495626]
[2019-03-26 23:51:56,897] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:51:56,900] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.23191124 0.18079653 0.15633236 0.14791271 0.28304717], sampled 0.4337248060197324
[2019-03-26 23:52:03,534] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:52:03,535] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.2, 85.66666666666667, 1.0, 2.0, 0.6547239242597539, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 914968.081849031, 914968.0818490303, 211375.43221831]
[2019-03-26 23:52:03,536] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:52:03,538] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.26478618 0.21933861 0.17144427 0.13064598 0.21378496], sampled 0.0274409116462887
[2019-03-26 23:52:03,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 914968.081849031 W.
[2019-03-26 23:52:03,698] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:52:03,703] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.9, 87.0, 1.0, 2.0, 0.3662166244080561, 1.0, 1.0, 0.3662166244080561, 1.0, 1.0, 0.626583226935522, 6.9112, 6.9112, 178.6582176852504, 1535736.848417422, 1535736.848417422, 335809.882830264]
[2019-03-26 23:52:03,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:52:03,708] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.2524777  0.23882584 0.11544158 0.1552861  0.23796883], sampled 0.66584061130182
[2019-03-26 23:52:49,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:52:49,206] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.6, 60.0, 1.0, 2.0, 0.4601653218855857, 1.0, 2.0, 0.4601653218855857, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1407466.84133568, 1407466.84133568, 301846.5966400157]
[2019-03-26 23:52:49,208] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:52:49,211] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.24119376 0.24028409 0.14324799 0.13634369 0.23893045], sampled 0.8982189301401778
[2019-03-26 23:52:53,611] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:52:53,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.75, 87.5, 1.0, 1.0, 0.2796159084604704, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4810039580889935, 6.911200000000001, 6.9112, 168.912956510431, 781470.0229435835, 781470.022943583, 215213.3231194629]
[2019-03-26 23:52:53,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:52:53,615] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.34398282 0.15450081 0.14587143 0.10539755 0.2502474 ], sampled 0.008593266081372453
[2019-03-26 23:53:00,605] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:53:00,606] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.87365905, 95.71089584333333, 1.0, 2.0, 0.1833784299367729, 0.0, 1.0, 0.0, 1.0, 2.0, 0.3246366905066315, 6.911199999999999, 6.9112, 168.912956510431, 561649.3848651599, 561649.3848651604, 196920.3779533676]
[2019-03-26 23:53:00,607] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:53:00,610] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.28382796 0.150062   0.12295895 0.14063126 0.3025198 ], sampled 0.3122957471366826
[2019-03-26 23:53:05,472] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3459.1404 3490314944.1940 1451.0000
[2019-03-26 23:53:05,735] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3731.9674 3147327809.2826 747.0000
[2019-03-26 23:53:05,754] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3552.3858 3337657315.5671 1391.0000
[2019-03-26 23:53:05,821] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3656.6054 3273124005.9341 1109.0000
[2019-03-26 23:53:05,968] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3634.2134 3187893906.6486 885.0000
[2019-03-26 23:53:06,983] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3459.1403775347603, 3490314944.193979, 1451.0, 3656.6053667670994, 3273124005.9341054, 1109.0, 3731.967437560424, 3147327809.282636, 747.0, 3552.385756486506, 3337657315.567139, 1391.0, 3634.213435054316, 3187893906.648612, 885.0]
[2019-03-26 23:53:09,866] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.2439156  0.18572481 0.15424159 0.17504402 0.241074  ], sum to 1.0000
[2019-03-26 23:53:09,872] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4871
[2019-03-26 23:53:09,986] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.1, 85.0, 1.0, 1.0, 0.3203802490487699, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 517453.0357388292, 517453.0357388292, 168388.0513315896], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 12600.0000, 
sim time next is 13200.0000, 
raw observation next is [21.13333333333333, 85.0, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 1.0, 1.0, 0.2866375063199341, 6.911199999999999, 6.9112, 168.912956510431, 506286.0496819881, 506286.0496819887, 191009.8718256814], 
processed observation next is [1.0, 0.13043478260869565, 0.20063191153238533, 0.85, 1.0, 1.0, 0.0, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.1300457394145538, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14063501380055224, 0.14063501380055243, 0.28508936093385284], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38723403], dtype=float32), -0.122467384]. 
=============================================
[2019-03-26 23:53:12,389] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.10635708 0.24109739 0.20430663 0.14165373 0.30658516], sum to 1.0000
[2019-03-26 23:53:12,397] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6455
[2019-03-26 23:53:12,488] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.1, 68.0, 1.0, 2.0, 0.4958513874144028, 1.0, 1.0, 0.4958513874144028, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1437769.56982367, 1437769.56982367, 305047.4239092545], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [27.01666666666667, 68.5, 1.0, 2.0, 0.5083791634475632, 1.0, 2.0, 0.5083791634475632, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1470609.395403408, 1470609.395403408, 308702.7024215817], 
processed observation next is [1.0, 0.6956521739130435, 0.4794628751974725, 0.685, 1.0, 1.0, 0.4076857390934496, 1.0, 1.0, 0.4076857390934496, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.40850260983428, 0.40850260983428, 0.46075030212176377], 
reward next is 0.5392, 
noisyNet noise sample is [array([-0.27164584], dtype=float32), -0.10289412]. 
=============================================
[2019-03-26 23:53:18,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.8988625e-03 4.0398052e-04 2.7758093e-03 9.8637724e-01 2.5440808e-03], sum to 1.0000
[2019-03-26 23:53:18,754] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8529
[2019-03-26 23:53:18,867] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.23333333333333, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 477854.3970252721, 477854.3970252721, 229167.8419745108], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 175800.0000, 
sim time next is 176400.0000, 
raw observation next is [20.2, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 476145.5269092839, 476145.5269092845, 228828.4889778278], 
processed observation next is [0.0, 0.043478260869565216, 0.15639810426540288, 0.96, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.13226264636368998, 0.13226264636369015, 0.3415350581758624], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.212386], dtype=float32), -0.29919797]. 
=============================================
[2019-03-26 23:53:23,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2486718e-05 9.9992120e-01 2.3490358e-08 3.6266028e-05 1.9294043e-10], sum to 1.0000
[2019-03-26 23:53:23,259] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9227
[2019-03-26 23:53:23,363] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 91.00000000000001, 1.0, 2.0, 0.289635091541419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464814.8271679669, 464814.8271679669, 164596.5636014907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 252600.0000, 
sim time next is 253200.0000, 
raw observation next is [20.63333333333333, 91.0, 1.0, 2.0, 0.2879269450021249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462374.4486972084, 462374.4486972084, 164430.6735337546], 
processed observation next is [0.0, 0.9565217391304348, 0.17693522906793036, 0.91, 1.0, 1.0, 0.14208065662906613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12843734686033567, 0.12843734686033567, 0.2454189157220218], 
reward next is 0.7546, 
noisyNet noise sample is [array([-0.20310234], dtype=float32), -1.4772568]. 
=============================================
[2019-03-26 23:53:23,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.7851979e-05 9.9990737e-01 4.3187415e-11 4.7214953e-06 4.4673383e-11], sum to 1.0000
[2019-03-26 23:53:23,881] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9481
[2019-03-26 23:53:23,887] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2860133779352211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459525.246932158, 459525.246932158, 164237.2160093909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 259800.0000, 
sim time next is 260400.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2865217789590404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460338.4855236855, 460338.4855236855, 164292.6638229974], 
processed observation next is [0.0, 0.0, 0.1706161137440759, 0.92, 1.0, 1.0, 0.14038768549281977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1278718015343571, 0.1278718015343571, 0.24521293107910058], 
reward next is 0.7548, 
noisyNet noise sample is [array([-0.9096368], dtype=float32), -0.21135126]. 
=============================================
[2019-03-26 23:53:26,133] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7890: loss 0.0491
[2019-03-26 23:53:26,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7890: learning rate 0.0005
[2019-03-26 23:53:26,220] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7909: loss 0.0115
[2019-03-26 23:53:26,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7909: learning rate 0.0005
[2019-03-26 23:53:26,297] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7945: loss 0.0546
[2019-03-26 23:53:26,302] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7947: learning rate 0.0005
[2019-03-26 23:53:26,308] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7951: loss 0.0616
[2019-03-26 23:53:26,309] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7951: loss 0.0294
[2019-03-26 23:53:26,309] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7951: learning rate 0.0005
[2019-03-26 23:53:26,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7951: learning rate 0.0005
[2019-03-26 23:53:26,335] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7961: loss 0.1082
[2019-03-26 23:53:26,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7961: learning rate 0.0005
[2019-03-26 23:53:26,346] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7965: loss 0.1162
[2019-03-26 23:53:26,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7965: learning rate 0.0005
[2019-03-26 23:53:26,355] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7967: loss 0.2307
[2019-03-26 23:53:26,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7967: learning rate 0.0005
[2019-03-26 23:53:26,358] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7968: loss 0.0457
[2019-03-26 23:53:26,363] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7968: learning rate 0.0005
[2019-03-26 23:53:26,443] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8005: loss 0.1875
[2019-03-26 23:53:26,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8006: learning rate 0.0005
[2019-03-26 23:53:26,470] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8021: loss 0.1810
[2019-03-26 23:53:26,471] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8021: loss 0.1705
[2019-03-26 23:53:26,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8021: learning rate 0.0005
[2019-03-26 23:53:26,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8022: learning rate 0.0005
[2019-03-26 23:53:26,479] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 8022: loss 0.1120
[2019-03-26 23:53:26,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 8022: learning rate 0.0005
[2019-03-26 23:53:26,558] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8058: loss 0.0068
[2019-03-26 23:53:26,563] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8059: learning rate 0.0005
[2019-03-26 23:53:26,586] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8069: loss 0.0019
[2019-03-26 23:53:26,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8069: learning rate 0.0005
[2019-03-26 23:53:26,588] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8069: loss 0.0007
[2019-03-26 23:53:26,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8071: learning rate 0.0005
[2019-03-26 23:53:27,383] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.2218370e-06 9.9999356e-01 3.6077516e-10 1.1568253e-06 1.7093026e-13], sum to 1.0000
[2019-03-26 23:53:27,392] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4502
[2019-03-26 23:53:27,398] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 76.83333333333334, 1.0, 2.0, 0.3162925045152192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497579.1848846025, 497579.1848846025, 166786.9531768659], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 312600.0000, 
sim time next is 313200.0000, 
raw observation next is [23.4, 77.0, 1.0, 2.0, 0.3150392842453921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496097.0192038848, 496097.0192038848, 166688.1536447444], 
processed observation next is [0.0, 0.6521739130434783, 0.30805687203791465, 0.77, 1.0, 1.0, 0.17474612559685798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13780472755663467, 0.13780472755663467, 0.24878828902200656], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.9411346], dtype=float32), 0.7064184]. 
=============================================
[2019-03-26 23:53:30,549] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.7788884e-06 9.9999475e-01 3.3907377e-10 1.4422873e-06 1.4390626e-10], sum to 1.0000
[2019-03-26 23:53:30,555] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9756
[2019-03-26 23:53:30,560] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 88.0, 1.0, 2.0, 0.2562858282114025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417437.4633412592, 417437.4633412592, 161470.1096790693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [20.25, 87.5, 1.0, 2.0, 0.2567408921909044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418251.4206480645, 418251.4206480645, 161518.4713780084], 
processed observation next is [1.0, 0.21739130434782608, 0.1587677725118484, 0.875, 1.0, 1.0, 0.10450709902518601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11618095018001792, 0.11618095018001792, 0.24107234534031105], 
reward next is 0.7589, 
noisyNet noise sample is [array([-1.9001069], dtype=float32), -0.50700766]. 
=============================================
[2019-03-26 23:53:31,240] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4548989e-06 9.9999857e-01 1.6799719e-10 5.1060621e-08 5.0779545e-12], sum to 1.0000
[2019-03-26 23:53:31,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7456
[2019-03-26 23:53:31,257] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 72.5, 1.0, 2.0, 0.4646197689237915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756174.7748838082, 756174.7748838076, 189495.3206721872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 384600.0000, 
sim time next is 385200.0000, 
raw observation next is [22.4, 72.0, 1.0, 2.0, 0.467161369936644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760201.3799058652, 760201.3799058646, 189919.8213142541], 
processed observation next is [1.0, 0.4782608695652174, 0.2606635071090047, 0.72, 1.0, 1.0, 0.3580257469116193, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21116704997385144, 0.21116704997385127, 0.28346241987202103], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.3297881], dtype=float32), -0.011942325]. 
=============================================
[2019-03-26 23:53:43,989] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15895: loss 0.7791
[2019-03-26 23:53:43,995] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15896: learning rate 0.0005
[2019-03-26 23:53:44,014] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15905: loss 0.6464
[2019-03-26 23:53:44,018] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15905: learning rate 0.0005
[2019-03-26 23:53:44,075] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15934: loss 0.4151
[2019-03-26 23:53:44,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15934: learning rate 0.0005
[2019-03-26 23:53:44,110] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15950: loss 0.2863
[2019-03-26 23:53:44,111] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15950: learning rate 0.0005
[2019-03-26 23:53:44,121] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15953: loss 0.1812
[2019-03-26 23:53:44,124] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15954: learning rate 0.0005
[2019-03-26 23:53:44,143] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15962: loss 0.2121
[2019-03-26 23:53:44,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15964: learning rate 0.0005
[2019-03-26 23:53:44,151] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15967: loss 0.1403
[2019-03-26 23:53:44,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15967: learning rate 0.0005
[2019-03-26 23:53:44,173] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15971: loss 0.0939
[2019-03-26 23:53:44,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15972: learning rate 0.0005
[2019-03-26 23:53:44,183] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15979: loss 0.0548
[2019-03-26 23:53:44,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15979: learning rate 0.0005
[2019-03-26 23:53:44,225] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15995: loss 0.0595
[2019-03-26 23:53:44,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15995: learning rate 0.0005
[2019-03-26 23:53:44,262] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16013: loss 0.0020
[2019-03-26 23:53:44,263] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16013: loss 0.0056
[2019-03-26 23:53:44,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16013: learning rate 0.0005
[2019-03-26 23:53:44,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16014: learning rate 0.0005
[2019-03-26 23:53:44,329] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16040: loss 0.0087
[2019-03-26 23:53:44,331] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 16040: loss 0.0450
[2019-03-26 23:53:44,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16040: learning rate 0.0005
[2019-03-26 23:53:44,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 16044: learning rate 0.0005
[2019-03-26 23:53:44,421] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16082: loss 0.1181
[2019-03-26 23:53:44,422] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16082: learning rate 0.0005
[2019-03-26 23:53:44,444] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 16095: loss 0.1024
[2019-03-26 23:53:44,446] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 16095: learning rate 0.0005
[2019-03-26 23:53:56,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.8758827e-10 1.0000000e+00 6.8103911e-11 4.4802001e-12 5.4454213e-14], sum to 1.0000
[2019-03-26 23:53:56,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4582
[2019-03-26 23:53:56,270] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.15, 88.83333333333334, 1.0, 2.0, 0.2669562477402215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434605.4772025349, 434605.4772025342, 162550.7975552343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 798600.0000, 
sim time next is 799200.0000, 
raw observation next is [20.3, 88.0, 1.0, 2.0, 0.2680936876375796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 436119.7882263062, 436119.7882263068, 162655.294652], 
processed observation next is [0.0, 0.2608695652173913, 0.16113744075829392, 0.88, 1.0, 1.0, 0.11818516582840918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1211443856184184, 0.12114438561841855, 0.24276909649552242], 
reward next is 0.7572, 
noisyNet noise sample is [array([-0.5190835], dtype=float32), -0.2895224]. 
=============================================
[2019-03-26 23:54:01,763] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23847: loss 0.1550
[2019-03-26 23:54:01,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23847: learning rate 0.0005
[2019-03-26 23:54:01,800] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23862: loss 0.0561
[2019-03-26 23:54:01,803] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23862: learning rate 0.0005
[2019-03-26 23:54:01,857] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23887: loss 0.0303
[2019-03-26 23:54:01,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23888: learning rate 0.0005
[2019-03-26 23:54:01,896] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23905: loss 0.0156
[2019-03-26 23:54:01,899] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23906: learning rate 0.0005
[2019-03-26 23:54:01,962] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23934: loss 0.0105
[2019-03-26 23:54:01,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23934: learning rate 0.0005
[2019-03-26 23:54:02,015] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23959: loss 0.0098
[2019-03-26 23:54:02,017] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23959: learning rate 0.0005
[2019-03-26 23:54:02,102] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23997: loss 0.1315
[2019-03-26 23:54:02,105] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23997: learning rate 0.0005
[2019-03-26 23:54:02,117] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24000: loss 0.1387
[2019-03-26 23:54:02,118] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24000: learning rate 0.0005
[2019-03-26 23:54:02,132] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24006: loss 0.1226
[2019-03-26 23:54:02,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24006: learning rate 0.0005
[2019-03-26 23:54:02,190] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24036: loss 0.0431
[2019-03-26 23:54:02,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24037: learning rate 0.0005
[2019-03-26 23:54:02,210] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24043: loss 0.0311
[2019-03-26 23:54:02,211] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24044: loss 0.0212
[2019-03-26 23:54:02,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24044: learning rate 0.0005
[2019-03-26 23:54:02,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24044: learning rate 0.0005
[2019-03-26 23:54:02,250] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24061: loss 0.0148
[2019-03-26 23:54:02,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24062: learning rate 0.0005
[2019-03-26 23:54:02,275] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24072: loss 0.0013
[2019-03-26 23:54:02,275] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24072: loss 0.0047
[2019-03-26 23:54:02,280] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24074: learning rate 0.0005
[2019-03-26 23:54:02,281] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24074: learning rate 0.0005
[2019-03-26 23:54:02,282] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24074: loss 0.0029
[2019-03-26 23:54:02,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24074: learning rate 0.0005
[2019-03-26 23:54:04,338] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 23:54:04,340] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:54:04,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:54:04,341] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:54:04,341] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:54:04,342] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:54:04,343] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:54:04,343] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:54:04,344] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:54:04,347] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:54:04,348] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:54:04,367] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-26 23:54:04,367] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-26 23:54:04,369] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-26 23:54:04,387] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-26 23:54:04,404] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-26 23:54:12,838] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:54:12,840] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.2, 75.83333333333334, 1.0, 2.0, 0.2590879149295057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425233.3000414897, 425233.3000414897, 161820.055339191]
[2019-03-26 23:54:12,841] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:54:12,844] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8381683e-09 1.0000000e+00 5.9846772e-10 3.2529437e-10 3.6341468e-12], sampled 0.5133585069948903
[2019-03-26 23:54:28,140] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:54:28,144] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.7, 90.16666666666666, 1.0, 2.0, 0.3298121731139325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518533.8750696515, 518533.8750696522, 168379.4633056991]
[2019-03-26 23:54:28,145] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:54:28,148] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2355552e-09 1.0000000e+00 3.9600007e-10 2.1270917e-10 2.1830671e-12], sampled 0.8431488084417974
[2019-03-26 23:54:29,834] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:54:29,835] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.50907642, 99.20221543833334, 1.0, 2.0, 0.4485952550469937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 641897.7758944646, 641897.7758944639, 177873.4105992614]
[2019-03-26 23:54:29,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:54:29,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.8889173e-10 1.0000000e+00 2.5505389e-10 1.2903359e-10 1.1982773e-12], sampled 0.4728927100651321
[2019-03-26 23:54:34,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:54:34,987] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.56666666666667, 91.33333333333334, 1.0, 2.0, 0.4565563986044713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650187.5451315361, 650187.5451315355, 178644.3613611816]
[2019-03-26 23:54:34,990] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:54:34,992] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.8594077e-10 1.0000000e+00 3.2966727e-10 1.6284887e-10 1.5996538e-12], sampled 0.21358093487283902
[2019-03-26 23:54:59,655] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:54:59,656] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.93395478666667, 73.94007485, 1.0, 2.0, 0.5117631081570101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715114.9647739447, 715114.9647739453, 185458.0968723469]
[2019-03-26 23:54:59,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:54:59,660] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.5419388e-10 1.0000000e+00 1.3188382e-10 8.4624391e-11 6.8530327e-13], sampled 0.6042053165202458
[2019-03-26 23:55:15,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:55:15,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [38.40000000000001, 39.66666666666667, 1.0, 2.0, 0.5843300388389178, 0.0, 2.0, 0.0, 1.0, 1.0, 1.012648099540185, 6.911200000000001, 6.9112, 168.9124845538565, 1633739.7073749, 1633739.7073749, 357243.2154370162]
[2019-03-26 23:55:15,426] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:55:15,427] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.3174531e-10 1.0000000e+00 1.4976297e-10 1.2315102e-10 9.6995968e-13], sampled 0.896635284643251
[2019-03-26 23:55:40,677] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00749399], dtype=float32), -0.023924747]
[2019-03-26 23:55:40,678] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.18197261166667, 75.17448481666666, 1.0, 2.0, 0.4262613012528451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626525.9165443446, 626525.9165443453, 176788.5656174476]
[2019-03-26 23:55:40,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:55:40,681] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0750160e-10 1.0000000e+00 1.9113042e-10 1.1211487e-10 9.5021246e-13], sampled 0.6184043156282926
[2019-03-26 23:55:55,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-26 23:55:55,982] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2722 3007609378.0304 1766.0000
[2019-03-26 23:55:56,181] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1728 3163995799.7723 1778.0000
[2019-03-26 23:55:56,206] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6811 2927325981.4431 1338.0000
[2019-03-26 23:55:56,309] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6275 2779190785.3580 933.0000
[2019-03-26 23:55:57,324] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 25000, evaluation results [25000.0, 7884.17279234024, 3163995799.772337, 1778.0, 8253.681111202164, 2927325981.443095, 1338.0, 8660.627510841134, 2779190785.357956, 933.0, 7998.272155887525, 3007609378.0303907, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-26 23:56:01,328] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6778813e-09 1.0000000e+00 2.1931661e-09 5.4719401e-10 2.8478931e-11], sum to 1.0000
[2019-03-26 23:56:01,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2535
[2019-03-26 23:56:01,341] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.5202789958299762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804925.5168352509, 804925.5168352509, 196103.687232592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1003800.0000, 
sim time next is 1004400.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.6037050698019224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 934008.8718070281, 934008.8718070281, 212330.1906743973], 
processed observation next is [1.0, 0.6521739130434783, 0.22274881516587688, 0.96, 1.0, 1.0, 0.5225362286770149, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2594469088352856, 0.2594469088352856, 0.31691073234984674], 
reward next is 0.6831, 
noisyNet noise sample is [array([1.4155908], dtype=float32), -1.374717]. 
=============================================
[2019-03-26 23:56:02,064] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.8868553e-09 1.0000000e+00 1.0485440e-09 7.8502556e-09 4.7315069e-10], sum to 1.0000
[2019-03-26 23:56:02,075] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9452
[2019-03-26 23:56:02,081] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.71666666666667, 96.83333333333334, 1.0, 2.0, 0.3598122273471944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553296.8323661025, 553296.8323661019, 170896.1081969232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1015800.0000, 
sim time next is 1016400.0000, 
raw observation next is [21.73333333333333, 96.66666666666667, 1.0, 2.0, 0.3568415315265809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548783.4748986302, 548783.4748986296, 170518.6940436217], 
processed observation next is [1.0, 0.782608695652174, 0.22906793048973137, 0.9666666666666667, 1.0, 1.0, 0.22511027894768781, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15243985413850838, 0.15243985413850822, 0.25450551349794287], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.29818127], dtype=float32), 1.15878]. 
=============================================
[2019-03-26 23:56:07,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3604581e-09 1.0000000e+00 5.4159144e-09 1.2597609e-09 6.1577089e-12], sum to 1.0000
[2019-03-26 23:56:07,845] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3300
[2019-03-26 23:56:07,969] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333333, 79.0, 1.0, 2.0, 0.3163582521154338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500184.4699186072, 500184.4699186066, 167037.5268476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1111200.0000, 
sim time next is 1111800.0000, 
raw observation next is [22.81666666666667, 79.5, 1.0, 2.0, 0.3149041220441833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498399.0415846922, 498399.0415846922, 166914.0830814503], 
processed observation next is [1.0, 0.8695652173913043, 0.28041074249605075, 0.795, 1.0, 1.0, 0.17458327957130515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13844417821797006, 0.13844417821797006, 0.249125497136493], 
reward next is 0.7509, 
noisyNet noise sample is [array([0.06839555], dtype=float32), 0.2310261]. 
=============================================
[2019-03-26 23:56:08,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4277057e-09 1.0000000e+00 1.4547850e-09 1.6633698e-09 4.8194903e-11], sum to 1.0000
[2019-03-26 23:56:08,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1968
[2019-03-26 23:56:09,012] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.35, 92.0, 1.0, 2.0, 0.2849804203682356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459130.1705108368, 459130.1705108368, 164214.1599275443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1132200.0000, 
sim time next is 1132800.0000, 
raw observation next is [20.3, 92.33333333333334, 1.0, 2.0, 0.2818650904555028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 454208.1355815878, 454208.1355815872, 163881.7139531384], 
processed observation next is [1.0, 0.08695652173913043, 0.16113744075829392, 0.9233333333333335, 1.0, 1.0, 0.1347772174162684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12616892655044104, 0.1261689265504409, 0.24459957306438565], 
reward next is 0.7554, 
noisyNet noise sample is [array([0.290508], dtype=float32), -1.1056219]. 
=============================================
[2019-03-26 23:56:11,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2966375e-09 1.0000000e+00 8.1235807e-10 5.2709925e-10 3.3752146e-12], sum to 1.0000
[2019-03-26 23:56:11,469] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1673
[2019-03-26 23:56:11,578] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 58.0, 1.0, 2.0, 0.9273329028324265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1423921.034399745, 1423921.034399745, 295614.1797955311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1180800.0000, 
sim time next is 1181400.0000, 
raw observation next is [27.6, 57.83333333333334, 1.0, 2.0, 0.8785985113006702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1350195.598470133, 1350195.598470133, 280907.5849148553], 
processed observation next is [1.0, 0.6956521739130435, 0.5071090047393366, 0.5783333333333335, 1.0, 1.0, 0.8537331461453859, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3750543329083703, 0.3750543329083703, 0.4192650521117243], 
reward next is 0.5807, 
noisyNet noise sample is [array([0.48978624], dtype=float32), -0.43847474]. 
=============================================
[2019-03-26 23:56:12,703] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31770: loss 0.2483
[2019-03-26 23:56:12,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31770: learning rate 0.0005
[2019-03-26 23:56:12,878] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31848: loss 0.3890
[2019-03-26 23:56:12,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31848: learning rate 0.0005
[2019-03-26 23:56:13,055] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31927: loss 0.0351
[2019-03-26 23:56:13,062] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31927: learning rate 0.0005
[2019-03-26 23:56:13,074] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31933: loss 0.0010
[2019-03-26 23:56:13,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31935: learning rate 0.0005
[2019-03-26 23:56:13,125] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31955: loss 0.0101
[2019-03-26 23:56:13,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31955: learning rate 0.0005
[2019-03-26 23:56:13,148] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31967: loss 0.0385
[2019-03-26 23:56:13,151] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31968: learning rate 0.0005
[2019-03-26 23:56:13,186] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31982: loss 0.0331
[2019-03-26 23:56:13,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31983: learning rate 0.0005
[2019-03-26 23:56:13,217] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31998: loss 0.0316
[2019-03-26 23:56:13,218] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31999: learning rate 0.0005
[2019-03-26 23:56:13,248] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32009: loss 0.0472
[2019-03-26 23:56:13,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32010: learning rate 0.0005
[2019-03-26 23:56:13,264] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32014: loss 0.0379
[2019-03-26 23:56:13,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32015: learning rate 0.0005
[2019-03-26 23:56:13,289] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32025: loss 0.0436
[2019-03-26 23:56:13,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32025: learning rate 0.0005
[2019-03-26 23:56:13,323] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 32040: loss 0.0058
[2019-03-26 23:56:13,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 32041: learning rate 0.0005
[2019-03-26 23:56:13,366] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32058: loss 0.0009
[2019-03-26 23:56:13,367] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32058: learning rate 0.0005
[2019-03-26 23:56:13,382] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32064: loss 0.0003
[2019-03-26 23:56:13,388] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32066: learning rate 0.0005
[2019-03-26 23:56:13,457] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32102: loss 0.0005
[2019-03-26 23:56:13,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32102: learning rate 0.0005
[2019-03-26 23:56:13,507] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 32120: loss 0.0017
[2019-03-26 23:56:13,509] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 32121: learning rate 0.0005
[2019-03-26 23:56:14,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5316214e-10 1.0000000e+00 2.6784601e-09 9.0855284e-10 8.7853058e-11], sum to 1.0000
[2019-03-26 23:56:14,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2670
[2019-03-26 23:56:14,067] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 88.66666666666666, 1.0, 2.0, 0.3459292872044848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535871.303702833, 535871.3037028336, 169564.7609892271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1212000.0000, 
sim time next is 1212600.0000, 
raw observation next is [22.43333333333333, 88.83333333333334, 1.0, 2.0, 0.3455641444637538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535479.6854406107, 535479.6854406114, 169537.7444667488], 
processed observation next is [1.0, 0.0, 0.2622432859399683, 0.8883333333333334, 1.0, 1.0, 0.21152306561898046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1487443570668363, 0.1487443570668365, 0.25304140965186384], 
reward next is 0.7470, 
noisyNet noise sample is [array([1.0137742], dtype=float32), -1.1908084]. 
=============================================
[2019-03-26 23:56:22,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2138094e-15 1.0000000e+00 2.9557163e-21 1.8117307e-18 1.1489714e-21], sum to 1.0000
[2019-03-26 23:56:22,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1476
[2019-03-26 23:56:22,086] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 90.0, 1.0, 2.0, 0.6095514060263363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954581.580616492, 954581.5806164913, 214705.5538799792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1344000.0000, 
sim time next is 1344600.0000, 
raw observation next is [21.75, 89.5, 1.0, 2.0, 0.6325786714045516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 994203.1120139954, 994203.1120139954, 220021.807208456], 
processed observation next is [1.0, 0.5652173913043478, 0.2298578199052133, 0.895, 1.0, 1.0, 0.5573237004874115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2761675311149987, 0.2761675311149987, 0.3283907570275463], 
reward next is 0.6716, 
noisyNet noise sample is [array([-0.73074454], dtype=float32), -1.6965728]. 
=============================================
[2019-03-26 23:56:22,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.31426004e-17 1.00000000e+00 1.00319826e-22 1.60656069e-20
 2.57914018e-23], sum to 1.0000
[2019-03-26 23:56:22,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2110
[2019-03-26 23:56:22,449] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 91.0, 1.0, 2.0, 0.7091606161431733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1130087.578516714, 1130087.578516714, 239288.0164668727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1353600.0000, 
sim time next is 1354200.0000, 
raw observation next is [20.96666666666667, 91.16666666666667, 1.0, 2.0, 0.6389362483018863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1018434.12291141, 1018434.122911409, 222670.5488732789], 
processed observation next is [1.0, 0.6956521739130435, 0.1927330173775673, 0.9116666666666667, 1.0, 1.0, 0.5649834316890197, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2828983674753917, 0.2828983674753914, 0.3323441027959386], 
reward next is 0.6677, 
noisyNet noise sample is [array([0.34169844], dtype=float32), 0.20164847]. 
=============================================
[2019-03-26 23:56:26,942] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7092388e-19 1.0000000e+00 3.3408333e-27 8.1034405e-23 9.9752984e-26], sum to 1.0000
[2019-03-26 23:56:26,949] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1927
[2019-03-26 23:56:26,958] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 82.33333333333334, 1.0, 2.0, 0.4220432983190036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613200.330282592, 613200.3302825913, 175305.1974712079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1428000.0000, 
sim time next is 1428600.0000, 
raw observation next is [25.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4230837763388082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612587.0450986268, 612587.0450986268, 175183.2670321294], 
processed observation next is [0.0, 0.5217391304347826, 0.4154818325434442, 0.8066666666666668, 1.0, 1.0, 0.30492021245639545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17016306808295187, 0.17016306808295187, 0.2614675627345215], 
reward next is 0.7385, 
noisyNet noise sample is [array([-1.2779527], dtype=float32), 0.010584487]. 
=============================================
[2019-03-26 23:56:27,066] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4756704e-19 1.0000000e+00 6.1009855e-27 1.0244373e-22 4.6694087e-27], sum to 1.0000
[2019-03-26 23:56:27,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3416
[2019-03-26 23:56:27,185] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 85.66666666666667, 1.0, 2.0, 0.4131055965282594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 604681.370537805, 604681.3705378043, 174630.8839070348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1426800.0000, 
sim time next is 1427400.0000, 
raw observation next is [25.0, 84.0, 1.0, 2.0, 0.4177568822889548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 609179.9531312601, 609179.9531312594, 174987.1664184877], 
processed observation next is [0.0, 0.5217391304347826, 0.38388625592417064, 0.84, 1.0, 1.0, 0.2985022678180178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16921665364757227, 0.16921665364757207, 0.2611748752514742], 
reward next is 0.7388, 
noisyNet noise sample is [array([1.979011], dtype=float32), -0.68334985]. 
=============================================
[2019-03-26 23:56:29,438] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3229596e-18 1.0000000e+00 1.1849983e-25 1.0588093e-21 2.1768782e-25], sum to 1.0000
[2019-03-26 23:56:29,449] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3255
[2019-03-26 23:56:29,454] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.73333333333333, 96.0, 1.0, 2.0, 0.3533754330075666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 544562.0618384372, 544562.0618384365, 170199.0565142596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1470000.0000, 
sim time next is 1470600.0000, 
raw observation next is [21.7, 96.0, 1.0, 2.0, 0.3513367139601113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541942.9637036734, 541942.9637036734, 169997.4966864819], 
processed observation next is [0.0, 0.0, 0.2274881516587678, 0.96, 1.0, 1.0, 0.2184779686266401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1505397121399093, 0.1505397121399093, 0.2537276069947491], 
reward next is 0.7463, 
noisyNet noise sample is [array([-1.0704527], dtype=float32), -0.4832641]. 
=============================================
[2019-03-26 23:56:30,766] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39805: loss 0.0385
[2019-03-26 23:56:30,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39805: learning rate 0.0005
[2019-03-26 23:56:30,779] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39811: loss 0.0099
[2019-03-26 23:56:30,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39812: learning rate 0.0005
[2019-03-26 23:56:30,924] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39874: loss 0.0264
[2019-03-26 23:56:30,926] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39875: learning rate 0.0005
[2019-03-26 23:56:31,143] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39975: loss 0.0205
[2019-03-26 23:56:31,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39975: learning rate 0.0005
[2019-03-26 23:56:31,165] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39986: loss 0.0234
[2019-03-26 23:56:31,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39986: learning rate 0.0005
[2019-03-26 23:56:31,172] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39986: loss 0.0454
[2019-03-26 23:56:31,176] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39989: learning rate 0.0005
[2019-03-26 23:56:31,194] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39997: loss 0.0104
[2019-03-26 23:56:31,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39997: learning rate 0.0005
[2019-03-26 23:56:31,200] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39999: loss 0.0018
[2019-03-26 23:56:31,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40000: learning rate 0.0005
[2019-03-26 23:56:31,204] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40000: loss 0.0037
[2019-03-26 23:56:31,206] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40000: learning rate 0.0005
[2019-03-26 23:56:31,216] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40004: loss 0.0025
[2019-03-26 23:56:31,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40005: learning rate 0.0005
[2019-03-26 23:56:31,228] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40009: loss 0.0082
[2019-03-26 23:56:31,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40009: learning rate 0.0005
[2019-03-26 23:56:31,260] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40019: loss 0.0014
[2019-03-26 23:56:31,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40019: learning rate 0.0005
[2019-03-26 23:56:31,277] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40026: loss 0.0057
[2019-03-26 23:56:31,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40028: learning rate 0.0005
[2019-03-26 23:56:31,311] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40044: loss 0.0020
[2019-03-26 23:56:31,314] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40045: learning rate 0.0005
[2019-03-26 23:56:31,411] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40088: loss 0.0678
[2019-03-26 23:56:31,412] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40088: learning rate 0.0005
[2019-03-26 23:56:31,561] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40156: loss 0.0419
[2019-03-26 23:56:31,565] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40157: learning rate 0.0005
[2019-03-26 23:56:32,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5384532e-17 1.0000000e+00 9.2358333e-23 5.2611026e-20 8.1028529e-23], sum to 1.0000
[2019-03-26 23:56:32,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6637
[2019-03-26 23:56:32,841] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 55.00000000000001, 1.0, 2.0, 0.3644598655128068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 553951.0811976786, 553951.0811976792, 170757.5575697083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1524000.0000, 
sim time next is 1524600.0000, 
raw observation next is [28.45, 55.5, 1.0, 2.0, 0.3641859483448473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 554235.9988482147, 554235.9988482147, 170804.1367881757], 
processed observation next is [0.0, 0.6521739130434783, 0.54739336492891, 0.555, 1.0, 1.0, 0.23395897390945458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1539544441245041, 0.1539544441245041, 0.2549315474450384], 
reward next is 0.7451, 
noisyNet noise sample is [array([0.2615031], dtype=float32), 2.6300042]. 
=============================================
[2019-03-26 23:56:37,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1686848e-17 1.0000000e+00 1.1517305e-23 1.1762192e-20 3.9964034e-23], sum to 1.0000
[2019-03-26 23:56:37,753] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3701
[2019-03-26 23:56:37,756] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 85.0, 1.0, 2.0, 0.8052610820602031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206092.929919036, 1206092.929919036, 256171.913510364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1604400.0000, 
sim time next is 1605000.0000, 
raw observation next is [24.08333333333334, 85.0, 1.0, 2.0, 0.817020652302006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223013.416345706, 1223013.416345706, 259234.0670737856], 
processed observation next is [1.0, 0.5652173913043478, 0.34044233807267016, 0.85, 1.0, 1.0, 0.7795429545807302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3397259489849183, 0.3397259489849183, 0.38691651802057553], 
reward next is 0.6131, 
noisyNet noise sample is [array([1.1120632], dtype=float32), 1.0668323]. 
=============================================
[2019-03-26 23:56:37,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.446396]
 [70.41646 ]
 [70.36291 ]
 [70.32187 ]
 [70.25523 ]], R is [[70.37528229]
 [70.28918457]
 [70.20991516]
 [70.14363861]
 [70.1153183 ]].
[2019-03-26 23:56:40,921] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3303378e-18 1.0000000e+00 1.1830573e-23 1.5362098e-21 5.3558808e-24], sum to 1.0000
[2019-03-26 23:56:40,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0766
[2019-03-26 23:56:40,934] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 98.0, 1.0, 2.0, 0.4341939420624599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619786.8178648998, 619786.817864899, 175622.3809237987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1668600.0000, 
sim time next is 1669200.0000, 
raw observation next is [23.66666666666667, 98.0, 1.0, 2.0, 0.4493481692295083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640961.0280242809, 640961.0280242809, 177726.5250660207], 
processed observation next is [1.0, 0.30434782608695654, 0.3206951026856243, 0.98, 1.0, 1.0, 0.33656405931266065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1780447300067447, 0.1780447300067447, 0.26526347024779207], 
reward next is 0.7347, 
noisyNet noise sample is [array([0.2366921], dtype=float32), -0.49945384]. 
=============================================
[2019-03-26 23:56:43,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9419174e-07 9.9999905e-01 5.4393830e-11 8.6311900e-09 4.5636089e-10], sum to 1.0000
[2019-03-26 23:56:43,538] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5177
[2019-03-26 23:56:43,543] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.135767827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1706400.0000, 
sim time next is 1707000.0000, 
raw observation next is [28.06666666666667, 78.66666666666667, 1.0, 2.0, 0.5064215132278067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707648.3718230554, 707648.371823056, 184609.1211601371], 
processed observation next is [1.0, 0.782608695652174, 0.529225908372828, 0.7866666666666667, 1.0, 1.0, 0.40532712437085144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19656899217307094, 0.1965689921730711, 0.2755360017315479], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.53690803], dtype=float32), -0.19472866]. 
=============================================
[2019-03-26 23:56:43,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[9.584453 ]
 [8.897799 ]
 [7.740116 ]
 [6.869507 ]
 [6.5467563]], R is [[11.06679344]
 [11.68102837]
 [12.29073429]
 [12.89649868]
 [13.49827385]].
[2019-03-26 23:56:46,395] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4197365e-23 1.0000000e+00 8.5138037e-31 1.7689970e-28 1.0490434e-29], sum to 1.0000
[2019-03-26 23:56:46,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0127
[2019-03-26 23:56:46,529] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 90.66666666666667, 1.0, 2.0, 0.4827320287737497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678251.1483006797, 678251.1483006797, 181413.1688061994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1753800.0000, 
sim time next is 1754400.0000, 
raw observation next is [25.0, 90.33333333333334, 1.0, 2.0, 0.4699067524590962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660096.2476125918, 660096.2476125924, 179465.7918225708], 
processed observation next is [1.0, 0.30434782608695654, 0.38388625592417064, 0.9033333333333334, 1.0, 1.0, 0.36133343669770623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1833600687812755, 0.18336006878127567, 0.26785939077995646], 
reward next is 0.7321, 
noisyNet noise sample is [array([0.45286986], dtype=float32), 0.56893635]. 
=============================================
[2019-03-26 23:56:47,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4588847e-18 1.0000000e+00 1.2750288e-24 1.0534751e-21 1.6834530e-23], sum to 1.0000
[2019-03-26 23:56:47,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4549
[2019-03-26 23:56:47,211] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 85.33333333333334, 1.0, 2.0, 0.5787107684086624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 892389.9307546557, 892389.9307546552, 206922.1475052646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [22.96666666666667, 85.16666666666667, 1.0, 2.0, 0.5477983218924735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846861.7074088061, 846861.7074088061, 201136.9342255705], 
processed observation next is [1.0, 0.5217391304347826, 0.2875197472353872, 0.8516666666666667, 1.0, 1.0, 0.4551787010752692, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2352393631691128, 0.2352393631691128, 0.30020437944114997], 
reward next is 0.6998, 
noisyNet noise sample is [array([0.71255547], dtype=float32), 0.6560036]. 
=============================================
[2019-03-26 23:56:48,724] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47798: loss 0.1724
[2019-03-26 23:56:48,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47798: learning rate 0.0005
[2019-03-26 23:56:48,795] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47829: loss 0.0221
[2019-03-26 23:56:48,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47830: learning rate 0.0005
[2019-03-26 23:56:48,910] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47879: loss 0.0154
[2019-03-26 23:56:48,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47879: learning rate 0.0005
[2019-03-26 23:56:49,021] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47929: loss 0.1232
[2019-03-26 23:56:49,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47929: learning rate 0.0005
[2019-03-26 23:56:49,070] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47951: loss 0.1648
[2019-03-26 23:56:49,073] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47952: learning rate 0.0005
[2019-03-26 23:56:49,133] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47979: loss 0.1141
[2019-03-26 23:56:49,134] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47979: learning rate 0.0005
[2019-03-26 23:56:49,155] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47989: loss 0.0592
[2019-03-26 23:56:49,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47990: learning rate 0.0005
[2019-03-26 23:56:49,160] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47991: loss 0.0549
[2019-03-26 23:56:49,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47991: learning rate 0.0005
[2019-03-26 23:56:49,188] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48000: loss 0.0354
[2019-03-26 23:56:49,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48000: learning rate 0.0005
[2019-03-26 23:56:49,224] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48018: loss 0.0237
[2019-03-26 23:56:49,226] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48018: learning rate 0.0005
[2019-03-26 23:56:49,240] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48027: loss 0.0175
[2019-03-26 23:56:49,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48027: learning rate 0.0005
[2019-03-26 23:56:49,262] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48037: loss 0.0056
[2019-03-26 23:56:49,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48038: learning rate 0.0005
[2019-03-26 23:56:49,298] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48052: loss 0.0207
[2019-03-26 23:56:49,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48052: learning rate 0.0005
[2019-03-26 23:56:49,315] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48058: loss 0.0129
[2019-03-26 23:56:49,317] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48059: learning rate 0.0005
[2019-03-26 23:56:49,457] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48121: loss 0.0611
[2019-03-26 23:56:49,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48121: learning rate 0.0005
[2019-03-26 23:56:49,530] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 48154: loss 0.0036
[2019-03-26 23:56:49,534] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 48155: learning rate 0.0005
[2019-03-26 23:56:53,639] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 23:56:53,640] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:56:53,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:56:53,642] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:56:53,643] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:56:53,644] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:56:53,646] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:56:53,644] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:56:53,648] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:56:53,648] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:56:53,649] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:56:53,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-26 23:56:53,681] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-26 23:56:53,681] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-26 23:56:53,697] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-26 23:56:53,731] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-26 23:57:03,780] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:03,781] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.68583583, 85.37778318000001, 1.0, 2.0, 0.2273128280036219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 379933.6132869901, 379933.6132869908, 157716.2101265493]
[2019-03-26 23:57:03,782] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:57:03,786] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7644806e-14 1.0000000e+00 1.4323879e-18 3.0057288e-17 3.7226238e-18], sampled 0.5937343425142614
[2019-03-26 23:57:13,437] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:13,438] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.36666666666667, 48.0, 1.0, 2.0, 0.2096133712260814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 350304.5725023419, 350304.5725023426, 156209.0485221967]
[2019-03-26 23:57:13,439] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:57:13,442] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4706094e-14 1.0000000e+00 5.6416108e-18 1.2524826e-16 1.2745276e-17], sampled 0.1234858886642346
[2019-03-26 23:57:22,173] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:22,175] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.45892558, 92.31215481, 1.0, 2.0, 0.9778383027403763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510406, 1366806.801953418, 1366806.801953419, 292248.7566163734]
[2019-03-26 23:57:22,177] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:57:22,179] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.3392112e-14 1.0000000e+00 6.3987847e-18 2.6941341e-16 1.6608458e-17], sampled 0.7486491504285486
[2019-03-26 23:57:23,375] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:23,378] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.89243539166667, 72.77483399333335, 1.0, 2.0, 0.6437652327706289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 899646.968138181, 899646.9681381816, 209173.8473044411]
[2019-03-26 23:57:23,378] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:57:23,383] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2180945e-14 1.0000000e+00 1.9945008e-18 5.7940031e-17 4.6300533e-18], sampled 0.9934413812432319
[2019-03-26 23:57:34,251] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:34,251] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 85.0, 1.0, 2.0, 0.4938749828513089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690110.7760360574, 690110.776036058, 182641.2469405641]
[2019-03-26 23:57:34,252] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:57:34,256] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3098658e-14 1.0000000e+00 1.1267931e-18 2.8047269e-17 2.6838022e-18], sampled 0.821550534253311
[2019-03-26 23:57:52,130] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:52,133] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.288835305, 72.07767122999999, 1.0, 2.0, 0.7176603890698047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1002962.495950165, 1002962.495950165, 224715.3383220554]
[2019-03-26 23:57:52,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:57:52,138] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3448671e-14 1.0000000e+00 1.8141651e-18 6.5151187e-17 4.2856616e-18], sampled 0.3365450764691458
[2019-03-26 23:57:58,416] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.08951259]
[2019-03-26 23:57:58,417] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.74988257333333, 85.21653490333334, 1.0, 2.0, 0.6931490882293797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 968691.2333678192, 968691.2333678186, 219382.125401291]
[2019-03-26 23:57:58,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:57:58,423] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5426750e-14 1.0000000e+00 1.0696135e-18 4.0427066e-17 2.6047254e-18], sampled 0.172189056155431
[2019-03-26 23:58:47,746] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-26 23:58:47,982] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3240 2927318304.0717 1338.0000
[2019-03-26 23:58:47,982] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-26 23:58:47,987] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 23:58:48,018] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8724 2842497929.8887 1131.0000
[2019-03-26 23:58:49,029] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 50000, evaluation results [50000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.324045648182, 2927318304.071684, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.87235025343, 2842497929.888741, 1131.0]
[2019-03-26 23:58:51,599] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2202444e-22 1.0000000e+00 1.6867069e-29 3.3486290e-27 8.5343260e-30], sum to 1.0000
[2019-03-26 23:58:51,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4142
[2019-03-26 23:58:51,614] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 94.83333333333334, 1.0, 2.0, 0.4553097877842079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659884.8310636559, 659884.8310636552, 179907.7735464447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1915800.0000, 
sim time next is 1916400.0000, 
raw observation next is [23.6, 94.66666666666667, 1.0, 2.0, 0.4337006350218194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630318.5950626275, 630318.5950626275, 176973.4965908111], 
processed observation next is [1.0, 0.17391304347826086, 0.3175355450236968, 0.9466666666666668, 1.0, 1.0, 0.3177116084600234, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17508849862850762, 0.17508849862850762, 0.26413954715046434], 
reward next is 0.7359, 
noisyNet noise sample is [array([0.5360892], dtype=float32), -1.5961673]. 
=============================================
[2019-03-26 23:58:55,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1258782e-22 1.0000000e+00 5.6273476e-30 2.2862099e-25 1.1752865e-27], sum to 1.0000
[2019-03-26 23:58:55,699] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2441
[2019-03-26 23:58:55,702] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 97.33333333333333, 1.0, 2.0, 0.4512865427725447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643499.0266313003, 643499.0266313003, 177979.0370970285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1986000.0000, 
sim time next is 1986600.0000, 
raw observation next is [23.83333333333333, 97.16666666666667, 1.0, 2.0, 0.4535014760161311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 645374.6808852643, 645374.6808852637, 178138.3604221271], 
processed observation next is [1.0, 1.0, 0.32859399684044216, 0.9716666666666667, 1.0, 1.0, 0.341568043392929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1792707446903512, 0.17927074469035104, 0.2658781498837718], 
reward next is 0.7341, 
noisyNet noise sample is [array([-1.0032636], dtype=float32), -1.4682906]. 
=============================================
[2019-03-26 23:58:58,833] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8444263e-22 1.0000000e+00 1.2863014e-28 1.3847264e-26 9.4203060e-28], sum to 1.0000
[2019-03-26 23:58:58,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0819
[2019-03-26 23:58:58,966] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.66666666666667, 1.0, 2.0, 0.508118286132569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710020.1507385301, 710020.1507385301, 184878.0600177137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2042400.0000, 
sim time next is 2043000.0000, 
raw observation next is [27.05, 84.0, 1.0, 2.0, 0.5078796389559456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709686.5652108587, 709686.5652108587, 184840.1169328549], 
processed observation next is [0.0, 0.6521739130434783, 0.4810426540284361, 0.84, 1.0, 1.0, 0.4070839023565609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19713515700301631, 0.19713515700301631, 0.2758807715415745], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.9377528], dtype=float32), -0.4587875]. 
=============================================
[2019-03-26 23:58:58,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.63867 ]
 [79.620186]
 [79.5989  ]
 [79.58288 ]
 [79.5779  ]], R is [[79.59034729]
 [79.51850128]
 [79.44718933]
 [79.37644196]
 [79.30612946]].
[2019-03-26 23:59:01,706] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5208472e-19 1.0000000e+00 2.1429232e-26 3.5089653e-23 4.1822811e-26], sum to 1.0000
[2019-03-26 23:59:01,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3759
[2019-03-26 23:59:01,719] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 97.16666666666667, 1.0, 2.0, 0.4599709408056562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650217.9598427864, 650217.9598427871, 178529.635563173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2088600.0000, 
sim time next is 2089200.0000, 
raw observation next is [23.93333333333333, 97.33333333333334, 1.0, 2.0, 0.4583683932920895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648304.8621784303, 648304.8621784303, 178340.272852537], 
processed observation next is [0.0, 0.17391304347826086, 0.3333333333333332, 0.9733333333333334, 1.0, 1.0, 0.3474317991470958, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18008468393845287, 0.18008468393845287, 0.2661795117202045], 
reward next is 0.7338, 
noisyNet noise sample is [array([-0.17300044], dtype=float32), 0.14936455]. 
=============================================
[2019-03-26 23:59:01,908] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55744: loss 0.0009
[2019-03-26 23:59:01,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55746: learning rate 0.0005
[2019-03-26 23:59:02,102] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55829: loss 0.0290
[2019-03-26 23:59:02,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55831: learning rate 0.0005
[2019-03-26 23:59:02,197] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55879: loss 0.0178
[2019-03-26 23:59:02,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55879: learning rate 0.0005
[2019-03-26 23:59:02,312] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55924: loss 0.0286
[2019-03-26 23:59:02,317] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55924: learning rate 0.0005
[2019-03-26 23:59:02,355] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 55945: loss 0.0201
[2019-03-26 23:59:02,358] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 55947: learning rate 0.0005
[2019-03-26 23:59:02,364] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55948: loss 0.0080
[2019-03-26 23:59:02,367] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55948: learning rate 0.0005
[2019-03-26 23:59:02,395] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55960: loss 0.0094
[2019-03-26 23:59:02,398] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55961: learning rate 0.0005
[2019-03-26 23:59:02,441] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55981: loss 0.0009
[2019-03-26 23:59:02,443] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55981: learning rate 0.0005
[2019-03-26 23:59:02,540] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56025: loss 0.0503
[2019-03-26 23:59:02,541] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56025: learning rate 0.0005
[2019-03-26 23:59:02,557] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56032: loss 0.0387
[2019-03-26 23:59:02,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56033: learning rate 0.0005
[2019-03-26 23:59:02,589] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56049: loss 0.0679
[2019-03-26 23:59:02,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56049: learning rate 0.0005
[2019-03-26 23:59:02,611] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56058: loss 0.1358
[2019-03-26 23:59:02,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56058: learning rate 0.0005
[2019-03-26 23:59:02,627] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56063: loss 0.1353
[2019-03-26 23:59:02,629] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56063: learning rate 0.0005
[2019-03-26 23:59:02,677] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56085: loss 0.0491
[2019-03-26 23:59:02,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56086: learning rate 0.0005
[2019-03-26 23:59:02,794] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56135: loss 0.0001
[2019-03-26 23:59:02,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56136: learning rate 0.0005
[2019-03-26 23:59:02,855] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56161: loss 0.0089
[2019-03-26 23:59:02,857] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56161: learning rate 0.0005
[2019-03-26 23:59:04,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7479879e-20 1.0000000e+00 9.2167784e-26 4.3504981e-24 2.8551160e-26], sum to 1.0000
[2019-03-26 23:59:04,129] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0991
[2019-03-26 23:59:04,134] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.25, 76.16666666666667, 1.0, 2.0, 0.5739163567625523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 801997.9322146008, 801997.9322146003, 195987.6671177533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2127000.0000, 
sim time next is 2127600.0000, 
raw observation next is [30.3, 76.0, 1.0, 2.0, 0.5748022414015747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 803236.3466301463, 803236.3466301457, 196145.977948463], 
processed observation next is [0.0, 0.6521739130434783, 0.6350710900473934, 0.76, 1.0, 1.0, 0.48771354385731885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22312120739726285, 0.22312120739726268, 0.29275519096785524], 
reward next is 0.7072, 
noisyNet noise sample is [array([0.59190077], dtype=float32), 0.68979084]. 
=============================================
[2019-03-26 23:59:18,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9237294e-15 1.0000000e+00 1.4933199e-21 7.6363767e-14 4.2148987e-22], sum to 1.0000
[2019-03-26 23:59:18,245] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2650
[2019-03-26 23:59:18,249] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.65, 71.5, 1.0, 2.0, 0.8092741896984313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1131065.067225003, 1131065.067225003, 246199.7353304236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [29.8, 71.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.822318600183165, 6.9112, 168.9081488063479, 2100558.197830642, 1454197.70221077, 311350.5057129195], 
processed observation next is [1.0, 0.34782608695652173, 0.6113744075829385, 0.71, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.09111186001831646, 0.0, 0.8294163371242782, 0.5834883882862895, 0.4039438061696583, 0.4647022473327157], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3475725], dtype=float32), 0.34133756]. 
=============================================
[2019-03-26 23:59:18,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3018768e-12 1.0000000e+00 6.9115903e-17 3.8728756e-13 3.6214818e-17], sum to 1.0000
[2019-03-26 23:59:18,626] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3440
[2019-03-26 23:59:18,632] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.53333333333333, 63.33333333333334, 1.0, 2.0, 0.3832461492502842, 1.0, 2.0, 0.3832461492502842, 1.0, 1.0, 0.6655718198094638, 6.911200000000001, 6.9112, 170.5573041426782, 1607256.266012718, 1607256.266012717, 343808.3493494684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2378400.0000, 
sim time next is 2379000.0000, 
raw observation next is [32.56666666666666, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.641274872300842, 6.9112, 168.9089429221964, 1972037.634830724, 1454109.700106274, 311353.573490856], 
processed observation next is [1.0, 0.5217391304347826, 0.7424960505529224, 0.6316666666666666, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.07300748723008424, 0.0, 0.8294202365966421, 0.5477882318974233, 0.4039193611406317, 0.4647068261057552], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34615877], dtype=float32), 0.13149066]. 
=============================================
[2019-03-26 23:59:18,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[44.958244]
 [45.086674]
 [44.41766 ]
 [43.73076 ]
 [43.667744]], R is [[46.09324646]
 [46.11916733]
 [46.16262436]
 [45.70100021]
 [45.24399185]].
[2019-03-26 23:59:19,875] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63769: loss 6.2166
[2019-03-26 23:59:19,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63769: learning rate 0.0005
[2019-03-26 23:59:20,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6573334e-06 9.9995100e-01 1.4379358e-09 4.2335942e-05 2.8730658e-09], sum to 1.0000
[2019-03-26 23:59:20,057] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63848: loss 8.4028
[2019-03-26 23:59:20,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63848: learning rate 0.0005
[2019-03-26 23:59:20,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6756
[2019-03-26 23:59:20,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2238233.878628549 W.
[2019-03-26 23:59:20,179] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.15000000000001, 60.5, 1.0, 2.0, 0.9594411720999535, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005991145384145, 6.9112, 168.9123931417482, 2238233.878628549, 2170986.001923519, 451061.37536617], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2392200.0000, 
sim time next is 2392800.0000, 
raw observation next is [33.16666666666667, 60.33333333333334, 1.0, 2.0, 0.8394404323294812, 1.0, 1.0, 0.8394404323294812, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2347750.781026902, 2347750.781026902, 439532.3101391398], 
processed observation next is [1.0, 0.6956521739130435, 0.7709320695102688, 0.6033333333333334, 1.0, 1.0, 0.8065547377463629, 1.0, 0.5, 0.8065547377463629, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.652152994729695, 0.652152994729695, 0.6560183733419997], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4284359], dtype=float32), -0.41063583]. 
=============================================
[2019-03-26 23:59:20,206] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63880: loss 6.5377
[2019-03-26 23:59:20,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63882: learning rate 0.0005
[2019-03-26 23:59:20,272] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63913: loss 3.5841
[2019-03-26 23:59:20,273] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63913: learning rate 0.0005
[2019-03-26 23:59:20,279] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63915: loss 4.8670
[2019-03-26 23:59:20,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63916: learning rate 0.0005
[2019-03-26 23:59:20,445] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 63993: loss 4.8091
[2019-03-26 23:59:20,446] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63993: loss 3.7179
[2019-03-26 23:59:20,448] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 63993: learning rate 0.0005
[2019-03-26 23:59:20,451] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63995: loss 3.0767
[2019-03-26 23:59:20,453] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63995: learning rate 0.0005
[2019-03-26 23:59:20,455] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63995: learning rate 0.0005
[2019-03-26 23:59:20,468] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64000: loss 3.7257
[2019-03-26 23:59:20,469] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64000: learning rate 0.0005
[2019-03-26 23:59:20,500] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64010: loss 1.9201
[2019-03-26 23:59:20,504] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64011: loss 5.5706
[2019-03-26 23:59:20,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64011: learning rate 0.0005
[2019-03-26 23:59:20,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64011: learning rate 0.0005
[2019-03-26 23:59:20,573] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64043: loss 4.8238
[2019-03-26 23:59:20,576] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64045: learning rate 0.0005
[2019-03-26 23:59:20,625] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64071: loss 2.2129
[2019-03-26 23:59:20,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64072: learning rate 0.0005
[2019-03-26 23:59:20,662] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64084: loss 0.7492
[2019-03-26 23:59:20,666] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64085: learning rate 0.0005
[2019-03-26 23:59:20,817] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64152: loss 2.0014
[2019-03-26 23:59:20,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64152: learning rate 0.0005
[2019-03-26 23:59:20,852] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 64168: loss 2.0951
[2019-03-26 23:59:20,854] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 64168: learning rate 0.0005
[2019-03-26 23:59:20,940] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9566027e-10 9.9999964e-01 5.9607822e-15 3.6623749e-07 1.3393474e-15], sum to 1.0000
[2019-03-26 23:59:20,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2198
[2019-03-26 23:59:20,951] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.56666666666667, 75.33333333333333, 1.0, 2.0, 0.5865709141981166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 819688.3932872053, 819688.3932872047, 198270.7730353634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2407200.0000, 
sim time next is 2407800.0000, 
raw observation next is [30.48333333333333, 75.66666666666667, 1.0, 2.0, 0.5849447147882951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817415.0284312845, 817415.0284312845, 197974.7118572583], 
processed observation next is [1.0, 0.8695652173913043, 0.6437598736176934, 0.7566666666666667, 1.0, 1.0, 0.4999333913111989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22705973011980124, 0.22705973011980124, 0.2954846445630721], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.6706642], dtype=float32), -0.36954117]. 
=============================================
[2019-03-26 23:59:25,850] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0038149e-13 1.0000000e+00 3.5282875e-19 7.0177364e-11 1.6807110e-18], sum to 1.0000
[2019-03-26 23:59:25,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0586
[2019-03-26 23:59:25,866] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 93.0, 1.0, 2.0, 0.5525950173015179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772192.3941970215, 772192.3941970215, 192246.8533653164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2497800.0000, 
sim time next is 2498400.0000, 
raw observation next is [26.9, 93.0, 1.0, 2.0, 0.552380441864277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771892.4390403318, 771892.4390403318, 192209.9112043129], 
processed observation next is [1.0, 0.9565217391304348, 0.4739336492890995, 0.93, 1.0, 1.0, 0.4606993275473217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21441456640009215, 0.21441456640009215, 0.2868804644840491], 
reward next is 0.7131, 
noisyNet noise sample is [array([1.3632561], dtype=float32), -0.49112263]. 
=============================================
[2019-03-26 23:59:29,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9761887e-10 1.0000000e+00 1.1256257e-16 1.0720816e-09 2.0964679e-14], sum to 1.0000
[2019-03-26 23:59:29,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7469
[2019-03-26 23:59:29,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2053289.287687015 W.
[2019-03-26 23:59:29,112] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.7342496645039449, 1.0, 2.0, 0.7342496645039449, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2053289.287687015, 2053289.287687015, 389129.544748885], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2540400.0000, 
sim time next is 2541000.0000, 
raw observation next is [27.68333333333333, 84.83333333333333, 1.0, 2.0, 0.7429646854199574, 1.0, 2.0, 0.7429646854199574, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2077684.003915774, 2077684.003915773, 393057.8869649526], 
processed observation next is [1.0, 0.391304347826087, 0.5110584518167456, 0.8483333333333333, 1.0, 1.0, 0.6903188980963342, 1.0, 1.0, 0.6903188980963342, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5771344455321594, 0.5771344455321592, 0.5866535626342576], 
reward next is 0.4133, 
noisyNet noise sample is [array([0.35723776], dtype=float32), -0.31842604]. 
=============================================
[2019-03-26 23:59:29,119] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[44.98695 ]
 [44.974464]
 [45.305325]
 [45.956074]
 [47.07453 ]], R is [[45.44679642]
 [45.41153717]
 [45.36845016]
 [45.33876801]
 [44.88537979]].
[2019-03-26 23:59:32,168] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8059602e-20 1.0000000e+00 2.0223485e-27 4.4143508e-20 5.0695012e-28], sum to 1.0000
[2019-03-26 23:59:32,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5212
[2019-03-26 23:59:32,183] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 92.5, 1.0, 2.0, 0.4865391851448324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679856.8996106591, 679856.8996106591, 181513.3265422555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2593800.0000, 
sim time next is 2594400.0000, 
raw observation next is [24.86666666666667, 92.66666666666667, 1.0, 2.0, 0.4839522424962031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676240.9306145759, 676240.9306145759, 181119.3198428449], 
processed observation next is [0.0, 0.0, 0.3775671406003162, 0.9266666666666667, 1.0, 1.0, 0.37825571385084716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1878447029484933, 0.1878447029484933, 0.27032734304902223], 
reward next is 0.7297, 
noisyNet noise sample is [array([1.0481412], dtype=float32), -2.0996501]. 
=============================================
[2019-03-26 23:59:37,925] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71807: loss 0.3890
[2019-03-26 23:59:37,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71807: learning rate 0.0005
[2019-03-26 23:59:38,055] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71868: loss 0.1922
[2019-03-26 23:59:38,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71868: learning rate 0.0005
[2019-03-26 23:59:38,074] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71875: loss 0.1511
[2019-03-26 23:59:38,078] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71877: learning rate 0.0005
[2019-03-26 23:59:38,094] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71884: loss 0.1414
[2019-03-26 23:59:38,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71886: learning rate 0.0005
[2019-03-26 23:59:38,185] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71925: loss 0.1697
[2019-03-26 23:59:38,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71925: learning rate 0.0005
[2019-03-26 23:59:38,201] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71932: loss 0.1377
[2019-03-26 23:59:38,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71932: learning rate 0.0005
[2019-03-26 23:59:38,270] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71961: loss 0.1382
[2019-03-26 23:59:38,273] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71961: learning rate 0.0005
[2019-03-26 23:59:38,293] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 71970: loss 0.1628
[2019-03-26 23:59:38,294] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 71970: learning rate 0.0005
[2019-03-26 23:59:38,382] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72011: loss 0.0766
[2019-03-26 23:59:38,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72011: learning rate 0.0005
[2019-03-26 23:59:38,404] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72021: loss 0.1219
[2019-03-26 23:59:38,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72021: learning rate 0.0005
[2019-03-26 23:59:38,430] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72031: loss 0.0941
[2019-03-26 23:59:38,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72031: learning rate 0.0005
[2019-03-26 23:59:38,474] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 72049: loss 0.0608
[2019-03-26 23:59:38,476] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 72049: learning rate 0.0005
[2019-03-26 23:59:38,499] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72059: loss 0.0642
[2019-03-26 23:59:38,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72059: learning rate 0.0005
[2019-03-26 23:59:38,539] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72076: loss 0.0132
[2019-03-26 23:59:38,541] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72077: learning rate 0.0005
[2019-03-26 23:59:38,597] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72105: loss 0.0083
[2019-03-26 23:59:38,602] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72106: learning rate 0.0005
[2019-03-26 23:59:38,859] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72220: loss 0.0085
[2019-03-26 23:59:38,862] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72220: learning rate 0.0005
[2019-03-26 23:59:45,043] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 23:59:45,046] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:59:45,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:59:45,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:59:45,050] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:59:45,050] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:59:45,051] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:59:45,052] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:59:45,054] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:59:45,055] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:59:45,057] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:59:45,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-26 23:59:45,085] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-26 23:59:45,086] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-26 23:59:45,086] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-26 23:59:45,144] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-27 00:00:11,250] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:00:11,251] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.3, 94.0, 1.0, 2.0, 0.4629610546274237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656232.777746873, 656232.7777468737, 179198.3239041596]
[2019-03-27 00:00:11,252] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:00:11,255] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1141571e-17 1.0000000e+00 5.9267492e-23 1.9836752e-16 1.5639152e-24], sampled 0.012694525432040749
[2019-03-27 00:00:19,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:00:19,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.98333333333333, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.619413191430892, 6.9112, 168.9093961885304, 2792780.473864133, 2290360.251207287, 474831.2974291897]
[2019-03-27 00:00:19,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:00:19,348] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.3718215e-16 1.0000000e+00 9.3437805e-22 1.6173688e-15 3.0506081e-23], sampled 0.041632176004802735
[2019-03-27 00:00:19,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2792780.473864133 W.
[2019-03-27 00:00:23,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:00:23,554] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.35223948, 74.24063307, 1.0, 2.0, 0.5265975648350614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763403.3048103156, 763403.3048103162, 191355.5318099832]
[2019-03-27 00:00:23,556] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:00:23,558] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2961012e-16 1.0000000e+00 1.4228872e-22 4.0666676e-16 4.1817749e-24], sampled 0.1250142237420926
[2019-03-27 00:00:29,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:00:29,524] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.56666666666667, 71.83333333333333, 1.0, 2.0, 0.5015924683604013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700898.277223034, 700898.2772230345, 183845.6661811807]
[2019-03-27 00:00:29,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:00:29,529] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.6606546e-17 1.0000000e+00 7.2923350e-23 2.4586862e-16 1.9654915e-24], sampled 0.010040895566003072
[2019-03-27 00:00:51,886] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:00:51,887] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.96510868666667, 82.74142607333333, 1.0, 2.0, 0.4681214631033261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664389.5892039635, 664389.5892039628, 180073.7062406469]
[2019-03-27 00:00:51,887] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:00:51,890] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8713034e-17 1.0000000e+00 4.0805919e-23 1.5884406e-16 1.0189681e-24], sampled 0.9087779785813124
[2019-03-27 00:00:57,724] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:00:57,726] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.09765244333333, 86.44914308166666, 1.0, 2.0, 0.5210334067551229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728073.3063447383, 728073.3063447383, 186957.6108480707]
[2019-03-27 00:00:57,728] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:00:57,730] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.3816497e-17 1.0000000e+00 2.5357666e-23 1.1144104e-16 6.1107044e-25], sampled 0.987875802374725
[2019-03-27 00:01:03,656] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:01:03,658] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.51666666666667, 81.66666666666667, 1.0, 2.0, 0.5852508628324261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817843.0113683267, 817843.0113683267, 198030.4800677657]
[2019-03-27 00:01:03,660] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:01:03,663] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3340426e-17 1.0000000e+00 1.4196227e-23 7.7566354e-17 3.3710102e-25], sampled 0.48833316245188374
[2019-03-27 00:01:37,567] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02206697], dtype=float32), -0.119014755]
[2019-03-27 00:01:37,570] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.15, 62.5, 1.0, 2.0, 0.3240521367641627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516345.2587888142, 516345.2587888136, 168316.1893577748]
[2019-03-27 00:01:37,571] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:01:37,573] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0000266e-16 1.0000000e+00 3.0298615e-22 6.2571240e-16 9.0292838e-24], sampled 0.5431073443247872
[2019-03-27 00:01:38,724] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4155 3164135224.3988 1778.0000
[2019-03-27 00:01:38,941] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 00:01:39,304] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 00:01:39,384] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:01:39,538] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3240 2927318304.0717 1338.0000
[2019-03-27 00:01:40,553] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 75000, evaluation results [75000.0, 7883.4154806281485, 3164135224.3988295, 1778.0, 8254.324045648182, 2927318304.071684, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 00:01:48,777] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6628491e-17 1.0000000e+00 1.3912144e-23 3.9252053e-18 1.1926676e-24], sum to 1.0000
[2019-03-27 00:01:48,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2591
[2019-03-27 00:01:48,789] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.0, 1.0, 2.0, 0.3073100318811158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486690.1820198278, 486690.1820198271, 166057.2253637769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3099701987121949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490351.5515316635, 490351.5515316641, 166313.7923316537], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16863879362915046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13620876431435097, 0.13620876431435114, 0.248229540793513], 
reward next is 0.7518, 
noisyNet noise sample is [array([-1.7978028], dtype=float32), 0.51530486]. 
=============================================
[2019-03-27 00:01:51,220] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79811: loss 0.1274
[2019-03-27 00:01:51,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79813: learning rate 0.0005
[2019-03-27 00:01:51,474] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79891: loss 0.3982
[2019-03-27 00:01:51,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79892: learning rate 0.0005
[2019-03-27 00:01:51,575] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79903: loss 0.3752
[2019-03-27 00:01:51,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79903: learning rate 0.0005
[2019-03-27 00:01:51,685] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79923: loss 0.4235
[2019-03-27 00:01:51,686] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79923: loss 0.4501
[2019-03-27 00:01:51,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79923: learning rate 0.0005
[2019-03-27 00:01:51,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79925: learning rate 0.0005
[2019-03-27 00:01:51,858] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79932: loss 0.3719
[2019-03-27 00:01:51,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79932: learning rate 0.0005
[2019-03-27 00:01:51,948] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79938: loss 0.3774
[2019-03-27 00:01:51,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79938: learning rate 0.0005
[2019-03-27 00:01:52,045] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79948: loss 0.3046
[2019-03-27 00:01:52,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79948: learning rate 0.0005
[2019-03-27 00:01:52,179] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79979: loss 0.2306
[2019-03-27 00:01:52,180] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79979: learning rate 0.0005
[2019-03-27 00:01:52,284] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79997: loss 0.1924
[2019-03-27 00:01:52,286] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79998: learning rate 0.0005
[2019-03-27 00:01:52,408] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80022: loss 0.0546
[2019-03-27 00:01:52,408] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80022: loss 0.0584
[2019-03-27 00:01:52,409] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80022: learning rate 0.0005
[2019-03-27 00:01:52,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80022: learning rate 0.0005
[2019-03-27 00:01:52,607] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80040: loss 0.0457
[2019-03-27 00:01:52,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80040: learning rate 0.0005
[2019-03-27 00:01:52,770] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80090: loss 0.0328
[2019-03-27 00:01:52,771] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80090: loss 0.0372
[2019-03-27 00:01:52,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80090: learning rate 0.0005
[2019-03-27 00:01:52,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80090: learning rate 0.0005
[2019-03-27 00:01:53,288] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 80254: loss 0.1467
[2019-03-27 00:01:53,290] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 80254: learning rate 0.0005
[2019-03-27 00:02:00,468] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0132185e-18 1.0000000e+00 8.9758428e-25 4.9837020e-18 1.7969377e-27], sum to 1.0000
[2019-03-27 00:02:00,479] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7716
[2019-03-27 00:02:00,484] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 98.0, 1.0, 2.0, 0.3470690678087817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537272.8732681094, 537272.8732681094, 169668.783994089], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3127200.0000, 
sim time next is 3127800.0000, 
raw observation next is [21.16666666666666, 99.0, 1.0, 2.0, 0.3432213617658628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532063.7906343807, 532063.79063438, 169267.1162802483], 
processed observation next is [1.0, 0.17391304347826086, 0.2022116903633489, 0.99, 1.0, 1.0, 0.20870043586248532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14779549739843906, 0.14779549739843886, 0.25263748698544525], 
reward next is 0.7474, 
noisyNet noise sample is [array([-0.12627622], dtype=float32), 0.30159867]. 
=============================================
[2019-03-27 00:02:10,223] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87821: loss 0.0156
[2019-03-27 00:02:10,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87824: learning rate 0.0005
[2019-03-27 00:02:10,316] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87864: loss 0.0700
[2019-03-27 00:02:10,319] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87865: learning rate 0.0005
[2019-03-27 00:02:10,338] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87873: loss 0.1402
[2019-03-27 00:02:10,343] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87874: learning rate 0.0005
[2019-03-27 00:02:10,390] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87896: loss 0.1079
[2019-03-27 00:02:10,395] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87898: learning rate 0.0005
[2019-03-27 00:02:10,433] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87917: loss 0.1519
[2019-03-27 00:02:10,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87917: learning rate 0.0005
[2019-03-27 00:02:10,459] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87927: loss 0.0947
[2019-03-27 00:02:10,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87927: learning rate 0.0005
[2019-03-27 00:02:10,489] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87938: loss 0.0814
[2019-03-27 00:02:10,495] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87939: learning rate 0.0005
[2019-03-27 00:02:10,496] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87939: loss 0.0492
[2019-03-27 00:02:10,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87939: learning rate 0.0005
[2019-03-27 00:02:10,639] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88007: loss 0.0027
[2019-03-27 00:02:10,642] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88008: learning rate 0.0005
[2019-03-27 00:02:10,665] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88016: loss 0.0244
[2019-03-27 00:02:10,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88016: learning rate 0.0005
[2019-03-27 00:02:10,699] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88031: loss 0.0558
[2019-03-27 00:02:10,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88032: learning rate 0.0005
[2019-03-27 00:02:10,731] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88045: loss 0.0280
[2019-03-27 00:02:10,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88046: learning rate 0.0005
[2019-03-27 00:02:10,773] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88066: loss 0.0260
[2019-03-27 00:02:10,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88066: learning rate 0.0005
[2019-03-27 00:02:10,779] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88067: loss 0.0358
[2019-03-27 00:02:10,780] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88068: loss 0.0448
[2019-03-27 00:02:10,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88069: learning rate 0.0005
[2019-03-27 00:02:10,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88069: learning rate 0.0005
[2019-03-27 00:02:11,278] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 88288: loss 0.0310
[2019-03-27 00:02:11,280] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 88288: learning rate 0.0005
[2019-03-27 00:02:12,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5525092e-23 1.0000000e+00 1.6352545e-30 1.4686259e-22 2.4471944e-32], sum to 1.0000
[2019-03-27 00:02:12,652] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2588
[2019-03-27 00:02:12,660] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5968682171159964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834083.7436432667, 834083.7436432667, 200164.2127618514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3337800.0000, 
sim time next is 3338400.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5964550567010019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833506.1528253664, 833506.1528253664, 200087.6766423961], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5138012731337371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2315294868959351, 0.2315294868959351, 0.29863832334685986], 
reward next is 0.7014, 
noisyNet noise sample is [array([0.30153546], dtype=float32), 0.16050966]. 
=============================================
[2019-03-27 00:02:24,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8362951e-13 9.9999523e-01 3.0042228e-18 4.7366043e-06 4.2134364e-20], sum to 1.0000
[2019-03-27 00:02:25,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4182
[2019-03-27 00:02:25,008] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.83333333333334, 1.0, 2.0, 0.4995048238479894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697980.1569303558, 697980.1569303565, 183518.2135709811], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3545400.0000, 
sim time next is 3546000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.494663753188603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691213.3141815951, 691213.3141815951, 182763.8815167615], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.74, 1.0, 1.0, 0.3911611484200036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19200369838377643, 0.19200369838377643, 0.2727819127115843], 
reward next is 0.7272, 
noisyNet noise sample is [array([1.411513], dtype=float32), 1.4476122]. 
=============================================
[2019-03-27 00:02:25,027] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.088387]
 [58.168606]
 [58.205822]
 [58.281628]
 [58.411343]], R is [[58.21155167]
 [58.35552979]
 [58.49700928]
 [58.6361618 ]
 [58.77311325]].
[2019-03-27 00:02:26,933] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6338512e-10 9.9936360e-01 3.7947128e-12 6.3639256e-04 4.9901937e-17], sum to 1.0000
[2019-03-27 00:02:26,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1920
[2019-03-27 00:02:26,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2099367.176924505 W.
[2019-03-27 00:02:27,052] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.5004738933159152, 1.0, 2.0, 0.5004738933159152, 1.0, 1.0, 0.8635595549535526, 6.911199999999999, 6.9112, 170.5573041426782, 2099367.176924505, 2099367.176924506, 414461.020837601], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3575400.0000, 
sim time next is 3576000.0000, 
raw observation next is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.5165748237207624, 1.0, 2.0, 0.5165748237207624, 1.0, 2.0, 0.8925916603561443, 6.9112, 6.9112, 170.5573041426782, 2166975.008912194, 2166975.008912194, 425949.1957345368], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783573, 0.6866666666666668, 1.0, 1.0, 0.41756002857923175, 1.0, 1.0, 0.41756002857923175, 1.0, 1.0, 0.8690142199465175, 0.0, 0.0, 0.8375144448122397, 0.6019375024756095, 0.6019375024756095, 0.6357450682605027], 
reward next is 0.3643, 
noisyNet noise sample is [array([2.2831638], dtype=float32), -0.15197514]. 
=============================================
[2019-03-27 00:02:27,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[44.000805]
 [45.390335]
 [46.775124]
 [47.380966]
 [48.63188 ]], R is [[42.8674736 ]
 [42.43880081]
 [42.41725922]
 [42.00047302]
 [41.58046722]].
[2019-03-27 00:02:28,341] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95881: loss 25.9559
[2019-03-27 00:02:28,342] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95881: learning rate 0.0005
[2019-03-27 00:02:28,363] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95891: loss -56.2377
[2019-03-27 00:02:28,364] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95891: loss 8.4089
[2019-03-27 00:02:28,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95891: learning rate 0.0005
[2019-03-27 00:02:28,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95891: learning rate 0.0005
[2019-03-27 00:02:28,406] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95909: loss 4.1516
[2019-03-27 00:02:28,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95910: learning rate 0.0005
[2019-03-27 00:02:28,415] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95913: loss 8.8279
[2019-03-27 00:02:28,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95913: learning rate 0.0005
[2019-03-27 00:02:28,477] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95940: loss -11.6550
[2019-03-27 00:02:28,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95940: learning rate 0.0005
[2019-03-27 00:02:28,515] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95957: loss 11.6954
[2019-03-27 00:02:28,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95957: learning rate 0.0005
[2019-03-27 00:02:28,557] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95975: loss -48.2190
[2019-03-27 00:02:28,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95975: learning rate 0.0005
[2019-03-27 00:02:28,607] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95996: loss -11.0214
[2019-03-27 00:02:28,610] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95998: learning rate 0.0005
[2019-03-27 00:02:28,627] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96004: loss 17.1144
[2019-03-27 00:02:28,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96006: learning rate 0.0005
[2019-03-27 00:02:28,661] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96021: loss -29.6247
[2019-03-27 00:02:28,665] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96023: loss -71.3331
[2019-03-27 00:02:28,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96022: learning rate 0.0005
[2019-03-27 00:02:28,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96023: learning rate 0.0005
[2019-03-27 00:02:28,680] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96031: loss -16.5247
[2019-03-27 00:02:28,682] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96032: learning rate 0.0005
[2019-03-27 00:02:28,724] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96047: loss -12.5083
[2019-03-27 00:02:28,728] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96049: learning rate 0.0005
[2019-03-27 00:02:28,831] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96097: loss -38.3262
[2019-03-27 00:02:28,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96098: learning rate 0.0005
[2019-03-27 00:02:29,115] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 96223: loss 1.6442
[2019-03-27 00:02:29,116] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 96224: learning rate 0.0005
[2019-03-27 00:02:33,975] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2185218e-06 8.9053553e-01 5.1332663e-08 1.0946317e-01 7.7358869e-11], sum to 1.0000
[2019-03-27 00:02:33,983] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5943
[2019-03-27 00:02:33,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2384199.734767539 W.
[2019-03-27 00:02:33,996] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 63.0, 1.0, 2.0, 0.8524603686452591, 1.0, 2.0, 0.8524603686452591, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2384199.734767539, 2384199.73476754, 446221.2424347982], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.5780838268611537, 1.0, 2.0, 0.5780838268611537, 1.0, 1.0, 1.003940432014952, 6.9112, 6.9112, 170.5573041426782, 2425256.312677193, 2425256.312677193, 473341.367884715], 
processed observation next is [1.0, 0.6956521739130435, 0.7314375987361774, 0.6433333333333333, 1.0, 1.0, 0.49166726127849836, 1.0, 1.0, 0.49166726127849836, 1.0, 0.5, 1.0048054048962831, 0.0, 0.0, 0.8375144448122397, 0.673682309076998, 0.673682309076998, 0.7064796535592761], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.13722093], dtype=float32), -0.20266476]. 
=============================================
[2019-03-27 00:02:37,537] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 00:02:37,539] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:02:37,540] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:02:37,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:02:37,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:02:37,541] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:02:37,542] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:02:37,543] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:02:37,546] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:02:37,547] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:02:37,549] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:02:37,562] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-27 00:02:37,563] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-27 00:02:37,600] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-27 00:02:37,627] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-27 00:02:37,628] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-27 00:02:44,695] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05184205], dtype=float32), -0.24826272]
[2019-03-27 00:02:44,697] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.1, 90.33333333333334, 1.0, 2.0, 0.2620375470848427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 431537.2052928276, 431537.2052928283, 162106.5199445472]
[2019-03-27 00:02:44,697] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:02:44,701] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7859921e-17 1.0000000e+00 1.3720899e-20 3.9106541e-15 2.9250537e-26], sampled 0.8648619978721263
[2019-03-27 00:02:46,368] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05184205], dtype=float32), -0.24826272]
[2019-03-27 00:02:46,370] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.1500055, 70.88362319666666, 1.0, 2.0, 0.2038414170740585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 340899.9184471912, 340899.9184471912, 149902.1826488697]
[2019-03-27 00:02:46,372] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:02:46,377] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.68532789e-17 1.00000000e+00 1.29564385e-20 3.71101694e-15
 2.66267210e-26], sampled 0.793392955352287
[2019-03-27 00:03:03,965] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05184205], dtype=float32), -0.24826272]
[2019-03-27 00:03:03,966] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.25, 93.83333333333334, 1.0, 2.0, 0.5569936859691708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792635.6746041526, 792635.674604152, 194838.0820007253]
[2019-03-27 00:03:03,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:03:03,970] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1734161e-16 1.0000000e+00 1.1814902e-19 2.0059045e-14 4.9330874e-25], sampled 0.44928385704527685
[2019-03-27 00:03:36,941] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05184205], dtype=float32), -0.24826272]
[2019-03-27 00:03:36,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.78208022166667, 77.5285272, 1.0, 2.0, 0.5143821188036525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718775.891728465, 718775.8917284644, 185877.1753837026]
[2019-03-27 00:03:36,944] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:03:36,946] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.4289285e-18 1.0000000e+00 2.2325902e-21 1.2714113e-15 2.7358773e-27], sampled 0.19592470967380715
[2019-03-27 00:03:56,098] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05184205], dtype=float32), -0.24826272]
[2019-03-27 00:03:56,100] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.76666666666667, 51.33333333333333, 1.0, 2.0, 0.7211842248731427, 1.0, 1.0, 0.7211842248731427, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2016692.793200629, 2016692.793200629, 383819.4425635726]
[2019-03-27 00:03:56,103] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:03:56,106] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.1592541e-13 1.0000000e+00 1.2732703e-15 3.2966474e-10 8.4030816e-20], sampled 0.04952570341374041
[2019-03-27 00:03:56,108] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2016692.793200629 W.
[2019-03-27 00:04:05,812] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05184205], dtype=float32), -0.24826272]
[2019-03-27 00:04:05,812] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.15, 63.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.194460774908743, 6.9112, 168.9108913298228, 1654845.043342281, 1453892.557474111, 311349.856757259]
[2019-03-27 00:04:05,814] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:04:05,816] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7438031e-15 1.0000000e+00 2.3541610e-18 2.3897442e-13 2.4028678e-23], sampled 0.3994913298880194
[2019-03-27 00:04:31,693] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 00:04:31,893] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 00:04:32,002] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4596 3007660329.2749 1766.0000
[2019-03-27 00:04:32,130] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4154 3164069856.8990 1778.0000
[2019-03-27 00:04:32,195] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:04:33,210] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 100000, evaluation results [100000.0, 7883.415429794, 3164069856.898958, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7997.459627403906, 3007660329.2748713, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:04:33,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0700611e-12 1.0000000e+00 8.2627640e-15 3.4654331e-08 3.5189876e-18], sum to 1.0000
[2019-03-27 00:04:33,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2336
[2019-03-27 00:04:33,319] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2487069.818013376 W.
[2019-03-27 00:04:33,324] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 65.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.183152320045016, 6.9112, 168.9113490058139, 2487069.818013376, 2294139.318388305, 476011.7034708666], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3754200.0000, 
sim time next is 3754800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.547452053941406, 6.9112, 168.9090724786835, 2746764.690047088, 2295395.964012708, 475248.1585760105], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.06362520539414059, 0.0, 0.829420872778305, 0.7629901916797467, 0.63760999000353, 0.7093256098149411], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24004696], dtype=float32), -0.59832174]. 
=============================================
[2019-03-27 00:04:41,525] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2281465e-23 1.0000000e+00 4.3542772e-27 6.0106893e-18 6.9825720e-36], sum to 1.0000
[2019-03-27 00:04:41,532] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3367
[2019-03-27 00:04:41,539] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 90.5, 1.0, 2.0, 0.5781903139495733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 807972.6853141182, 807972.6853141176, 196752.6146647437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [27.66666666666666, 91.0, 1.0, 2.0, 0.577627009638801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807185.2154711109, 807185.2154711109, 196651.3604342198], 
processed observation next is [0.0, 0.043478260869565216, 0.5102685624012636, 0.91, 1.0, 1.0, 0.4911168790828927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22421811540864192, 0.22421811540864192, 0.2935094931854027], 
reward next is 0.7065, 
noisyNet noise sample is [array([-0.39056075], dtype=float32), -0.79844946]. 
=============================================
[2019-03-27 00:04:41,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.23495 ]
 [72.394455]
 [72.54431 ]
 [72.67784 ]
 [72.71363 ]], R is [[72.10308838]
 [72.08839417]
 [72.07349396]
 [72.05839539]
 [72.04381561]].
[2019-03-27 00:04:41,808] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103866: loss 0.0145
[2019-03-27 00:04:41,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103866: learning rate 0.0005
[2019-03-27 00:04:41,816] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103869: loss 0.0202
[2019-03-27 00:04:41,818] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103869: learning rate 0.0005
[2019-03-27 00:04:41,918] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103915: loss 0.0303
[2019-03-27 00:04:41,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103916: learning rate 0.0005
[2019-03-27 00:04:41,939] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103925: loss 0.0322
[2019-03-27 00:04:41,941] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103926: learning rate 0.0005
[2019-03-27 00:04:41,995] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103948: loss 0.0674
[2019-03-27 00:04:41,998] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103948: learning rate 0.0005
[2019-03-27 00:04:42,018] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103957: loss 0.0582
[2019-03-27 00:04:42,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103957: learning rate 0.0005
[2019-03-27 00:04:42,036] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103965: loss 0.0248
[2019-03-27 00:04:42,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103966: learning rate 0.0005
[2019-03-27 00:04:42,059] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103976: loss 0.0074
[2019-03-27 00:04:42,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103976: learning rate 0.0005
[2019-03-27 00:04:42,080] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103986: loss 0.0093
[2019-03-27 00:04:42,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103986: learning rate 0.0005
[2019-03-27 00:04:42,088] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4855530e-21 1.0000000e+00 2.5239598e-26 1.2784919e-16 3.9134071e-34], sum to 1.0000
[2019-03-27 00:04:42,097] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2613
[2019-03-27 00:04:42,104] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 90.66666666666667, 1.0, 2.0, 0.576724529448313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805923.5967779416, 805923.5967779416, 196489.2596508685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3903600.0000, 
sim time next is 3904200.0000, 
raw observation next is [27.5, 91.5, 1.0, 2.0, 0.5759854728022732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804890.4365129758, 804890.4365129764, 196356.6652942462], 
processed observation next is [0.0, 0.17391304347826086, 0.5023696682464456, 0.915, 1.0, 1.0, 0.48913912385816044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22358067680915997, 0.2235806768091601, 0.2930696496929048], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.66126746], dtype=float32), 0.9444181]. 
=============================================
[2019-03-27 00:04:42,133] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104006: loss 0.0025
[2019-03-27 00:04:42,136] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104007: loss 0.0035
[2019-03-27 00:04:42,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104006: learning rate 0.0005
[2019-03-27 00:04:42,138] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104007: learning rate 0.0005
[2019-03-27 00:04:42,147] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104013: loss 0.0052
[2019-03-27 00:04:42,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104014: learning rate 0.0005
[2019-03-27 00:04:42,196] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104034: loss 0.0314
[2019-03-27 00:04:42,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104035: learning rate 0.0005
[2019-03-27 00:04:42,253] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104058: loss 0.0426
[2019-03-27 00:04:42,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104059: learning rate 0.0005
[2019-03-27 00:04:42,310] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104087: loss 0.0414
[2019-03-27 00:04:42,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104087: learning rate 0.0005
[2019-03-27 00:04:42,511] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 104175: loss 0.0025
[2019-03-27 00:04:42,512] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 104175: learning rate 0.0005
[2019-03-27 00:04:43,589] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5383053e-21 1.0000000e+00 3.5000984e-25 1.1947906e-15 9.5198374e-34], sum to 1.0000
[2019-03-27 00:04:43,599] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3510
[2019-03-27 00:04:43,723] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5855827289608689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818306.9473715558, 818306.9473715564, 198090.7564418991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3925200.0000, 
sim time next is 3925800.0000, 
raw observation next is [32.5, 65.0, 1.0, 2.0, 0.5848134190571245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 817231.4822105966, 817231.4822105971, 197950.9395632618], 
processed observation next is [0.0, 0.43478260869565216, 0.7393364928909952, 0.65, 1.0, 1.0, 0.4997752036832825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22700874505849905, 0.22700874505849922, 0.2954491635272564], 
reward next is 0.7046, 
noisyNet noise sample is [array([-1.8570434], dtype=float32), -0.9273544]. 
=============================================
[2019-03-27 00:04:50,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8660289e-17 1.0000000e+00 3.1865802e-20 1.7851351e-12 5.4884320e-26], sum to 1.0000
[2019-03-27 00:04:50,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4043
[2019-03-27 00:04:50,736] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 79.0, 1.0, 2.0, 0.5701696397281947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796760.2590043277, 796760.2590043277, 195320.6360147131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [29.5, 79.0, 1.0, 2.0, 0.5668084612773808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792061.5663031598, 792061.5663031598, 194725.6000038597], 
processed observation next is [1.0, 0.782608695652174, 0.5971563981042655, 0.79, 1.0, 1.0, 0.4780824834667239, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2200171017508777, 0.2200171017508777, 0.29063522388635776], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.05025042], dtype=float32), -0.6067645]. 
=============================================
[2019-03-27 00:04:50,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[42.80116 ]
 [42.452816]
 [42.091476]
 [41.22877 ]
 [39.88112 ]], R is [[43.16075897]
 [43.4376297 ]
 [43.71175385]
 [43.9843483 ]
 [44.25605392]].
[2019-03-27 00:04:52,789] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1826891e-20 1.0000000e+00 3.5766961e-25 8.7255495e-16 4.7883979e-31], sum to 1.0000
[2019-03-27 00:04:52,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6615
[2019-03-27 00:04:52,803] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6942198967818454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970188.3956380525, 970188.3956380525, 219602.720940052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4079400.0000, 
sim time next is 4080000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6541168429467781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 914119.3285823171, 914119.3285823165, 211253.337682386], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.5832733047551543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2539220357173103, 0.25392203571731015, 0.31530348907818806], 
reward next is 0.6847, 
noisyNet noise sample is [array([1.028626], dtype=float32), 0.05379144]. 
=============================================
[2019-03-27 00:04:52,819] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[57.772694]
 [57.70058 ]
 [57.52741 ]
 [57.59879 ]
 [57.900795]], R is [[57.9569397 ]
 [58.04960632]
 [58.1231041 ]
 [58.18759537]
 [58.24858856]].
[2019-03-27 00:04:53,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1768598e-22 1.0000000e+00 3.2239341e-27 1.2273488e-16 1.7344586e-34], sum to 1.0000
[2019-03-27 00:04:53,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9717
[2019-03-27 00:04:53,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2684309.508619515 W.
[2019-03-27 00:04:53,686] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.66666666666667, 82.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.64473704841129, 6.9112, 168.9032533276808, 2684309.508619515, 1454549.310208439, 310148.2285322465], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4090800.0000, 
sim time next is 4091400.0000, 
raw observation next is [29.0, 81.5, 1.0, 2.0, 0.5465153798356912, 1.0, 1.0, 0.5465153798356912, 1.0, 1.0, 0.9491164793766852, 6.9112, 6.9112, 170.5573041426782, 2292694.555973839, 2292694.555973839, 448802.754167368], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.815, 1.0, 1.0, 0.4536329877538448, 1.0, 0.5, 0.4536329877538448, 1.0, 0.5, 0.9379469260691281, 0.0, 0.0, 0.8375144448122397, 0.6368595988816219, 0.6368595988816219, 0.6698548569662209], 
reward next is 0.3301, 
noisyNet noise sample is [array([-1.6792561], dtype=float32), 0.9329148]. 
=============================================
[2019-03-27 00:04:59,722] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111824: loss -117.2388
[2019-03-27 00:04:59,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111826: learning rate 0.0005
[2019-03-27 00:04:59,983] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111944: loss -118.6117
[2019-03-27 00:04:59,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111944: learning rate 0.0005
[2019-03-27 00:04:59,989] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111944: loss -80.2177
[2019-03-27 00:04:59,992] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111945: learning rate 0.0005
[2019-03-27 00:05:00,001] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111949: loss -88.1150
[2019-03-27 00:05:00,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111950: learning rate 0.0005
[2019-03-27 00:05:00,021] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111956: loss -51.8840
[2019-03-27 00:05:00,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111956: learning rate 0.0005
[2019-03-27 00:05:00,046] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111967: loss -62.8064
[2019-03-27 00:05:00,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111967: learning rate 0.0005
[2019-03-27 00:05:00,062] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 111974: loss -79.9944
[2019-03-27 00:05:00,067] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 111976: learning rate 0.0005
[2019-03-27 00:05:00,068] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111977: loss -70.1177
[2019-03-27 00:05:00,069] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111978: learning rate 0.0005
[2019-03-27 00:05:00,112] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111996: loss -69.2688
[2019-03-27 00:05:00,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111998: learning rate 0.0005
[2019-03-27 00:05:00,146] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112012: loss -74.7460
[2019-03-27 00:05:00,148] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112012: loss -83.8947
[2019-03-27 00:05:00,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112014: learning rate 0.0005
[2019-03-27 00:05:00,154] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112014: learning rate 0.0005
[2019-03-27 00:05:00,166] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112020: loss -76.3616
[2019-03-27 00:05:00,168] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112020: loss -96.0406
[2019-03-27 00:05:00,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112021: learning rate 0.0005
[2019-03-27 00:05:00,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112021: learning rate 0.0005
[2019-03-27 00:05:00,195] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112032: loss -89.8131
[2019-03-27 00:05:00,197] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112032: loss -57.3473
[2019-03-27 00:05:00,199] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112032: learning rate 0.0005
[2019-03-27 00:05:00,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112033: learning rate 0.0005
[2019-03-27 00:05:00,501] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 112168: loss -64.3334
[2019-03-27 00:05:00,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 112168: learning rate 0.0005
[2019-03-27 00:05:16,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0568701e-25 1.0000000e+00 5.5067655e-32 6.9003316e-20 2.6959266e-36], sum to 1.0000
[2019-03-27 00:05:16,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2612
[2019-03-27 00:05:16,236] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 77.66666666666667, 1.0, 2.0, 0.5927703594218329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828355.0262931172, 828355.0262931172, 199406.8003517087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4470600.0000, 
sim time next is 4471200.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5905113158979588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825196.9450086606, 825196.9450086606, 198991.3931387019], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.79, 1.0, 1.0, 0.5066401396360949, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22922137361351683, 0.22922137361351683, 0.2970020793114954], 
reward next is 0.7030, 
noisyNet noise sample is [array([-0.18457554], dtype=float32), 0.35595414]. 
=============================================
[2019-03-27 00:05:17,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.02485395e-25 1.00000000e+00 3.14013736e-33 1.08741826e-20
 1.26610713e-37], sum to 1.0000
[2019-03-27 00:05:17,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2389
[2019-03-27 00:05:17,460] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 84.0, 1.0, 2.0, 0.5266037821918546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735859.8443584514, 735859.844358452, 187868.5678555646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4488600.0000, 
sim time next is 4489200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.521256030414582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728384.499280889, 728384.4992808898, 186992.7269796804], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4232000366440747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20232902757802473, 0.20232902757802493, 0.27909362235773194], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.3382298], dtype=float32), 0.93232054]. 
=============================================
[2019-03-27 00:05:17,536] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119771: loss 0.6112
[2019-03-27 00:05:17,539] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119771: learning rate 0.0005
[2019-03-27 00:05:17,758] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119874: loss 0.3414
[2019-03-27 00:05:17,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119875: learning rate 0.0005
[2019-03-27 00:05:17,857] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119915: loss 0.3158
[2019-03-27 00:05:17,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119917: learning rate 0.0005
[2019-03-27 00:05:17,944] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119956: loss 0.1372
[2019-03-27 00:05:17,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119956: learning rate 0.0005
[2019-03-27 00:05:17,951] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 119959: loss 0.1089
[2019-03-27 00:05:17,954] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 119960: learning rate 0.0005
[2019-03-27 00:05:17,964] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119965: loss 0.0941
[2019-03-27 00:05:17,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119965: learning rate 0.0005
[2019-03-27 00:05:17,966] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119965: loss 0.0449
[2019-03-27 00:05:17,973] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119967: learning rate 0.0005
[2019-03-27 00:05:18,016] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119987: loss 0.0835
[2019-03-27 00:05:18,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119988: learning rate 0.0005
[2019-03-27 00:05:18,041] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119997: loss 0.0321
[2019-03-27 00:05:18,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119999: learning rate 0.0005
[2019-03-27 00:05:18,084] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120014: loss 0.0386
[2019-03-27 00:05:18,089] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120014: learning rate 0.0005
[2019-03-27 00:05:18,106] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120023: loss 0.0093
[2019-03-27 00:05:18,111] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120024: learning rate 0.0005
[2019-03-27 00:05:18,141] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120038: loss 0.0078
[2019-03-27 00:05:18,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120040: learning rate 0.0005
[2019-03-27 00:05:18,186] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120056: loss 0.0049
[2019-03-27 00:05:18,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120056: learning rate 0.0005
[2019-03-27 00:05:18,221] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120076: loss 0.0144
[2019-03-27 00:05:18,223] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120077: learning rate 0.0005
[2019-03-27 00:05:18,240] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120083: loss 0.0207
[2019-03-27 00:05:18,246] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120083: learning rate 0.0005
[2019-03-27 00:05:18,357] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120134: loss 0.0174
[2019-03-27 00:05:18,359] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120135: learning rate 0.0005
[2019-03-27 00:05:18,741] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4603731e-21 1.0000000e+00 2.0667426e-25 2.9653881e-18 1.1422973e-30], sum to 1.0000
[2019-03-27 00:05:18,753] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8752
[2019-03-27 00:05:18,762] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.83333333333333, 1.0, 2.0, 0.4870135491722845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680519.9560845736, 680519.9560845731, 181585.7409667126], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4507800.0000, 
sim time next is 4508400.0000, 
raw observation next is [26.0, 85.66666666666667, 1.0, 2.0, 0.4890695399877618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683393.7833797967, 683393.7833797967, 181900.7411829323], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.8566666666666667, 1.0, 1.0, 0.38442113251537563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18983160649438796, 0.18983160649438796, 0.27149364355661537], 
reward next is 0.7285, 
noisyNet noise sample is [array([0.20703813], dtype=float32), -2.0837684]. 
=============================================
[2019-03-27 00:05:19,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.2335294e-26 1.0000000e+00 7.3436141e-31 7.1588904e-19 5.3953610e-37], sum to 1.0000
[2019-03-27 00:05:19,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1569
[2019-03-27 00:05:19,642] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 70.33333333333334, 1.0, 2.0, 0.5496363099008391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768056.4224806846, 768056.4224806853, 191738.3165138381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4534800.0000, 
sim time next is 4535400.0000, 
raw observation next is [30.66666666666667, 68.16666666666666, 1.0, 2.0, 0.5490514145734481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767238.8002684446, 767238.8002684452, 191637.9977070911], 
processed observation next is [0.0, 0.4782608695652174, 0.6524486571879939, 0.6816666666666665, 1.0, 1.0, 0.4566884512933109, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21312188896345682, 0.213121888963457, 0.2860268622493897], 
reward next is 0.7140, 
noisyNet noise sample is [array([-0.11045197], dtype=float32), 1.2706025]. 
=============================================
[2019-03-27 00:05:23,129] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0047467e-22 1.0000000e+00 7.6771150e-27 1.6857467e-16 7.2386166e-32], sum to 1.0000
[2019-03-27 00:05:23,131] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3049
[2019-03-27 00:05:23,138] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 85.66666666666667, 1.0, 2.0, 0.5554147059510528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776134.0468403613, 776134.0468403606, 192733.7021832155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4580400.0000, 
sim time next is 4581000.0000, 
raw observation next is [28.0, 86.5, 1.0, 2.0, 0.5606127076914825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783400.390023372, 783400.3900233727, 193637.4480344224], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.865, 1.0, 1.0, 0.4706177201102199, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21761121945093667, 0.21761121945093687, 0.2890111164692872], 
reward next is 0.7110, 
noisyNet noise sample is [array([-1.783082], dtype=float32), -0.24295701]. 
=============================================
[2019-03-27 00:05:23,180] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[63.41905]
 [63.84396]
 [64.42999]
 [65.32864]
 [65.30088]], R is [[63.09381104]
 [63.17521286]
 [63.25697327]
 [63.33850098]
 [63.41915512]].
[2019-03-27 00:05:29,254] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 00:05:29,256] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:05:29,258] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:05:29,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:05:29,262] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:05:29,262] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:05:29,265] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:05:29,265] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:05:29,266] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:05:29,266] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:05:29,267] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:05:29,283] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-27 00:05:29,300] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-27 00:05:29,302] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-27 00:05:29,338] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-27 00:05:29,340] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-27 00:05:42,970] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06206692], dtype=float32), -0.22583088]
[2019-03-27 00:05:42,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.21616365166667, 94.05917315333333, 1.0, 2.0, 0.3653707428743949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548233.8356509055, 548233.8356509049, 170027.2302075395]
[2019-03-27 00:05:42,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:05:42,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1600892e-21 1.0000000e+00 2.3300745e-25 8.5293703e-17 4.8240079e-29], sampled 0.38584298160160824
[2019-03-27 00:05:44,032] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06206692], dtype=float32), -0.22583088]
[2019-03-27 00:05:44,033] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.83333333333334, 93.66666666666667, 1.0, 2.0, 0.3309185664671869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512739.7196782659, 512739.7196782652, 167725.6890182652]
[2019-03-27 00:05:44,034] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:05:44,036] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1308143e-19 1.0000000e+00 8.7877787e-24 1.2149120e-15 3.9171998e-27], sampled 0.940986119850909
[2019-03-27 00:06:30,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06206692], dtype=float32), -0.22583088]
[2019-03-27 00:06:30,059] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.35722035, 85.38277449666666, 1.0, 2.0, 0.7376509091426735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1030913.702536847, 1030913.702536847, 229181.7222499506]
[2019-03-27 00:06:30,062] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:06:30,063] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8084378e-17 1.0000000e+00 2.2081627e-21 1.9849651e-13 4.5779133e-24], sampled 0.47845028418556645
[2019-03-27 00:06:31,191] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06206692], dtype=float32), -0.22583088]
[2019-03-27 00:06:31,192] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.14832103666667, 77.68401249, 1.0, 2.0, 1.019185832550949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1424640.50114347, 1424640.50114347, 304824.6959095551]
[2019-03-27 00:06:31,193] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:06:31,196] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.0083805e-18 1.0000000e+00 8.6036588e-22 8.1507250e-14 1.3045385e-24], sampled 0.5543063401987073
[2019-03-27 00:06:33,811] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06206692], dtype=float32), -0.22583088]
[2019-03-27 00:06:33,813] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.66666666666667, 72.33333333333333, 1.0, 2.0, 0.6385049556519021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 892292.7649808782, 892292.7649808788, 208139.5210176102]
[2019-03-27 00:06:33,813] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:06:33,815] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1972425e-18 1.0000000e+00 2.3601267e-22 2.1096650e-14 2.4726803e-25], sampled 0.3304186774421034
[2019-03-27 00:07:23,061] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.8285 3164036075.7457 1776.0000
[2019-03-27 00:07:23,518] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3535 2842572453.9316 1131.0000
[2019-03-27 00:07:23,530] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1320 2927522702.9456 1338.0000
[2019-03-27 00:07:23,556] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 00:07:23,649] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.3311 2779360271.7044 934.0000
[2019-03-27 00:07:24,662] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 125000, evaluation results [125000.0, 7883.82845755634, 3164036075.7457314, 1776.0, 8254.132014516563, 2927522702.9455853, 1338.0, 8660.331104826895, 2779360271.7043743, 934.0, 7998.955910945021, 3007647868.858331, 1766.0, 8496.353453476098, 2842572453.9316177, 1131.0]
[2019-03-27 00:07:30,864] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127785: loss -252.5269
[2019-03-27 00:07:30,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127785: learning rate 0.0005
[2019-03-27 00:07:31,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0798077e-12 9.9999964e-01 3.0169382e-14 4.0125849e-07 9.1155519e-18], sum to 1.0000
[2019-03-27 00:07:31,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8212
[2019-03-27 00:07:31,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2407072.518724877 W.
[2019-03-27 00:07:31,031] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.069411831846271, 6.9112, 168.9118545961272, 2407072.518724877, 2294832.332192819, 476283.3624992569], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4791000.0000, 
sim time next is 4791600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8738960494961664, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990315528667751, 6.9112, 168.9123569798244, 2118498.828791368, 2062371.749023275, 427504.6182850457], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.8480675295134534, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007911552866775117, 0.0, 0.829437001182558, 0.588471896886491, 0.5728810413953541, 0.6380665944552921], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7255478], dtype=float32), -0.53935385]. 
=============================================
[2019-03-27 00:07:31,118] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127898: loss -269.4947
[2019-03-27 00:07:31,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127898: learning rate 0.0005
[2019-03-27 00:07:31,181] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127927: loss -169.6939
[2019-03-27 00:07:31,184] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127927: learning rate 0.0005
[2019-03-27 00:07:31,244] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127954: loss -283.7442
[2019-03-27 00:07:31,246] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127955: learning rate 0.0005
[2019-03-27 00:07:31,251] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127957: loss -199.8569
[2019-03-27 00:07:31,255] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127957: learning rate 0.0005
[2019-03-27 00:07:31,269] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127963: loss -265.0406
[2019-03-27 00:07:31,270] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127964: learning rate 0.0005
[2019-03-27 00:07:31,302] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127978: loss -184.4904
[2019-03-27 00:07:31,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127979: learning rate 0.0005
[2019-03-27 00:07:31,323] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127987: loss -243.1633
[2019-03-27 00:07:31,326] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127988: loss -45.2608
[2019-03-27 00:07:31,329] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127988: loss -155.6306
[2019-03-27 00:07:31,330] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127988: learning rate 0.0005
[2019-03-27 00:07:31,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127988: learning rate 0.0005
[2019-03-27 00:07:31,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127988: learning rate 0.0005
[2019-03-27 00:07:31,368] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128005: loss -231.5060
[2019-03-27 00:07:31,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128006: learning rate 0.0005
[2019-03-27 00:07:31,437] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128036: loss -104.3626
[2019-03-27 00:07:31,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128036: learning rate 0.0005
[2019-03-27 00:07:31,541] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 128084: loss -97.9237
[2019-03-27 00:07:31,543] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 128084: learning rate 0.0005
[2019-03-27 00:07:31,548] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128085: loss -217.2725
[2019-03-27 00:07:31,551] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128085: loss -134.1284
[2019-03-27 00:07:31,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128085: learning rate 0.0005
[2019-03-27 00:07:31,554] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128085: learning rate 0.0005
[2019-03-27 00:07:31,620] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 128116: loss -137.3289
[2019-03-27 00:07:31,623] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 128116: learning rate 0.0005
[2019-03-27 00:07:32,372] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0940996e-18 1.0000000e+00 2.5961917e-20 8.8327782e-17 4.1757312e-25], sum to 1.0000
[2019-03-27 00:07:32,384] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1413
[2019-03-27 00:07:32,390] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 70.0, 1.0, 2.0, 0.5105331950420744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713395.7618843167, 713395.7618843167, 185264.1819900584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4817400.0000, 
sim time next is 4818000.0000, 
raw observation next is [29.66666666666667, 70.0, 1.0, 2.0, 0.5098914425048809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712498.7054627757, 712498.7054627764, 185161.3104872922], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590839, 0.7, 1.0, 1.0, 0.4095077620540734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19791630707299324, 0.19791630707299343, 0.2763601649064063], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.7261706], dtype=float32), -1.0313433]. 
=============================================
[2019-03-27 00:07:32,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[43.82148 ]
 [43.296486]
 [42.02547 ]
 [40.18144 ]
 [37.730022]], R is [[44.22584534]
 [44.50707626]
 [44.78598404]
 [45.06327438]
 [45.33909988]].
[2019-03-27 00:07:38,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.1258219e-27 1.0000000e+00 3.9752680e-31 1.9376120e-24 1.5533157e-37], sum to 1.0000
[2019-03-27 00:07:38,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8553
[2019-03-27 00:07:38,146] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 76.5, 1.0, 2.0, 0.4890598876174903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683380.2914506076, 683380.2914506069, 181899.5067941759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4912200.0000, 
sim time next is 4912800.0000, 
raw observation next is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4870278561144585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 181588.3701370411], 
processed observation next is [1.0, 0.8695652173913043, 0.4944707740916275, 0.7733333333333333, 1.0, 1.0, 0.38196127242705846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1890388761239808, 0.1890388761239806, 0.27102741811498676], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.09897581], dtype=float32), -0.15039232]. 
=============================================
[2019-03-27 00:07:42,130] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8924688e-15 1.0000000e+00 1.4323552e-18 3.9760604e-14 3.5521856e-22], sum to 1.0000
[2019-03-27 00:07:42,137] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2719
[2019-03-27 00:07:42,147] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2157581.718582312 W.
[2019-03-27 00:07:42,152] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.63333333333334, 63.83333333333334, 1.0, 2.0, 0.7715067825273418, 1.0, 2.0, 0.7715067825273418, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2157581.718582312, 2157581.718582312, 406231.4990898446], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4975800.0000, 
sim time next is 4976400.0000, 
raw observation next is [30.66666666666667, 63.66666666666667, 1.0, 2.0, 0.9440937459790752, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979252964763722, 6.9112, 168.912551397113, 2216753.535547278, 2168474.53593209, 447521.4661426086], 
processed observation next is [1.0, 0.6086956521739131, 0.6524486571879939, 0.6366666666666667, 1.0, 1.0, 0.932643067444669, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006805296476372203, 0.0, 0.8294379558604495, 0.6157648709853549, 0.6023540377589139, 0.6679424867800129], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46452722], dtype=float32), 0.65737325]. 
=============================================
[2019-03-27 00:07:47,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9838493e-29 1.0000000e+00 3.6935309e-31 4.8065305e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:07:47,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0846
[2019-03-27 00:07:47,523] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 68.83333333333333, 1.0, 2.0, 0.5281310652742466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737994.7638898012, 737994.7638898012, 188121.4172582703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5071800.0000, 
sim time next is 5072400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5299414630530386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740525.4421315795, 740525.4421315801, 188420.6451424913], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4336644133169139, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20570151170321652, 0.20570151170321668, 0.28122484349625565], 
reward next is 0.7188, 
noisyNet noise sample is [array([-0.5901052], dtype=float32), -0.058469642]. 
=============================================
[2019-03-27 00:07:48,706] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135710: loss 4.9551
[2019-03-27 00:07:48,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135710: learning rate 0.0005
[2019-03-27 00:07:49,101] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 135889: loss 1.7747
[2019-03-27 00:07:49,103] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 135890: learning rate 0.0005
[2019-03-27 00:07:49,134] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135902: loss 2.4897
[2019-03-27 00:07:49,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135903: learning rate 0.0005
[2019-03-27 00:07:49,167] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135920: loss 2.4591
[2019-03-27 00:07:49,169] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135920: learning rate 0.0005
[2019-03-27 00:07:49,217] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135939: loss 2.5553
[2019-03-27 00:07:49,219] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135939: learning rate 0.0005
[2019-03-27 00:07:49,224] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135940: loss 2.8783
[2019-03-27 00:07:49,228] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135940: learning rate 0.0005
[2019-03-27 00:07:49,276] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 135966: loss 3.6816
[2019-03-27 00:07:49,280] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 135967: learning rate 0.0005
[2019-03-27 00:07:49,287] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135969: loss 3.0527
[2019-03-27 00:07:49,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135970: learning rate 0.0005
[2019-03-27 00:07:49,353] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136000: loss 3.8968
[2019-03-27 00:07:49,357] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136000: learning rate 0.0005
[2019-03-27 00:07:49,368] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136008: loss 3.9745
[2019-03-27 00:07:49,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136009: learning rate 0.0005
[2019-03-27 00:07:49,384] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136016: loss 3.6962
[2019-03-27 00:07:49,389] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136016: learning rate 0.0005
[2019-03-27 00:07:49,504] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 136071: loss 3.7436
[2019-03-27 00:07:49,506] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 136071: learning rate 0.0005
[2019-03-27 00:07:49,523] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136076: loss 2.9327
[2019-03-27 00:07:49,526] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136078: learning rate 0.0005
[2019-03-27 00:07:49,613] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136117: loss 2.5292
[2019-03-27 00:07:49,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136117: learning rate 0.0005
[2019-03-27 00:07:49,653] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136135: loss 2.4223
[2019-03-27 00:07:49,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136135: learning rate 0.0005
[2019-03-27 00:07:49,695] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 136150: loss 2.1524
[2019-03-27 00:07:49,698] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 136150: learning rate 0.0005
[2019-03-27 00:07:54,619] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.5677861e-27 1.0000000e+00 4.6446950e-28 4.9866773e-22 8.4208182e-37], sum to 1.0000
[2019-03-27 00:07:54,629] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6322
[2019-03-27 00:07:54,634] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5171155593673222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722596.7855046365, 722596.7855046365, 186320.7193050761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5186400.0000, 
sim time next is 5187000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5168586989980881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722237.7369859146, 722237.7369859153, 186279.1976108113], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4179020469856483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006215936071985, 0.2006215936071987, 0.27802865315046466], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.77877134], dtype=float32), 0.77220553]. 
=============================================
[2019-03-27 00:07:54,657] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.63798 ]
 [69.18498 ]
 [69.85952 ]
 [70.57959 ]
 [71.516594]], R is [[68.15333557]
 [68.19371033]
 [68.23384857]
 [68.27384186]
 [68.313797  ]].
[2019-03-27 00:07:59,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7124418e-21 1.0000000e+00 6.6762117e-25 1.7449716e-18 1.9545936e-28], sum to 1.0000
[2019-03-27 00:07:59,762] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7020
[2019-03-27 00:07:59,770] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.51666666666667, 84.33333333333333, 1.0, 2.0, 0.563889616088756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787981.2414684966, 787981.2414684966, 194211.5965354411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5271000.0000, 
sim time next is 5271600.0000, 
raw observation next is [28.53333333333334, 84.66666666666667, 1.0, 2.0, 0.5661469796991381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791136.8637080308, 791136.8637080308, 194608.7906780089], 
processed observation next is [1.0, 0.0, 0.5513428120063194, 0.8466666666666667, 1.0, 1.0, 0.4772855177098049, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21976023991889743, 0.21976023991889743, 0.2904608816089685], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.16210139], dtype=float32), -0.7215884]. 
=============================================
[2019-03-27 00:08:04,535] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4270049e-22 1.0000000e+00 2.5336297e-23 2.6090015e-18 1.0877840e-34], sum to 1.0000
[2019-03-27 00:08:04,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5149
[2019-03-27 00:08:04,549] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.98333333333333, 83.5, 1.0, 2.0, 0.6136009548963426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857476.0736089996, 857476.0736089996, 203307.4196781815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5352600.0000, 
sim time next is 5353200.0000, 
raw observation next is [29.9, 84.0, 1.0, 2.0, 0.6127049561961891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 856223.4563763249, 856223.4563763243, 203137.0524924579], 
processed observation next is [1.0, 1.0, 0.6161137440758293, 0.84, 1.0, 1.0, 0.5333794652966134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23783984899342359, 0.23783984899342342, 0.3031896305857581], 
reward next is 0.6968, 
noisyNet noise sample is [array([-0.16602743], dtype=float32), -0.024626229]. 
=============================================
[2019-03-27 00:08:04,741] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2585036e-23 1.0000000e+00 2.0052255e-27 1.2911990e-19 2.5452874e-35], sum to 1.0000
[2019-03-27 00:08:04,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9485
[2019-03-27 00:08:04,760] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 84.0, 1.0, 2.0, 0.605396113386456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846005.6591666734, 846005.6591666734, 201755.7752975868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5356800.0000, 
sim time next is 5357400.0000, 
raw observation next is [29.66666666666666, 84.16666666666667, 1.0, 2.0, 0.606432858766389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 847455.0281843399, 847455.0281843405, 201950.649182032], 
processed observation next is [1.0, 0.0, 0.6050552922590835, 0.8416666666666667, 1.0, 1.0, 0.5258227214052879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23540417449564996, 0.23540417449565013, 0.3014188793761672], 
reward next is 0.6986, 
noisyNet noise sample is [array([1.3567485], dtype=float32), -0.7567578]. 
=============================================
[2019-03-27 00:08:05,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6946112e-17 1.0000000e+00 2.8498579e-20 3.7372563e-15 7.9758144e-27], sum to 1.0000
[2019-03-27 00:08:05,782] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8681
[2019-03-27 00:08:05,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2028205.138446692 W.
[2019-03-27 00:08:05,793] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.73333333333333, 91.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.72039523911099, 6.9112, 168.908845771469, 2028205.138446692, 1454148.15631027, 311356.1457542186], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5370000.0000, 
sim time next is 5370600.0000, 
raw observation next is [28.66666666666667, 91.5, 1.0, 2.0, 0.6221228919030325, 1.0, 1.0, 0.6221228919030325, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1739478.327178713, 1739478.327178713, 342580.6624070461], 
processed observation next is [1.0, 0.13043478260869565, 0.5576619273301741, 0.915, 1.0, 1.0, 0.5447263757867861, 1.0, 0.5, 0.5447263757867861, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48318842421630914, 0.48318842421630914, 0.511314421503054], 
reward next is 0.4887, 
noisyNet noise sample is [array([-0.35916686], dtype=float32), -0.5125394]. 
=============================================
[2019-03-27 00:08:06,679] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143739: loss 8.3156
[2019-03-27 00:08:06,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143739: learning rate 0.0005
[2019-03-27 00:08:07,056] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143906: loss -11.2614
[2019-03-27 00:08:07,060] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 143906: loss -48.4542
[2019-03-27 00:08:07,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143906: learning rate 0.0005
[2019-03-27 00:08:07,064] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 143907: learning rate 0.0005
[2019-03-27 00:08:07,077] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 143916: loss 3.1829
[2019-03-27 00:08:07,081] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 143916: learning rate 0.0005
[2019-03-27 00:08:07,117] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143930: loss -31.8983
[2019-03-27 00:08:07,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143930: learning rate 0.0005
[2019-03-27 00:08:07,145] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143941: loss -30.0854
[2019-03-27 00:08:07,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143941: learning rate 0.0005
[2019-03-27 00:08:07,294] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144009: loss -28.2620
[2019-03-27 00:08:07,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144009: learning rate 0.0005
[2019-03-27 00:08:07,319] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144015: loss -29.1458
[2019-03-27 00:08:07,321] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144016: learning rate 0.0005
[2019-03-27 00:08:07,334] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144022: loss -4.9154
[2019-03-27 00:08:07,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144022: learning rate 0.0005
[2019-03-27 00:08:07,337] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144023: loss -4.3840
[2019-03-27 00:08:07,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144024: learning rate 0.0005
[2019-03-27 00:08:07,353] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144029: loss -11.5002
[2019-03-27 00:08:07,354] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144029: learning rate 0.0005
[2019-03-27 00:08:07,355] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144029: loss -24.1414
[2019-03-27 00:08:07,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144029: learning rate 0.0005
[2019-03-27 00:08:07,449] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 144075: loss -6.8977
[2019-03-27 00:08:07,450] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 144076: learning rate 0.0005
[2019-03-27 00:08:07,457] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144079: loss -27.0292
[2019-03-27 00:08:07,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144079: learning rate 0.0005
[2019-03-27 00:08:07,504] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144096: loss -11.7337
[2019-03-27 00:08:07,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144096: learning rate 0.0005
[2019-03-27 00:08:07,538] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 144106: loss 15.9362
[2019-03-27 00:08:07,542] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 144108: learning rate 0.0005
[2019-03-27 00:08:20,738] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 00:08:20,741] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:08:20,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:08:20,744] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:08:20,746] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:08:20,746] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:08:20,747] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:08:20,748] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:08:20,750] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:08:20,751] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:08:20,752] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:08:20,770] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-27 00:08:20,771] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-27 00:08:20,789] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-27 00:08:20,825] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-27 00:08:20,826] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-27 00:08:22,594] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:08:22,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.6, 75.0, 1.0, 2.0, 0.3830790173771025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 572194.3413916377, 572194.3413916371, 172026.4423224937]
[2019-03-27 00:08:22,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:08:22,598] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.18794116e-26 1.00000000e+00 1.18676734e-32 1.36744778e-23
 0.00000000e+00], sampled 0.9826187098027893
[2019-03-27 00:08:24,181] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:08:24,183] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.22887276666667, 95.01452770666667, 1.0, 2.0, 0.3929017218224894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591927.7751745739, 591927.7751745745, 173957.5667791312]
[2019-03-27 00:08:24,186] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:08:24,189] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.3893277e-29 1.0000000e+00 7.9185716e-36 3.1622426e-26 0.0000000e+00], sampled 0.7699587455330561
[2019-03-27 00:08:30,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:08:30,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.35620656333333, 80.86247533, 1.0, 2.0, 0.2238924005891341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 372476.095207257, 372476.0952072564, 158065.1661943387]
[2019-03-27 00:08:30,550] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:08:30,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0759953e-31 1.0000000e+00 4.5730285e-38 3.4639112e-28 0.0000000e+00], sampled 0.06742431588659636
[2019-03-27 00:09:20,507] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:09:20,509] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.2, 58.5, 1.0, 2.0, 0.9373468041376533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1310173.505762834, 1310173.505762834, 280424.4222729744]
[2019-03-27 00:09:20,510] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:09:20,514] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.3025088e-27 1.0000000e+00 2.7409250e-33 2.5012847e-24 0.0000000e+00], sampled 0.7206833021046141
[2019-03-27 00:09:28,355] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:09:28,357] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.6, 47.0, 1.0, 2.0, 0.5590836251629913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781262.8637610411, 781262.8637610411, 193370.3347134588]
[2019-03-27 00:09:28,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:09:28,362] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0276318e-27 1.0000000e+00 1.4985036e-33 2.9143979e-24 0.0000000e+00], sampled 0.13325569064530896
[2019-03-27 00:09:35,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:09:35,648] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.66666666666667, 84.0, 1.0, 2.0, 0.4974532312340174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695112.4379774907, 695112.4379774913, 183198.0284346595]
[2019-03-27 00:09:35,648] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:09:35,651] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5108497e-26 1.0000000e+00 9.5752383e-33 7.5771573e-24 0.0000000e+00], sampled 0.45946122386027755
[2019-03-27 00:09:49,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:09:49,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.85, 65.83333333333333, 1.0, 2.0, 0.9976959166268431, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.989320289578028, 6.9112, 168.912492064305, 2291777.04901073, 2236355.979278409, 463216.1114242952]
[2019-03-27 00:09:49,584] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:09:49,586] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.3319767e-21 1.0000000e+00 2.4949355e-26 2.2818354e-18 1.5098072e-33], sampled 0.2824729810472091
[2019-03-27 00:09:49,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2291777.04901073 W.
[2019-03-27 00:09:57,458] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:09:57,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.66666666666667, 91.0, 1.0, 2.0, 0.4961977556352681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693357.5362922495, 693357.5362922495, 183002.5576879114]
[2019-03-27 00:09:57,462] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:09:57,463] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.3091202e-28 1.0000000e+00 1.6807870e-34 4.9299083e-25 0.0000000e+00], sampled 0.6860477761114872
[2019-03-27 00:09:57,702] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:09:57,703] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.8, 94.0, 1.0, 2.0, 0.5111793242981645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714298.936949272, 714298.9369492728, 185364.2629409365]
[2019-03-27 00:09:57,704] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:09:57,706] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6420401e-27 1.0000000e+00 1.9570433e-33 3.0280446e-24 0.0000000e+00], sampled 0.8408204198234586
[2019-03-27 00:10:07,958] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:10:07,959] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 61.5, 1.0, 2.0, 0.4595679941296346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656262.8457540879, 656262.8457540872, 179314.8206682997]
[2019-03-27 00:10:07,960] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:10:07,964] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3944320e-28 1.0000000e+00 3.2699486e-34 5.6899393e-25 0.0000000e+00], sampled 0.6609742206871979
[2019-03-27 00:10:13,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.2433709]
[2019-03-27 00:10:13,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.3, 84.66666666666667, 1.0, 2.0, 0.8840033863124621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1235569.527885873, 1235569.527885874, 265572.1338029551]
[2019-03-27 00:10:13,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:10:13,221] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6651676e-25 1.0000000e+00 6.2768229e-31 3.9634904e-22 0.0000000e+00], sampled 0.9981729066601392
[2019-03-27 00:10:14,796] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 00:10:15,030] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:10:15,253] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1203 2842500573.0697 1131.0000
[2019-03-27 00:10:15,436] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6673 3164067602.5339 1778.0000
[2019-03-27 00:10:15,449] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 00:10:16,468] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 150000, evaluation results [150000.0, 7882.66734019175, 3164067602.533886, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8496.120309717919, 2842500573.0697403, 1131.0]
[2019-03-27 00:10:20,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8085138e-36 1.0000000e+00 0.0000000e+00 3.5281429e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:10:20,335] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8318
[2019-03-27 00:10:20,343] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 78.5, 1.0, 2.0, 0.535332250997685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748061.0315799187, 748061.0315799187, 189317.072221138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5686200.0000, 
sim time next is 5686800.0000, 
raw observation next is [28.33333333333333, 80.0, 1.0, 2.0, 0.5360825558341042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749109.8600838475, 749109.8600838475, 189442.5785884544], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8, 1.0, 1.0, 0.4410633202820533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2080860722455132, 0.2080860722455132, 0.2827501172962006], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.67249537], dtype=float32), 1.8318342]. 
=============================================
[2019-03-27 00:10:20,372] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151748: loss 0.3790
[2019-03-27 00:10:20,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151748: learning rate 0.0005
[2019-03-27 00:10:20,592] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151844: loss 0.0580
[2019-03-27 00:10:20,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151845: learning rate 0.0005
[2019-03-27 00:10:20,629] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151865: loss 0.0361
[2019-03-27 00:10:20,634] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151866: learning rate 0.0005
[2019-03-27 00:10:20,676] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 151883: loss 0.0156
[2019-03-27 00:10:20,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 151884: learning rate 0.0005
[2019-03-27 00:10:20,812] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151944: loss 0.0044
[2019-03-27 00:10:20,815] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151945: learning rate 0.0005
[2019-03-27 00:10:20,892] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151980: loss 0.0048
[2019-03-27 00:10:20,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151980: learning rate 0.0005
[2019-03-27 00:10:20,898] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151980: loss 0.0128
[2019-03-27 00:10:20,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151982: learning rate 0.0005
[2019-03-27 00:10:20,916] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151990: loss 0.0003
[2019-03-27 00:10:20,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151990: learning rate 0.0005
[2019-03-27 00:10:20,937] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 151999: loss 0.0025
[2019-03-27 00:10:20,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 151999: learning rate 0.0005
[2019-03-27 00:10:20,997] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152024: loss 0.0005
[2019-03-27 00:10:21,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152025: learning rate 0.0005
[2019-03-27 00:10:21,018] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 152036: loss 0.0016
[2019-03-27 00:10:21,019] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 152036: learning rate 0.0005
[2019-03-27 00:10:21,064] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152056: loss 0.0048
[2019-03-27 00:10:21,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152057: learning rate 0.0005
[2019-03-27 00:10:21,168] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152102: loss 0.0006
[2019-03-27 00:10:21,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152103: learning rate 0.0005
[2019-03-27 00:10:21,196] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152117: loss 0.0167
[2019-03-27 00:10:21,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152117: learning rate 0.0005
[2019-03-27 00:10:21,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9140576e-36 1.0000000e+00 0.0000000e+00 2.4458163e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:10:21,231] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 152132: loss 0.0012
[2019-03-27 00:10:21,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2273
[2019-03-27 00:10:21,234] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 152133: learning rate 0.0005
[2019-03-27 00:10:21,244] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152137: loss 0.0103
[2019-03-27 00:10:21,245] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5186722328170297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724772.7613659343, 724772.761365935, 186572.6404854767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5705400.0000, 
sim time next is 5706000.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5168852551501861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722274.8581099614, 722274.8581099614, 186283.3548427695], 
processed observation next is [0.0, 0.043478260869565216, 0.4549763033175356, 0.87, 1.0, 1.0, 0.41793404234962184, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20063190503054482, 0.20063190503054482, 0.27803485797428285], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.43405235], dtype=float32), -0.532386]. 
=============================================
[2019-03-27 00:10:21,246] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152137: learning rate 0.0005
[2019-03-27 00:10:21,270] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.40598 ]
 [72.835434]
 [73.29855 ]
 [73.84965 ]
 [74.548416]], R is [[72.12050629]
 [72.12083435]
 [72.12097931]
 [72.12160492]
 [72.12284088]].
[2019-03-27 00:10:25,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0487578e-32 1.0000000e+00 0.0000000e+00 5.0647773e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:10:25,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0359
[2019-03-27 00:10:25,546] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 86.5, 1.0, 2.0, 0.5406072735995959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755434.8485502941, 755434.8485502935, 190202.6357142658], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5783400.0000, 
sim time next is 5784000.0000, 
raw observation next is [27.3, 86.66666666666667, 1.0, 2.0, 0.5397328615130403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754212.5270855926, 754212.527085592, 190055.2146745392], 
processed observation next is [0.0, 0.9565217391304348, 0.4928909952606636, 0.8666666666666667, 1.0, 1.0, 0.4454612789313739, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20950347974599795, 0.20950347974599778, 0.2836644995142376], 
reward next is 0.7163, 
noisyNet noise sample is [array([2.1089706], dtype=float32), -1.7200536]. 
=============================================
[2019-03-27 00:10:25,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.38867]
 [86.33373]
 [86.27747]
 [86.20162]
 [86.10228]], R is [[86.29872894]
 [86.1518631 ]
 [86.00621796]
 [85.86155701]
 [85.71812439]].
[2019-03-27 00:10:29,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7909822e-20 1.0000000e+00 6.3853816e-23 8.1046706e-17 3.2311336e-29], sum to 1.0000
[2019-03-27 00:10:29,345] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5920
[2019-03-27 00:10:29,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2836817.352417134 W.
[2019-03-27 00:10:29,357] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.8, 61.33333333333334, 1.0, 2.0, 1.014108339077098, 1.0, 2.0, 1.014108339077098, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2836817.352417134, 2836817.352417133, 537566.9169899833], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5840400.0000, 
sim time next is 5841000.0000, 
raw observation next is [32.75, 61.5, 1.0, 2.0, 0.7359317419947886, 1.0, 2.0, 0.6885559105116568, 1.0, 1.0, 1.03, 7.005100565511464, 6.9112, 170.5573041426782, 2889259.809153663, 2821995.014447009, 532917.5727374038], 
processed observation next is [1.0, 0.6086956521739131, 0.7511848341232228, 0.615, 1.0, 1.0, 0.6818454722828778, 1.0, 1.0, 0.6247661572429599, 1.0, 0.5, 1.0365853658536586, 0.009390056551146397, 0.0, 0.8375144448122397, 0.8025721692093508, 0.7838875040130581, 0.7953993622946325], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08890615], dtype=float32), -1.6931031]. 
=============================================
[2019-03-27 00:10:29,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[37.588837]
 [36.99676 ]
 [37.186935]
 [37.947968]
 [38.47426 ]], R is [[37.48297119]
 [37.30580521]
 [36.93274689]
 [36.56341934]
 [36.19778442]].
[2019-03-27 00:10:36,923] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.8355275e-30 1.0000000e+00 2.3094067e-37 1.5062402e-26 0.0000000e+00], sum to 1.0000
[2019-03-27 00:10:36,931] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3696
[2019-03-27 00:10:36,938] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 91.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.974264896367111, 6.9112, 168.9602238291896, 1498538.21340181, 1453785.285576813, 311358.1659200625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5969400.0000, 
sim time next is 5970000.0000, 
raw observation next is [26.33333333333334, 91.33333333333334, 1.0, 2.0, 0.954837199340371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129051603066, 1334636.016869436, 1334636.016869436, 285466.4439005664], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271754, 0.9133333333333334, 1.0, 1.0, 0.9455869871570736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294396929996837, 0.3707322269081767, 0.3707322269081767, 0.4260693192545767], 
reward next is 0.5739, 
noisyNet noise sample is [array([0.16732316], dtype=float32), -1.9450196]. 
=============================================
[2019-03-27 00:10:36,964] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[65.73659 ]
 [67.44293 ]
 [67.448135]
 [67.66551 ]
 [67.8463  ]], R is [[65.34568787]
 [64.9121933 ]
 [64.98329163]
 [65.05322266]
 [65.12200165]].
[2019-03-27 00:10:37,707] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9499205e-25 1.0000000e+00 1.0920953e-27 8.8074450e-24 1.8042163e-37], sum to 1.0000
[2019-03-27 00:10:37,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1622
[2019-03-27 00:10:37,727] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 90.66666666666666, 1.0, 2.0, 0.6207729797828606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 867502.7060887204, 867502.7060887197, 204673.5221518826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5982600.0000, 
sim time next is 5983200.0000, 
raw observation next is [26.8, 90.0, 1.0, 2.0, 0.7027348165219205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 982093.6955925846, 982093.6955925851, 221436.6562364791], 
processed observation next is [1.0, 0.2608695652173913, 0.4691943127962086, 0.9, 1.0, 1.0, 0.6418491765324343, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2728038043312735, 0.27280380433127366, 0.3305024719947449], 
reward next is 0.6695, 
noisyNet noise sample is [array([0.6980791], dtype=float32), 0.037494972]. 
=============================================
[2019-03-27 00:10:38,360] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159775: loss 108.4542
[2019-03-27 00:10:38,363] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159775: learning rate 0.0005
[2019-03-27 00:10:38,443] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159811: loss -23.2828
[2019-03-27 00:10:38,445] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159811: learning rate 0.0005
[2019-03-27 00:10:38,590] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159880: loss -33.1926
[2019-03-27 00:10:38,592] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159880: learning rate 0.0005
[2019-03-27 00:10:38,746] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 159948: loss 3.5796
[2019-03-27 00:10:38,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 159948: learning rate 0.0005
[2019-03-27 00:10:38,785] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159963: loss -34.3340
[2019-03-27 00:10:38,788] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159964: learning rate 0.0005
[2019-03-27 00:10:38,793] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159966: loss -93.5613
[2019-03-27 00:10:38,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159967: learning rate 0.0005
[2019-03-27 00:10:38,808] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159970: loss -60.0628
[2019-03-27 00:10:38,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159970: learning rate 0.0005
[2019-03-27 00:10:38,876] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159996: loss -91.4554
[2019-03-27 00:10:38,877] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159997: learning rate 0.0005
[2019-03-27 00:10:38,885] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160001: loss -69.8062
[2019-03-27 00:10:38,888] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160001: learning rate 0.0005
[2019-03-27 00:10:38,948] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 160030: loss 31.8725
[2019-03-27 00:10:38,950] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 160031: learning rate 0.0005
[2019-03-27 00:10:38,971] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160040: loss -23.6600
[2019-03-27 00:10:38,974] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160041: learning rate 0.0005
[2019-03-27 00:10:39,030] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 160064: loss -10.6107
[2019-03-27 00:10:39,031] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160064: loss -80.5191
[2019-03-27 00:10:39,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160064: learning rate 0.0005
[2019-03-27 00:10:39,034] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 160064: learning rate 0.0005
[2019-03-27 00:10:39,052] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160072: loss -10.1701
[2019-03-27 00:10:39,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160073: learning rate 0.0005
[2019-03-27 00:10:39,090] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160088: loss 38.9003
[2019-03-27 00:10:39,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160089: learning rate 0.0005
[2019-03-27 00:10:39,177] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160126: loss -19.7175
[2019-03-27 00:10:39,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160127: learning rate 0.0005
[2019-03-27 00:10:44,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4464789e-20 1.0000000e+00 4.2686798e-24 4.0302610e-19 4.2951795e-29], sum to 1.0000
[2019-03-27 00:10:44,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3983
[2019-03-27 00:10:44,614] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1696603.358175374 W.
[2019-03-27 00:10:44,619] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.08333333333333, 65.0, 1.0, 2.0, 0.4045338842064264, 1.0, 1.0, 0.4045338842064264, 1.0, 2.0, 0.6974461982981641, 6.911199999999999, 6.9112, 170.5573041426782, 1696603.358175374, 1696603.358175375, 354690.710788691], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6094200.0000, 
sim time next is 6094800.0000, 
raw observation next is [31.1, 65.0, 1.0, 2.0, 0.364311195442476, 1.0, 2.0, 0.364311195442476, 1.0, 2.0, 0.628473944145749, 6.9112, 6.9112, 170.5573041426782, 1527790.328072276, 1527790.328072276, 333486.348540328], 
processed observation next is [1.0, 0.5652173913043478, 0.6729857819905214, 0.65, 1.0, 1.0, 0.23410987402707947, 1.0, 1.0, 0.23410987402707947, 1.0, 1.0, 0.5469194440801817, 0.0, 0.0, 0.8375144448122397, 0.4243862022422989, 0.4243862022422989, 0.49774081871690745], 
reward next is 0.5023, 
noisyNet noise sample is [array([-0.4080937], dtype=float32), -0.06372545]. 
=============================================
[2019-03-27 00:10:47,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1808664e-23 1.0000000e+00 8.0953599e-27 5.9806239e-22 3.6097526e-35], sum to 1.0000
[2019-03-27 00:10:47,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1963
[2019-03-27 00:10:47,421] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.0, 1.0, 2.0, 0.7691406544920268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1074944.883617544, 1074944.883617544, 236477.1538081265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6145200.0000, 
sim time next is 6145800.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.7764128767016052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1085113.676703261, 1085113.676703261, 238203.5778158413], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.7306179237368737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3014204657509058, 0.3014204657509058, 0.3555277280833452], 
reward next is 0.6445, 
noisyNet noise sample is [array([-1.3640087], dtype=float32), -0.1144047]. 
=============================================
[2019-03-27 00:10:51,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3247500e-29 1.0000000e+00 2.1557449e-35 1.6465814e-26 0.0000000e+00], sum to 1.0000
[2019-03-27 00:10:51,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7644
[2019-03-27 00:10:51,391] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 88.66666666666666, 1.0, 2.0, 0.521727724133706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729043.8533712581, 729043.8533712574, 187070.6405458976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6219600.0000, 
sim time next is 6220200.0000, 
raw observation next is [26.71666666666667, 88.83333333333334, 1.0, 2.0, 0.5217213575510007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729034.9538810509, 729034.9538810509, 187069.6210340689], 
processed observation next is [1.0, 1.0, 0.46524486571879947, 0.8883333333333334, 1.0, 1.0, 0.4237606717481936, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20250970941140303, 0.20250970941140303, 0.2792083896030879], 
reward next is 0.7208, 
noisyNet noise sample is [array([-0.4451178], dtype=float32), 0.7068856]. 
=============================================
[2019-03-27 00:10:54,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.421516e-35 1.000000e+00 0.000000e+00 9.703083e-34 0.000000e+00], sum to 1.0000
[2019-03-27 00:10:54,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9684
[2019-03-27 00:10:54,218] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 68.0, 1.0, 2.0, 0.5401834855089805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754842.4443038256, 754842.444303825, 190131.8280194779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6262200.0000, 
sim time next is 6262800.0000, 
raw observation next is [30.66666666666666, 67.66666666666667, 1.0, 2.0, 0.5385036327451984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752494.2175479198, 752494.2175479204, 189849.0417139439], 
processed observation next is [0.0, 0.4782608695652174, 0.6524486571879934, 0.6766666666666667, 1.0, 1.0, 0.44398028041590165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20902617154108885, 0.20902617154108902, 0.28335677867752823], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.182973], dtype=float32), -1.3061634]. 
=============================================
[2019-03-27 00:10:54,391] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:10:54,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6993
[2019-03-27 00:10:54,407] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.33333333333334, 1.0, 2.0, 0.5419008612283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757243.1288852462, 757243.1288852456, 190422.0233671023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6258000.0000, 
sim time next is 6258600.0000, 
raw observation next is [30.15, 71.5, 1.0, 2.0, 0.5420248353274145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757416.4299662584, 757416.4299662578, 190442.9864727354], 
processed observation next is [0.0, 0.43478260869565216, 0.6279620853080569, 0.715, 1.0, 1.0, 0.4482226931655596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2103934527684051, 0.21039345276840493, 0.2842432633921424], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.68380153], dtype=float32), -0.46901187]. 
=============================================
[2019-03-27 00:10:56,146] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167730: loss 0.0540
[2019-03-27 00:10:56,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167730: learning rate 0.0005
[2019-03-27 00:10:56,285] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 167794: loss 0.0084
[2019-03-27 00:10:56,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 167794: learning rate 0.0005
[2019-03-27 00:10:56,408] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167851: loss 0.0630
[2019-03-27 00:10:56,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167851: learning rate 0.0005
[2019-03-27 00:10:56,575] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 167925: loss 0.1545
[2019-03-27 00:10:56,578] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 167925: learning rate 0.0005
[2019-03-27 00:10:56,614] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167940: loss 0.1744
[2019-03-27 00:10:56,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167941: learning rate 0.0005
[2019-03-27 00:10:56,667] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167965: loss 0.0558
[2019-03-27 00:10:56,670] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167966: learning rate 0.0005
[2019-03-27 00:10:56,686] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167973: loss 0.0401
[2019-03-27 00:10:56,688] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167974: learning rate 0.0005
[2019-03-27 00:10:56,701] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167979: loss 0.0659
[2019-03-27 00:10:56,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167980: learning rate 0.0005
[2019-03-27 00:10:56,789] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168018: loss 0.0038
[2019-03-27 00:10:56,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168020: learning rate 0.0005
[2019-03-27 00:10:56,837] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168040: loss 0.0012
[2019-03-27 00:10:56,840] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168042: learning rate 0.0005
[2019-03-27 00:10:56,886] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 168064: loss 0.0081
[2019-03-27 00:10:56,887] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 168064: learning rate 0.0005
[2019-03-27 00:10:56,896] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168066: loss 0.0197
[2019-03-27 00:10:56,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168067: learning rate 0.0005
[2019-03-27 00:10:56,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 168077: loss 0.0120
[2019-03-27 00:10:56,924] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 168078: learning rate 0.0005
[2019-03-27 00:10:56,934] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168084: loss 0.0006
[2019-03-27 00:10:56,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168086: learning rate 0.0005
[2019-03-27 00:10:56,963] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168097: loss 0.0100
[2019-03-27 00:10:56,965] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168098: learning rate 0.0005
[2019-03-27 00:10:57,169] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168189: loss 0.0023
[2019-03-27 00:10:57,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168191: learning rate 0.0005
[2019-03-27 00:11:01,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:11:01,653] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3657
[2019-03-27 00:11:01,659] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 82.0, 1.0, 2.0, 0.5123752175565013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715970.587428095, 715970.5874280944, 185557.364716329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6390000.0000, 
sim time next is 6390600.0000, 
raw observation next is [27.18333333333333, 82.16666666666667, 1.0, 2.0, 0.5121313591848884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715629.715685423, 715629.715685423, 185518.3057289357], 
processed observation next is [0.0, 1.0, 0.48736176935229053, 0.8216666666666668, 1.0, 1.0, 0.4122064568492631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19878603213483972, 0.19878603213483972, 0.27689299362527714], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.8407526], dtype=float32), -0.29765597]. 
=============================================
[2019-03-27 00:11:02,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3811578e-33 1.0000000e+00 0.0000000e+00 1.9088872e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 00:11:02,409] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3035
[2019-03-27 00:11:02,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.5126055159279782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716292.5047409645, 716292.5047409652, 185594.4002182946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6394200.0000, 
sim time next is 6394800.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 0.5124905265649867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716131.769476908, 716131.769476908, 185575.9632581578], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.83, 1.0, 1.0, 0.4126391886325141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19892549152136335, 0.19892549152136335, 0.2769790496390415], 
reward next is 0.7230, 
noisyNet noise sample is [array([-0.66505504], dtype=float32), -0.024334108]. 
=============================================
[2019-03-27 00:11:02,700] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3286898e-28 1.0000000e+00 8.1424113e-33 1.0472943e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:11:02,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5293
[2019-03-27 00:11:02,713] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 83.0, 1.0, 2.0, 0.5120595916267434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715529.397097342, 715529.3970973414, 185506.7909953896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6399000.0000, 
sim time next is 6399600.0000, 
raw observation next is [27.03333333333333, 83.0, 1.0, 2.0, 0.5110430279115776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714108.4185074257, 714108.4185074263, 185344.0614224369], 
processed observation next is [1.0, 0.043478260869565216, 0.48025276461295413, 0.83, 1.0, 1.0, 0.41089521435129833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19836344958539603, 0.1983634495853962, 0.27663292749617446], 
reward next is 0.7234, 
noisyNet noise sample is [array([-2.932418], dtype=float32), -1.1623195]. 
=============================================
[2019-03-27 00:11:06,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9874526e-32 1.0000000e+00 0.0000000e+00 1.4452273e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:11:06,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8432
[2019-03-27 00:11:06,429] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 80.5, 1.0, 2.0, 0.5136221093835424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717713.5278245476, 717713.5278245476, 185757.980834713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6469800.0000, 
sim time next is 6470400.0000, 
raw observation next is [27.63333333333333, 81.0, 1.0, 2.0, 0.5161202523848935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721205.510839934, 721205.510839934, 186160.3445189343], 
processed observation next is [1.0, 0.9130434782608695, 0.5086887835703, 0.81, 1.0, 1.0, 0.41701235227095595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2003348641222039, 0.2003348641222039, 0.2778512604760213], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.33632773], dtype=float32), 0.2996981]. 
=============================================
[2019-03-27 00:11:09,075] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1643641e-24 1.0000000e+00 4.6959457e-30 1.8648135e-23 0.0000000e+00], sum to 1.0000
[2019-03-27 00:11:09,076] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6492
[2019-03-27 00:11:09,085] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 87.0, 1.0, 2.0, 0.7811419519900361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1091726.424045043, 1091726.424045043, 239334.3687124852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6508800.0000, 
sim time next is 6509400.0000, 
raw observation next is [27.41666666666667, 85.16666666666667, 1.0, 2.0, 0.9250014301611457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1292907.291572742, 1292907.291572741, 276907.1900368137], 
processed observation next is [1.0, 0.34782608695652173, 0.4984202211690366, 0.8516666666666667, 1.0, 1.0, 0.9096402773025851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3591409143257617, 0.3591409143257614, 0.4132943134877817], 
reward next is 0.5867, 
noisyNet noise sample is [array([-1.2442327], dtype=float32), 1.302956]. 
=============================================
[2019-03-27 00:11:10,285] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2627226e-18 1.0000000e+00 2.8020905e-21 9.5066425e-16 3.4816725e-30], sum to 1.0000
[2019-03-27 00:11:10,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1280
[2019-03-27 00:11:10,296] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.85, 55.83333333333334, 1.0, 2.0, 0.5564101130065732, 1.0, 1.0, 0.5564101130065732, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1555609.644227545, 1555609.644227545, 318702.944277936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6527400.0000, 
sim time next is 6528000.0000, 
raw observation next is [31.9, 55.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 9.623283484205803, 6.9112, 168.897300058783, 3378824.346160029, 1454957.318491244, 307510.5639371395], 
processed observation next is [1.0, 0.5652173913043478, 0.7109004739336492, 0.5566666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.27120834842058034, 0.0, 0.8293630648071564, 0.9385623183777858, 0.4041548106920122, 0.4589709909509545], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8077867], dtype=float32), 0.67784667]. 
=============================================
[2019-03-27 00:11:10,308] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[42.805904]
 [41.897564]
 [40.468548]
 [41.504337]
 [40.966846]], R is [[42.01548767]
 [41.5953331 ]
 [41.17937851]
 [40.76758575]
 [40.35990906]].
[2019-03-27 00:11:12,398] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:11:12,399] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:11:12,400] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:11:12,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:11:12,401] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:11:12,401] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:11:12,403] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:11:12,405] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:11:12,405] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:11:12,407] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:11:12,408] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:11:12,428] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-27 00:11:12,429] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-27 00:11:12,447] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-27 00:11:12,488] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-27 00:11:12,506] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-27 00:11:18,186] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:11:18,186] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 54.83333333333333, 1.0, 2.0, 0.2092848977704102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 349296.7506862957, 349296.7506862951, 156437.5121361042]
[2019-03-27 00:11:18,187] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:11:18,189] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.867776e-31 1.000000e+00 2.414070e-36 8.234476e-30 0.000000e+00], sampled 0.3346048602953431
[2019-03-27 00:11:36,560] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:11:36,563] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.71531276666667, 94.86431762666666, 1.0, 2.0, 0.4338039866876404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 627013.8970666322, 627013.8970666316, 176555.2904636265]
[2019-03-27 00:11:36,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:11:36,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4428875e-29 1.0000000e+00 6.7336008e-34 1.1686252e-27 0.0000000e+00], sampled 0.7933189636539384
[2019-03-27 00:11:54,532] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:11:54,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.7987055202241818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1176574.980325607, 1176574.980325607, 251876.8028516413]
[2019-03-27 00:11:54,535] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:11:54,542] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9718429e-26 1.0000000e+00 5.3797580e-31 4.9460893e-25 0.0000000e+00], sampled 0.06119132389780413
[2019-03-27 00:12:04,435] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:12:04,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.43333333333333, 74.33333333333333, 1.0, 2.0, 0.58201713586717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813322.3968343734, 813322.3968343734, 197443.1194119334]
[2019-03-27 00:12:04,437] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:12:04,441] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.5315161e-30 1.0000000e+00 6.6881576e-35 1.9273318e-28 0.0000000e+00], sampled 0.70566642706123
[2019-03-27 00:12:23,390] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:12:23,392] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.98845213833334, 76.38665837500001, 1.0, 2.0, 0.430186886851735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 629145.7749361773, 629145.7749361766, 176961.9382358017]
[2019-03-27 00:12:23,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:12:23,396] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6495337e-29 1.0000000e+00 1.6004446e-34 4.1632076e-28 0.0000000e+00], sampled 0.9268930230197855
[2019-03-27 00:12:34,199] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:12:34,200] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.63333333333334, 90.66666666666667, 1.0, 2.0, 0.5770907172355771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 806435.5073987488, 806435.5073987482, 196554.8607724489]
[2019-03-27 00:12:34,201] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:12:34,203] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4516565e-29 1.0000000e+00 6.6441434e-34 1.2230654e-27 0.0000000e+00], sampled 0.8314021511087587
[2019-03-27 00:12:47,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:12:47,677] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.52212389333333, 74.52137201333333, 1.0, 2.0, 0.4546665311059173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656135.8233283532, 656135.8233283532, 179459.5306937596]
[2019-03-27 00:12:47,679] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:12:47,681] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9164242e-30 1.0000000e+00 2.1539037e-35 7.4946348e-29 0.0000000e+00], sampled 0.05777469309807193
[2019-03-27 00:13:02,848] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.22724958]
[2019-03-27 00:13:02,850] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.53547044833334, 78.31858704666668, 1.0, 2.0, 0.7294905070369584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1019503.551091479, 1019503.551091479, 227337.0183158371]
[2019-03-27 00:13:02,852] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:13:02,855] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.2552042e-28 1.0000000e+00 9.7199439e-33 9.8567084e-27 0.0000000e+00], sampled 0.024121924216097046
[2019-03-27 00:13:06,116] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:13:06,792] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:13:06,833] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 00:13:06,951] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5883 2927327297.4404 1338.0000
[2019-03-27 00:13:06,962] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5752 2842451949.6408 1131.0000
[2019-03-27 00:13:07,974] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 175000, evaluation results [175000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8253.588267098503, 2927327297.440425, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.575228127671, 2842451949.640752, 1131.0]
[2019-03-27 00:13:09,685] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175766: loss -194.8312
[2019-03-27 00:13:09,690] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175768: learning rate 0.0005
[2019-03-27 00:13:09,759] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175800: loss -305.7025
[2019-03-27 00:13:09,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175800: learning rate 0.0005
[2019-03-27 00:13:09,823] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175826: loss -229.4966
[2019-03-27 00:13:09,827] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175827: learning rate 0.0005
[2019-03-27 00:13:10,025] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175917: loss -173.2910
[2019-03-27 00:13:10,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175918: learning rate 0.0005
[2019-03-27 00:13:10,060] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175931: loss -289.3647
[2019-03-27 00:13:10,064] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175931: learning rate 0.0005
[2019-03-27 00:13:10,135] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175968: loss -124.0508
[2019-03-27 00:13:10,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175968: learning rate 0.0005
[2019-03-27 00:13:10,143] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 175969: loss -170.3475
[2019-03-27 00:13:10,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 175969: learning rate 0.0005
[2019-03-27 00:13:10,194] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175989: loss -170.3694
[2019-03-27 00:13:10,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175989: learning rate 0.0005
[2019-03-27 00:13:10,286] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176032: loss -231.8671
[2019-03-27 00:13:10,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176032: learning rate 0.0005
[2019-03-27 00:13:10,296] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176037: loss -257.6895
[2019-03-27 00:13:10,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176037: learning rate 0.0005
[2019-03-27 00:13:10,309] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 176042: loss -283.0394
[2019-03-27 00:13:10,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 176042: learning rate 0.0005
[2019-03-27 00:13:10,354] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176058: loss -134.9456
[2019-03-27 00:13:10,356] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176058: learning rate 0.0005
[2019-03-27 00:13:10,378] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176067: loss -93.5327
[2019-03-27 00:13:10,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176068: learning rate 0.0005
[2019-03-27 00:13:10,409] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176081: loss -94.6418
[2019-03-27 00:13:10,411] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176081: learning rate 0.0005
[2019-03-27 00:13:10,507] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 176128: loss -322.2821
[2019-03-27 00:13:10,509] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 176128: learning rate 0.0005
[2019-03-27 00:13:10,523] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176132: loss -54.1668
[2019-03-27 00:13:10,525] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176133: learning rate 0.0005
[2019-03-27 00:13:15,413] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2079221e-24 1.0000000e+00 7.5360069e-28 3.1663291e-22 1.0087510e-37], sum to 1.0000
[2019-03-27 00:13:15,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1734
[2019-03-27 00:13:15,426] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1730548.414733676 W.
[2019-03-27 00:13:15,435] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.86666666666667, 86.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.301098715168418, 6.9112, 168.910803126166, 1730548.414733676, 1453944.373242454, 311349.028635154], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6684000.0000, 
sim time next is 6684600.0000, 
raw observation next is [27.03333333333333, 85.0, 1.0, 2.0, 0.3858068085302862, 1.0, 1.0, 0.3858068085302862, 1.0, 1.0, 0.6586689604086107, 6.9112, 6.9112, 170.5573041426782, 1618003.255834149, 1618003.255834149, 343636.0908204885], 
processed observation next is [1.0, 0.34782608695652173, 0.48025276461295413, 0.85, 1.0, 1.0, 0.2600082030485376, 1.0, 0.5, 0.2600082030485376, 1.0, 0.5, 0.5837426346446472, 0.0, 0.0, 0.8375144448122397, 0.44944534884281917, 0.44944534884281917, 0.5128896877917739], 
reward next is 0.4871, 
noisyNet noise sample is [array([1.1135217], dtype=float32), -0.19249347]. 
=============================================
[2019-03-27 00:13:16,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8597270e-23 1.0000000e+00 6.1748299e-28 4.7707731e-21 1.3377215e-34], sum to 1.0000
[2019-03-27 00:13:16,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4132
[2019-03-27 00:13:16,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1932457.767122881 W.
[2019-03-27 00:13:16,511] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.15, 60.66666666666666, 1.0, 2.0, 0.7409670659797447, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.962907859702709, 6.9112, 168.912654528936, 1932457.767122881, 1895774.497384825, 395150.5855572782], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6702600.0000, 
sim time next is 6703200.0000, 
raw observation next is [30.2, 60.0, 1.0, 2.0, 0.6761903157885689, 1.0, 1.0, 0.6761903157885689, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1890786.201549381, 1890786.20154938, 364086.3297601363], 
processed observation next is [1.0, 0.6086956521739131, 0.6303317535545023, 0.6, 1.0, 1.0, 0.6098678503476733, 1.0, 0.5, 0.6098678503476733, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5252183893192724, 0.5252183893192722, 0.5434124324778153], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.2844005], dtype=float32), 0.20121267]. 
=============================================
[2019-03-27 00:13:19,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5082134e-25 1.0000000e+00 7.6960276e-27 2.2151208e-23 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:19,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5133
[2019-03-27 00:13:19,228] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 83.83333333333333, 1.0, 2.0, 0.3336939028776395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529080.5901565697, 529080.5901565697, 169277.7967602219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6749400.0000, 
sim time next is 6750000.0000, 
raw observation next is [22.1, 84.0, 1.0, 2.0, 0.3335881990232148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 529244.9349324272, 529244.9349324278, 169294.4908170113], 
processed observation next is [1.0, 0.13043478260869565, 0.24644549763033188, 0.84, 1.0, 1.0, 0.19709421569062022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14701248192567423, 0.1470124819256744, 0.252678344503002], 
reward next is 0.7473, 
noisyNet noise sample is [array([-1.1185294], dtype=float32), 0.6619975]. 
=============================================
[2019-03-27 00:13:19,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.046616]
 [66.070656]
 [65.957756]
 [65.6174  ]
 [65.22539 ]], R is [[66.02822113]
 [66.11528778]
 [66.19991302]
 [66.27909851]
 [66.34922791]].
[2019-03-27 00:13:21,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3233432e-18 1.0000000e+00 3.7179045e-22 2.7835338e-12 1.0728971e-29], sum to 1.0000
[2019-03-27 00:13:21,849] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7725
[2019-03-27 00:13:21,855] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.4, 45.0, 1.0, 2.0, 0.9470836105389412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1487683.571324547, 1487683.571324547, 305814.7340321878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6789600.0000, 
sim time next is 6790200.0000, 
raw observation next is [29.38333333333333, 45.33333333333334, 1.0, 2.0, 0.9564329756309038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1501416.535329425, 1501416.535329425, 308765.8662549229], 
processed observation next is [1.0, 0.6086956521739131, 0.5916271721958924, 0.4533333333333334, 1.0, 1.0, 0.94750960919386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.41706014870261804, 0.41706014870261804, 0.46084457649988486], 
reward next is 0.5392, 
noisyNet noise sample is [array([-1.0390935], dtype=float32), 0.5138364]. 
=============================================
[2019-03-27 00:13:22,842] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7602044e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:22,851] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7899
[2019-03-27 00:13:22,858] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 68.66666666666667, 1.0, 2.0, 0.3298525330430171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517358.6362171899, 517358.6362171905, 168260.4403935623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6819600.0000, 
sim time next is 6820200.0000, 
raw observation next is [24.75, 69.5, 1.0, 2.0, 0.3304498398699051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 518073.9918309319, 518073.9918309313, 168310.9737173225], 
processed observation next is [1.0, 0.9565217391304348, 0.3720379146919432, 0.695, 1.0, 1.0, 0.19331306008422303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14390944217525886, 0.1439094421752587, 0.25121040853331716], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.42253965], dtype=float32), 0.5981426]. 
=============================================
[2019-03-27 00:13:25,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5896220e-29 1.0000000e+00 1.8628478e-34 3.7769647e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:25,151] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4035
[2019-03-27 00:13:25,157] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 83.33333333333334, 1.0, 2.0, 0.3474344319468126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538128.9982267176, 538128.9982267176, 169746.7032450415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6844800.0000, 
sim time next is 6845400.0000, 
raw observation next is [23.3, 83.0, 1.0, 2.0, 0.3493246992825027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 540260.4933673695, 540260.4933673688, 169899.2951815642], 
processed observation next is [0.0, 0.21739130434782608, 0.3033175355450238, 0.83, 1.0, 1.0, 0.21605385455723217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15007235926871376, 0.15007235926871357, 0.2535810375844242], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.39057818], dtype=float32), 1.1962413]. 
=============================================
[2019-03-27 00:13:25,378] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8513591e-32 1.0000000e+00 3.1310745e-38 2.7084370e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:25,392] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8913
[2019-03-27 00:13:25,398] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 78.66666666666667, 1.0, 2.0, 0.3776137079235777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570824.3978629658, 570824.3978629658, 172123.2898209765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6853200.0000, 
sim time next is 6853800.0000, 
raw observation next is [24.8, 78.33333333333333, 1.0, 2.0, 0.3801581559053236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573762.7834650266, 573762.7834650266, 172355.1561369583], 
processed observation next is [0.0, 0.30434782608695654, 0.3744075829383887, 0.7833333333333333, 1.0, 1.0, 0.2532025974762935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15937855096250741, 0.15937855096250741, 0.2572465016969527], 
reward next is 0.7428, 
noisyNet noise sample is [array([-1.0375301], dtype=float32), -0.2661199]. 
=============================================
[2019-03-27 00:13:26,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9730247e-33 1.0000000e+00 0.0000000e+00 9.7302344e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:26,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7190
[2019-03-27 00:13:26,647] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.56666666666667, 34.66666666666667, 1.0, 2.0, 0.2648799904228961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 432452.1975484979, 432452.1975484979, 162377.8519226228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6873600.0000, 
sim time next is 6874200.0000, 
raw observation next is [29.65, 33.5, 1.0, 2.0, 0.2608519516318573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427038.6891723659, 427038.6891723666, 161990.3035745148], 
processed observation next is [0.0, 0.5652173913043478, 0.6042654028436019, 0.335, 1.0, 1.0, 0.1094601826889847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11862185810343497, 0.11862185810343516, 0.24177657249927584], 
reward next is 0.7582, 
noisyNet noise sample is [array([-2.1583982], dtype=float32), -0.3916746]. 
=============================================
[2019-03-27 00:13:27,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9459054e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:27,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9377
[2019-03-27 00:13:27,050] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 30.0, 1.0, 2.0, 0.2544687688644385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419707.3587315264, 419707.3587315264, 161319.5096549368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6876000.0000, 
sim time next is 6876600.0000, 
raw observation next is [29.86666666666667, 31.16666666666666, 1.0, 2.0, 0.2530402991085502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 416885.9759888996, 416885.9759889003, 161192.1970138435], 
processed observation next is [0.0, 0.6086956521739131, 0.6145339652448659, 0.3116666666666666, 1.0, 1.0, 0.10004855314283155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11580165999691655, 0.11580165999691674, 0.24058536867737837], 
reward next is 0.7594, 
noisyNet noise sample is [array([0.7448431], dtype=float32), -0.8995345]. 
=============================================
[2019-03-27 00:13:27,589] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183775: loss 0.0031
[2019-03-27 00:13:27,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183775: learning rate 0.0005
[2019-03-27 00:13:27,597] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183777: loss 0.0101
[2019-03-27 00:13:27,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183777: learning rate 0.0005
[2019-03-27 00:13:27,642] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183798: loss 0.0120
[2019-03-27 00:13:27,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183798: learning rate 0.0005
[2019-03-27 00:13:27,862] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 183895: loss 0.0446
[2019-03-27 00:13:27,867] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 183896: learning rate 0.0005
[2019-03-27 00:13:27,929] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183926: loss 0.1726
[2019-03-27 00:13:27,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183926: learning rate 0.0005
[2019-03-27 00:13:27,955] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183937: loss 0.1415
[2019-03-27 00:13:27,956] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183937: learning rate 0.0005
[2019-03-27 00:13:28,071] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 183989: loss 0.0449
[2019-03-27 00:13:28,076] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 183990: learning rate 0.0005
[2019-03-27 00:13:28,085] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183996: loss 0.0128
[2019-03-27 00:13:28,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183996: learning rate 0.0005
[2019-03-27 00:13:28,143] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184018: loss 0.0010
[2019-03-27 00:13:28,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184018: learning rate 0.0005
[2019-03-27 00:13:28,187] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184039: loss 0.0009
[2019-03-27 00:13:28,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184039: learning rate 0.0005
[2019-03-27 00:13:28,246] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184064: loss 0.0123
[2019-03-27 00:13:28,251] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 184064: loss 0.0029
[2019-03-27 00:13:28,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184064: learning rate 0.0005
[2019-03-27 00:13:28,256] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184068: learning rate 0.0005
[2019-03-27 00:13:28,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 184086: loss 0.0509
[2019-03-27 00:13:28,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 184086: learning rate 0.0005
[2019-03-27 00:13:28,403] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 184131: loss 0.0786
[2019-03-27 00:13:28,408] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 184131: learning rate 0.0005
[2019-03-27 00:13:28,425] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184141: loss 0.0242
[2019-03-27 00:13:28,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184143: learning rate 0.0005
[2019-03-27 00:13:28,469] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184159: loss 0.0152
[2019-03-27 00:13:28,471] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184160: learning rate 0.0005
[2019-03-27 00:13:40,393] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2351863e-26 1.0000000e+00 3.3617299e-30 3.0305267e-18 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:40,401] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7207
[2019-03-27 00:13:40,407] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 94.16666666666667, 1.0, 2.0, 0.5966713864400905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846837.2712282994, 846837.2712282994, 201808.3221804269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7099800.0000, 
sim time next is 7100400.0000, 
raw observation next is [24.2, 94.33333333333334, 1.0, 2.0, 0.4998698147767043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710274.5482731145, 710274.5482731145, 185083.5276788748], 
processed observation next is [1.0, 0.17391304347826086, 0.3459715639810427, 0.9433333333333335, 1.0, 1.0, 0.3974335117791618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1972984856314207, 0.1972984856314207, 0.27624407116249966], 
reward next is 0.7238, 
noisyNet noise sample is [array([0.03747921], dtype=float32), 0.14324082]. 
=============================================
[2019-03-27 00:13:45,505] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191788: loss 0.0114
[2019-03-27 00:13:45,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191789: learning rate 0.0005
[2019-03-27 00:13:45,556] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191811: loss 0.0077
[2019-03-27 00:13:45,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191811: learning rate 0.0005
[2019-03-27 00:13:45,565] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191815: loss 0.0096
[2019-03-27 00:13:45,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191815: learning rate 0.0005
[2019-03-27 00:13:45,773] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191909: loss 0.1281
[2019-03-27 00:13:45,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191909: learning rate 0.0005
[2019-03-27 00:13:45,818] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191929: loss 0.1267
[2019-03-27 00:13:45,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191930: learning rate 0.0005
[2019-03-27 00:13:45,840] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191936: loss 0.0596
[2019-03-27 00:13:45,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191936: learning rate 0.0005
[2019-03-27 00:13:45,880] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191952: loss 0.0164
[2019-03-27 00:13:45,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191954: learning rate 0.0005
[2019-03-27 00:13:46,002] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192008: loss 0.0116
[2019-03-27 00:13:46,004] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192008: learning rate 0.0005
[2019-03-27 00:13:46,007] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 192009: loss 0.0121
[2019-03-27 00:13:46,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 192010: learning rate 0.0005
[2019-03-27 00:13:46,034] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192020: loss 0.0316
[2019-03-27 00:13:46,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192020: learning rate 0.0005
[2019-03-27 00:13:46,075] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 192037: loss 0.0373
[2019-03-27 00:13:46,078] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 192038: learning rate 0.0005
[2019-03-27 00:13:46,116] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192059: loss 0.0296
[2019-03-27 00:13:46,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192059: learning rate 0.0005
[2019-03-27 00:13:46,146] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192071: loss 0.0213
[2019-03-27 00:13:46,150] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192072: learning rate 0.0005
[2019-03-27 00:13:46,164] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 192079: loss 0.0087
[2019-03-27 00:13:46,167] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 192080: learning rate 0.0005
[2019-03-27 00:13:46,210] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192097: loss 0.0082
[2019-03-27 00:13:46,211] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192097: learning rate 0.0005
[2019-03-27 00:13:46,292] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 192132: loss 0.0113
[2019-03-27 00:13:46,294] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 192133: learning rate 0.0005
[2019-03-27 00:13:47,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1620562e-25 1.0000000e+00 1.8821888e-29 3.6479896e-26 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:47,958] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3634
[2019-03-27 00:13:47,962] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 87.33333333333334, 1.0, 2.0, 0.3723478901464754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565879.6792143573, 565879.6792143566, 171782.4665505339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7239000.0000, 
sim time next is 7239600.0000, 
raw observation next is [23.2, 88.0, 1.0, 2.0, 0.3725794176418219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566430.6070968946, 566430.6070968946, 171836.3003067081], 
processed observation next is [1.0, 0.8260869565217391, 0.29857819905213273, 0.88, 1.0, 1.0, 0.24407158752026734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15734183530469295, 0.15734183530469295, 0.2564720900100121], 
reward next is 0.7435, 
noisyNet noise sample is [array([1.6301609], dtype=float32), 0.6078053]. 
=============================================
[2019-03-27 00:13:48,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5856495e-22 1.0000000e+00 2.2987503e-25 5.1408176e-20 1.6011460e-35], sum to 1.0000
[2019-03-27 00:13:48,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7322
[2019-03-27 00:13:48,080] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 82.66666666666667, 1.0, 2.0, 0.8681887734309692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1301619.070462619, 1301619.070462619, 273715.6453516032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7231200.0000, 
sim time next is 7231800.0000, 
raw observation next is [24.36666666666666, 81.83333333333333, 1.0, 2.0, 0.860910835118904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1294263.649352784, 1294263.649352785, 272103.8541152108], 
processed observation next is [1.0, 0.6956521739130435, 0.3538704581358607, 0.8183333333333332, 1.0, 1.0, 0.8324226929143421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35951768037577336, 0.3595176803757736, 0.40612515539583705], 
reward next is 0.5939, 
noisyNet noise sample is [array([-0.9002657], dtype=float32), 0.648933]. 
=============================================
[2019-03-27 00:13:52,123] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5462961e-21 1.0000000e+00 5.2514300e-23 1.2825470e-15 1.4336022e-32], sum to 1.0000
[2019-03-27 00:13:52,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7757
[2019-03-27 00:13:52,142] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 61.0, 1.0, 2.0, 0.5723463913351585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873091.0886159511, 873091.0886159511, 204640.9253716278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7303200.0000, 
sim time next is 7303800.0000, 
raw observation next is [27.4, 60.5, 1.0, 2.0, 0.5502614071739451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839317.2188722782, 839317.2188722782, 200397.156903787], 
processed observation next is [1.0, 0.5217391304347826, 0.4976303317535545, 0.605, 1.0, 1.0, 0.4581462737035483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23314367190896618, 0.23314367190896618, 0.29910023418475673], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.75424236], dtype=float32), -2.0185292]. 
=============================================
[2019-03-27 00:13:57,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1589632e-27 1.0000000e+00 2.4565263e-34 1.0232495e-23 0.0000000e+00], sum to 1.0000
[2019-03-27 00:13:57,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7487
[2019-03-27 00:13:57,780] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 92.0, 1.0, 2.0, 0.6590100484177079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1049587.856301528, 1049587.856301527, 227189.844411565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7394400.0000, 
sim time next is 7395000.0000, 
raw observation next is [20.86666666666667, 91.83333333333334, 1.0, 2.0, 0.5525854446070985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 880894.8556144383, 880894.8556144377, 204494.5465874934], 
processed observation next is [1.0, 0.6086956521739131, 0.18799368088467638, 0.9183333333333334, 1.0, 1.0, 0.46094631880373305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2446930154484551, 0.24469301544845493, 0.30521574117536326], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.0686603], dtype=float32), 0.12253019]. 
=============================================
[2019-03-27 00:13:57,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.05112 ]
 [75.977234]
 [75.979996]
 [75.99332 ]
 [76.03675 ]], R is [[76.29897308]
 [76.19689941]
 [76.08376312]
 [75.97636414]
 [75.88314056]].
[2019-03-27 00:14:03,393] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199780: loss 0.0011
[2019-03-27 00:14:03,396] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199780: learning rate 0.0005
[2019-03-27 00:14:03,406] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199785: loss 0.0019
[2019-03-27 00:14:03,408] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199786: learning rate 0.0005
[2019-03-27 00:14:03,487] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199823: loss 0.0368
[2019-03-27 00:14:03,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199826: learning rate 0.0005
[2019-03-27 00:14:03,530] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 199845: loss 0.0826
[2019-03-27 00:14:03,536] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 199847: learning rate 0.0005
[2019-03-27 00:14:03,606] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199878: loss 0.1304
[2019-03-27 00:14:03,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199878: learning rate 0.0005
[2019-03-27 00:14:03,786] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199960: loss 0.0061
[2019-03-27 00:14:03,787] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199960: learning rate 0.0005
[2019-03-27 00:14:03,791] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199961: loss 0.0014
[2019-03-27 00:14:03,795] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199961: learning rate 0.0005
[2019-03-27 00:14:03,858] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199993: loss 0.0011
[2019-03-27 00:14:03,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199994: learning rate 0.0005
[2019-03-27 00:14:03,872] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 00:14:03,875] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:14:03,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:14:03,879] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:14:03,880] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:14:03,881] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:14:03,882] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:14:03,882] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:14:03,883] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:14:03,883] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:14:03,885] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:14:03,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-27 00:14:03,902] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-27 00:14:03,903] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-27 00:14:03,903] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-27 00:14:03,938] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-27 00:14:06,635] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.17670421]
[2019-03-27 00:14:06,636] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.98060183333333, 89.46680718333334, 1.0, 2.0, 0.3605062802617944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552853.0616155458, 552853.0616155451, 170816.2240136471]
[2019-03-27 00:14:06,638] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:14:06,639] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9564272e-37 1.0000000e+00 0.0000000e+00 1.7178034e-37 0.0000000e+00], sampled 0.3303718258089249
[2019-03-27 00:14:48,802] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.17670421]
[2019-03-27 00:14:48,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3065240606037272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 488121.7145664596, 488121.714566459, 166207.6890647149]
[2019-03-27 00:14:48,804] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:14:48,810] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5556631806952863
[2019-03-27 00:15:11,007] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.17670421]
[2019-03-27 00:15:11,008] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.40000000000001, 50.0, 1.0, 2.0, 0.5467700473124344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764049.6945575806, 764049.69455758, 191247.8462912747]
[2019-03-27 00:15:11,010] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:15:11,016] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.998574852610891
[2019-03-27 00:15:21,604] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.17670421]
[2019-03-27 00:15:21,605] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.76666666666667, 84.0, 1.0, 2.0, 0.6082961798061985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850059.9541302391, 850059.9541302391, 202301.9235507029]
[2019-03-27 00:15:21,606] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:15:21,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0747146010141776
[2019-03-27 00:15:41,971] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.17670421]
[2019-03-27 00:15:41,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.29699225333333, 50.55608788666667, 1.0, 2.0, 0.4018698179233864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595989.3478025694, 595989.3478025694, 174060.9114270321]
[2019-03-27 00:15:41,975] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:15:41,979] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5172058e-37 1.0000000e+00 0.0000000e+00 2.0738060e-37 0.0000000e+00], sampled 0.8692565578347816
[2019-03-27 00:15:51,843] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.17670421]
[2019-03-27 00:15:51,844] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.16780537666667, 84.3553984, 1.0, 2.0, 0.2936792496082931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473624.8606932202, 473624.8606932202, 165210.7388647055]
[2019-03-27 00:15:51,844] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:15:51,846] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.055302942770289576
[2019-03-27 00:15:58,209] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:15:58,253] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 00:15:58,513] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:15:58,545] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:15:58,617] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 00:15:59,633] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 200000, evaluation results [200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:15:59,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 200035: loss 0.0384
[2019-03-27 00:15:59,714] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 200035: learning rate 0.0005
[2019-03-27 00:15:59,762] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200056: loss 0.0236
[2019-03-27 00:15:59,764] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200057: learning rate 0.0005
[2019-03-27 00:15:59,804] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200077: loss 0.0237
[2019-03-27 00:15:59,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200079: learning rate 0.0005
[2019-03-27 00:15:59,837] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 200096: loss 0.0005
[2019-03-27 00:15:59,841] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200096: loss 0.0004
[2019-03-27 00:15:59,842] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 200096: learning rate 0.0005
[2019-03-27 00:15:59,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200097: learning rate 0.0005
[2019-03-27 00:15:59,870] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 200106: loss 0.0012
[2019-03-27 00:15:59,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 200107: learning rate 0.0005
[2019-03-27 00:15:59,987] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200159: loss 0.0646
[2019-03-27 00:15:59,989] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200160: learning rate 0.0005
[2019-03-27 00:16:00,025] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200175: loss 0.0632
[2019-03-27 00:16:00,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200176: learning rate 0.0005
[2019-03-27 00:16:02,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:16:02,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0038
[2019-03-27 00:16:02,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 90.0, 1.0, 2.0, 0.3927705050899581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583607.899235491, 583607.8992354916, 172956.4466228047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7543800.0000, 
sim time next is 7544400.0000, 
raw observation next is [23.7, 90.0, 1.0, 2.0, 0.3955017707588144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586627.6564882618, 586627.6564882625, 173199.4673738753], 
processed observation next is [0.0, 0.30434782608695654, 0.3222748815165877, 0.9, 1.0, 1.0, 0.2716888804323065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16295212680229493, 0.16295212680229512, 0.25850666772220193], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.2419503], dtype=float32), 0.6854585]. 
=============================================
[2019-03-27 00:16:02,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7068712e-36 1.0000000e+00 0.0000000e+00 1.6309758e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:02,512] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6127
[2019-03-27 00:16:02,522] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 90.0, 1.0, 2.0, 0.3853932460323327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576839.8275490114, 576839.8275490114, 172480.00652421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7540800.0000, 
sim time next is 7541400.0000, 
raw observation next is [23.46666666666667, 90.0, 1.0, 2.0, 0.3865041213515579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577846.516870782, 577846.516870782, 172549.472001667], 
processed observation next is [0.0, 0.2608695652173913, 0.31121642969984215, 0.9, 1.0, 1.0, 0.26084833897778065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.160512921352995, 0.160512921352995, 0.2575365253756224], 
reward next is 0.7425, 
noisyNet noise sample is [array([1.4039276], dtype=float32), -0.22278988]. 
=============================================
[2019-03-27 00:16:02,949] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0795366e-32 1.0000000e+00 8.5727937e-38 3.8295878e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:02,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8758
[2019-03-27 00:16:02,964] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 78.16666666666667, 1.0, 2.0, 0.4630433475360153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649402.900580505, 649402.900580505, 178317.5645487464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7553400.0000, 
sim time next is 7554000.0000, 
raw observation next is [27.06666666666667, 77.33333333333334, 1.0, 2.0, 0.4687798867984823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655033.6082073301, 655033.6082073307, 178849.1823778388], 
processed observation next is [0.0, 0.43478260869565216, 0.48183254344391807, 0.7733333333333334, 1.0, 1.0, 0.3599757672270872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1819537800575917, 0.18195378005759186, 0.2669390781758788], 
reward next is 0.7331, 
noisyNet noise sample is [array([2.9162734], dtype=float32), 0.72375673]. 
=============================================
[2019-03-27 00:16:02,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.13581 ]
 [76.17468 ]
 [76.181076]
 [76.23458 ]
 [76.2719  ]], R is [[76.05483246]
 [76.02814484]
 [76.00267029]
 [75.978302  ]
 [75.95492554]].
[2019-03-27 00:16:11,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6561073e-32 1.0000000e+00 5.1671210e-36 2.6953151e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:11,741] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7166
[2019-03-27 00:16:11,745] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 93.0, 1.0, 2.0, 0.495363730226292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692191.7385777482, 692191.7385777482, 182871.6693297447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7708800.0000, 
sim time next is 7709400.0000, 
raw observation next is [25.0, 92.5, 1.0, 2.0, 0.4969029648518459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694343.2761949665, 694343.2761949671, 183111.2153675945], 
processed observation next is [1.0, 0.21739130434782608, 0.38388625592417064, 0.925, 1.0, 1.0, 0.3938589937974047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1928731322763796, 0.19287313227637976, 0.2733003214441709], 
reward next is 0.7267, 
noisyNet noise sample is [array([-1.1090759], dtype=float32), -0.569491]. 
=============================================
[2019-03-27 00:16:12,027] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1776397e-32 1.0000000e+00 9.8039661e-38 6.4250082e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:12,039] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9202
[2019-03-27 00:16:12,045] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.495466891035004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 696387.5950469036, 696387.5950469042, 183409.2943589505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7705800.0000, 
sim time next is 7706400.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.4868345722466326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684259.3430898135, 684259.3430898128, 182072.2308730523], 
processed observation next is [1.0, 0.17391304347826086, 0.3601895734597157, 0.94, 1.0, 1.0, 0.3817284002971477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1900720397471704, 0.1900720397471702, 0.2717495983179885], 
reward next is 0.7283, 
noisyNet noise sample is [array([1.5840483], dtype=float32), -0.33317423]. 
=============================================
[2019-03-27 00:16:16,852] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9546993e-30 1.0000000e+00 1.0216636e-35 1.6396106e-27 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:16,859] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0891
[2019-03-27 00:16:16,862] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.5, 1.0, 2.0, 0.5824384932220675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813911.4356362551, 813911.4356362551, 197513.6046262231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7795800.0000, 
sim time next is 7796400.0000, 
raw observation next is [25.93333333333333, 89.0, 1.0, 2.0, 0.5773719780615019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 806828.6949117149, 806828.6949117156, 196600.3436667412], 
processed observation next is [1.0, 0.21739130434782608, 0.42812006319115314, 0.89, 1.0, 1.0, 0.4908096121222914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2241190819199208, 0.224119081919921, 0.29343334875633015], 
reward next is 0.7066, 
noisyNet noise sample is [array([1.7156918], dtype=float32), -0.49828854]. 
=============================================
[2019-03-27 00:16:17,006] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207761: loss 0.1392
[2019-03-27 00:16:17,007] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207761: learning rate 0.0005
[2019-03-27 00:16:17,055] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207777: loss 0.1264
[2019-03-27 00:16:17,056] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207777: learning rate 0.0005
[2019-03-27 00:16:17,101] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207799: loss 0.1058
[2019-03-27 00:16:17,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207799: learning rate 0.0005
[2019-03-27 00:16:17,154] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207826: loss 0.0533
[2019-03-27 00:16:17,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207826: learning rate 0.0005
[2019-03-27 00:16:17,170] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 207833: loss 0.0913
[2019-03-27 00:16:17,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 207833: learning rate 0.0005
[2019-03-27 00:16:17,384] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207925: loss 0.0224
[2019-03-27 00:16:17,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207928: learning rate 0.0005
[2019-03-27 00:16:17,481] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207966: loss 0.0228
[2019-03-27 00:16:17,483] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207966: learning rate 0.0005
[2019-03-27 00:16:17,494] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207973: loss 0.0211
[2019-03-27 00:16:17,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207973: learning rate 0.0005
[2019-03-27 00:16:17,587] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208013: loss 0.0221
[2019-03-27 00:16:17,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208014: learning rate 0.0005
[2019-03-27 00:16:17,735] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 208081: loss 0.0421
[2019-03-27 00:16:17,737] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208081: loss 0.0301
[2019-03-27 00:16:17,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208081: learning rate 0.0005
[2019-03-27 00:16:17,740] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 208081: learning rate 0.0005
[2019-03-27 00:16:17,754] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 208087: loss 0.0216
[2019-03-27 00:16:17,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 208088: learning rate 0.0005
[2019-03-27 00:16:17,769] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 208091: loss 0.0214
[2019-03-27 00:16:17,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 208091: learning rate 0.0005
[2019-03-27 00:16:17,926] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208163: loss 0.0285
[2019-03-27 00:16:17,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208166: learning rate 0.0005
[2019-03-27 00:16:17,962] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 208176: loss 0.0185
[2019-03-27 00:16:17,966] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 208176: learning rate 0.0005
[2019-03-27 00:16:18,057] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 208225: loss 0.0207
[2019-03-27 00:16:18,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 208225: learning rate 0.0005
[2019-03-27 00:16:26,814] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:26,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:26,816] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-27 00:16:26,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:26,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:26,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-27 00:16:26,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:26,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:26,966] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-27 00:16:26,980] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:26,981] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:26,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-27 00:16:26,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,004] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-27 00:16:27,134] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,134] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,135] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-27 00:16:27,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-27 00:16:27,233] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,233] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,235] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-27 00:16:27,254] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,255] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-27 00:16:27,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-27 00:16:27,373] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,373] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,373] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-27 00:16:27,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,423] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-27 00:16:27,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-27 00:16:27,469] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-27 00:16:27,491] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,491] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-27 00:16:27,524] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:16:27,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:27,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-27 00:16:29,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:16:29,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7236
[2019-03-27 00:16:29,339] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 84.0, 1.0, 2.0, 0.3294673707365959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520693.2050942397, 520693.2050942397, 168598.3714326102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 28800.0000, 
sim time next is 29400.0000, 
raw observation next is [22.51666666666667, 83.0, 1.0, 2.0, 0.3567915519874122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562941.8508880638, 562941.8508880638, 171995.7415537664], 
processed observation next is [1.0, 0.34782608695652173, 0.2661927330173777, 0.83, 1.0, 1.0, 0.22505006263543637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1563727363577955, 0.1563727363577955, 0.25671006202054686], 
reward next is 0.7433, 
noisyNet noise sample is [array([-0.06201022], dtype=float32), -1.4503579]. 
=============================================
[2019-03-27 00:16:30,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2880281e-31 1.0000000e+00 0.0000000e+00 1.3417750e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:30,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8045
[2019-03-27 00:16:30,848] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 91.0, 1.0, 2.0, 0.4854247404875331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734863.4458652891, 734863.4458652891, 188265.2080226837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [22.96666666666667, 91.0, 1.0, 2.0, 0.4732006339763806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715966.8271768078, 715966.8271768078, 186217.6687613569], 
processed observation next is [1.0, 0.30434782608695654, 0.2875197472353872, 0.91, 1.0, 1.0, 0.36530196864624176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19887967421577996, 0.19887967421577996, 0.27793681904680134], 
reward next is 0.7221, 
noisyNet noise sample is [array([-1.6575993], dtype=float32), -1.7984059]. 
=============================================
[2019-03-27 00:16:30,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[61.94083 ]
 [62.13655 ]
 [62.329952]
 [62.673008]
 [62.490585]], R is [[61.97252274]
 [62.07180405]
 [62.17460251]
 [62.27603912]
 [62.39798737]].
[2019-03-27 00:16:33,313] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2810548e-35 1.0000000e+00 2.6675195e-35 1.8160072e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:33,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7557
[2019-03-27 00:16:33,332] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 89.66666666666667, 1.0, 2.0, 0.3878659190502755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598953.3325073369, 598953.3325073362, 174934.9719188603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [22.48333333333333, 89.83333333333333, 1.0, 2.0, 0.3817879508531122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588945.2340165173, 588945.2340165167, 174030.2213120737], 
processed observation next is [1.0, 0.08695652173913043, 0.26461295418641384, 0.8983333333333333, 1.0, 1.0, 0.25516620584712313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16359589833792146, 0.16359589833792132, 0.2597465989732443], 
reward next is 0.7403, 
noisyNet noise sample is [array([3.6090925], dtype=float32), 0.027563069]. 
=============================================
[2019-03-27 00:16:34,632] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4868183e-28 1.0000000e+00 6.0212352e-29 2.1505917e-23 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:34,639] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0107
[2019-03-27 00:16:34,643] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 89.0, 1.0, 2.0, 0.3446691038097438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535770.6797168511, 535770.6797168511, 169606.1960093649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 88200.0000, 
sim time next is 88800.0000, 
raw observation next is [22.3, 89.0, 1.0, 2.0, 0.3449381809840169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536190.0994677328, 536190.0994677328, 169640.2261771646], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 1.0, 1.0, 0.21076889275182759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14894169429659246, 0.14894169429659246, 0.25319436742860385], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.14365567], dtype=float32), -0.09330995]. 
=============================================
[2019-03-27 00:16:36,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4346613e-32 1.0000000e+00 1.9979388e-37 9.9260862e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:36,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0578
[2019-03-27 00:16:36,275] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 93.0, 1.0, 2.0, 0.2927604075809669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469301.1687311629, 469301.1687311629, 164903.6486414795], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 205200.0000, 
sim time next is 205800.0000, 
raw observation next is [20.53333333333333, 93.0, 1.0, 2.0, 0.2935388023309903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 470281.9536565233, 470281.9536565239, 164969.8636236899], 
processed observation next is [0.0, 0.391304347826087, 0.17219589257503945, 0.93, 1.0, 1.0, 0.1488419305192654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13063387601570092, 0.1306338760157011, 0.24622367705028342], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.74841297], dtype=float32), -0.70004684]. 
=============================================
[2019-03-27 00:16:40,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7373235e-35 1.0000000e+00 0.0000000e+00 2.0707005e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:40,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3461
[2019-03-27 00:16:40,020] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.93333333333333, 96.0, 1.0, 2.0, 0.288645434796434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464586.690818226, 464586.6908182266, 164586.425793016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 182400.0000, 
sim time next is 183000.0000, 
raw observation next is [19.91666666666666, 96.0, 1.0, 2.0, 0.2878720947516818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463490.9715213497, 463490.9715213497, 164511.4154489585], 
processed observation next is [0.0, 0.08695652173913043, 0.14296998420221146, 0.96, 1.0, 1.0, 0.14201457198997805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1287474920892638, 0.1287474920892638, 0.24553942604322163], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.86040956], dtype=float32), 1.3924478]. 
=============================================
[2019-03-27 00:16:40,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.168236]
 [69.599556]
 [69.90106 ]
 [70.02963 ]
 [70.15118 ]], R is [[69.01693726]
 [69.08111572]
 [69.1444931 ]
 [69.20703125]
 [69.26873016]].
[2019-03-27 00:16:41,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:16:41,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6713
[2019-03-27 00:16:41,768] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 92.33333333333333, 1.0, 2.0, 0.3016456660709038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481606.5066932109, 481606.5066932116, 165753.8694256359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 211200.0000, 
sim time next is 211800.0000, 
raw observation next is [20.78333333333333, 92.16666666666667, 1.0, 2.0, 0.3010305345907621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480659.1345238702, 480659.1345238708, 165686.503816592], 
processed observation next is [0.0, 0.43478260869565216, 0.18404423380726695, 0.9216666666666667, 1.0, 1.0, 0.15786811396477363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1335164262566306, 0.13351642625663077, 0.24729328927849553], 
reward next is 0.7527, 
noisyNet noise sample is [array([0.9024048], dtype=float32), -1.4423598]. 
=============================================
[2019-03-27 00:16:43,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1610546e-36 1.0000000e+00 0.0000000e+00 1.9575106e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:43,622] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1131
[2019-03-27 00:16:43,627] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 90.5, 1.0, 2.0, 0.2907289948954743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465958.3967604042, 465958.3967604042, 164670.8623561137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 250200.0000, 
sim time next is 250800.0000, 
raw observation next is [20.76666666666667, 90.66666666666667, 1.0, 2.0, 0.2943811953996872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471934.3171387933, 471934.3171387933, 165087.7251199895], 
processed observation next is [0.0, 0.9130434782608695, 0.18325434439178534, 0.9066666666666667, 1.0, 1.0, 0.149856861927334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13109286587188704, 0.13109286587188704, 0.24639958973132764], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.3272376], dtype=float32), 1.1597118]. 
=============================================
[2019-03-27 00:16:44,826] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2191653e-35 1.0000000e+00 0.0000000e+00 1.5397382e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:44,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3599
[2019-03-27 00:16:44,840] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 83.66666666666667, 1.0, 2.0, 0.2960323952871879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471676.2812182143, 471676.2812182143, 165035.1444332457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 288600.0000, 
sim time next is 289200.0000, 
raw observation next is [22.03333333333333, 83.33333333333334, 1.0, 2.0, 0.2963295790978868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 471918.240543293, 471918.2405432923, 165048.3676505503], 
processed observation next is [0.0, 0.34782608695652173, 0.2432859399684044, 0.8333333333333335, 1.0, 1.0, 0.1522043121661287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13108840015091472, 0.13108840015091452, 0.24634084723962732], 
reward next is 0.7537, 
noisyNet noise sample is [array([-1.0622371], dtype=float32), 0.44933987]. 
=============================================
[2019-03-27 00:16:45,540] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.6629119e-38 1.0000000e+00 0.0000000e+00 1.2408146e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:45,540] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7934
[2019-03-27 00:16:45,547] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 95.5, 1.0, 2.0, 0.2650526154486161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 431053.1406491161, 431053.1406491161, 162334.7419220064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 275400.0000, 
sim time next is 276000.0000, 
raw observation next is [19.33333333333333, 95.66666666666667, 1.0, 2.0, 0.26342732567264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 428744.169440457, 428744.1694404576, 162181.6441650866], 
processed observation next is [0.0, 0.17391304347826086, 0.11532385466034739, 0.9566666666666667, 1.0, 1.0, 0.11256304297908432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11909560262234917, 0.11909560262234933, 0.24206215547027848], 
reward next is 0.7579, 
noisyNet noise sample is [array([-0.22533232], dtype=float32), -0.7890469]. 
=============================================
[2019-03-27 00:16:45,583] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.10891 ]
 [73.190384]
 [73.22566 ]
 [73.24526 ]
 [73.22533 ]], R is [[73.07407379]
 [73.10103607]
 [73.12747955]
 [73.15338898]
 [73.17880249]].
[2019-03-27 00:16:46,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6959982e-36 1.0000000e+00 2.2573555e-37 1.5966915e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:46,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7298
[2019-03-27 00:16:46,043] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 85.0, 1.0, 2.0, 0.2574934045363385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419474.0532968437, 419474.0532968437, 161593.9726072189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 369600.0000, 
sim time next is 370200.0000, 
raw observation next is [20.63333333333333, 84.5, 1.0, 2.0, 0.257264450710331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419078.1915287006, 419078.1915287006, 161570.0893810088], 
processed observation next is [1.0, 0.2608695652173913, 0.17693522906793036, 0.845, 1.0, 1.0, 0.10513789242208553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11641060875797238, 0.11641060875797238, 0.241149387135834], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.3988065], dtype=float32), -0.22950487]. 
=============================================
[2019-03-27 00:16:55,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8942735e-31 1.0000000e+00 3.1853072e-33 1.3382323e-26 0.0000000e+00], sum to 1.0000
[2019-03-27 00:16:55,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6635
[2019-03-27 00:16:55,833] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.66666666666667, 1.0, 2.0, 0.2282594086308865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377369.7702676977, 377369.7702676977, 158765.3333167458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 447600.0000, 
sim time next is 448200.0000, 
raw observation next is [19.7, 82.5, 1.0, 2.0, 0.2290585576323127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378771.8010226543, 378771.8010226543, 158832.2920210429], 
processed observation next is [1.0, 0.17391304347826086, 0.1327014218009479, 0.825, 1.0, 1.0, 0.07115488871362975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10521438917295953, 0.10521438917295953, 0.237063122419467], 
reward next is 0.7629, 
noisyNet noise sample is [array([-1.7184057], dtype=float32), 0.23714437]. 
=============================================
[2019-03-27 00:16:57,618] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 00:16:57,622] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:16:57,623] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:16:57,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:57,626] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:16:57,626] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:57,628] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:16:57,629] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:16:57,630] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:57,631] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:57,628] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:16:57,653] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-27 00:16:57,674] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-27 00:16:57,675] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-27 00:16:57,676] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-27 00:16:57,739] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-27 00:17:01,574] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.12127628]
[2019-03-27 00:17:01,575] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.1, 60.0, 1.0, 2.0, 0.2718636696638978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 443902.1487617302, 443902.1487617309, 163109.5562119813]
[2019-03-27 00:17:01,575] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:17:01,581] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6739541e-32 1.0000000e+00 2.9113273e-34 1.8888214e-28 0.0000000e+00], sampled 0.48068293294086384
[2019-03-27 00:17:12,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.12127628]
[2019-03-27 00:17:12,457] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.8, 94.0, 1.0, 2.0, 0.3374390906288617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 522751.8247870384, 522751.824787039, 168511.2479659854]
[2019-03-27 00:17:12,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:17:12,465] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.0175243e-34 1.0000000e+00 4.9205053e-36 1.8529092e-29 0.0000000e+00], sampled 0.12283662947920271
[2019-03-27 00:17:42,732] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.12127628]
[2019-03-27 00:17:42,734] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3539231406670067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563599.6850036548, 563599.6850036555, 172083.1644948465]
[2019-03-27 00:17:42,736] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:17:42,738] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.298730e-32 1.000000e+00 1.841476e-33 1.847548e-26 0.000000e+00], sampled 0.7157791470132852
[2019-03-27 00:18:17,622] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.12127628]
[2019-03-27 00:18:17,624] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.86666666666667, 66.5, 1.0, 2.0, 0.5590068765013044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 781155.5758262539, 781155.5758262544, 193358.7407476227]
[2019-03-27 00:18:17,625] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:18:17,628] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9759063e-36 1.0000000e+00 3.9673002e-38 8.1356509e-31 0.0000000e+00], sampled 0.3753360311395797
[2019-03-27 00:18:40,104] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.12127628]
[2019-03-27 00:18:40,105] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.75, 70.5, 1.0, 2.0, 0.5903392257385888, 0.0, 2.0, 0.0, 1.0, 2.0, 1.025224006942716, 6.911200000000001, 6.9112, 168.9128535521627, 1650553.971470619, 1650553.971470618, 361561.1091456781]
[2019-03-27 00:18:40,106] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:18:40,108] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7941442e-32 1.0000000e+00 1.8442116e-32 1.1755771e-21 0.0000000e+00], sampled 0.17874598151186993
[2019-03-27 00:18:44,846] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.12127628]
[2019-03-27 00:18:44,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.03333333333333, 94.16666666666667, 1.0, 2.0, 0.3152971131656389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498224.968041146, 498224.9680411454, 166885.4709497582]
[2019-03-27 00:18:44,849] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:18:44,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0532436e-33 1.0000000e+00 2.0741248e-35 9.7542845e-29 0.0000000e+00], sampled 0.6068398944442693
[2019-03-27 00:18:52,134] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:18:52,265] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:18:52,337] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:18:52,351] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:18:52,434] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:18:53,451] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 225000, evaluation results [225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:18:55,480] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.36354975e-33 1.00000000e+00 2.17242061e-35 2.92906371e-25
 0.00000000e+00], sum to 1.0000
[2019-03-27 00:18:55,489] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9010
[2019-03-27 00:18:55,496] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.23333333333333, 85.0, 1.0, 2.0, 0.2357627921104252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390452.6277067414, 390452.6277067414, 159402.8310996984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 512400.0000, 
sim time next is 513000.0000, 
raw observation next is [19.15, 85.5, 1.0, 2.0, 0.2350162099216474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 389299.1388566919, 389299.1388566919, 159325.0359620243], 
processed observation next is [1.0, 0.9565217391304348, 0.10663507109004738, 0.855, 1.0, 1.0, 0.07833278303812939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10813864968241442, 0.10813864968241442, 0.2377985611373497], 
reward next is 0.7622, 
noisyNet noise sample is [array([0.6990487], dtype=float32), 1.1118222]. 
=============================================
[2019-03-27 00:18:55,513] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[79.79417 ]
 [79.79471 ]
 [79.779655]
 [79.76302 ]
 [79.779655]], R is [[79.76260376]
 [79.72706604]
 [79.69179535]
 [79.65679932]
 [79.62210083]].
[2019-03-27 00:18:55,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2853351e-37 1.0000000e+00 1.9405294e-36 4.3621408e-27 0.0000000e+00], sum to 1.0000
[2019-03-27 00:18:55,662] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4376
[2019-03-27 00:18:55,671] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [18.8, 87.0, 1.0, 2.0, 0.2299358285746499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381481.9020643638, 381481.9020643638, 158786.4280343929], 
processed observation next is [1.0, 0.0, 0.09004739336492901, 0.87, 1.0, 1.0, 0.07221184165620467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10596719501787884, 0.10596719501787884, 0.2369946687080491], 
reward next is 0.7630, 
noisyNet noise sample is [array([1.1816823], dtype=float32), -0.76021653]. 
=============================================
[2019-03-27 00:18:56,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7776014e-29 1.0000000e+00 6.3934576e-28 2.8788162e-18 0.0000000e+00], sum to 1.0000
[2019-03-27 00:18:56,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3440
[2019-03-27 00:18:56,804] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 55.0, 1.0, 2.0, 0.633009382082244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1039117.682835463, 1039117.682835462, 222885.9367263796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [24.78333333333333, 54.5, 1.0, 2.0, 0.6412729187898063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051154.283686693, 1051154.283686693, 224749.5555619333], 
processed observation next is [1.0, 0.4782608695652174, 0.37361769352290675, 0.545, 1.0, 1.0, 0.567798697337116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2919873010240814, 0.2919873010240814, 0.33544709785363175], 
reward next is 0.6646, 
noisyNet noise sample is [array([1.8106557], dtype=float32), -1.2547234]. 
=============================================
[2019-03-27 00:18:56,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.445755]
 [68.46506 ]
 [68.48713 ]
 [68.55526 ]
 [68.87944 ]], R is [[68.41550446]
 [68.39868927]
 [68.38816833]
 [68.37859344]
 [68.36954498]].
[2019-03-27 00:19:01,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3474654e-30 1.0000000e+00 2.1762333e-30 1.6370960e-22 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:01,596] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5600
[2019-03-27 00:19:01,603] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.93333333333333, 92.0, 1.0, 2.0, 0.2001213600130119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 334604.8500839895, 334604.8500839895, 155294.3530568108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 621600.0000, 
sim time next is 622200.0000, 
raw observation next is [16.91666666666666, 92.0, 1.0, 2.0, 0.1994623048226262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 333520.3046226147, 333520.3046226153, 155224.9842087952], 
processed observation next is [1.0, 0.17391304347826086, 0.0007898894154816193, 0.92, 1.0, 1.0, 0.0354967527983448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09264452906183741, 0.09264452906183758, 0.23167908090864958], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.00225113], dtype=float32), 0.4414567]. 
=============================================
[2019-03-27 00:19:09,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2899678e-34 1.0000000e+00 2.6436958e-33 2.4496701e-24 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:09,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8598
[2019-03-27 00:19:09,368] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 60.5, 1.0, 2.0, 0.2436747180262885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402194.8189099152, 402194.8189099146, 160246.7239985138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 756600.0000, 
sim time next is 757200.0000, 
raw observation next is [22.9, 62.0, 1.0, 2.0, 0.2448529244925271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403930.1500833399, 403930.1500833393, 160369.7330999935], 
processed observation next is [1.0, 0.782608695652174, 0.2843601895734597, 0.62, 1.0, 1.0, 0.09018424637653867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11220281946759442, 0.11220281946759425, 0.23935781059700523], 
reward next is 0.7606, 
noisyNet noise sample is [array([0.7390994], dtype=float32), -0.55168253]. 
=============================================
[2019-03-27 00:19:11,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0947869e-31 1.0000000e+00 2.1025756e-32 5.6732479e-26 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:11,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1480
[2019-03-27 00:19:11,889] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 68.0, 1.0, 2.0, 0.2904095887404655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 464791.6358541481, 464791.6358541474, 164583.6680689902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 813000.0000, 
sim time next is 813600.0000, 
raw observation next is [24.2, 67.0, 1.0, 2.0, 0.2913297578531947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466062.1251034755, 466062.1251034748, 164669.3506602364], 
processed observation next is [0.0, 0.43478260869565216, 0.3459715639810427, 0.67, 1.0, 1.0, 0.14618043114842735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12946170141763208, 0.12946170141763189, 0.24577515023915883], 
reward next is 0.7542, 
noisyNet noise sample is [array([1.177211], dtype=float32), -0.5312822]. 
=============================================
[2019-03-27 00:19:18,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4603477e-34 1.0000000e+00 1.0429911e-35 1.2913797e-29 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:18,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6490
[2019-03-27 00:19:18,333] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2939403222194549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 469208.8858034216, 469208.8858034223, 164875.2402231568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 902400.0000, 
sim time next is 903000.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2941604343610053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469560.1879819987, 469560.1879819987, 164899.8162335354], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14959088477229554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1304333855505552, 0.1304333855505552, 0.24611912870676927], 
reward next is 0.7539, 
noisyNet noise sample is [array([0.15662439], dtype=float32), 1.0303508]. 
=============================================
[2019-03-27 00:19:18,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.53525]
 [76.49251]
 [76.44526]
 [76.39765]
 [76.33111]], R is [[76.55995941]
 [76.54827118]
 [76.53672791]
 [76.52529907]
 [76.51396179]].
[2019-03-27 00:19:22,513] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.26077205e-29 1.00000000e+00 2.11184703e-29 4.36945286e-19
 0.00000000e+00], sum to 1.0000
[2019-03-27 00:19:22,520] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6124
[2019-03-27 00:19:22,524] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.66666666666667, 1.0, 2.0, 0.3313461095703727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514104.4202601747, 514104.4202601747, 167854.2908459639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 966000.0000, 
sim time next is 966600.0000, 
raw observation next is [21.9, 92.5, 1.0, 2.0, 0.3266065538592764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507045.1089431385, 507045.1089431385, 167315.7245968776], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.925, 1.0, 1.0, 0.18868259501117637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14084586359531626, 0.14084586359531626, 0.24972496208489195], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.22717357], dtype=float32), 0.23016858]. 
=============================================
[2019-03-27 00:19:22,565] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5613062e-28 1.0000000e+00 1.4924903e-26 1.4264521e-15 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:22,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5238
[2019-03-27 00:19:22,575] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.66666666666667, 1.0, 2.0, 0.3313461095703727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514104.4202601747, 514104.4202601747, 167854.2908459639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 966000.0000, 
sim time next is 966600.0000, 
raw observation next is [21.9, 92.5, 1.0, 2.0, 0.3266065538592764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507045.1089431385, 507045.1089431385, 167315.7245968776], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.925, 1.0, 1.0, 0.18868259501117637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14084586359531626, 0.14084586359531626, 0.24972496208489195], 
reward next is 0.7503, 
noisyNet noise sample is [array([1.7154056], dtype=float32), -3.3230786]. 
=============================================
[2019-03-27 00:19:25,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3392973e-31 1.0000000e+00 1.2820272e-32 1.6206071e-20 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:25,505] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4674
[2019-03-27 00:19:25,510] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 96.0, 1.0, 2.0, 0.3546689625840897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 545224.731305105, 545224.7313051056, 170215.5432650581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1023600.0000, 
sim time next is 1024200.0000, 
raw observation next is [21.85, 96.0, 1.0, 2.0, 0.3547315860715979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545052.8240585913, 545052.8240585913, 170193.3016782987], 
processed observation next is [1.0, 0.8695652173913043, 0.23459715639810438, 0.96, 1.0, 1.0, 0.22256817598987697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15140356223849757, 0.15140356223849757, 0.2540198532511921], 
reward next is 0.7460, 
noisyNet noise sample is [array([1.469795], dtype=float32), -0.59848946]. 
=============================================
[2019-03-27 00:19:36,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4213905e-32 1.0000000e+00 1.4959056e-32 3.0353158e-26 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:36,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3458
[2019-03-27 00:19:36,966] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.0, 1.0, 2.0, 0.3420071343676661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531989.4497294846, 531989.4497294846, 169310.1688530309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1220400.0000, 
sim time next is 1221000.0000, 
raw observation next is [21.88333333333333, 92.16666666666667, 1.0, 2.0, 0.3928120115120785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610927.8457835277, 610927.8457835283, 176069.1309924747], 
processed observation next is [1.0, 0.13043478260869565, 0.2361769352290678, 0.9216666666666667, 1.0, 1.0, 0.26844820664105845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16970217938431326, 0.16970217938431342, 0.2627897477499622], 
reward next is 0.7372, 
noisyNet noise sample is [array([0.8528205], dtype=float32), -1.2750242]. 
=============================================
[2019-03-27 00:19:36,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.76247 ]
 [65.754974]
 [65.809906]
 [65.94407 ]
 [65.965454]], R is [[65.73121643]
 [65.82120514]
 [65.90962982]
 [65.99652863]
 [66.08345032]].
[2019-03-27 00:19:47,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2118684e-34 1.0000000e+00 1.2566485e-35 5.6302092e-28 0.0000000e+00], sum to 1.0000
[2019-03-27 00:19:47,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7510
[2019-03-27 00:19:47,181] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 97.33333333333334, 1.0, 2.0, 0.307379653664389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489091.8437951748, 489091.8437951742, 166272.6762947582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1387200.0000, 
sim time next is 1387800.0000, 
raw observation next is [20.3, 97.5, 1.0, 2.0, 0.3068023820804128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488327.5231009396, 488327.5231009396, 166219.2769416711], 
processed observation next is [0.0, 0.043478260869565216, 0.16113744075829392, 0.975, 1.0, 1.0, 0.16482214708483472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13564653419470543, 0.13564653419470543, 0.2480884730472703], 
reward next is 0.7519, 
noisyNet noise sample is [array([1.556386], dtype=float32), 0.42701697]. 
=============================================
[2019-03-27 00:19:49,161] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 00:19:49,162] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:19:49,163] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:19:49,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:19:49,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:19:49,164] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:19:49,165] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:19:49,168] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:19:49,169] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:19:49,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:19:49,172] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:19:49,195] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-27 00:19:49,195] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-27 00:19:49,196] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-27 00:19:49,240] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-27 00:19:49,241] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-27 00:19:53,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:19:53,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.52705829666667, 89.94086168000001, 1.0, 2.0, 0.3073646416677931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 488818.8900314408, 488818.8900314402, 166249.6768110008]
[2019-03-27 00:19:53,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:19:53,766] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.247074e-34 0.000000e+00], sampled 0.556112094024869
[2019-03-27 00:20:00,511] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:20:00,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 70.33333333333334, 1.0, 2.0, 0.2802641616765406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 455963.6375927905, 455963.6375927911, 163951.714717164]
[2019-03-27 00:20:00,515] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:20:00,518] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1050754e-37 1.0000000e+00 2.5412377e-38 1.9806516e-30 0.0000000e+00], sampled 0.20959256079825705
[2019-03-27 00:20:53,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:20:53,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.06666666666666, 71.0, 1.0, 2.0, 0.5503329918639122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769030.3114853748, 769030.3114853748, 191857.3923393706]
[2019-03-27 00:20:53,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:20:53,828] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8222198e-38 1.0000000e+00 0.0000000e+00 8.7132428e-31 0.0000000e+00], sampled 0.08313933231291204
[2019-03-27 00:20:57,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:20:57,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [38.3, 44.0, 1.0, 2.0, 0.9981884487549039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1395270.604943426, 1395270.604943425, 298379.8133129745]
[2019-03-27 00:20:57,425] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:20:57,427] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0271065e-36 1.0000000e+00 2.1955625e-37 2.5314701e-27 0.0000000e+00], sampled 0.22278921777305205
[2019-03-27 00:21:23,087] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:21:23,088] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.3419701, 72.38720126, 1.0, 2.0, 0.563075142750768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786842.6721340371, 786842.6721340371, 194068.3471292527]
[2019-03-27 00:21:23,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:21:23,092] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5248605e-32 0.0000000e+00], sampled 0.2804385574527338
[2019-03-27 00:21:31,684] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:21:31,685] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.98333333333333, 88.83333333333334, 1.0, 2.0, 0.5767639743437127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 805978.7385856485, 805978.7385856492, 196496.4808858201]
[2019-03-27 00:21:31,687] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:21:31,690] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.4046568e-37 1.0000000e+00 9.9670974e-38 3.7531757e-28 0.0000000e+00], sampled 0.5864535810646638
[2019-03-27 00:21:33,853] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:21:33,854] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.63333333333334, 61.66666666666667, 1.0, 2.0, 0.8903002407379449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340734.917294767, 1340734.917294767, 280916.8524653118]
[2019-03-27 00:21:33,856] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:21:33,858] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0500877e-34 1.0000000e+00 3.2083663e-34 9.2340673e-24 0.0000000e+00], sampled 0.36479939821599927
[2019-03-27 00:21:39,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:21:39,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.5, 94.0, 1.0, 2.0, 0.4832435453360206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679215.9111065073, 679215.9111065079, 181522.7966113308]
[2019-03-27 00:21:39,657] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:21:39,661] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4385235e-37 1.0000000e+00 0.0000000e+00 4.8674633e-30 0.0000000e+00], sampled 0.7337995989633265
[2019-03-27 00:21:40,881] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.0615221]
[2019-03-27 00:21:40,884] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.03333333333334, 87.33333333333333, 1.0, 2.0, 0.7174171810198852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1002622.441370717, 1002622.441370718, 224646.8171354372]
[2019-03-27 00:21:40,885] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:21:40,888] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4780350e-36 1.0000000e+00 2.0461769e-37 4.1667027e-28 0.0000000e+00], sampled 0.7154653347838109
[2019-03-27 00:21:43,222] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:21:43,256] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:21:43,483] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:21:43,561] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:21:43,622] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:21:44,639] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:21:55,620] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.19553968e-28 1.00000000e+00 3.04658813e-28 1.15743595e-20
 0.00000000e+00], sum to 1.0000
[2019-03-27 00:21:55,628] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7744
[2019-03-27 00:21:55,632] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 85.0, 1.0, 2.0, 0.8333633007763968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1262923.366575425, 1262923.366575424, 265653.1000151281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1594800.0000, 
sim time next is 1595400.0000, 
raw observation next is [23.73333333333333, 85.00000000000001, 1.0, 2.0, 0.8033237847524282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1216325.692660438, 1216325.692660439, 257306.5942499215], 
processed observation next is [1.0, 0.4782608695652174, 0.3238546603475513, 0.8500000000000001, 1.0, 1.0, 0.7630407045209978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33786824796123277, 0.33786824796123305, 0.3840396929103306], 
reward next is 0.6160, 
noisyNet noise sample is [array([-0.1866573], dtype=float32), 1.1737742]. 
=============================================
[2019-03-27 00:22:04,338] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.73340372e-30 1.00000000e+00 1.17789255e-30 5.91728983e-21
 0.00000000e+00], sum to 1.0000
[2019-03-27 00:22:04,348] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9629
[2019-03-27 00:22:04,353] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 93.5, 1.0, 2.0, 0.4873468549343948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691831.4654513524, 691831.465451353, 183027.3932835453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [24.4, 93.33333333333334, 1.0, 2.0, 0.5124576535202783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726592.295516293, 726592.295516293, 186915.6330045822], 
processed observation next is [1.0, 0.21739130434782608, 0.3554502369668246, 0.9333333333333335, 1.0, 1.0, 0.4125995825545521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20183119319897028, 0.20183119319897028, 0.278978556723257], 
reward next is 0.7210, 
noisyNet noise sample is [array([1.5186323], dtype=float32), -0.03017464]. 
=============================================
[2019-03-27 00:22:09,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5681245e-31 1.0000000e+00 1.3144250e-30 1.8475206e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:09,033] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6706
[2019-03-27 00:22:09,040] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 91.66666666666667, 1.0, 2.0, 0.8180472116404325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1189346.0457431, 1189346.0457431, 254830.9724059374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1847400.0000, 
sim time next is 1848000.0000, 
raw observation next is [24.13333333333333, 91.33333333333334, 1.0, 2.0, 0.8218425929990477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1191510.566723731, 1191510.566723732, 255367.1571649751], 
processed observation next is [1.0, 0.391304347826087, 0.3428120063191152, 0.9133333333333334, 1.0, 1.0, 0.7853525216855997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33097515742325867, 0.3309751574232589, 0.3811450106939927], 
reward next is 0.6189, 
noisyNet noise sample is [array([-1.0498843], dtype=float32), -0.525164]. 
=============================================
[2019-03-27 00:22:09,059] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.70255 ]
 [64.521706]
 [64.54852 ]
 [64.679405]
 [64.89005 ]], R is [[64.71836853]
 [64.69083405]
 [64.630867  ]
 [64.57195282]
 [64.52251434]].
[2019-03-27 00:22:12,433] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0622947e-32 1.0000000e+00 3.3073328e-34 4.5168196e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:12,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0692
[2019-03-27 00:22:12,450] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 89.0, 1.0, 2.0, 0.4575749553836169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650445.6121707486, 650445.612170748, 178641.9379266083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1891800.0000, 
sim time next is 1892400.0000, 
raw observation next is [24.83333333333334, 89.33333333333334, 1.0, 2.0, 0.457008147605568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650229.9721294543, 650229.9721294548, 178634.0645105311], 
processed observation next is [1.0, 0.9130434782608695, 0.3759873617693526, 0.8933333333333334, 1.0, 1.0, 0.3457929489223711, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18061943670262617, 0.18061943670262634, 0.266618006732136], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.1607061], dtype=float32), 0.52156585]. 
=============================================
[2019-03-27 00:22:14,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5655936e-30 1.0000000e+00 4.2048564e-32 5.2868918e-21 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:14,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2197
[2019-03-27 00:22:14,594] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 82.0, 1.0, 2.0, 0.9743470879759385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390709.438635488, 1390709.438635488, 295639.1465482148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1933200.0000, 
sim time next is 1933800.0000, 
raw observation next is [25.85, 81.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.009109463974895, 6.9112, 168.9124048539953, 1553421.696664891, 1483961.579989437, 316169.3551255277], 
processed observation next is [1.0, 0.391304347826087, 0.4241706161137442, 0.8166666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.009790946397489541, 0.0, 0.829437236266654, 0.4315060268513586, 0.41221154999706583, 0.4718945598888473], 
reward next is 0.0386, 
noisyNet noise sample is [array([-0.8212083], dtype=float32), 0.06234048]. 
=============================================
[2019-03-27 00:22:17,991] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.927487e-33 0.000000e+00], sum to 1.0000
[2019-03-27 00:22:18,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7165
[2019-03-27 00:22:18,013] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 96.33333333333333, 1.0, 2.0, 0.412213088772087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607604.927948057, 607604.927948057, 175031.0478428343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1977600.0000, 
sim time next is 1978200.0000, 
raw observation next is [23.1, 96.5, 1.0, 2.0, 0.4148082964729517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610197.1089894206, 610197.1089894206, 175239.8477476627], 
processed observation next is [1.0, 0.9130434782608695, 0.2938388625592418, 0.965, 1.0, 1.0, 0.2949497547866888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16949919694150573, 0.16949919694150573, 0.26155201156367563], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.62508017], dtype=float32), -0.51185733]. 
=============================================
[2019-03-27 00:22:21,913] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1383001e-36 1.0000000e+00 0.0000000e+00 2.9266714e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:21,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1183
[2019-03-27 00:22:21,930] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 84.66666666666667, 1.0, 2.0, 0.5084988919999548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710552.1690202173, 710552.1690202173, 184938.6034889804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2044200.0000, 
sim time next is 2044800.0000, 
raw observation next is [26.9, 85.0, 1.0, 2.0, 0.5087270310955853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710871.0664116039, 710871.0664116046, 184974.9145333862], 
processed observation next is [0.0, 0.6956521739130435, 0.4739336492890995, 0.85, 1.0, 1.0, 0.40810485674166896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19746418511433442, 0.1974641851143346, 0.2760819619901287], 
reward next is 0.7239, 
noisyNet noise sample is [array([-1.4562112], dtype=float32), 0.46519294]. 
=============================================
[2019-03-27 00:22:29,615] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7163915e-26 1.0000000e+00 1.0977172e-27 4.4319299e-21 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:29,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5937
[2019-03-27 00:22:29,628] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 86.0, 1.0, 2.0, 0.686270494815913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 959073.9057718285, 959073.9057718279, 217906.9106131316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2256600.0000, 
sim time next is 2257200.0000, 
raw observation next is [26.2, 86.0, 1.0, 2.0, 0.6677336840332496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933157.0374939744, 933157.0374939751, 214031.9107158564], 
processed observation next is [1.0, 0.13043478260869565, 0.44075829383886256, 0.86, 1.0, 1.0, 0.5996791373894573, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2592102881927707, 0.25921028819277087, 0.3194506130087409], 
reward next is 0.6805, 
noisyNet noise sample is [array([0.53913605], dtype=float32), -0.76326084]. 
=============================================
[2019-03-27 00:22:30,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6841225e-28 1.0000000e+00 1.2509661e-28 1.5505368e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:30,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4976
[2019-03-27 00:22:30,233] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.0, 1.0, 2.0, 0.6954466490191373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 971903.5948098795, 971903.5948098802, 219864.8691363885], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2185200.0000, 
sim time next is 2185800.0000, 
raw observation next is [27.21666666666667, 86.16666666666667, 1.0, 2.0, 0.7708029283147513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1077269.242752012, 1077269.242752012, 236869.8254263724], 
processed observation next is [1.0, 0.30434782608695654, 0.48894154818325447, 0.8616666666666667, 1.0, 1.0, 0.7238589497768088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2992414563200034, 0.2992414563200034, 0.3535370528751827], 
reward next is 0.6465, 
noisyNet noise sample is [array([-0.6832038], dtype=float32), 1.1246972]. 
=============================================
[2019-03-27 00:22:33,777] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3056168e-27 1.0000000e+00 1.3566047e-30 3.7571781e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:33,784] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4530
[2019-03-27 00:22:33,789] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 85.00000000000001, 1.0, 2.0, 0.5256722112767287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734557.6458683417, 734557.6458683423, 187716.0619513254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2243400.0000, 
sim time next is 2244000.0000, 
raw observation next is [27.2, 85.0, 1.0, 2.0, 0.5227447013851051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730465.4300006226, 730465.430000622, 187236.4340996137], 
processed observation next is [1.0, 1.0, 0.4881516587677725, 0.85, 1.0, 1.0, 0.4249936161266326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20290706388906185, 0.20290706388906166, 0.2794573643277816], 
reward next is 0.7205, 
noisyNet noise sample is [array([0.74469614], dtype=float32), 0.29442313]. 
=============================================
[2019-03-27 00:22:33,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[57.900013]
 [57.900406]
 [57.87716 ]
 [57.89639 ]
 [57.91303 ]], R is [[58.03409958]
 [58.17358398]
 [58.31108856]
 [58.44695282]
 [58.58124542]].
[2019-03-27 00:22:35,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5251310e-25 1.0000000e+00 1.5649467e-25 1.5013343e-20 2.1990085e-36], sum to 1.0000
[2019-03-27 00:22:35,971] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9335
[2019-03-27 00:22:35,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2089911.2050024 W.
[2019-03-27 00:22:35,988] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.03333333333333, 62.66666666666667, 1.0, 2.0, 0.4982218550831368, 1.0, 2.0, 0.4982218550831368, 1.0, 2.0, 0.8652465977941838, 6.911199999999999, 6.9112, 170.5573041426782, 2089911.2050024, 2089911.205002401, 413918.1185848867], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2288400.0000, 
sim time next is 2289000.0000, 
raw observation next is [32.01666666666667, 62.83333333333333, 1.0, 2.0, 0.7272593543048202, 1.0, 2.0, 0.7272593543048202, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2033722.709891233, 2033722.709891233, 386014.1785119713], 
processed observation next is [1.0, 0.4782608695652174, 0.7164296998420224, 0.6283333333333333, 1.0, 1.0, 0.671396812415446, 1.0, 1.0, 0.671396812415446, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5649229749697869, 0.5649229749697869, 0.5761405649432407], 
reward next is 0.4239, 
noisyNet noise sample is [array([1.5513834], dtype=float32), -1.1468004]. 
=============================================
[2019-03-27 00:22:35,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[55.15203 ]
 [54.27069 ]
 [54.50227 ]
 [54.457664]
 [55.12846 ]], R is [[55.63354111]
 [55.45941925]
 [54.90482712]
 [54.71378326]
 [54.16664505]].
[2019-03-27 00:22:40,372] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5674790e-30 1.0000000e+00 8.0410433e-33 3.5518053e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 00:22:40,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0795
[2019-03-27 00:22:40,386] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 79.5, 1.0, 2.0, 0.7255714514771513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014023.838048129, 1014023.838048129, 226460.0582356986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2353800.0000, 
sim time next is 2354400.0000, 
raw observation next is [27.9, 79.0, 1.0, 2.0, 0.733048010728053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024477.758532929, 1024477.758532929, 228138.8730248544], 
processed observation next is [1.0, 0.2608695652173913, 0.5213270142180094, 0.79, 1.0, 1.0, 0.6783710972627145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28457715514803583, 0.28457715514803583, 0.340505780634111], 
reward next is 0.6595, 
noisyNet noise sample is [array([-0.4429748], dtype=float32), 0.6688593]. 
=============================================
[2019-03-27 00:22:40,522] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:22:40,526] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:22:40,527] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:22:40,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:22:40,530] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:22:40,532] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:22:40,533] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:22:40,534] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:22:40,532] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:22:40,535] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:22:40,536] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:22:40,548] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-27 00:22:40,549] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-27 00:22:40,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-27 00:22:40,550] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-27 00:22:40,550] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-27 00:23:04,492] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.06523917]
[2019-03-27 00:23:04,494] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.84674370666666, 97.60712588, 1.0, 2.0, 0.4014303228490287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595488.124400908, 595488.1244009074, 174019.8700545626]
[2019-03-27 00:23:04,497] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:23:04,501] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.2200064e-33 1.0000000e+00 2.8896728e-35 6.9747593e-32 0.0000000e+00], sampled 0.08862999761913892
[2019-03-27 00:23:29,607] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.06523917]
[2019-03-27 00:23:29,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.93333333333334, 70.33333333333333, 1.0, 2.0, 0.5420389295984864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757436.1320867295, 757436.1320867295, 190444.2816294958]
[2019-03-27 00:23:29,610] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:23:29,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.9306724e-34 1.0000000e+00 2.5731384e-36 6.2024115e-33 0.0000000e+00], sampled 0.7427476646568268
[2019-03-27 00:23:32,101] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.06523917]
[2019-03-27 00:23:32,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.030118294521294, 6.9112, 168.9120326312334, 2379645.691511907, 2295281.421964455, 476384.1592447076]
[2019-03-27 00:23:32,104] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:23:32,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5738947e-28 1.0000000e+00 1.2001219e-29 4.7902030e-25 0.0000000e+00], sampled 0.2557168723852301
[2019-03-27 00:23:32,110] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2379645.691511907 W.
[2019-03-27 00:23:55,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.06523917]
[2019-03-27 00:23:55,722] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.35, 72.33333333333333, 1.0, 2.0, 0.6063227649164314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847301.1169969224, 847301.1169969224, 201929.1128022019]
[2019-03-27 00:23:55,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:23:55,726] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1412148e-34 1.0000000e+00 4.1879251e-37 1.3096733e-33 0.0000000e+00], sampled 0.8954046586039502
[2019-03-27 00:24:01,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04089416], dtype=float32), -0.06523917]
[2019-03-27 00:24:01,460] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.86745591, 67.96973577, 1.0, 2.0, 0.5213674167407951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728540.1999175757, 728540.199917575, 187011.3938908396]
[2019-03-27 00:24:01,463] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:24:01,467] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8645277e-33 1.0000000e+00 2.2172969e-35 6.1113123e-32 0.0000000e+00], sampled 0.3677590783804622
[2019-03-27 00:24:35,080] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:24:35,085] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:24:35,186] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:24:35,333] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:24:35,340] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:24:36,358] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 275000, evaluation results [275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:24:36,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5550634e-28 1.0000000e+00 1.7741853e-30 1.1607979e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:24:36,700] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3534
[2019-03-27 00:24:36,704] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.855149821951676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1195218.267409612, 1195218.267409612, 257897.9041275255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2361600.0000, 
sim time next is 2362200.0000, 
raw observation next is [29.65, 71.5, 1.0, 2.0, 0.8092741896984313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1131065.067225003, 1131065.067225003, 246199.7353304236], 
processed observation next is [1.0, 0.34782608695652173, 0.6042654028436019, 0.715, 1.0, 1.0, 0.7702098671065438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31418474089583415, 0.31418474089583415, 0.3674622915379457], 
reward next is 0.6325, 
noisyNet noise sample is [array([-0.5257055], dtype=float32), 1.1711485]. 
=============================================
[2019-03-27 00:24:43,046] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5629047e-19 1.0000000e+00 3.1518663e-23 1.4038628e-19 1.5280220e-28], sum to 1.0000
[2019-03-27 00:24:43,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4735
[2019-03-27 00:24:43,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1795463.110207902 W.
[2019-03-27 00:24:43,069] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.5, 84.66666666666667, 1.0, 2.0, 0.6421290393037661, 1.0, 2.0, 0.6421290393037661, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1795463.110207902, 1795463.110207902, 350331.9422893524], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [27.6, 84.33333333333333, 1.0, 2.0, 0.4147484023428136, 1.0, 2.0, 0.4147484023428136, 1.0, 1.0, 0.7134383082001322, 6.9112, 6.9112, 170.5573041426782, 1739477.520178595, 1739477.520178595, 360202.5234824367], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.8433333333333333, 1.0, 1.0, 0.29487759318411283, 1.0, 1.0, 0.29487759318411283, 1.0, 0.5, 0.6505345221952831, 0.0, 0.0, 0.8375144448122397, 0.4831882000496098, 0.4831882000496098, 0.5376157066902041], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1154467], dtype=float32), -0.73823506]. 
=============================================
[2019-03-27 00:24:47,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1309000e-20 1.0000000e+00 1.3808040e-23 3.5644295e-22 3.7370854e-30], sum to 1.0000
[2019-03-27 00:24:47,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6074
[2019-03-27 00:24:47,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1811481.46193655 W.
[2019-03-27 00:24:47,935] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.45, 73.5, 1.0, 2.0, 0.431902000491057, 1.0, 2.0, 0.431902000491057, 1.0, 1.0, 0.7445812023885592, 6.911200000000001, 6.9112, 170.5573041426782, 1811481.46193655, 1811481.46193655, 370411.1680582972], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2550600.0000, 
sim time next is 2551200.0000, 
raw observation next is [29.56666666666667, 72.66666666666666, 1.0, 2.0, 0.6399226667709975, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.986423826875484, 6.9112, 168.9124936118325, 1789303.24764651, 1735937.022104946, 372808.3639786445], 
processed observation next is [1.0, 0.5217391304347826, 0.6003159557661929, 0.7266666666666666, 1.0, 1.0, 0.5661718876759007, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00752238268754839, 0.0, 0.8294376721082681, 0.4970286799018083, 0.482204728362485, 0.5564303939979769], 
reward next is 0.0675, 
noisyNet noise sample is [array([0.11413332], dtype=float32), -0.89755124]. 
=============================================
[2019-03-27 00:24:59,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:24:59,308] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6118
[2019-03-27 00:24:59,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3937032667045529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587464.4672788488, 587464.4672788488, 173385.4817238476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2741400.0000, 
sim time next is 2742000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3931210197645402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586595.8043169726, 586595.804316972, 173306.165648713], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2688205057404099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16294327897693683, 0.16294327897693667, 0.2586659188786761], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.42912814], dtype=float32), -0.06318487]. 
=============================================
[2019-03-27 00:24:59,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.15806 ]
 [79.11339 ]
 [79.06828 ]
 [79.014496]
 [78.92871 ]], R is [[79.16411591]
 [79.11369324]
 [79.06370544]
 [79.01423645]
 [78.96524048]].
[2019-03-27 00:25:00,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4277886e-36 1.0000000e+00 0.0000000e+00 1.1434186e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:25:00,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3669
[2019-03-27 00:25:00,786] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 97.0, 1.0, 2.0, 0.3664723963152283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557692.7297589987, 557692.7297589987, 171098.6210555367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2759400.0000, 
sim time next is 2760000.0000, 
raw observation next is [22.0, 96.0, 1.0, 2.0, 0.361777469942939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552609.0492098521, 552609.0492098528, 170729.5282432186], 
processed observation next is [0.0, 0.9565217391304348, 0.2417061611374408, 0.96, 1.0, 1.0, 0.23105719270233616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15350251366940337, 0.15350251366940357, 0.25482019140778894], 
reward next is 0.7452, 
noisyNet noise sample is [array([0.32087162], dtype=float32), 1.2642896]. 
=============================================
[2019-03-27 00:25:00,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.0351 ]
 [76.02736]
 [76.00739]
 [75.97889]
 [75.914  ]], R is [[76.02496338]
 [76.00934601]
 [75.99344635]
 [75.97743225]
 [75.96150208]].
[2019-03-27 00:25:05,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9146493e-36 1.0000000e+00 0.0000000e+00 8.5783069e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 00:25:05,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7039
[2019-03-27 00:25:05,021] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4059744611742992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598168.7936836937, 598168.793683693, 174143.7409613761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2828400.0000, 
sim time next is 2829000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4040913762003742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595394.5450848016, 595394.545084801, 173887.1380199762], 
processed observation next is [1.0, 0.7391304347826086, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28203780265105327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16538737363466713, 0.16538737363466696, 0.25953304182086], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.26195136], dtype=float32), -0.39861473]. 
=============================================
[2019-03-27 00:25:05,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.669205]
 [76.28839 ]
 [75.78286 ]
 [74.88932 ]
 [73.59668 ]], R is [[76.98899841]
 [76.95919037]
 [76.93009186]
 [76.90113068]
 [76.85502625]].
[2019-03-27 00:25:05,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:25:05,242] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0334
[2019-03-27 00:25:05,247] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4091672451503332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602870.9496955557, 602870.9496955563, 174580.6887437804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4093354669916394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 603119.3580655652, 603119.3580655647, 174603.8807527346], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.2883559843272764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16753315501821256, 0.1675331550182124, 0.26060280709363376], 
reward next is 0.7394, 
noisyNet noise sample is [array([-0.32361373], dtype=float32), 1.0281203]. 
=============================================
[2019-03-27 00:25:05,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.30494]
 [89.14601]
 [88.93989]
 [88.78543]
 [88.71758]], R is [[89.28665161]
 [89.13321686]
 [88.98126221]
 [88.82993317]
 [88.67997742]].
[2019-03-27 00:25:07,835] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7521194e-36 1.0000000e+00 4.5784941e-37 2.8751272e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 00:25:07,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2176
[2019-03-27 00:25:07,852] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 89.0, 1.0, 2.0, 0.5395591959381556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844189.1259799454, 844189.1259799454, 200583.1075371584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2973000.0000, 
sim time next is 2973600.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.5608023860212256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880231.2487849245, 880231.2487849245, 204935.8393119968], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.88, 1.0, 1.0, 0.47084624821834403, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2445086802180346, 0.2445086802180346, 0.30587438703283104], 
reward next is 0.6941, 
noisyNet noise sample is [array([0.07670692], dtype=float32), -0.65461123]. 
=============================================
[2019-03-27 00:25:22,087] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5416174e-35 1.0000000e+00 0.0000000e+00 8.8250764e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:25:22,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9498
[2019-03-27 00:25:22,107] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3852086289591706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583957.1103113506, 583957.1103113499, 173337.6641927693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.3820660893101436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581410.119889785, 581410.119889785, 173170.2989534034], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.97, 1.0, 1.0, 0.25550131242185975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16150281108049583, 0.16150281108049583, 0.25846313276627375], 
reward next is 0.7415, 
noisyNet noise sample is [array([1.211204], dtype=float32), 0.48228142]. 
=============================================
[2019-03-27 00:25:22,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.74035 ]
 [61.717873]
 [61.8658  ]
 [61.792465]
 [61.801357]], R is [[61.87059784]
 [61.99317932]
 [62.10582733]
 [62.22784042]
 [62.34812927]].
[2019-03-27 00:25:25,874] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0166535e-28 1.0000000e+00 2.6916394e-30 2.8545918e-27 0.0000000e+00], sum to 1.0000
[2019-03-27 00:25:25,884] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7507
[2019-03-27 00:25:25,886] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.4974510962164605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695109.4536510713, 695109.453651072, 183198.4501563019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3175200.0000, 
sim time next is 3175800.0000, 
raw observation next is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5022602342015936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701831.6854603944, 701831.685460395, 183951.3551340682], 
processed observation next is [1.0, 0.782608695652174, 0.470774091627172, 0.8483333333333333, 1.0, 1.0, 0.40031353518264284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19495324596122066, 0.19495324596122082, 0.27455426139413164], 
reward next is 0.7254, 
noisyNet noise sample is [array([-2.1711383], dtype=float32), -1.1244162]. 
=============================================
[2019-03-27 00:25:26,496] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:25:26,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0422
[2019-03-27 00:25:26,520] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.00000000000001, 1.0, 2.0, 0.4919456088401552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687413.9141745166, 687413.9141745173, 182343.4337176877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3193800.0000, 
sim time next is 3194400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4894258108128207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683891.7731756506, 683891.77317565, 181955.7322039544], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.384850374473278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18996993699323628, 0.1899699369932361, 0.27157571970739464], 
reward next is 0.7284, 
noisyNet noise sample is [array([-1.4567287], dtype=float32), 0.8764774]. 
=============================================
[2019-03-27 00:25:28,676] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:25:28,687] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9306
[2019-03-27 00:25:28,696] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4692845440200683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656322.6936613327, 656322.6936613327, 178999.0570207532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3222000.0000, 
sim time next is 3222600.0000, 
raw observation next is [26.16666666666667, 83.16666666666667, 1.0, 2.0, 0.4711421527747118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658335.4629072178, 658335.4629072185, 179198.1220709058], 
processed observation next is [0.0, 0.30434782608695654, 0.4391785150078992, 0.8316666666666667, 1.0, 1.0, 0.3628218708129058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1828709619186716, 0.1828709619186718, 0.2674598836879191], 
reward next is 0.7325, 
noisyNet noise sample is [array([-1.1526134], dtype=float32), -0.3641068]. 
=============================================
[2019-03-27 00:25:28,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:25:28,873] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8316
[2019-03-27 00:25:28,879] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 75.66666666666667, 1.0, 2.0, 0.4981132724309033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696035.0436176839, 696035.0436176833, 183301.7897510382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3230400.0000, 
sim time next is 3231000.0000, 
raw observation next is [28.5, 76.5, 1.0, 2.0, 0.5067806721666409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708150.4100429899, 708150.4100429906, 184666.289243801], 
processed observation next is [0.0, 0.391304347826087, 0.5497630331753555, 0.765, 1.0, 1.0, 0.4057598459839047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19670844723416386, 0.19670844723416406, 0.27562132722955374], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.7197975], dtype=float32), -0.21902485]. 
=============================================
[2019-03-27 00:25:28,899] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.26402]
 [70.26989]
 [70.26639]
 [70.22237]
 [70.23416]], R is [[70.27793121]
 [70.30156708]
 [70.32691956]
 [70.35352325]
 [70.37999725]].
[2019-03-27 00:25:30,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:25:30,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3457
[2019-03-27 00:25:30,609] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 77.66666666666667, 1.0, 2.0, 0.5705677971913229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 797316.8567040372, 797316.8567040366, 195390.7187670969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3264000.0000, 
sim time next is 3264600.0000, 
raw observation next is [29.33333333333333, 78.33333333333334, 1.0, 2.0, 0.5638418887388965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787914.5223730415, 787914.5223730408, 194202.8663751448], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494469, 0.7833333333333334, 1.0, 1.0, 0.47450829968541747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21886514510362265, 0.21886514510362245, 0.28985502444051464], 
reward next is 0.7101, 
noisyNet noise sample is [array([1.524443], dtype=float32), -0.36441517]. 
=============================================
[2019-03-27 00:25:32,715] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 00:25:32,716] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:25:32,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:25:32,719] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:25:32,720] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:25:32,721] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:25:32,721] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:25:32,722] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:25:32,724] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:25:32,719] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:25:32,734] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:25:32,740] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-27 00:25:32,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-27 00:25:32,761] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-27 00:25:32,780] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-27 00:25:32,816] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-27 00:25:34,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:25:34,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.99095972, 75.25590267, 1.0, 2.0, 0.7249798916473273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1013196.708031669, 1013196.70803167, 226326.9444070113]
[2019-03-27 00:25:34,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:25:34,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7021747e-38 1.0000000e+00 0.0000000e+00 2.8788433e-37 0.0000000e+00], sampled 0.5734210011639238
[2019-03-27 00:25:43,111] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:25:43,113] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.64890469, 85.69990258, 1.0, 2.0, 0.2364279748117202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 393912.7559770458, 393912.7559770458, 159075.4199491137]
[2019-03-27 00:25:43,114] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:25:43,119] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.6618563e-37 1.0000000e+00 0.0000000e+00 5.7575906e-37 0.0000000e+00], sampled 0.1937205346595159
[2019-03-27 00:26:34,880] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:26:34,881] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.15000000000001, 47.5, 1.0, 2.0, 0.8314482824092604, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00531367152853, 6.9112, 168.9123206326423, 2059087.415531918, 1992320.189155121, 415769.1797409155]
[2019-03-27 00:26:34,883] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:26:34,885] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8349612268898601
[2019-03-27 00:26:34,886] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2059087.415531918 W.
[2019-03-27 00:26:41,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:26:41,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.83111048, 53.94662054, 1.0, 2.0, 0.4030715345679898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607042.1026577882, 607042.1026577889, 175332.3493390636]
[2019-03-27 00:26:41,792] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:26:41,793] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6874178280516526
[2019-03-27 00:27:02,614] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:27:02,619] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.91391915333334, 72.77358038666667, 1.0, 2.0, 0.604497671082315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844749.6393120718, 844749.6393120718, 201587.9551824171]
[2019-03-27 00:27:02,622] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:27:02,625] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6438178267557795
[2019-03-27 00:27:12,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:27:12,395] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.65884419333333, 81.17867226333333, 1.0, 2.0, 0.4353102636823881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631240.5835527035, 631240.5835527029, 177027.3778481398]
[2019-03-27 00:27:12,397] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:27:12,400] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5631983061924474
[2019-03-27 00:27:13,396] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:27:13,397] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.01551073166667, 59.85508344166666, 1.0, 2.0, 0.4169497169556086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610902.3916622589, 610902.3916622582, 175236.5479869203]
[2019-03-27 00:27:13,398] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:27:13,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5354581578407998
[2019-03-27 00:27:15,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:27:15,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.83333333333334, 87.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.010808539488163, 6.9112, 168.9121576942658, 1524468.71425469, 1453803.322269866, 311354.6907366864]
[2019-03-27 00:27:15,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:27:15,588] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4247788e-38 0.0000000e+00], sampled 0.4930055436112889
[2019-03-27 00:27:24,750] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.04200586]
[2019-03-27 00:27:24,752] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.99768238166667, 92.20449489, 1.0, 2.0, 0.3835151464082563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578407.797052589, 578407.797052589, 172755.750374882]
[2019-03-27 00:27:24,754] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:27:24,756] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8968237907240212
[2019-03-27 00:27:27,526] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:27:27,683] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:27:27,944] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:27:28,032] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:27:28,047] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:27:29,062] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 300000, evaluation results [300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:27:35,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6329881e-31 1.0000000e+00 5.5084277e-31 5.9231122e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 00:27:35,212] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4285
[2019-03-27 00:27:35,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1811596.855511853 W.
[2019-03-27 00:27:35,225] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.415267658261342, 6.9112, 168.9101480798178, 1811596.855511853, 1453999.855755337, 311353.8031757549], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.3929227981384718, 1.0, 1.0, 0.3929227981384718, 1.0, 1.0, 0.6823769588115577, 6.911199999999999, 6.9112, 170.5573041426782, 1647869.360069792, 1647869.360069792, 349005.9175207075], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.84, 1.0, 1.0, 0.2685816845041829, 1.0, 0.5, 0.2685816845041829, 1.0, 0.5, 0.6126548278189727, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.45774148890827554, 0.45774148890827554, 0.5209043545085186], 
reward next is 0.4791, 
noisyNet noise sample is [array([0.31411126], dtype=float32), 0.46378654]. 
=============================================
[2019-03-27 00:27:37,454] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9664217e-31 1.0000000e+00 1.5497034e-32 4.3882942e-27 0.0000000e+00], sum to 1.0000
[2019-03-27 00:27:37,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1253
[2019-03-27 00:27:37,501] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 73.33333333333334, 1.0, 2.0, 0.5277161059006845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737414.7106371127, 737414.7106371133, 188052.9137473982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3437400.0000, 
sim time next is 3438000.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.521772456269125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729106.38192588, 729106.38192588, 187077.9046272713], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.74, 1.0, 1.0, 0.42382223646882533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20252955053496666, 0.20252955053496666, 0.27922075317503176], 
reward next is 0.7208, 
noisyNet noise sample is [array([1.0726192], dtype=float32), -1.763309]. 
=============================================
[2019-03-27 00:27:37,521] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[51.817642]
 [51.95607 ]
 [52.304684]
 [52.81611 ]
 [52.535732]], R is [[51.25112534]
 [51.45793533]
 [51.66124725]
 [51.8611908 ]
 [52.05807114]].
[2019-03-27 00:27:40,083] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.634161e-37 1.000000e+00 2.538951e-38 5.257045e-37 0.000000e+00], sum to 1.0000
[2019-03-27 00:27:40,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3855
[2019-03-27 00:27:40,098] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.7951577808035653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1111325.240513955, 1111325.240513955, 242723.3068325832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3479400.0000, 
sim time next is 3480000.0000, 
raw observation next is [27.66666666666666, 79.0, 1.0, 2.0, 0.8064513657963535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1127117.712805864, 1127117.712805865, 245498.5105834061], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.79, 1.0, 1.0, 0.7668088744534379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31308825355718445, 0.3130882535571847, 0.3664156874379196], 
reward next is 0.6336, 
noisyNet noise sample is [array([0.9618126], dtype=float32), 0.89974093]. 
=============================================
[2019-03-27 00:27:40,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[66.8764  ]
 [67.319466]
 [67.788216]
 [67.87815 ]
 [67.90494 ]], R is [[66.56215668]
 [66.53426361]
 [66.52297974]
 [66.54197693]
 [66.56842804]].
[2019-03-27 00:27:41,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3314542e-18 1.0000000e+00 8.0584814e-19 1.0473178e-11 1.2190236e-25], sum to 1.0000
[2019-03-27 00:27:41,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3282
[2019-03-27 00:27:41,116] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2196676.075650483 W.
[2019-03-27 00:27:41,119] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5236478664927536, 1.0, 2.0, 0.5236478664927536, 1.0, 1.0, 0.9094031711022253, 6.9112, 6.9112, 170.5573041426782, 2196676.075650483, 2196676.075650483, 431884.3344837918], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3582000.0000, 
sim time next is 3582600.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.5674656138056888, 1.0, 2.0, 0.5674656138056888, 1.0, 2.0, 0.9855001074343257, 6.9112, 6.9112, 170.5573041426782, 2380666.899687275, 2380666.899687275, 464934.568507009], 
processed observation next is [1.0, 0.4782608695652174, 0.6761453396524489, 0.695, 1.0, 1.0, 0.4788742335008299, 1.0, 1.0, 0.4788742335008299, 1.0, 1.0, 0.9823172041882021, 0.0, 0.0, 0.8375144448122397, 0.661296361024243, 0.661296361024243, 0.693932191801506], 
reward next is 0.3061, 
noisyNet noise sample is [array([0.8201269], dtype=float32), -0.28884202]. 
=============================================
[2019-03-27 00:27:41,578] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2031585e-16 1.0000000e+00 7.1139020e-16 2.8294358e-11 1.2851816e-23], sum to 1.0000
[2019-03-27 00:27:41,585] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5861
[2019-03-27 00:27:41,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2326804.682188787 W.
[2019-03-27 00:27:41,599] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 1.022721051951661, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005994590514376, 6.9112, 168.9122927088433, 2326804.682188787, 2259554.401384034, 470364.7851427609], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3499200.0000, 
sim time next is 3499800.0000, 
raw observation next is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.641799708069479, 6.9112, 168.9089996047504, 2802456.346706386, 2284155.910339925, 474454.7680512081], 
processed observation next is [1.0, 0.5217391304347826, 0.7235387045813588, 0.6633333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07305997080694793, 0.0, 0.8294205149339317, 0.7784600963073295, 0.6344877528722014, 0.7081414448525494], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.36363363], dtype=float32), -2.083132]. 
=============================================
[2019-03-27 00:27:48,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1278114e-27 1.0000000e+00 1.6483440e-30 5.6288864e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 00:27:48,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3390
[2019-03-27 00:27:49,002] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5191185488877897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725396.6393389122, 725396.6393389116, 186645.7186011872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [28.0, 78.16666666666667, 1.0, 2.0, 0.5162032013253437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721321.4596799125, 721321.4596799125, 186173.5513085935], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7816666666666667, 1.0, 1.0, 0.4171122907534261, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20036707213330904, 0.20036707213330904, 0.2778709721023784], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.31081438], dtype=float32), 0.737845]. 
=============================================
[2019-03-27 00:27:52,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.405030e-20 1.000000e+00 4.844682e-21 8.106311e-13 3.169945e-28], sum to 1.0000
[2019-03-27 00:27:52,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8291
[2019-03-27 00:27:52,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2384201.009960529 W.
[2019-03-27 00:27:52,620] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.5, 63.0, 1.0, 2.0, 0.8524608241504976, 1.0, 2.0, 0.8524608241504976, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2384201.009960529, 2384201.009960529, 446221.4828729453], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.8671257403849977, 1.0, 2.0, 0.8671257403849977, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2425256.312938304, 2425256.312938304, 453876.219128867], 
processed observation next is [1.0, 0.6956521739130435, 0.7314375987361774, 0.6433333333333333, 1.0, 1.0, 0.8399105305843345, 1.0, 1.0, 0.8399105305843345, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.673682309149529, 0.673682309149529, 0.6774271927296522], 
reward next is 0.3226, 
noisyNet noise sample is [array([-0.13766436], dtype=float32), -1.1828711]. 
=============================================
[2019-03-27 00:27:59,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1439363e-35 1.0000000e+00 0.0000000e+00 1.4783925e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 00:27:59,390] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7796
[2019-03-27 00:27:59,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5172292827914047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722755.7521721631, 722755.7521721631, 186339.6530535103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3807600.0000, 
sim time next is 3808200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5169078942163573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722306.5037944618, 722306.5037944611, 186287.6925481673], 
processed observation next is [0.0, 0.043478260869565216, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4179613183329606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006406954984616, 0.20064069549846142, 0.2780413321614437], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.56490284], dtype=float32), -0.98955625]. 
=============================================
[2019-03-27 00:28:04,102] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7595672e-32 1.0000000e+00 0.0000000e+00 9.1576720e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:04,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0951
[2019-03-27 00:28:04,121] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 71.5, 1.0, 2.0, 0.5427197607385906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758387.8537620524, 758387.8537620524, 190559.7242899053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3877800.0000, 
sim time next is 3878400.0000, 
raw observation next is [29.66666666666667, 73.0, 1.0, 2.0, 0.5404450279479359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755208.0488084322, 755208.0488084329, 190175.6901563578], 
processed observation next is [0.0, 0.9130434782608695, 0.6050552922590839, 0.73, 1.0, 1.0, 0.4463193107806457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20978001355789783, 0.20978001355789802, 0.28384431366620566], 
reward next is 0.7162, 
noisyNet noise sample is [array([-0.17031099], dtype=float32), -1.4148419]. 
=============================================
[2019-03-27 00:28:04,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3834156e-29 1.0000000e+00 1.1097005e-33 1.2209904e-28 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:04,915] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9515
[2019-03-27 00:28:04,919] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.576169738306765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805148.0293902752, 805148.0293902752, 196390.060684455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3902400.0000, 
sim time next is 3903000.0000, 
raw observation next is [27.83333333333334, 89.83333333333334, 1.0, 2.0, 0.576277442121096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805298.5933324341, 805298.5933324341, 196409.2361476552], 
processed observation next is [0.0, 0.17391304347826086, 0.5181674565560824, 0.8983333333333334, 1.0, 1.0, 0.48949089412180236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2236940537034539, 0.2236940537034539, 0.29314811365321675], 
reward next is 0.7069, 
noisyNet noise sample is [array([1.8547702], dtype=float32), 0.27755308]. 
=============================================
[2019-03-27 00:28:04,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[57.147057]
 [57.30277 ]
 [57.49    ]
 [57.641987]
 [57.795338]], R is [[57.17598724]
 [57.31110764]
 [57.44536972]
 [57.57868576]
 [57.71090698]].
[2019-03-27 00:28:05,581] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6563692e-36 1.0000000e+00 3.2158586e-38 2.9990517e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:05,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6547
[2019-03-27 00:28:05,596] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 91.5, 1.0, 2.0, 0.5593800536740255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 781677.2450658308, 781677.2450658302, 193421.7347716822], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3907800.0000, 
sim time next is 3908400.0000, 
raw observation next is [27.0, 90.66666666666666, 1.0, 2.0, 0.5548512991716928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775346.4573599938, 775346.4573599938, 192635.5245040893], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.9066666666666666, 1.0, 1.0, 0.4636762640622805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2153740159333316, 0.2153740159333316, 0.2875157082150586], 
reward next is 0.7125, 
noisyNet noise sample is [array([0.5637113], dtype=float32), 2.4360118]. 
=============================================
[2019-03-27 00:28:06,329] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4255222e-37 1.0000000e+00 7.4642136e-38 1.5127293e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:06,339] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7818
[2019-03-27 00:28:06,346] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6059374622129535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846762.4640449545, 846762.4640449551, 201857.6914519609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3920400.0000, 
sim time next is 3921000.0000, 
raw observation next is [32.0, 70.33333333333334, 1.0, 2.0, 0.6280266165448627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877643.5298606829, 877643.5298606829, 206082.2154341878], 
processed observation next is [0.0, 0.391304347826087, 0.7156398104265403, 0.7033333333333335, 1.0, 1.0, 0.5518392970420032, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24378986940574526, 0.24378986940574526, 0.3075853961704295], 
reward next is 0.6924, 
noisyNet noise sample is [array([2.0534918], dtype=float32), 0.080802985]. 
=============================================
[2019-03-27 00:28:06,364] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[67.41605 ]
 [67.423706]
 [67.48528 ]
 [67.53957 ]
 [67.594055]], R is [[67.33156586]
 [67.35697174]
 [67.38286591]
 [67.40919495]
 [67.43577576]].
[2019-03-27 00:28:08,640] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0611825e-34 1.0000000e+00 2.3275828e-36 1.8009486e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:08,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9403
[2019-03-27 00:28:08,654] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5919392236872658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 827193.1200939014, 827193.1200939008, 199253.0817238713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3978000.0000, 
sim time next is 3978600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5909416752724208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825798.5749622694, 825798.5749622694, 199069.8323283244], 
processed observation next is [1.0, 0.043478260869565216, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5071586449065311, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22938849304507483, 0.22938849304507483, 0.29711915272884243], 
reward next is 0.7029, 
noisyNet noise sample is [array([-0.36307696], dtype=float32), 0.14538598]. 
=============================================
[2019-03-27 00:28:09,392] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6378390e-36 1.0000000e+00 1.3865209e-37 4.1018640e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:09,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7929
[2019-03-27 00:28:09,403] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 76.33333333333333, 1.0, 2.0, 0.6164178685432905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861414.1632499291, 861414.1632499298, 203844.9507847506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3969600.0000, 
sim time next is 3970200.0000, 
raw observation next is [31.16666666666667, 77.66666666666667, 1.0, 2.0, 0.6188350292305955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864793.4035982647, 864793.4035982647, 204307.8921634679], 
processed observation next is [0.0, 0.9565217391304348, 0.6761453396524489, 0.7766666666666667, 1.0, 1.0, 0.5407650954585488, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24022038988840685, 0.24022038988840685, 0.3049371524827879], 
reward next is 0.6951, 
noisyNet noise sample is [array([0.42732748], dtype=float32), -0.85423416]. 
=============================================
[2019-03-27 00:28:13,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3126091e-20 1.0000000e+00 4.1343802e-24 1.3948549e-15 4.7375610e-31], sum to 1.0000
[2019-03-27 00:28:13,314] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1027
[2019-03-27 00:28:13,322] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2341527.877640123 W.
[2019-03-27 00:28:13,326] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 1.033239679046829, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005995098977388, 6.9112, 168.9123903871312, 2341527.877640123, 2274277.197226051, 473693.7110067045], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4035600.0000, 
sim time next is 4036200.0000, 
raw observation next is [30.0, 79.00000000000001, 1.0, 2.0, 0.3890109881367778, 1.0, 1.0, 0.3890109881367778, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1087358.3363046, 1087358.3363046, 269405.633033236], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.7900000000000001, 1.0, 1.0, 0.2638686604057564, 1.0, 0.5, 0.2638686604057564, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3020439823068333, 0.3020439823068333, 0.4020979597510985], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6234964], dtype=float32), -0.7247526]. 
=============================================
[2019-03-27 00:28:14,604] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6495373e-32 1.0000000e+00 2.0827488e-37 1.3460065e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:14,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3157
[2019-03-27 00:28:14,615] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 85.0, 1.0, 2.0, 0.5417378231236277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757015.2210022269, 757015.2210022275, 190394.0819614282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4064400.0000, 
sim time next is 4065000.0000, 
raw observation next is [27.76666666666667, 85.16666666666667, 1.0, 2.0, 0.5412760465771025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756369.7125669847, 756369.7125669853, 190316.0600203178], 
processed observation next is [1.0, 0.043478260869565216, 0.515007898894155, 0.8516666666666667, 1.0, 1.0, 0.4473205380447018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21010269793527353, 0.2101026979352737, 0.28405382092584747], 
reward next is 0.7159, 
noisyNet noise sample is [array([1.1575303], dtype=float32), 0.054718368]. 
=============================================
[2019-03-27 00:28:14,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.806587]
 [63.935482]
 [64.10568 ]
 [64.281845]
 [64.543076]], R is [[63.68433762]
 [63.76332092]
 [63.84152603]
 [63.91891479]
 [63.99540329]].
[2019-03-27 00:28:15,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7028043e-29 1.0000000e+00 2.0006951e-34 7.3372936e-29 0.0000000e+00], sum to 1.0000
[2019-03-27 00:28:15,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3702
[2019-03-27 00:28:15,278] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 84.66666666666667, 1.0, 2.0, 0.5418295375273203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757143.4268265029, 757143.4268265023, 190409.6253148704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4063200.0000, 
sim time next is 4063800.0000, 
raw observation next is [27.83333333333334, 84.83333333333334, 1.0, 2.0, 0.5416925017101607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756951.8670642086, 756951.867064208, 190386.4460029551], 
processed observation next is [1.0, 0.0, 0.5181674565560824, 0.8483333333333334, 1.0, 1.0, 0.4478222912170611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21026440751783573, 0.21026440751783557, 0.28415887463127626], 
reward next is 0.7158, 
noisyNet noise sample is [array([-1.2801493], dtype=float32), 0.6991589]. 
=============================================
[2019-03-27 00:28:20,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2856345e-25 1.0000000e+00 1.3262524e-28 5.2676065e-21 4.1223191e-38], sum to 1.0000
[2019-03-27 00:28:20,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1969
[2019-03-27 00:28:20,915] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1703713.217283246 W.
[2019-03-27 00:28:20,921] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 81.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.263297578968324, 6.9112, 168.9109766956921, 1703713.217283246, 1453926.004269188, 311358.1073930356], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [31.33333333333334, 80.66666666666667, 1.0, 2.0, 0.538723416837185, 1.0, 1.0, 0.538723416837185, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1506126.490958303, 1506126.490958303, 312742.9420290756], 
processed observation next is [1.0, 0.30434782608695654, 0.6840442338072673, 0.8066666666666668, 1.0, 1.0, 0.44424508052672895, 1.0, 0.5, 0.44424508052672895, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.41836846971063973, 0.41836846971063973, 0.4667805104911576], 
reward next is 0.5332, 
noisyNet noise sample is [array([-1.1329027], dtype=float32), -1.2141875]. 
=============================================
[2019-03-27 00:28:25,007] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 00:28:25,008] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:28:25,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:28:25,009] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:28:25,010] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:28:25,010] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:28:25,012] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:28:25,014] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:28:25,012] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:28:25,015] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:28:25,016] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:28:25,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-27 00:28:25,036] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-27 00:28:25,084] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-27 00:28:25,085] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-27 00:28:25,106] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-27 00:29:10,234] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:29:10,235] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.058265665, 95.17373096499999, 1.0, 2.0, 0.2846656100402948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 464433.5550475363, 464433.5550475357, 164481.0536836167]
[2019-03-27 00:29:10,235] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:29:10,237] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7259134e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.49587196556685187
[2019-03-27 00:29:20,353] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:29:20,354] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.94217272, 81.6312576, 1.0, 2.0, 0.5782660031719423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808078.4949164586, 808078.4949164586, 196765.8517872879]
[2019-03-27 00:29:20,355] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:29:20,358] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4906901794646539
[2019-03-27 00:29:25,329] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:29:25,331] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.6942198967818454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970188.3956380525, 970188.3956380525, 219602.720940052]
[2019-03-27 00:29:25,333] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:29:25,335] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7102015e-33 1.0000000e+00 2.4090445e-38 1.0776202e-33 0.0000000e+00], sampled 0.36320105343754794
[2019-03-27 00:29:30,770] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:29:30,771] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.71731329333333, 60.80479875, 1.0, 2.0, 0.5510236573345346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769995.7909400731, 769995.7909400731, 191975.1352429845]
[2019-03-27 00:29:30,772] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:29:30,774] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.0728107e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4589454384303202
[2019-03-27 00:29:41,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:29:41,300] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.3, 55.66666666666667, 1.0, 2.0, 1.005264876375378, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564375855, 1405168.60178911, 1405168.601789109, 300539.8000087814]
[2019-03-27 00:29:41,302] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:29:41,304] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2062434e-31 1.0000000e+00 2.0807567e-36 1.2591367e-31 0.0000000e+00], sampled 0.8560345170367675
[2019-03-27 00:30:08,570] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:30:08,571] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.0, 74.33333333333334, 1.0, 2.0, 0.6719220950146619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939012.9245584757, 939012.9245584764, 214905.9979133092]
[2019-03-27 00:30:08,571] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:30:08,573] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4894228e-36 1.0000000e+00 0.0000000e+00 2.9053970e-38 0.0000000e+00], sampled 0.27448745070017944
[2019-03-27 00:30:15,379] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.057952534]
[2019-03-27 00:30:15,380] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.5, 84.0, 1.0, 2.0, 0.5708135325548224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797660.3785733518, 797660.3785733518, 195433.9547625683]
[2019-03-27 00:30:15,382] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:30:15,387] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0667477e-36 1.0000000e+00 0.0000000e+00 3.1355105e-37 0.0000000e+00], sampled 0.9744317253537069
[2019-03-27 00:30:19,505] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:30:19,864] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:30:19,882] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:30:19,903] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:30:19,975] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:30:20,991] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 325000, evaluation results [325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:30:24,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5384599e-31 1.0000000e+00 6.1002796e-37 3.0553978e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:30:24,978] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7088
[2019-03-27 00:30:24,982] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.0, 50.0, 1.0, 2.0, 0.5725718040855148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800118.3297242044, 800118.3297242044, 195748.5385102798], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4299600.0000, 
sim time next is 4300200.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.5709728603557979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 797883.1084150588, 797883.1084150594, 195464.1266514716], 
processed observation next is [1.0, 0.782608695652174, 0.9052132701421801, 0.5, 1.0, 1.0, 0.4830998317539733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22163419678196078, 0.22163419678196095, 0.291737502464883], 
reward next is 0.7083, 
noisyNet noise sample is [array([-1.4202504], dtype=float32), 0.51715666]. 
=============================================
[2019-03-27 00:30:31,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:30:31,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2411
[2019-03-27 00:30:31,177] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 76.33333333333334, 1.0, 2.0, 0.5887426322615759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822724.3800377763, 822724.3800377763, 198668.8642275119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4395000.0000, 
sim time next is 4395600.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6045746050301529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844857.1927000735, 844857.1927000735, 201602.8412600016], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5235838614821119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23468255352779818, 0.23468255352779818, 0.30089976307462923], 
reward next is 0.6991, 
noisyNet noise sample is [array([0.04849387], dtype=float32), 1.9466101]. 
=============================================
[2019-03-27 00:30:33,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:30:33,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3965
[2019-03-27 00:30:33,348] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 76.33333333333334, 1.0, 2.0, 0.6275837579465035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 877024.396137832, 877024.3961378313, 205998.0354304218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4441200.0000, 
sim time next is 4441800.0000, 
raw observation next is [31.83333333333333, 75.66666666666666, 1.0, 2.0, 0.6294337730479428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 879610.7923422247, 879610.7923422254, 206358.3176680486], 
processed observation next is [0.0, 0.391304347826087, 0.7077409162717218, 0.7566666666666666, 1.0, 1.0, 0.5535346663228227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24433633120617354, 0.24433633120617373, 0.307997489056789], 
reward next is 0.6920, 
noisyNet noise sample is [array([-0.9612313], dtype=float32), -1.4824702]. 
=============================================
[2019-03-27 00:30:35,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:30:35,765] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9303
[2019-03-27 00:30:35,768] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5527037252724505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772344.3571794131, 772344.3571794131, 192265.0418269546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4478400.0000, 
sim time next is 4479000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5521228776564142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771532.3899624025, 771532.3899624019, 192165.0534474425], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4603890092245954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21431455276733402, 0.21431455276733388, 0.28681351260812316], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.65090585], dtype=float32), 1.2729857]. 
=============================================
[2019-03-27 00:30:35,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.46231 ]
 [75.35437 ]
 [75.28764 ]
 [75.21097 ]
 [75.137566]], R is [[75.48384857]
 [75.44204712]
 [75.40002441]
 [75.35792542]
 [75.31607819]].
[2019-03-27 00:30:43,100] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.3997481e-22 1.0000000e+00 3.6206660e-25 8.7342344e-25 1.4800544e-29], sum to 1.0000
[2019-03-27 00:30:43,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8518
[2019-03-27 00:30:43,121] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.9217203584982252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1288318.438848386, 1288318.438848385, 275984.2101772494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4597200.0000, 
sim time next is 4597800.0000, 
raw observation next is [27.16666666666666, 94.00000000000001, 1.0, 2.0, 0.9450630098866242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1320965.517572562, 1320965.517572562, 282636.4787974789], 
processed observation next is [1.0, 0.21739130434782608, 0.4865718799368086, 0.9400000000000002, 1.0, 1.0, 0.9338108552850893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36693486599237835, 0.36693486599237835, 0.4218454907425058], 
reward next is 0.5782, 
noisyNet noise sample is [array([-1.4142542], dtype=float32), 0.5764015]. 
=============================================
[2019-03-27 00:30:44,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0546938e-08 1.0000000e+00 8.8413259e-09 4.5595527e-09 1.5419419e-11], sum to 1.0000
[2019-03-27 00:30:44,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8723
[2019-03-27 00:30:44,800] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3075280.243588501 W.
[2019-03-27 00:30:44,807] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.8244858981355911, 1.0, 2.0, 0.7328329885820581, 1.0, 1.0, 1.03, 7.005107549807001, 6.9112, 170.5573041426782, 3075280.243588501, 3008010.445746851, 563531.8013861523], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.8497121920778876, 1.0, 2.0, 0.7454461355532065, 1.0, 2.0, 1.03, 7.005109539793688, 6.9112, 170.5573041426782, 3128276.685513288, 3061005.462163226, 572787.1093119393], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 0.8189303519010694, 1.0, 1.0, 0.693308597052056, 1.0, 1.0, 1.0365853658536586, 0.009390953979368798, 0.0, 0.8375144448122397, 0.8689657459759133, 0.8502792950453406, 0.854906133301402], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.66060585], dtype=float32), -1.3356091]. 
=============================================
[2019-03-27 00:30:46,631] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4471444e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:30:46,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5765
[2019-03-27 00:30:46,651] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 88.16666666666667, 1.0, 2.0, 0.5249932849139376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733608.6081372208, 733608.6081372208, 187605.2002836865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4672200.0000, 
sim time next is 4672800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5290199997329466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739237.3668997014, 739237.366899702, 188268.3894604913], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.89, 1.0, 1.0, 0.43255421654571874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20534371302769483, 0.205343713027695, 0.2809975962096885], 
reward next is 0.7190, 
noisyNet noise sample is [array([0.7824255], dtype=float32), 0.11822896]. 
=============================================
[2019-03-27 00:30:47,035] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8000198e-27 1.0000000e+00 1.0238416e-33 2.7421292e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:30:47,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2067
[2019-03-27 00:30:47,046] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 86.5, 1.0, 2.0, 0.9592886103637915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340861.963088486, 1340861.963088486, 286767.4255070675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4689000.0000, 
sim time next is 4689600.0000, 
raw observation next is [27.66666666666666, 85.66666666666667, 1.0, 2.0, 0.830988013210304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161429.53381116, 1161429.53381116, 251659.916831695], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.8566666666666667, 1.0, 1.0, 0.7963711002533783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3226193149475444, 0.3226193149475444, 0.37561181616670897], 
reward next is 0.6244, 
noisyNet noise sample is [array([0.32266074], dtype=float32), 1.5484821]. 
=============================================
[2019-03-27 00:30:49,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2470740e-27 1.0000000e+00 6.3309435e-34 5.8360391e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 00:30:49,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1414
[2019-03-27 00:30:49,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2585341.761482093 W.
[2019-03-27 00:30:49,731] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.322155230355108, 6.9112, 168.9106189604854, 2585341.761482093, 2293800.011719906, 475694.2398242467], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4716000.0000, 
sim time next is 4716600.0000, 
raw observation next is [30.83333333333334, 67.5, 1.0, 2.0, 0.5314473043944735, 1.0, 1.0, 0.5314473043944735, 1.0, 2.0, 0.9213933712943365, 6.911200000000001, 6.9112, 170.5573041426782, 2229425.873364415, 2229425.873364415, 437273.8096532026], 
processed observation next is [1.0, 0.6086956521739131, 0.6603475513428123, 0.675, 1.0, 1.0, 0.43547867999334156, 1.0, 0.5, 0.43547867999334156, 1.0, 1.0, 0.9041382576760202, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6192849648234486, 0.6192849648234486, 0.6526474770943322], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34620467], dtype=float32), 0.6332638]. 
=============================================
[2019-03-27 00:30:52,962] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.786578e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 00:30:52,972] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6243
[2019-03-27 00:30:52,981] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.5970820639323688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 834382.6977638899, 834382.6977638893, 200195.5653108609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4770000.0000, 
sim time next is 4770600.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.6504004919059506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908923.556545309, 908923.5565453096, 210500.1410584522], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.5787957733806633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25247876570703026, 0.25247876570703043, 0.3141793150126152], 
reward next is 0.6858, 
noisyNet noise sample is [array([0.2669618], dtype=float32), -0.980382]. 
=============================================
[2019-03-27 00:30:57,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7275063e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:30:57,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4972
[2019-03-27 00:30:57,531] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4848186778890622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677452.0127284338, 677452.0127284338, 181251.4184775893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4845600.0000, 
sim time next is 4846200.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.9806421946334349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1370728.567619118, 1370728.567619118, 293079.7629098151], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.9766773429318493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38075793544975495, 0.38075793544975495, 0.43743248195494794], 
reward next is 0.5626, 
noisyNet noise sample is [array([-0.30290115], dtype=float32), -0.64002705]. 
=============================================
[2019-03-27 00:31:00,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6795755e-27 1.0000000e+00 3.2759699e-32 9.2009215e-29 4.0477711e-38], sum to 1.0000
[2019-03-27 00:31:00,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6351
[2019-03-27 00:31:00,068] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2417335.317593603 W.
[2019-03-27 00:31:00,073] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5761976037924497, 1.0, 2.0, 0.5761976037924497, 1.0, 1.0, 1.000664686328114, 6.9112, 6.9112, 170.5573041426782, 2417335.317593603, 2417335.317593603, 471836.0990944469], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4888800.0000, 
sim time next is 4889400.0000, 
raw observation next is [31.91666666666667, 63.33333333333333, 1.0, 2.0, 0.9704971244461419, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.000024315607049, 6.9112, 168.9124284522061, 2253708.080350648, 2190693.250760522, 454581.9813288723], 
processed observation next is [1.0, 0.6086956521739131, 0.7116903633491314, 0.6333333333333333, 1.0, 1.0, 0.9644543668025806, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008882431560704873, 0.0, 0.8294373521446723, 0.6260300223196245, 0.608525902989034, 0.6784805691475706], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.306874], dtype=float32), 1.0943025]. 
=============================================
[2019-03-27 00:31:01,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:31:01,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4251
[2019-03-27 00:31:01,569] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5097186140793517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712257.1220534906, 712257.1220534906, 185132.8167083495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4923000.0000, 
sim time next is 4923600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5093462078983533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711736.5646145464, 711736.5646145471, 185073.4390484642], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40885085288958223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19770460128181844, 0.19770460128181863, 0.2762290135051705], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.6737561], dtype=float32), 0.41762212]. 
=============================================
[2019-03-27 00:31:09,904] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:31:09,912] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3984
[2019-03-27 00:31:09,917] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5177056254068089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723421.6011666113, 723421.6011666113, 186417.1241739583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5054400.0000, 
sim time next is 5055000.0000, 
raw observation next is [31.16666666666667, 63.0, 1.0, 2.0, 0.5227724539329466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730504.2237892565, 730504.2237892571, 187241.4677359061], 
processed observation next is [0.0, 0.5217391304347826, 0.6761453396524489, 0.63, 1.0, 1.0, 0.4250270529312609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20291783994146015, 0.20291783994146032, 0.2794648772177703], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.77756864], dtype=float32), -1.2615933]. 
=============================================
[2019-03-27 00:31:09,928] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[83.54868 ]
 [83.32378 ]
 [83.17409 ]
 [83.01978 ]
 [82.889465]], R is [[83.58939362]
 [83.4752655 ]
 [83.36217499]
 [83.25017548]
 [83.13934326]].
[2019-03-27 00:31:10,357] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:31:10,365] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7426
[2019-03-27 00:31:10,372] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 63.0, 1.0, 2.0, 0.5346565109051925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747116.4357692233, 747116.4357692233, 189205.0323169144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5056800.0000, 
sim time next is 5057400.0000, 
raw observation next is [31.83333333333333, 63.0, 1.0, 2.0, 0.5393228908244629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753639.438542164, 753639.438542164, 189987.4066432035], 
processed observation next is [0.0, 0.5217391304347826, 0.7077409162717218, 0.63, 1.0, 1.0, 0.4449673383427264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20934428848393447, 0.20934428848393447, 0.2835632934973186], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.31282675], dtype=float32), -0.7180393]. 
=============================================
[2019-03-27 00:31:14,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:31:14,771] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7942
[2019-03-27 00:31:14,777] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5104630775478667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713297.7500051999, 713297.7500052005, 185251.6245727143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5131800.0000, 
sim time next is 5132400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5103689724725529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713166.2077364186, 713166.2077364192, 185236.5954961804], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4100830993645215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1981017243712274, 0.19810172437122756, 0.27647253059131405], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.7757291], dtype=float32), 1.7297106]. 
=============================================
[2019-03-27 00:31:16,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:31:16,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6266
[2019-03-27 00:31:16,667] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5406710671499779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755524.0242093195, 755524.0242093201, 190213.936129436], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5161800.0000, 
sim time next is 5162400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5409040698056533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755849.733772564, 755849.7337725633, 190253.2434629186], 
processed observation next is [0.0, 0.782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44687237325982326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20995825938126778, 0.20995825938126758, 0.28396006487002773], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.9657355], dtype=float32), -0.7471682]. 
=============================================
[2019-03-27 00:31:16,934] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:31:16,935] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:31:16,936] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:31:16,937] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:31:16,938] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:31:16,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:31:16,939] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:31:16,937] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:31:16,940] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:31:16,943] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:31:16,943] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:31:16,969] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-27 00:31:16,971] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-27 00:31:16,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-27 00:31:17,010] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-27 00:31:17,029] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-27 00:31:21,370] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:31:21,371] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.171178375, 79.14597868499999, 1.0, 2.0, 0.3496840494016797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537432.3712704045, 537432.371270404, 169568.0096797214]
[2019-03-27 00:31:21,372] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:31:21,375] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.813994421056472
[2019-03-27 00:31:34,331] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:31:34,332] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.26666666666667, 69.0, 1.0, 2.0, 0.3304070660160498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512324.9343247216, 512324.9343247223, 167705.3275194789]
[2019-03-27 00:31:34,334] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:31:34,335] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42779874282996333
[2019-03-27 00:31:53,462] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:31:53,465] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.64834156, 80.13728129, 1.0, 2.0, 0.5568012030090832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778072.2413005605, 778072.2413005605, 192970.7824816824]
[2019-03-27 00:31:53,468] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:31:53,471] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5628073940652459
[2019-03-27 00:32:25,525] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:32:25,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 63.5, 1.0, 2.0, 0.9722937550753633, 1.0, 2.0, 0.9722937550753633, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2719719.940570633, 2719719.940570634, 512476.8170051872]
[2019-03-27 00:32:25,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:32:25,531] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2197551e-33 1.0000000e+00 0.0000000e+00 1.1320994e-36 0.0000000e+00], sampled 0.5763904796192846
[2019-03-27 00:32:25,531] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2719719.940570633 W.
[2019-03-27 00:32:32,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:32:32,587] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.05764740333333, 61.70992633333333, 1.0, 2.0, 0.5409194070870801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755871.173442582, 755871.173442582, 190256.3227725853]
[2019-03-27 00:32:32,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:32:32,592] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.38479401279307135
[2019-03-27 00:32:33,233] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:32:33,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.039671755, 86.271984175, 1.0, 2.0, 0.828300912941705, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983289763036, 6.9112, 168.9123268535439, 2054682.377021815, 1987440.099734841, 414946.1411547148]
[2019-03-27 00:32:33,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:32:33,240] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2817986e-32 1.0000000e+00 0.0000000e+00 1.0244578e-35 0.0000000e+00], sampled 0.21257985193412632
[2019-03-27 00:32:33,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2054682.377021815 W.
[2019-03-27 00:32:36,524] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:32:36,530] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 74.66666666666666, 1.0, 2.0, 0.8063678812344627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1127000.970658606, 1127000.970658606, 245482.012971427]
[2019-03-27 00:32:36,530] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:32:36,532] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2172438e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.1839192072876864
[2019-03-27 00:32:44,034] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.09645756]
[2019-03-27 00:32:44,037] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.56666666666667, 80.66666666666667, 1.0, 2.0, 0.5706608332825299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797446.9150249014, 797446.9150249014, 195408.2689505976]
[2019-03-27 00:32:44,037] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:32:44,041] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8073490009503155
[2019-03-27 00:33:11,108] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:33:11,488] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:33:11,713] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:33:11,716] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:33:11,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:33:12,865] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 350000, evaluation results [350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:33:16,405] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6160377e-18 1.0000000e+00 1.7715820e-22 9.9992552e-20 1.5005297e-24], sum to 1.0000
[2019-03-27 00:33:16,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8139
[2019-03-27 00:33:16,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2436415.064866477 W.
[2019-03-27 00:33:16,427] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5807410353163763, 1.0, 2.0, 0.5807410353163763, 1.0, 1.0, 1.00855512434941, 6.911200000000001, 6.9112, 170.5573041426782, 2436415.064866477, 2436415.064866477, 475469.5989708871], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5228400.0000, 
sim time next is 5229000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5526520139702416, 1.0, 2.0, 0.5526520139702416, 1.0, 2.0, 0.9597737834524794, 6.911199999999999, 6.9112, 170.5573041426782, 2318462.318397507, 2318462.318397508, 453465.9078279763], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.46102652285571266, 1.0, 1.0, 0.46102652285571266, 1.0, 1.0, 0.950943638356682, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6440173106659742, 0.6440173106659745, 0.6768147878029497], 
reward next is 0.3232, 
noisyNet noise sample is [array([0.2846937], dtype=float32), 0.0069011645]. 
=============================================
[2019-03-27 00:33:16,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[27.216276]
 [26.79438 ]
 [26.779068]
 [26.936392]
 [27.36342 ]], R is [[27.95447731]
 [27.96527672]
 [27.90630722]
 [27.62724495]
 [27.35097313]].
[2019-03-27 00:33:18,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5798555e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:33:18,726] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6880
[2019-03-27 00:33:18,730] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.33333333333333, 1.0, 2.0, 0.5497328995329734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768191.4446982018, 768191.4446982018, 191754.9222320282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5263800.0000, 
sim time next is 5264400.0000, 
raw observation next is [28.5, 81.66666666666667, 1.0, 2.0, 0.5508259599999857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769719.4302636089, 769719.4302636089, 191942.5302566618], 
processed observation next is [1.0, 0.9565217391304348, 0.5497630331753555, 0.8166666666666668, 1.0, 1.0, 0.4588264578313081, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2138109528510025, 0.2138109528510025, 0.2864813884427788], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.78096825], dtype=float32), 1.4336624]. 
=============================================
[2019-03-27 00:33:24,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4510392e-25 1.0000000e+00 1.0258767e-29 3.4151437e-27 4.8888744e-33], sum to 1.0000
[2019-03-27 00:33:24,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3106
[2019-03-27 00:33:24,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2772511.527075974 W.
[2019-03-27 00:33:24,640] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.2, 87.0, 1.0, 2.0, 0.6803475333278061, 1.0, 1.0, 0.6607638061781657, 1.0, 1.0, 1.03, 7.005096182628445, 6.9112, 170.5573041426782, 2772511.527075974, 2705249.87200671, 515193.8932437238], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5365200.0000, 
sim time next is 5365800.0000, 
raw observation next is [29.15, 87.5, 1.0, 2.0, 0.6452566339811776, 1.0, 2.0, 0.6452566339811776, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1804215.569904958, 1804215.569904958, 351582.0732663954], 
processed observation next is [1.0, 0.08695652173913043, 0.5805687203791469, 0.875, 1.0, 1.0, 0.5725983541941899, 1.0, 1.0, 0.5725983541941899, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5011709916402661, 0.5011709916402661, 0.5247493630841723], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11831029], dtype=float32), 0.47419724]. 
=============================================
[2019-03-27 00:33:25,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0748542e-16 1.0000000e+00 3.7808099e-20 5.6049696e-19 6.2677088e-23], sum to 1.0000
[2019-03-27 00:33:25,753] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9476
[2019-03-27 00:33:25,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1686343.671717935 W.
[2019-03-27 00:33:25,768] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.4, 75.0, 1.0, 2.0, 0.6031342700447806, 1.0, 1.0, 0.6031342700447806, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1686343.671717935, 1686343.671717936, 335427.1702263721], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5384400.0000, 
sim time next is 5385000.0000, 
raw observation next is [31.6, 74.0, 1.0, 2.0, 0.3801028961420969, 1.0, 2.0, 0.3801028961420969, 1.0, 1.0, 0.6601130286502298, 6.9112, 6.9112, 170.5573041426782, 1594064.299340367, 1594064.299340367, 342149.118230533], 
processed observation next is [1.0, 0.30434782608695654, 0.6966824644549764, 0.74, 1.0, 1.0, 0.25313601944830955, 1.0, 1.0, 0.25313601944830955, 1.0, 0.5, 0.5855036934758899, 0.0, 0.0, 0.8375144448122397, 0.4427956387056575, 0.4427956387056575, 0.5106703257172134], 
reward next is 0.4893, 
noisyNet noise sample is [array([-1.0536164], dtype=float32), 0.13730918]. 
=============================================
[2019-03-27 00:33:25,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[27.332697]
 [27.514719]
 [28.655725]
 [30.564234]
 [31.747425]], R is [[26.20257187]
 [25.94054604]
 [25.6811409 ]
 [25.42432976]
 [25.17008591]].
[2019-03-27 00:33:28,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.06900355e-26 1.00000000e+00 1.21634955e-30 3.82401913e-28
 2.69410821e-35], sum to 1.0000
[2019-03-27 00:33:28,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2986
[2019-03-27 00:33:28,069] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.88333333333333, 75.66666666666667, 1.0, 2.0, 0.5821301382215632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813480.3690571458, 813480.3690571458, 197465.325133494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5422200.0000, 
sim time next is 5422800.0000, 
raw observation next is [30.86666666666667, 76.33333333333334, 1.0, 2.0, 0.5872359064941861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820618.0285986264, 820618.0285986264, 198393.2900724954], 
processed observation next is [1.0, 0.782608695652174, 0.6619273301737759, 0.7633333333333334, 1.0, 1.0, 0.5026938632460073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22794945238850733, 0.22794945238850733, 0.2961093881679036], 
reward next is 0.7039, 
noisyNet noise sample is [array([-0.20749378], dtype=float32), -1.4347011]. 
=============================================
[2019-03-27 00:33:36,683] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4176301e-07 9.9992788e-01 5.8718586e-10 7.1706971e-05 3.2318870e-10], sum to 1.0000
[2019-03-27 00:33:36,691] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3689
[2019-03-27 00:33:36,699] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2843173.220588084 W.
[2019-03-27 00:33:36,702] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.86666666666667, 70.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.69699534115713, 6.9112, 168.9086562021441, 2843173.220588084, 2285717.153064108, 474394.8233310293], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5564400.0000, 
sim time next is 5565000.0000, 
raw observation next is [31.08333333333334, 69.16666666666667, 1.0, 2.0, 0.9181506060703791, 1.0, 1.0, 0.9181506060703791, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2568113.964032419, 2568113.964032419, 481490.063282572], 
processed observation next is [1.0, 0.391304347826087, 0.6721958925750398, 0.6916666666666668, 1.0, 1.0, 0.9013862723739507, 1.0, 0.5, 0.9013862723739507, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7133649900090053, 0.7133649900090053, 0.7186418854963762], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27790296], dtype=float32), -0.69766134]. 
=============================================
[2019-03-27 00:33:36,710] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[18.484072]
 [19.211777]
 [19.16375 ]
 [18.914925]
 [19.108625]], R is [[18.51377869]
 [18.32864189]
 [18.46543884]
 [18.58039474]
 [18.39459038]].
[2019-03-27 00:33:44,810] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0086792e-27 1.0000000e+00 4.9977034e-37 1.9645700e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:33:44,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1082
[2019-03-27 00:33:44,824] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.81666666666667, 89.0, 1.0, 2.0, 0.5347091481231504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747190.0156496192, 747190.0156496197, 189212.6271551946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5795400.0000, 
sim time next is 5796000.0000, 
raw observation next is [26.8, 89.0, 1.0, 2.0, 0.5339738909064046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746162.2236068867, 746162.2236068867, 189089.9487435603], 
processed observation next is [1.0, 0.08695652173913043, 0.4691943127962086, 0.89, 1.0, 1.0, 0.43852276012819824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2072672843352463, 0.2072672843352463, 0.2822238040948661], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.5760329], dtype=float32), -2.5187209]. 
=============================================
[2019-03-27 00:33:44,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[46.467594]
 [46.796112]
 [47.26258 ]
 [47.822853]
 [48.40081 ]], R is [[46.79383469]
 [47.04349136]
 [47.29043961]
 [47.53471756]
 [47.77643967]].
[2019-03-27 00:33:55,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2878053e-23 1.0000000e+00 2.6690952e-29 1.7304962e-23 1.6844877e-30], sum to 1.0000
[2019-03-27 00:33:55,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3637
[2019-03-27 00:33:55,944] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 95.0, 1.0, 2.0, 0.9543050645449079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333891.751063515, 1333891.751063515, 285311.5487068324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5886600.0000, 
sim time next is 5887200.0000, 
raw observation next is [25.83333333333334, 95.0, 1.0, 2.0, 0.9649197290024352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348737.95474605, 1348737.95474605, 288418.6792180605], 
processed observation next is [1.0, 0.13043478260869565, 0.42338072669826254, 0.95, 1.0, 1.0, 0.957734613255946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3746494318739028, 0.3746494318739028, 0.4304756406239709], 
reward next is 0.5695, 
noisyNet noise sample is [array([0.6971574], dtype=float32), -1.593307]. 
=============================================
[2019-03-27 00:33:57,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4081338e-09 1.0000000e+00 7.5666487e-15 7.6104127e-09 3.6309212e-14], sum to 1.0000
[2019-03-27 00:33:57,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1903
[2019-03-27 00:33:57,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2549890.1106028 W.
[2019-03-27 00:33:57,365] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.26666666666667, 73.33333333333334, 1.0, 2.0, 0.6077612440915631, 1.0, 2.0, 0.6077612440915631, 1.0, 2.0, 1.03, 6.939844078588678, 6.9112, 170.5573041426782, 2549890.1106028, 2529371.19194311, 490810.3780091716], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5911800.0000, 
sim time next is 5912400.0000, 
raw observation next is [31.43333333333334, 72.66666666666667, 1.0, 2.0, 0.8433282458588472, 1.0, 2.0, 0.8433282458588472, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2358634.496780186, 2358634.496780186, 441524.0220698584], 
processed observation next is [1.0, 0.43478260869565216, 0.6887835703001584, 0.7266666666666667, 1.0, 1.0, 0.8112388504323461, 1.0, 1.0, 0.8112388504323461, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6551762491056072, 0.6551762491056072, 0.6589910777162066], 
reward next is 0.3410, 
noisyNet noise sample is [array([1.2873992], dtype=float32), -1.8467474]. 
=============================================
[2019-03-27 00:34:08,856] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 00:34:08,857] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:34:08,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:34:08,858] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:34:08,859] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:34:08,859] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:34:08,860] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:34:08,862] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:34:08,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:34:08,863] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:34:08,864] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:34:08,887] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-27 00:34:08,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-27 00:34:08,890] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-27 00:34:08,890] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-27 00:34:08,957] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-27 00:34:38,554] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.14542642]
[2019-03-27 00:34:38,557] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.911832375, 81.60311497666667, 1.0, 2.0, 0.5670657611477046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811078.172209038, 811078.1722090387, 197158.8473814329]
[2019-03-27 00:34:38,560] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:34:38,562] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9657670e-26 1.0000000e+00 4.8679150e-35 3.7107518e-28 4.3604426e-35], sampled 0.6905196045010118
[2019-03-27 00:34:41,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.14542642]
[2019-03-27 00:34:41,521] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.06666666666667, 85.0, 1.0, 2.0, 0.444890102755168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 641599.7119761524, 641599.7119761518, 177970.1220067226]
[2019-03-27 00:34:41,522] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:34:41,527] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5720668e-30 1.0000000e+00 0.0000000e+00 1.6838435e-32 0.0000000e+00], sampled 0.4976197296672267
[2019-03-27 00:35:09,543] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.14542642]
[2019-03-27 00:35:09,545] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 91.5, 1.0, 2.0, 0.8671886007029677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1212054.125597888, 1212054.125597888, 261072.4786098207]
[2019-03-27 00:35:09,546] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:35:09,547] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.3493956e-21 1.0000000e+00 7.9239219e-28 5.3897865e-22 1.3406024e-27], sampled 0.026479260372736624
[2019-03-27 00:35:16,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.14542642]
[2019-03-27 00:35:16,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.21506117333333, 79.43942894333334, 1.0, 2.0, 0.9689738311459061, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564464882, 1354408.280737598, 1354408.280737597, 289625.0446384558]
[2019-03-27 00:35:16,767] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:35:16,771] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.4182242e-20 1.0000000e+00 1.6419961e-26 4.3011838e-20 3.6324647e-26], sampled 0.5489615281875446
[2019-03-27 00:35:32,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.14542642]
[2019-03-27 00:35:32,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.15, 89.16666666666667, 1.0, 2.0, 0.5113257757610636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714503.6504446712, 714503.6504446717, 185389.3033097809]
[2019-03-27 00:35:32,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:35:32,414] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.0245737e-29 1.0000000e+00 2.9698974e-38 8.3841119e-31 2.4270501e-38], sampled 0.7190708303103702
[2019-03-27 00:36:03,769] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:36:03,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:36:03,910] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:36:03,911] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 00:36:04,023] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:36:05,039] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 375000, evaluation results [375000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:36:07,258] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5992495e-24 1.0000000e+00 6.9673901e-33 6.0650244e-25 6.7362505e-33], sum to 1.0000
[2019-03-27 00:36:07,271] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6938
[2019-03-27 00:36:07,278] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 91.0, 1.0, 2.0, 0.538071823698464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751890.6026538422, 751890.6026538422, 189776.2377056621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6139800.0000, 
sim time next is 6140400.0000, 
raw observation next is [26.76666666666667, 91.33333333333333, 1.0, 2.0, 0.5392792475745962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753578.4306708007, 753578.4306708013, 189979.2090079504], 
processed observation next is [1.0, 0.043478260869565216, 0.46761453396524505, 0.9133333333333333, 1.0, 1.0, 0.4449147561139713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2093273418530002, 0.20932734185300036, 0.2835510582208215], 
reward next is 0.7164, 
noisyNet noise sample is [array([1.2310508], dtype=float32), 1.3422245]. 
=============================================
[2019-03-27 00:36:18,974] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:36:18,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0975
[2019-03-27 00:36:18,985] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 63.0, 1.0, 2.0, 0.5382711482541161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752169.2333426882, 752169.2333426889, 189809.7982757424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6352200.0000, 
sim time next is 6352800.0000, 
raw observation next is [31.33333333333333, 63.0, 1.0, 2.0, 0.5339955561636565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746192.5087479838, 746192.5087479833, 189093.9941111217], 
processed observation next is [0.0, 0.5217391304347826, 0.6840442338072668, 0.63, 1.0, 1.0, 0.43854886284777883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20727569687443995, 0.2072756968744398, 0.28222984195689804], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.6909791], dtype=float32), 0.3371492]. 
=============================================
[2019-03-27 00:36:26,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2920015e-34 1.0000000e+00 0.0000000e+00 8.3688764e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:36:26,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0214
[2019-03-27 00:36:26,190] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.65, 68.5, 1.0, 2.0, 0.470068317360236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656834.5093519087, 656834.5093519087, 179041.6723747082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6456600.0000, 
sim time next is 6457200.0000, 
raw observation next is [29.53333333333333, 69.0, 1.0, 2.0, 0.4793719089041941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669838.6757475933, 669838.6757475926, 180429.0990033234], 
processed observation next is [1.0, 0.7391304347826086, 0.598736176935229, 0.69, 1.0, 1.0, 0.37273723964360733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1860662988187759, 0.1860662988187757, 0.2692971626915275], 
reward next is 0.7307, 
noisyNet noise sample is [array([0.42688268], dtype=float32), 0.17723608]. 
=============================================
[2019-03-27 00:36:26,586] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:36:26,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7545
[2019-03-27 00:36:26,608] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 76.66666666666667, 1.0, 2.0, 0.5053332822338025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706127.2262472076, 706127.2262472083, 184435.7964293429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6466200.0000, 
sim time next is 6466800.0000, 
raw observation next is [27.9, 77.33333333333334, 1.0, 2.0, 0.5068638940061424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708266.7389208385, 708266.7389208392, 184678.4127319815], 
processed observation next is [1.0, 0.8695652173913043, 0.5213270142180094, 0.7733333333333334, 1.0, 1.0, 0.4058601132604125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.196740760811344, 0.1967407608113442, 0.2756394219880321], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.1371624], dtype=float32), -0.53143096]. 
=============================================
[2019-03-27 00:36:27,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8465990e-31 1.0000000e+00 0.0000000e+00 1.3235267e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:36:27,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8391
[2019-03-27 00:36:27,688] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 89.16666666666667, 1.0, 2.0, 0.519268848784936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725606.734466834, 725606.7344668333, 186670.0259647582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6570600.0000, 
sim time next is 6571200.0000, 
raw observation next is [26.4, 89.33333333333334, 1.0, 2.0, 0.5185196867119601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724559.5265676586, 724559.526567658, 186548.4288508109], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.8933333333333334, 1.0, 1.0, 0.4199032370023616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20126653515768295, 0.20126653515768275, 0.2784304908221058], 
reward next is 0.7216, 
noisyNet noise sample is [array([1.0472163], dtype=float32), 0.039839514]. 
=============================================
[2019-03-27 00:36:31,093] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8903944e-22 1.0000000e+00 3.4373462e-29 1.9212871e-24 5.0168034e-30], sum to 1.0000
[2019-03-27 00:36:31,102] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6410
[2019-03-27 00:36:31,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1934981.540571157 W.
[2019-03-27 00:36:31,117] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.48333333333333, 61.66666666666667, 1.0, 2.0, 0.4613209101967675, 1.0, 1.0, 0.4613209101967675, 1.0, 2.0, 0.7834059195950027, 6.9112, 6.9112, 170.5573041426782, 1934981.540571157, 1934981.540571157, 386457.3112359151], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6538200.0000, 
sim time next is 6538800.0000, 
raw observation next is [30.36666666666667, 62.33333333333334, 1.0, 2.0, 0.6834927301776966, 1.0, 2.0, 0.6834927301776966, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1911223.671603677, 1911223.671603677, 367129.1919556258], 
processed observation next is [1.0, 0.6956521739130435, 0.6382306477093209, 0.6233333333333334, 1.0, 1.0, 0.6186659399731285, 1.0, 1.0, 0.6186659399731285, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5308954643343547, 0.5308954643343547, 0.5479540178442176], 
reward next is 0.4520, 
noisyNet noise sample is [array([-0.00974398], dtype=float32), 0.24193798]. 
=============================================
[2019-03-27 00:36:33,021] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1711560e-28 1.0000000e+00 1.8455150e-38 2.2166078e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 00:36:33,032] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8131
[2019-03-27 00:36:33,039] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 89.0, 1.0, 2.0, 0.5195887127387313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726053.8530634192, 726053.8530634185, 186722.0346897692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6570000.0000, 
sim time next is 6570600.0000, 
raw observation next is [26.45, 89.16666666666667, 1.0, 2.0, 0.519268848784936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725606.734466834, 725606.7344668333, 186670.0259647582], 
processed observation next is [1.0, 0.043478260869565216, 0.45260663507109006, 0.8916666666666667, 1.0, 1.0, 0.4208058419095614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2015574262407872, 0.201557426240787, 0.27861197905187796], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.22446959], dtype=float32), -0.19463444]. 
=============================================
[2019-03-27 00:36:33,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5835939e-32 1.0000000e+00 0.0000000e+00 2.5822156e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 00:36:33,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0001
[2019-03-27 00:36:33,406] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 91.33333333333334, 1.0, 2.0, 0.667947790105773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933456.3819723948, 933456.3819723953, 214077.2668831671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6578400.0000, 
sim time next is 6579000.0000, 
raw observation next is [25.95, 91.5, 1.0, 2.0, 0.7075476536035944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 988822.9160131195, 988822.9160131201, 222481.1445952237], 
processed observation next is [1.0, 0.13043478260869565, 0.42890995260663506, 0.915, 1.0, 1.0, 0.6476477754260174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27467303222586653, 0.2746730322258667, 0.3320614098436175], 
reward next is 0.6679, 
noisyNet noise sample is [array([1.2351171], dtype=float32), -0.5091049]. 
=============================================
[2019-03-27 00:36:33,414] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[61.611813]
 [61.58076 ]
 [62.31592 ]
 [62.35539 ]
 [62.439014]], R is [[61.24617386]
 [61.31419373]
 [61.35583496]
 [61.43333817]
 [61.50936127]].
[2019-03-27 00:36:58,074] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0145988e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:36:58,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7795
[2019-03-27 00:36:58,093] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 72.5, 1.0, 2.0, 0.4346398957204802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 628941.992582633, 628941.9925826325, 176764.8646300449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6993000.0000, 
sim time next is 6993600.0000, 
raw observation next is [26.83333333333334, 73.66666666666667, 1.0, 2.0, 0.4386159301385224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632901.8878519399, 632901.8878519406, 177109.0428965779], 
processed observation next is [0.0, 0.9565217391304348, 0.4707740916271725, 0.7366666666666667, 1.0, 1.0, 0.32363365076930406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17580607995887218, 0.17580607995887237, 0.2643418550695193], 
reward next is 0.7357, 
noisyNet noise sample is [array([0.93259746], dtype=float32), 0.03822348]. 
=============================================
[2019-03-27 00:36:59,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3286676e-22 1.0000000e+00 3.9977665e-35 5.5211017e-20 7.8268366e-33], sum to 1.0000
[2019-03-27 00:36:59,509] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9600
[2019-03-27 00:36:59,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1703754.925381673 W.
[2019-03-27 00:36:59,523] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.45, 59.66666666666667, 1.0, 2.0, 0.4062377316312656, 1.0, 2.0, 0.4062377316312656, 1.0, 1.0, 0.6761156912365218, 6.9112, 6.9112, 170.5573041426782, 1703754.925381673, 1703754.925381673, 352017.2082997135], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7033800.0000, 
sim time next is 7034400.0000, 
raw observation next is [29.6, 59.0, 1.0, 2.0, 0.6553690382077151, 1.0, 2.0, 0.6553690382077151, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1832515.253804019, 1832515.253804019, 355587.1209010551], 
processed observation next is [1.0, 0.43478260869565216, 0.6018957345971565, 0.59, 1.0, 1.0, 0.584781973744235, 1.0, 1.0, 0.584781973744235, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5090320149455608, 0.5090320149455608, 0.5307270461209778], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.1274624], dtype=float32), 0.8387604]. 
=============================================
[2019-03-27 00:36:59,587] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.5940678e-28 1.0000000e+00 0.0000000e+00 2.0994632e-27 2.5247150e-38], sum to 1.0000
[2019-03-27 00:36:59,598] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0288
[2019-03-27 00:36:59,603] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 86.66666666666667, 1.0, 2.0, 0.6158495117229907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876285.895155288, 876285.895155288, 205765.4420471193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7014000.0000, 
sim time next is 7014600.0000, 
raw observation next is [25.15, 87.0, 1.0, 2.0, 0.6098453087967037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 867883.0369732823, 867883.0369732829, 204619.6271205548], 
processed observation next is [1.0, 0.17391304347826086, 0.3909952606635071, 0.87, 1.0, 1.0, 0.5299341069839804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24107862138146732, 0.2410786213814675, 0.3054024285381415], 
reward next is 0.6946, 
noisyNet noise sample is [array([-0.71060854], dtype=float32), -0.5574321]. 
=============================================
[2019-03-27 00:37:00,972] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 00:37:00,975] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:37:00,975] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:37:00,976] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:37:00,977] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:37:00,977] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:37:00,978] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:37:00,980] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:37:00,981] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:37:00,978] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:37:00,984] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:37:01,000] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-27 00:37:01,020] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-27 00:37:01,042] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-27 00:37:01,060] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-27 00:37:01,060] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-27 00:37:59,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.11769134]
[2019-03-27 00:37:59,552] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.428870985, 59.50822302, 1.0, 2.0, 0.700592623991513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 979098.5350507107, 979098.5350507107, 220986.3938731123]
[2019-03-27 00:37:59,553] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:37:59,557] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.0526762e-27 1.0000000e+00 4.3492792e-35 6.3228680e-31 2.6568767e-35], sampled 0.25695952652749765
[2019-03-27 00:38:21,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.11769134]
[2019-03-27 00:38:21,597] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 87.0, 1.0, 2.0, 0.550308261473757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768995.7409539418, 768995.7409539425, 191853.4611998173]
[2019-03-27 00:38:21,598] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:38:21,602] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.3438062e-26 1.0000000e+00 1.3865661e-33 1.5233129e-29 9.0720494e-34], sampled 0.5390442712388622
[2019-03-27 00:38:28,208] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.11769134]
[2019-03-27 00:38:28,210] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.26666666666667, 72.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.291537980544928, 6.9112, 168.9107695602481, 2553747.92615606, 2283926.566011356, 475265.2108484572]
[2019-03-27 00:38:28,212] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:38:28,215] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5053965e-10 1.0000000e+00 3.9456734e-15 1.8002878e-10 1.8051668e-14], sampled 0.20814423099479595
[2019-03-27 00:38:28,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2553747.92615606 W.
[2019-03-27 00:38:55,714] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:38:55,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:38:55,876] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:38:55,933] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:38:56,056] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:38:57,076] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 400000, evaluation results [400000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:38:59,464] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:38:59,475] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0640
[2019-03-27 00:38:59,480] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 88.83333333333334, 1.0, 2.0, 0.4723686625885019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662500.8905011335, 662500.8905011329, 179696.5471681743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7084200.0000, 
sim time next is 7084800.0000, 
raw observation next is [25.2, 89.0, 1.0, 2.0, 0.4718344121776715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662063.976337504, 662063.9763375047, 179657.2734908038], 
processed observation next is [1.0, 0.0, 0.3933649289099526, 0.89, 1.0, 1.0, 0.3636559182863512, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18390666009375112, 0.1839066600937513, 0.2681451843146326], 
reward next is 0.7319, 
noisyNet noise sample is [array([-0.9062522], dtype=float32), -0.0102211535]. 
=============================================
[2019-03-27 00:39:00,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:39:00,489] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2443
[2019-03-27 00:39:00,495] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 93.66666666666667, 1.0, 2.0, 0.4963683472389043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703030.7176296974, 703030.717629698, 184238.2353270157], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7098000.0000, 
sim time next is 7098600.0000, 
raw observation next is [24.33333333333333, 93.83333333333334, 1.0, 2.0, 0.490802998810062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695510.1720054905, 695510.1720054899, 183410.4509354638], 
processed observation next is [1.0, 0.13043478260869565, 0.35229067930489716, 0.9383333333333335, 1.0, 1.0, 0.3865096371205567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19319727000152515, 0.19319727000152498, 0.2737469416947221], 
reward next is 0.7263, 
noisyNet noise sample is [array([0.367417], dtype=float32), 0.61599904]. 
=============================================
[2019-03-27 00:39:01,804] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7000268e-29 1.0000000e+00 0.0000000e+00 1.6755577e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 00:39:01,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1881
[2019-03-27 00:39:01,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1737224.567428996 W.
[2019-03-27 00:39:01,828] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.1, 70.0, 1.0, 2.0, 0.6213174895666703, 1.0, 1.0, 0.6213174895666703, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1737224.567428996, 1737224.567428996, 342246.4809365264], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7120800.0000, 
sim time next is 7121400.0000, 
raw observation next is [28.18333333333333, 69.5, 1.0, 2.0, 0.7189815428255263, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.957503160514332, 6.9112, 168.9126810380232, 1901691.022123968, 1868842.019995327, 390294.6301434648], 
processed observation next is [1.0, 0.43478260869565216, 0.5347551342812005, 0.695, 1.0, 1.0, 0.6614235455729233, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.004630316051433159, 0.0, 0.8294385924566686, 0.5282475061455467, 0.5191227833320353, 0.5825292987215892], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29021263], dtype=float32), -0.97543555]. 
=============================================
[2019-03-27 00:39:08,867] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:39:08,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9477
[2019-03-27 00:39:08,882] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 92.0, 1.0, 2.0, 0.3688638858436427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562204.3888377986, 562204.3888377986, 171512.7275364236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7243200.0000, 
sim time next is 7243800.0000, 
raw observation next is [22.58333333333334, 92.0, 1.0, 2.0, 0.3679991789024917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 561167.8791016368, 561167.8791016361, 171431.8244643848], 
processed observation next is [1.0, 0.8695652173913043, 0.2693522906793052, 0.92, 1.0, 1.0, 0.23855322759336345, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15587996641712135, 0.15587996641712115, 0.25586839472296236], 
reward next is 0.7441, 
noisyNet noise sample is [array([-1.4552416], dtype=float32), -0.6445921]. 
=============================================
[2019-03-27 00:39:21,926] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:39:21,938] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5764
[2019-03-27 00:39:21,951] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 95.0, 1.0, 2.0, 0.3359611040407641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521705.286226209, 521705.2862262096, 168465.0910193421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7459200.0000, 
sim time next is 7459800.0000, 
raw observation next is [21.7, 94.66666666666667, 1.0, 2.0, 0.3381506709639861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524440.2747357157, 524440.2747357157, 168662.9221081334], 
processed observation next is [0.0, 0.34782608695652173, 0.2274881516587678, 0.9466666666666668, 1.0, 1.0, 0.20259116983612782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14567785409325437, 0.14567785409325437, 0.2517357046390051], 
reward next is 0.7483, 
noisyNet noise sample is [array([-0.21851513], dtype=float32), 0.23662388]. 
=============================================
[2019-03-27 00:39:22,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:39:22,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5185
[2019-03-27 00:39:22,662] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 93.66666666666667, 1.0, 2.0, 0.3452236390077541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532743.9537262849, 532743.9537262843, 169251.9696557469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7461600.0000, 
sim time next is 7462200.0000, 
raw observation next is [22.1, 93.33333333333333, 1.0, 2.0, 0.3469143773739593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534445.9834828508, 534445.9834828508, 169362.8886042231], 
processed observation next is [0.0, 0.34782608695652173, 0.24644549763033188, 0.9333333333333332, 1.0, 1.0, 0.2131498522577823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14845721763412523, 0.14845721763412523, 0.2527804307525718], 
reward next is 0.7472, 
noisyNet noise sample is [array([-0.79005116], dtype=float32), 0.069451205]. 
=============================================
[2019-03-27 00:39:23,546] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:39:23,556] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6167
[2019-03-27 00:39:23,559] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 80.33333333333334, 1.0, 2.0, 0.4055258622927768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596066.4046566807, 596066.4046566807, 173904.4246032597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7480200.0000, 
sim time next is 7480800.0000, 
raw observation next is [25.4, 80.0, 1.0, 2.0, 0.4075194854067613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 597900.8704023627, 597900.8704023634, 174040.2020620153], 
processed observation next is [0.0, 0.6086956521739131, 0.4028436018957346, 0.8, 1.0, 1.0, 0.28616805470694134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1660835751117674, 0.1660835751117676, 0.2597614956149482], 
reward next is 0.7402, 
noisyNet noise sample is [array([1.8174473], dtype=float32), 0.6088802]. 
=============================================
[2019-03-27 00:39:34,007] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.685000e-32 1.000000e+00 0.000000e+00 2.586827e-37 0.000000e+00], sum to 1.0000
[2019-03-27 00:39:34,016] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5552
[2019-03-27 00:39:34,022] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 88.0, 1.0, 2.0, 0.494073798632051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690388.6793581739, 690388.6793581746, 182672.5519115474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7678800.0000, 
sim time next is 7679400.0000, 
raw observation next is [25.86666666666667, 88.00000000000001, 1.0, 2.0, 0.4925865389700069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688309.7998596545, 688309.7998596552, 182442.4780367959], 
processed observation next is [1.0, 0.9130434782608695, 0.42496050552922615, 0.8800000000000001, 1.0, 1.0, 0.3886584806867553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1911971666276818, 0.191197166627682, 0.2723022060250685], 
reward next is 0.7277, 
noisyNet noise sample is [array([-1.1690462], dtype=float32), -1.1086596]. 
=============================================
[2019-03-27 00:39:34,357] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6324215e-28 1.0000000e+00 3.8547434e-37 6.0016651e-32 6.9524018e-37], sum to 1.0000
[2019-03-27 00:39:34,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8870
[2019-03-27 00:39:34,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1816975.009043328 W.
[2019-03-27 00:39:34,385] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.7, 75.0, 1.0, 2.0, 0.658441265401949, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.978575194551282, 6.9112, 168.912499939599, 1816975.009043328, 1769176.856351918, 376869.1054427013], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7664400.0000, 
sim time next is 7665000.0000, 
raw observation next is [28.55, 75.66666666666667, 1.0, 2.0, 0.3251750169571651, 1.0, 1.0, 0.3251750169571651, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 908849.2630794056, 908849.2630794056, 254920.8147566795], 
processed observation next is [1.0, 0.7391304347826086, 0.552132701421801, 0.7566666666666667, 1.0, 1.0, 0.1869578517556206, 1.0, 0.5, 0.1869578517556206, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.25245812863316824, 0.25245812863316824, 0.380478827995044], 
reward next is 0.6195, 
noisyNet noise sample is [array([-0.31692636], dtype=float32), -2.4521794]. 
=============================================
[2019-03-27 00:39:34,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[45.00243 ]
 [44.123764]
 [44.448204]
 [44.00382 ]
 [44.042534]], R is [[47.25113678]
 [46.8792572 ]
 [46.50450134]
 [46.51665878]
 [46.05149078]].
[2019-03-27 00:39:37,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2004323e-31 1.0000000e+00 0.0000000e+00 6.7271244e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:39:37,239] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0528
[2019-03-27 00:39:37,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2293662.087783201 W.
[2019-03-27 00:39:37,252] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.3, 64.0, 1.0, 2.0, 0.8201187020717244, 1.0, 2.0, 0.8201187020717244, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2293662.087783201, 2293662.087783201, 429781.4282238567], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7731000.0000, 
sim time next is 7731600.0000, 
raw observation next is [31.36666666666667, 63.66666666666667, 1.0, 2.0, 1.02215143180401, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.990796065975566, 6.9112, 168.9124834571758, 2326007.372540172, 2269539.341995285, 470763.7143850386], 
processed observation next is [1.0, 0.4782608695652174, 0.6856240126382308, 0.6366666666666667, 1.0, 1.0, 1.026688472053024, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00795960659755659, 0.0, 0.8294376222442549, 0.6461131590389367, 0.6304275949986903, 0.7026324095299084], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4038099], dtype=float32), 0.56763464]. 
=============================================
[2019-03-27 00:39:42,417] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.940716e-32 1.000000e+00 0.000000e+00 8.962399e-32 0.000000e+00], sum to 1.0000
[2019-03-27 00:39:42,424] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0428
[2019-03-27 00:39:42,429] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 83.0, 1.0, 2.0, 0.7642823916688315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1068151.599059242, 1068151.599059242, 235331.2969281832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7803600.0000, 
sim time next is 7804200.0000, 
raw observation next is [27.56666666666666, 82.5, 1.0, 2.0, 0.7792158234416188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1089033.080385263, 1089033.080385263, 238872.0986803748], 
processed observation next is [1.0, 0.30434782608695654, 0.5055292259083725, 0.825, 1.0, 1.0, 0.7339949680019503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3025091889959064, 0.3025091889959064, 0.35652552041846985], 
reward next is 0.6435, 
noisyNet noise sample is [array([0.00038482], dtype=float32), 0.0968829]. 
=============================================
[2019-03-27 00:39:45,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2135085e-37 1.0000000e+00 0.0000000e+00 4.0734844e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:39:45,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-27 00:39:45,572] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 87.16666666666666, 1.0, 2.0, 0.5080128537097623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709872.7752950997, 709872.7752951003, 184860.9908938631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7858200.0000, 
sim time next is 7858800.0000, 
raw observation next is [26.4, 87.0, 1.0, 2.0, 0.5058889047323214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706903.8834219666, 706903.8834219672, 184523.7968878408], 
processed observation next is [1.0, 1.0, 0.45023696682464454, 0.87, 1.0, 1.0, 0.404685427388339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19636218983943515, 0.19636218983943532, 0.2754086520714042], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.5547732], dtype=float32), 0.10983695]. 
=============================================
[2019-03-27 00:39:46,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:46,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:46,721] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.9604106e-33 1.0000000e+00 0.0000000e+00 1.5954202e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:39:46,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-27 00:39:46,760] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8212
[2019-03-27 00:39:46,768] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 89.33333333333333, 1.0, 2.0, 0.693426089341136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969078.5251910627, 969078.5251910621, 219431.0477504276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7879200.0000, 
sim time next is 7879800.0000, 
raw observation next is [26.28333333333333, 89.16666666666667, 1.0, 2.0, 0.7334498719807727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1025039.654350252, 1025039.654350252, 228229.3575811335], 
processed observation next is [1.0, 0.17391304347826086, 0.4447077409162717, 0.8916666666666667, 1.0, 1.0, 0.6788552674467141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28473323731951444, 0.28473323731951444, 0.340640832210647], 
reward next is 0.6594, 
noisyNet noise sample is [array([0.28267998], dtype=float32), -0.3534]. 
=============================================
[2019-03-27 00:39:47,098] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1658572e-32 1.0000000e+00 0.0000000e+00 4.6582852e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:39:47,103] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6901
[2019-03-27 00:39:47,107] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333334, 86.0, 1.0, 2.0, 0.6243938726880052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872564.8218607658, 872564.8218607658, 205370.5781021543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7885200.0000, 
sim time next is 7885800.0000, 
raw observation next is [26.8, 85.5, 1.0, 2.0, 0.633665521371371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885526.9719471707, 885526.9719471707, 207176.9686434698], 
processed observation next is [1.0, 0.2608695652173913, 0.4691943127962086, 0.855, 1.0, 1.0, 0.5586331582787603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24597971442976962, 0.24597971442976962, 0.30921935618428326], 
reward next is 0.6908, 
noisyNet noise sample is [array([0.08132689], dtype=float32), 0.7086535]. 
=============================================
[2019-03-27 00:39:47,378] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5641989e-31 1.0000000e+00 0.0000000e+00 2.8425416e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 00:39:47,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2700
[2019-03-27 00:39:47,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1791228.950109042 W.
[2019-03-27 00:39:47,398] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.4270773332333739, 1.0, 2.0, 0.4270773332333739, 1.0, 1.0, 0.7348280713954859, 6.9112, 6.9112, 170.5573041426782, 1791228.950109042, 1791228.950109042, 367345.5385479694], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7900800.0000, 
sim time next is 7901400.0000, 
raw observation next is [29.46666666666667, 72.66666666666666, 1.0, 2.0, 0.6567959304946859, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.987298220456713, 6.9112, 168.912503366139, 1814672.731794814, 1760686.179934796, 376254.7540021305], 
processed observation next is [1.0, 0.43478260869565216, 0.5955766192733019, 0.7266666666666666, 1.0, 1.0, 0.5865011210779347, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007609822045671333, 0.0, 0.8294377200063786, 0.5040757588318928, 0.4890794944263322, 0.5615742597046723], 
reward next is 0.0579, 
noisyNet noise sample is [array([0.180768], dtype=float32), 0.4440338]. 
=============================================
[2019-03-27 00:39:48,471] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5370992e-25 1.0000000e+00 6.4589144e-34 7.3414367e-29 3.3448382e-34], sum to 1.0000
[2019-03-27 00:39:48,480] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6679
[2019-03-27 00:39:48,489] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2194840.211005143 W.
[2019-03-27 00:39:48,494] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.26666666666667, 69.0, 1.0, 2.0, 0.5232106774728398, 1.0, 1.0, 0.5232106774728398, 1.0, 2.0, 0.9033642981002569, 6.911199999999999, 6.9112, 170.5573041426782, 2194840.211005143, 2194840.211005143, 430564.5404714683], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7914000.0000, 
sim time next is 7914600.0000, 
raw observation next is [30.3, 68.5, 1.0, 2.0, 0.7858790307709813, 1.0, 2.0, 0.7858790307709813, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2197816.119671677, 2197816.119671677, 413051.8991658284], 
processed observation next is [1.0, 0.6086956521739131, 0.6350710900473934, 0.685, 1.0, 1.0, 0.7420229286397365, 1.0, 1.0, 0.7420229286397365, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6105044776865769, 0.6105044776865769, 0.6164953718892962], 
reward next is 0.3835, 
noisyNet noise sample is [array([-2.6509588], dtype=float32), 0.010636325]. 
=============================================
[2019-03-27 00:39:50,383] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:50,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:50,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-27 00:39:50,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:50,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:50,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-27 00:39:50,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:50,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:50,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-27 00:39:51,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-27 00:39:51,265] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,266] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,268] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-27 00:39:51,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,291] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-27 00:39:51,319] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,320] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,323] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-27 00:39:51,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,351] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-27 00:39:51,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-27 00:39:51,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,463] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-27 00:39:51,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-27 00:39:51,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,524] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-27 00:39:51,543] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-27 00:39:51,572] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,572] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:39:51,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:51,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-27 00:39:51,601] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-27 00:39:53,265] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 00:39:53,266] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:39:53,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:39:53,267] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:53,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:39:53,268] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:39:53,269] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:53,268] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:53,270] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:39:53,271] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:53,275] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:39:53,285] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-27 00:39:53,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-27 00:39:53,312] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-27 00:39:53,333] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-27 00:39:53,334] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-27 00:40:13,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:40:13,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.06666666666667, 95.0, 1.0, 2.0, 0.650022440921547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 963176.7206946, 963176.7206946007, 217447.8527135873]
[2019-03-27 00:40:13,559] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:40:13,563] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.696790644363866
[2019-03-27 00:40:39,098] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:40:39,099] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.0, 74.0, 1.0, 2.0, 0.6372356222240269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890518.1634581441, 890518.1634581441, 207880.2851791309]
[2019-03-27 00:40:39,100] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:40:39,105] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4624654526949855
[2019-03-27 00:40:47,139] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:40:47,140] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.8955024469003753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564978379, 1251651.21244208, 1251651.21244208, 268699.9941978502]
[2019-03-27 00:40:47,141] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:40:47,145] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8442533947183786
[2019-03-27 00:40:59,789] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:40:59,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.24134613, 88.0453019, 1.0, 2.0, 0.4926415251168852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692878.0718099168, 692878.0718099173, 183028.2185914336]
[2019-03-27 00:40:59,791] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:40:59,796] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9872837444516758
[2019-03-27 00:41:04,439] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:04,440] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.696556535, 83.232209325, 1.0, 2.0, 0.4169606115112856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 615421.3324987617, 615421.332498761, 175791.909354251]
[2019-03-27 00:41:04,442] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:41:04,445] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7454808123138821
[2019-03-27 00:41:05,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:05,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.3, 45.0, 1.0, 2.0, 0.5977667969717804, 0.0, 2.0, 0.0, 1.0, 1.0, 1.029597034818103, 6.911200000000001, 6.9112, 168.9121748984429, 1671337.383722635, 1671337.383722634, 364429.6244803837]
[2019-03-27 00:41:05,018] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:41:05,020] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.006488e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.9031926961279155
[2019-03-27 00:41:05,025] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1671337.383722635 W.
[2019-03-27 00:41:17,743] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:17,745] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.01166311, 49.31993996, 1.0, 2.0, 0.5389353819262155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753097.749100172, 753097.749100172, 189920.0989017418]
[2019-03-27 00:41:17,746] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:41:17,749] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7858253201965744
[2019-03-27 00:41:21,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:21,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.06657814666667, 97.51882512, 1.0, 2.0, 0.7301348480695072, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977165143063, 6.9112, 168.9123160334375, 1917298.953030464, 1850061.025050591, 391309.3179683808]
[2019-03-27 00:41:21,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:41:21,763] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.311133727969931
[2019-03-27 00:41:21,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1917298.953030464 W.
[2019-03-27 00:41:26,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:26,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.93333333333333, 68.66666666666667, 1.0, 2.0, 0.9124064733157896, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.981017657314771, 6.9112, 168.9124848333921, 2172402.273059727, 2122871.362334134, 438410.3396670989]
[2019-03-27 00:41:26,351] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:41:26,355] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6910328e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.45237055690594286
[2019-03-27 00:41:26,357] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2172402.273059727 W.
[2019-03-27 00:41:33,159] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:33,160] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.54626476833333, 28.54495445666667, 1.0, 2.0, 0.2437598582661374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 405212.4909765474, 405212.4909765474, 159968.5937419908]
[2019-03-27 00:41:33,162] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:41:33,167] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.551053455920689
[2019-03-27 00:41:37,752] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.052714385]
[2019-03-27 00:41:37,753] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 73.66666666666667, 1.0, 2.0, 0.5784648946159433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808356.5348770049, 808356.5348770049, 196801.9745717164]
[2019-03-27 00:41:37,753] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:41:37,756] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3548608230359297
[2019-03-27 00:41:47,692] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:41:48,050] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:41:48,228] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:41:48,273] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:41:48,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:41:49,377] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 425000, evaluation results [425000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:41:50,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:41:50,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5551
[2019-03-27 00:41:50,038] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 75.0, 1.0, 2.0, 0.3830790208322662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 572194.3413916377, 572194.3413916371, 172026.442151353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [25.43333333333334, 75.66666666666667, 1.0, 2.0, 0.3871612908418746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579468.8303373059, 579468.8303373066, 172715.4756283996], 
processed observation next is [1.0, 0.782608695652174, 0.40442338072669864, 0.7566666666666667, 1.0, 1.0, 0.2616401094480417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16096356398258496, 0.16096356398258516, 0.257784291982686], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.06573272], dtype=float32), -1.0825518]. 
=============================================
[2019-03-27 00:41:55,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2862482e-34 1.0000000e+00 0.0000000e+00 1.7133663e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:41:55,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4139
[2019-03-27 00:41:55,905] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 93.66666666666667, 1.0, 2.0, 0.6772672177176389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1015721.395939352, 1015721.395939352, 224885.0827546244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 121800.0000, 
sim time next is 122400.0000, 
raw observation next is [22.9, 94.0, 1.0, 2.0, 0.7523565089805304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1126767.662118722, 1126767.662118722, 242529.6785571532], 
processed observation next is [1.0, 0.43478260869565216, 0.2843601895734597, 0.94, 1.0, 1.0, 0.7016343481693137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31299101725520057, 0.31299101725520057, 0.3619845948614227], 
reward next is 0.6380, 
noisyNet noise sample is [array([0.04504943], dtype=float32), -0.7990782]. 
=============================================
[2019-03-27 00:41:56,153] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8495845e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:41:56,161] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2004
[2019-03-27 00:41:56,165] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.33333333333334, 1.0, 2.0, 0.7425352020826143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1119812.87522061, 1119812.87522061, 241048.7627609518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 119400.0000, 
sim time next is 120000.0000, 
raw observation next is [22.9, 92.66666666666667, 1.0, 2.0, 0.7633938777805619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1149690.533730069, 1149690.533730069, 246093.2961935163], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.9266666666666667, 1.0, 1.0, 0.7149323828681469, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31935848159168584, 0.31935848159168584, 0.3673034271545019], 
reward next is 0.6327, 
noisyNet noise sample is [array([-0.23275138], dtype=float32), -0.8519224]. 
=============================================
[2019-03-27 00:41:56,175] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[58.80743 ]
 [59.13144 ]
 [59.58384 ]
 [59.880146]
 [60.188107]], R is [[58.47070694]
 [58.52622604]
 [58.5841217 ]
 [58.65701294]
 [58.72920609]].
[2019-03-27 00:41:58,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:41:58,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2469
[2019-03-27 00:41:58,418] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 96.0, 1.0, 2.0, 0.3504340421043926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543278.860499487, 543278.8604994876, 170181.9920069493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 163800.0000, 
sim time next is 164400.0000, 
raw observation next is [21.4, 96.0, 1.0, 2.0, 0.3462904811820275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538335.8262998111, 538335.8262998117, 169815.5839444151], 
processed observation next is [1.0, 0.9130434782608695, 0.21327014218009477, 0.96, 1.0, 1.0, 0.2123981700988283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14953772952772532, 0.14953772952772548, 0.2534560954394255], 
reward next is 0.7465, 
noisyNet noise sample is [array([-0.4080871], dtype=float32), 0.46273893]. 
=============================================
[2019-03-27 00:42:03,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:03,903] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8439
[2019-03-27 00:42:03,908] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 91.16666666666667, 1.0, 2.0, 0.2831954340292483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 455922.9584193631, 455922.9584193638, 163996.5893461507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 256200.0000, 
sim time next is 256800.0000, 
raw observation next is [20.5, 91.33333333333334, 1.0, 2.0, 0.2833883711786864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456068.3070089751, 456068.3070089751, 164005.9628730823], 
processed observation next is [0.0, 1.0, 0.1706161137440759, 0.9133333333333334, 1.0, 1.0, 0.13661249539600773, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12668564083582642, 0.12668564083582642, 0.24478501921355567], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.6972606], dtype=float32), 0.8223593]. 
=============================================
[2019-03-27 00:42:08,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:08,972] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9567
[2019-03-27 00:42:08,978] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333333, 86.0, 1.0, 2.0, 0.2746407143913493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443667.5745969001, 443667.5745969001, 163180.4369165069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 337800.0000, 
sim time next is 338400.0000, 
raw observation next is [20.9, 86.0, 1.0, 2.0, 0.2723060848230452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440156.2608427943, 440156.2608427943, 162950.0363039614], 
processed observation next is [0.0, 0.9565217391304348, 0.1895734597156398, 0.86, 1.0, 1.0, 0.1232603431602954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12226562801188731, 0.12226562801188731, 0.2432090094088976], 
reward next is 0.7568, 
noisyNet noise sample is [array([-0.4668475], dtype=float32), 0.79760677]. 
=============================================
[2019-03-27 00:42:16,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:16,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2931
[2019-03-27 00:42:16,766] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.85, 87.0, 1.0, 2.0, 0.2325930176599129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385719.082205185, 385719.082205185, 159052.3526245966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 516600.0000, 
sim time next is 517200.0000, 
raw observation next is [18.83333333333333, 87.0, 1.0, 2.0, 0.2321318764183155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385011.7829843339, 385011.7829843339, 159002.7278875467], 
processed observation next is [1.0, 1.0, 0.0916271721958924, 0.87, 1.0, 1.0, 0.07485768243170542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10694771749564831, 0.10694771749564831, 0.2373175043097712], 
reward next is 0.7627, 
noisyNet noise sample is [array([-0.27259937], dtype=float32), -0.46638507]. 
=============================================
[2019-03-27 00:42:19,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:19,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6193
[2019-03-27 00:42:19,613] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.83333333333333, 87.0, 1.0, 2.0, 0.2321318764183155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385011.7829843339, 385011.7829843339, 159002.7278875467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
processed observation next is [1.0, 1.0, 0.09083728278041096, 0.87, 1.0, 1.0, 0.07366329259156455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10650673415106725, 0.10650673415106725, 0.23717051313799883], 
reward next is 0.7628, 
noisyNet noise sample is [array([-0.95291734], dtype=float32), -0.63609874]. 
=============================================
[2019-03-27 00:42:21,183] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8690102e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:42:21,194] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4662
[2019-03-27 00:42:21,198] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 78.33333333333334, 1.0, 2.0, 0.2407276706654041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397550.4897433306, 397550.4897433306, 159953.1145831218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 544800.0000, 
sim time next is 545400.0000, 
raw observation next is [20.6, 77.5, 1.0, 2.0, 0.2477203328967732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408707.3088508554, 408707.308850856, 160646.6954397053], 
processed observation next is [1.0, 0.30434782608695654, 0.17535545023696694, 0.775, 1.0, 1.0, 0.0936389552973171, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1135298080141265, 0.11352980801412665, 0.23977118722344076], 
reward next is 0.7602, 
noisyNet noise sample is [array([0.91566443], dtype=float32), -0.069593]. 
=============================================
[2019-03-27 00:42:25,600] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2046938e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:42:25,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9494
[2019-03-27 00:42:25,615] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.83333333333333, 88.33333333333333, 1.0, 2.0, 0.2123993107496134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 354389.3453061864, 354389.3453061858, 156746.184946145], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 607200.0000, 
sim time next is 607800.0000, 
raw observation next is [17.76666666666667, 88.66666666666667, 1.0, 2.0, 0.2114011963153425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 352776.7222180887, 352776.722218088, 156639.0532893801], 
processed observation next is [1.0, 0.0, 0.04107424960505548, 0.8866666666666667, 1.0, 1.0, 0.049880959416075274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09799353394946908, 0.09799353394946889, 0.23378963177519418], 
reward next is 0.7662, 
noisyNet noise sample is [array([0.3688642], dtype=float32), 1.5622284]. 
=============================================
[2019-03-27 00:42:31,727] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3583806e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:42:31,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3562
[2019-03-27 00:42:31,743] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.96666666666667, 66.33333333333334, 1.0, 2.0, 0.5335588132860438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872045.3916993379, 872045.3916993379, 202024.2447057555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 724800.0000, 
sim time next is 725400.0000, 
raw observation next is [23.15, 65.5, 1.0, 2.0, 0.5355168119886912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874631.3157602391, 874631.3157602391, 202384.6064567166], 
processed observation next is [1.0, 0.391304347826087, 0.2962085308056872, 0.655, 1.0, 1.0, 0.44038170119119413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2429531432667331, 0.2429531432667331, 0.3020665768010696], 
reward next is 0.6979, 
noisyNet noise sample is [array([0.3451783], dtype=float32), -1.4092892]. 
=============================================
[2019-03-27 00:42:31,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:31,796] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7226
[2019-03-27 00:42:31,804] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.96666666666667, 80.16666666666667, 1.0, 2.0, 0.2334841922283851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 386343.1201557997, 386343.1201557997, 159220.1731400012], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 715800.0000, 
sim time next is 716400.0000, 
raw observation next is [20.2, 79.0, 1.0, 2.0, 0.2355400699339977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 389425.749075381, 389425.7490753804, 159435.7178858593], 
processed observation next is [1.0, 0.30434782608695654, 0.15639810426540288, 0.79, 1.0, 1.0, 0.07896393967951529, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10817381918760584, 0.10817381918760566, 0.23796375803859598], 
reward next is 0.7620, 
noisyNet noise sample is [array([-0.13639085], dtype=float32), -0.7650945]. 
=============================================
[2019-03-27 00:42:39,190] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:39,202] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0951
[2019-03-27 00:42:39,208] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 79.0, 1.0, 2.0, 0.3004548252925558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478485.8758273748, 478485.8758273748, 165514.059338641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 846000.0000, 
sim time next is 846600.0000, 
raw observation next is [22.53333333333333, 79.33333333333334, 1.0, 2.0, 0.2996558232071326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477486.3531816378, 477486.3531816385, 165447.0151375067], 
processed observation next is [0.0, 0.8260869565217391, 0.26698262243285936, 0.7933333333333334, 1.0, 1.0, 0.15621183518931636, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1326350981060105, 0.1326350981060107, 0.24693584348881598], 
reward next is 0.7531, 
noisyNet noise sample is [array([1.5853878], dtype=float32), -0.55353266]. 
=============================================
[2019-03-27 00:42:40,894] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:40,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5425
[2019-03-27 00:42:40,903] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 89.0, 1.0, 2.0, 0.2999799249130229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 477609.8224887764, 477609.8224887764, 165449.7052868117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 867600.0000, 
sim time next is 868200.0000, 
raw observation next is [21.26666666666667, 89.0, 1.0, 2.0, 0.2990876043078745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 476476.1412247525, 476476.1412247518, 165373.5491908529], 
processed observation next is [0.0, 0.043478260869565216, 0.2069510268562403, 0.89, 1.0, 1.0, 0.1555272341058729, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13235448367354236, 0.13235448367354216, 0.24682619282216853], 
reward next is 0.7532, 
noisyNet noise sample is [array([-0.3983269], dtype=float32), -0.7661328]. 
=============================================
[2019-03-27 00:42:42,110] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:42,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9750
[2019-03-27 00:42:42,130] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 86.0, 1.0, 2.0, 0.278570810106858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 449665.4145380659, 449665.4145380653, 163577.5260373691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 883200.0000, 
sim time next is 883800.0000, 
raw observation next is [21.1, 85.5, 1.0, 2.0, 0.2791042447665226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 450229.5764741094, 450229.5764741094, 163615.5172352122], 
processed observation next is [0.0, 0.21739130434782608, 0.1990521327014219, 0.855, 1.0, 1.0, 0.1314508973090634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12506377124280815, 0.12506377124280815, 0.24420226453016747], 
reward next is 0.7558, 
noisyNet noise sample is [array([1.5942932], dtype=float32), 1.1289071]. 
=============================================
[2019-03-27 00:42:42,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:42,422] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6058
[2019-03-27 00:42:42,429] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.292241266409421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 466670.2044195034, 466670.204419504, 164700.6656892739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 892800.0000, 
sim time next is 893400.0000, 
raw observation next is [22.5, 79.00000000000001, 1.0, 2.0, 0.2925385832152782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466982.5633899804, 466982.5633899804, 164720.0440739929], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.7900000000000001, 1.0, 1.0, 0.14763684724732312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.129717378719439, 0.129717378719439, 0.24585081205073567], 
reward next is 0.7541, 
noisyNet noise sample is [array([0.73643935], dtype=float32), 0.356874]. 
=============================================
[2019-03-27 00:42:45,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:42:45,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3240
[2019-03-27 00:42:45,168] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 84.5, 1.0, 2.0, 0.3337932480252307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518922.6041117552, 518922.6041117552, 168262.5010783901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 934200.0000, 
sim time next is 934800.0000, 
raw observation next is [22.83333333333334, 85.33333333333334, 1.0, 2.0, 0.3345505630127777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 519477.9307774201, 519477.9307774206, 168288.1086678219], 
processed observation next is [0.0, 0.8260869565217391, 0.2812006319115327, 0.8533333333333334, 1.0, 1.0, 0.1982536903768406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14429942521595004, 0.14429942521595016, 0.25117628159376404], 
reward next is 0.7488, 
noisyNet noise sample is [array([1.1980995], dtype=float32), -0.65002257]. 
=============================================
[2019-03-27 00:42:45,340] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 00:42:45,342] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:42:45,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:42:45,344] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:42:45,344] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:42:45,345] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:42:45,345] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:42:45,347] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:42:45,348] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:42:45,348] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:42:45,346] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:42:45,372] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-27 00:42:45,372] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-27 00:42:45,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-27 00:42:45,435] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-27 00:42:45,452] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-27 00:42:53,153] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.007166677]
[2019-03-27 00:42:53,155] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.0, 86.0, 1.0, 2.0, 0.3923464887632082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586872.5116340881, 586872.5116340881, 173375.0959557385]
[2019-03-27 00:42:53,157] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:42:53,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8130287901783779
[2019-03-27 00:44:08,516] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.007166677]
[2019-03-27 00:44:08,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.45, 60.66666666666666, 1.0, 2.0, 0.5198684069498711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726444.820945256, 726444.8209452567, 186767.8669291593]
[2019-03-27 00:44:08,518] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:44:08,519] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20710601455867417
[2019-03-27 00:44:10,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.007166677]
[2019-03-27 00:44:10,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.9, 88.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.144488381178038, 6.9112, 168.9114840673984, 1619369.637782977, 1453868.27372503, 311352.6163177098]
[2019-03-27 00:44:10,528] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:44:10,529] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6372649382328123
[2019-03-27 00:44:37,032] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.007166677]
[2019-03-27 00:44:37,034] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.13333333333333, 80.33333333333334, 1.0, 2.0, 0.6172847995815888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862626.1494328486, 862626.1494328493, 204009.3798722005]
[2019-03-27 00:44:37,034] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:44:37,037] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8004981374788154
[2019-03-27 00:44:39,023] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:44:39,435] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:44:39,519] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:44:39,565] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:44:39,667] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:44:40,684] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 450000, evaluation results [450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:44:46,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:44:46,775] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3487
[2019-03-27 00:44:46,782] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.11666666666667, 83.33333333333333, 1.0, 2.0, 0.3094614475552037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491753.6787675507, 491753.6787675507, 166457.3621773816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1115400.0000, 
sim time next is 1116000.0000, 
raw observation next is [22.0, 84.0, 1.0, 2.0, 0.3076326729967183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489142.7409900075, 489142.7409900075, 166271.0507321882], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 0.84, 1.0, 1.0, 0.1658224975864076, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1358729836083354, 0.1358729836083354, 0.2481657473614749], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.0385717], dtype=float32), 1.0269514]. 
=============================================
[2019-03-27 00:44:46,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.52138 ]
 [74.52957 ]
 [74.43315 ]
 [74.28926 ]
 [74.090416]], R is [[74.53414154]
 [74.5403595 ]
 [74.54637146]
 [74.55238342]
 [74.55834198]].
[2019-03-27 00:44:46,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:44:46,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8357
[2019-03-27 00:44:46,863] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 96.5, 1.0, 2.0, 0.3325613554506282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517316.5723982737, 517316.5723982743, 168145.1584687701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1049400.0000, 
sim time next is 1050000.0000, 
raw observation next is [21.16666666666666, 96.33333333333334, 1.0, 2.0, 0.3280387250889422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512374.4978643529, 512374.4978643529, 167819.2174377652], 
processed observation next is [1.0, 0.13043478260869565, 0.2022116903633489, 0.9633333333333334, 1.0, 1.0, 0.19040810251679785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1423262494067647, 0.1423262494067647, 0.250476443936963], 
reward next is 0.7495, 
noisyNet noise sample is [array([2.3544183], dtype=float32), -0.15428586]. 
=============================================
[2019-03-27 00:44:46,876] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.98582]
 [71.03817]
 [71.06982]
 [71.07914]
 [71.02135]], R is [[70.94651794]
 [70.98609161]
 [71.02478027]
 [71.06021118]
 [71.09658051]].
[2019-03-27 00:44:49,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8619365e-36 1.0000000e+00 0.0000000e+00 1.2085110e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:44:49,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5526
[2019-03-27 00:44:49,947] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 67.33333333333333, 1.0, 2.0, 0.8016730572051164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1237864.74481765, 1237864.744817649, 259720.5075437787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1093200.0000, 
sim time next is 1093800.0000, 
raw observation next is [25.7, 67.16666666666667, 1.0, 2.0, 0.797217510878386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231854.706438085, 1231854.706438085, 258606.5675795001], 
processed observation next is [1.0, 0.6521739130434783, 0.4170616113744076, 0.6716666666666667, 1.0, 1.0, 0.7556837480462482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34218186289946806, 0.34218186289946806, 0.38597995161119414], 
reward next is 0.6140, 
noisyNet noise sample is [array([-1.2304806], dtype=float32), 1.5176504]. 
=============================================
[2019-03-27 00:44:55,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0676038e-33 1.0000000e+00 0.0000000e+00 8.6146983e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 00:44:55,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9071
[2019-03-27 00:44:55,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1742510.506860341 W.
[2019-03-27 00:44:55,911] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.15, 71.5, 1.0, 2.0, 0.6232064672827189, 1.0, 1.0, 0.6232064672827189, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1742510.506860341, 1742510.506860341, 342970.6789412025], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [28.16666666666666, 71.66666666666667, 1.0, 2.0, 0.6051546556671048, 1.0, 2.0, 0.6051546556671048, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1691997.063336905, 1691997.063336905, 336152.9444348365], 
processed observation next is [1.0, 0.4782608695652174, 0.5339652448657185, 0.7166666666666667, 1.0, 1.0, 0.5242827176712106, 1.0, 1.0, 0.5242827176712106, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4699991842602514, 0.4699991842602514, 0.5017208125893082], 
reward next is 0.4983, 
noisyNet noise sample is [array([0.17126991], dtype=float32), -0.09970577]. 
=============================================
[2019-03-27 00:45:04,337] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:04,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9315
[2019-03-27 00:45:04,355] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 95.0, 1.0, 2.0, 0.8139356302280932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1210813.706585817, 1210813.706585817, 257422.0193257338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1333800.0000, 
sim time next is 1334400.0000, 
raw observation next is [22.93333333333333, 95.0, 1.0, 2.0, 0.74319664946453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1106221.01304135, 1106221.01304135, 239407.4658106031], 
processed observation next is [1.0, 0.43478260869565216, 0.28593996840442326, 0.95, 1.0, 1.0, 0.6905983728488313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30728361473370835, 0.30728361473370835, 0.35732457583672106], 
reward next is 0.6427, 
noisyNet noise sample is [array([-2.2140856], dtype=float32), -0.6526095]. 
=============================================
[2019-03-27 00:45:05,764] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:05,771] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7010
[2019-03-27 00:45:05,776] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333333, 94.33333333333334, 1.0, 2.0, 0.3234996431887225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 509385.1160155715, 509385.1160155709, 167690.7516526353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1369200.0000, 
sim time next is 1369800.0000, 
raw observation next is [21.1, 94.5, 1.0, 2.0, 0.324238512798485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510714.784000715, 510714.7840007156, 167796.0366305613], 
processed observation next is [1.0, 0.8695652173913043, 0.1990521327014219, 0.945, 1.0, 1.0, 0.18582953349215056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1418652177779764, 0.14186521777797656, 0.2504418457172557], 
reward next is 0.7496, 
noisyNet noise sample is [array([-2.6013386], dtype=float32), -0.66034335]. 
=============================================
[2019-03-27 00:45:05,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:05,866] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4813
[2019-03-27 00:45:05,870] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 69.0, 1.0, 2.0, 0.4277090929814758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617149.4771918209, 617149.4771918209, 175561.0670608429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [27.66666666666667, 69.16666666666667, 1.0, 2.0, 0.4303329792422638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619344.9523717558, 619344.9523717551, 175728.4968300429], 
processed observation next is [0.0, 0.6521739130434783, 0.5102685624012641, 0.6916666666666668, 1.0, 1.0, 0.31365419185814924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17204026454770996, 0.17204026454770976, 0.26228133855230285], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.85140496], dtype=float32), -0.9153961]. 
=============================================
[2019-03-27 00:45:05,886] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[86.38841 ]
 [86.258156]
 [86.17242 ]
 [86.085785]
 [86.007965]], R is [[86.354599  ]
 [86.22901917]
 [86.10470581]
 [85.98162079]
 [85.85972595]].
[2019-03-27 00:45:12,685] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:12,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5780
[2019-03-27 00:45:12,699] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 90.0, 1.0, 2.0, 0.3444579342784178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 533972.0111294887, 533972.0111294887, 169421.1715931408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1550400.0000, 
sim time next is 1551000.0000, 
raw observation next is [22.18333333333333, 90.5, 1.0, 2.0, 0.3435328483465633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532828.480562943, 532828.480562943, 169336.7082593457], 
processed observation next is [0.0, 0.9565217391304348, 0.2503949447077408, 0.905, 1.0, 1.0, 0.20907572089947382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14800791126748417, 0.14800791126748417, 0.2527413556109637], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.12523797], dtype=float32), -1.0050356]. 
=============================================
[2019-03-27 00:45:12,720] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.81304 ]
 [76.8677  ]
 [76.92704 ]
 [76.984764]
 [77.04168 ]], R is [[76.74948883]
 [76.72912598]
 [76.70897675]
 [76.68897247]
 [76.66899109]].
[2019-03-27 00:45:16,762] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4173976e-32 1.0000000e+00 0.0000000e+00 2.6101011e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:45:16,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3549
[2019-03-27 00:45:16,777] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 85.0, 1.0, 2.0, 0.6383729827092052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 958392.0700820248, 958392.0700820254, 216470.8191801312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1602000.0000, 
sim time next is 1602600.0000, 
raw observation next is [24.01666666666667, 85.00000000000001, 1.0, 2.0, 0.650490053267038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975997.7207925948, 975997.7207925948, 219010.5520290937], 
processed observation next is [1.0, 0.5652173913043478, 0.33728278041074267, 0.8500000000000001, 1.0, 1.0, 0.5789036786349855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.271110477997943, 0.271110477997943, 0.32688142093894584], 
reward next is 0.6731, 
noisyNet noise sample is [array([0.97379583], dtype=float32), -1.6801689]. 
=============================================
[2019-03-27 00:45:22,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:22,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7804
[2019-03-27 00:45:22,406] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 84.66666666666666, 1.0, 2.0, 0.5090827636249359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711368.3166479467, 711368.3166479474, 185031.53148485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1712400.0000, 
sim time next is 1713000.0000, 
raw observation next is [26.81666666666667, 85.33333333333334, 1.0, 2.0, 0.5083002362419704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710274.4841059294, 710274.4841059294, 184906.9153788787], 
processed observation next is [1.0, 0.8260869565217391, 0.46998420221169057, 0.8533333333333334, 1.0, 1.0, 0.4075906460746631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19729846780720262, 0.19729846780720262, 0.27598047071474435], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.10228425], dtype=float32), 0.080317385]. 
=============================================
[2019-03-27 00:45:22,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.28353]
 [76.209  ]
 [75.88822]
 [75.69177]
 [75.90314]], R is [[75.9302063 ]
 [75.89473724]
 [75.85980988]
 [75.8254776 ]
 [75.79173279]].
[2019-03-27 00:45:23,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:23,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4723
[2019-03-27 00:45:23,184] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 99.0, 1.0, 2.0, 0.4810218100538433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688882.3257555472, 688882.3257555466, 182813.539430845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1661400.0000, 
sim time next is 1662000.0000, 
raw observation next is [23.46666666666667, 99.0, 1.0, 2.0, 0.4749539791025801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679709.2318490242, 679709.2318490242, 181816.7391612908], 
processed observation next is [1.0, 0.21739130434782608, 0.31121642969984215, 0.99, 1.0, 1.0, 0.36741443265371093, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18880811995806226, 0.18880811995806226, 0.27136826740491166], 
reward next is 0.7286, 
noisyNet noise sample is [array([-0.19360238], dtype=float32), 0.82852]. 
=============================================
[2019-03-27 00:45:23,216] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.24267]
 [73.27815]
 [73.26865]
 [73.43793]
 [73.3928 ]], R is [[73.26508331]
 [73.25957489]
 [73.25575256]
 [73.24717712]
 [73.25087738]].
[2019-03-27 00:45:27,305] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3231575e-35 1.0000000e+00 0.0000000e+00 1.2090171e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:45:27,312] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0695
[2019-03-27 00:45:27,325] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 84.66666666666666, 1.0, 2.0, 0.7629865618473617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1166042.850806639, 1166042.850806639, 248028.8281951654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1770000.0000, 
sim time next is 1770600.0000, 
raw observation next is [23.38333333333333, 85.33333333333334, 1.0, 2.0, 0.7552224976841946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1153933.697518288, 1153933.697518289, 246009.6674691382], 
processed observation next is [1.0, 0.4782608695652174, 0.30726698262243274, 0.8533333333333334, 1.0, 1.0, 0.7050873466074633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3205371381995244, 0.32053713819952473, 0.36717860816289283], 
reward next is 0.6328, 
noisyNet noise sample is [array([-0.47010183], dtype=float32), 0.1757968]. 
=============================================
[2019-03-27 00:45:27,653] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2871251e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:45:27,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1976
[2019-03-27 00:45:27,664] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 84.66666666666667, 1.0, 2.0, 0.681857240381397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1062414.05920471, 1062414.059204711, 230387.9684804505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1776000.0000, 
sim time next is 1776600.0000, 
raw observation next is [22.65, 84.5, 1.0, 2.0, 0.6980661025658906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090535.943897861, 1090535.943897861, 234538.0644342992], 
processed observation next is [1.0, 0.5652173913043478, 0.2725118483412322, 0.845, 1.0, 1.0, 0.6362242199589043, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3029266510827392, 0.3029266510827392, 0.35005681258850624], 
reward next is 0.6499, 
noisyNet noise sample is [array([1.2974756], dtype=float32), 0.5102169]. 
=============================================
[2019-03-27 00:45:28,289] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:45:28,299] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9448
[2019-03-27 00:45:28,306] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 93.16666666666666, 1.0, 2.0, 0.523230978863991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 188570.2929640439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [24.5, 93.0, 1.0, 2.0, 0.4871256155342776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 182666.2265049748], 
processed observation next is [1.0, 0.2608695652173913, 0.3601895734597157, 0.93, 1.0, 1.0, 0.38207905486057536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1913762893031219, 0.19137628930312206, 0.2726361589626489], 
reward next is 0.7274, 
noisyNet noise sample is [array([-1.0467736], dtype=float32), -0.3264787]. 
=============================================
[2019-03-27 00:45:36,589] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 00:45:36,589] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:45:36,590] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:45:36,591] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:45:36,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:45:36,592] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:45:36,593] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:45:36,593] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:45:36,592] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:45:36,595] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:45:36,594] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:45:36,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-27 00:45:36,635] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-27 00:45:36,653] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-27 00:45:36,674] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-27 00:45:36,674] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-27 00:46:50,974] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.013331717]
[2019-03-27 00:46:50,976] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.56666666666667, 77.0, 1.0, 2.0, 0.6013318297063597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 840323.8136685065, 840323.8136685059, 200994.0772361184]
[2019-03-27 00:46:50,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:46:50,980] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8250280342868372
[2019-03-27 00:47:09,182] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.013331717]
[2019-03-27 00:47:09,184] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.76666666666667, 59.66666666666667, 1.0, 2.0, 0.5550189507141996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775580.8183256356, 775580.8183256363, 192665.5656012809]
[2019-03-27 00:47:09,185] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:47:09,189] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43547819424440515
[2019-03-27 00:47:10,009] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.013331717]
[2019-03-27 00:47:10,011] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.491014745, 74.76752374, 1.0, 2.0, 0.4016454729569991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604948.1116153153, 604948.1116153153, 175142.1809787377]
[2019-03-27 00:47:10,013] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:47:10,017] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2083540762888496
[2019-03-27 00:47:10,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.013331717]
[2019-03-27 00:47:10,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.9, 74.66666666666667, 1.0, 2.0, 0.5335746153779879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 745604.0898065383, 745604.0898065389, 189023.1192390265]
[2019-03-27 00:47:10,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:47:10,776] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12320905106174329
[2019-03-27 00:47:11,639] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.013331717]
[2019-03-27 00:47:11,640] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.1, 85.66666666666667, 1.0, 2.0, 0.5202028244912935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726912.2835005831, 726912.2835005831, 186822.1572256739]
[2019-03-27 00:47:11,641] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:47:11,644] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6987887342823669
[2019-03-27 00:47:22,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.013331717]
[2019-03-27 00:47:22,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 82.5, 1.0, 2.0, 0.6163453387688914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861312.7652999338, 861312.7652999344, 203830.0405777085]
[2019-03-27 00:47:22,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:47:22,589] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4742557040281429
[2019-03-27 00:47:31,351] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:47:31,451] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:47:31,466] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:47:31,493] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:47:31,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:47:32,734] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 475000, evaluation results [475000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:47:40,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:47:40,091] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9599
[2019-03-27 00:47:40,102] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 97.33333333333334, 1.0, 2.0, 0.4669774741565006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656237.0641911187, 656237.064191118, 179064.9579631308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2006400.0000, 
sim time next is 2007000.0000, 
raw observation next is [24.2, 97.0, 1.0, 2.0, 0.4695583185565638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658112.1932833813, 658112.1932833806, 179221.2954939674], 
processed observation next is [0.0, 0.21739130434782608, 0.3459715639810427, 0.97, 1.0, 1.0, 0.3609136368151371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18280894257871702, 0.18280894257871683, 0.2674944708865185], 
reward next is 0.7325, 
noisyNet noise sample is [array([1.0998569], dtype=float32), 0.8650659]. 
=============================================
[2019-03-27 00:47:40,117] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.52114 ]
 [69.49528 ]
 [69.45249 ]
 [69.33746 ]
 [69.274826]], R is [[69.57232666]
 [69.60934448]
 [69.64617157]
 [69.68270111]
 [69.71870422]].
[2019-03-27 00:47:42,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:47:42,409] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3918
[2019-03-27 00:47:42,416] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 87.66666666666667, 1.0, 2.0, 0.4880166059696301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681922.010016076, 681922.010016076, 181739.8203442827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2054400.0000, 
sim time next is 2055000.0000, 
raw observation next is [25.85, 87.83333333333334, 1.0, 2.0, 0.4869621855938084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680448.1610901399, 680448.1610901406, 181578.5221855014], 
processed observation next is [0.0, 0.782608695652174, 0.4241706161137442, 0.8783333333333334, 1.0, 1.0, 0.3818821513178415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1890133780805944, 0.1890133780805946, 0.27101271967985285], 
reward next is 0.7290, 
noisyNet noise sample is [array([1.0463359], dtype=float32), -0.055774737]. 
=============================================
[2019-03-27 00:47:42,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.59339]
 [85.55191]
 [85.51264]
 [85.46899]
 [85.42126]], R is [[85.49372864]
 [85.36753845]
 [85.24232483]
 [85.11800385]
 [84.99453735]].
[2019-03-27 00:47:45,272] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:47:45,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6167
[2019-03-27 00:47:45,287] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 96.5, 1.0, 2.0, 0.470418714713117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661346.4960762226, 661346.4960762219, 179610.3312836227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2093400.0000, 
sim time next is 2094000.0000, 
raw observation next is [24.33333333333333, 96.0, 1.0, 2.0, 0.4764538974751408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667692.2994593258, 667692.2994593258, 180238.4013870275], 
processed observation next is [0.0, 0.21739130434782608, 0.35229067930489716, 0.96, 1.0, 1.0, 0.36922156322306116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18547008318314606, 0.18547008318314606, 0.26901253938362313], 
reward next is 0.7310, 
noisyNet noise sample is [array([2.486882], dtype=float32), 0.6748857]. 
=============================================
[2019-03-27 00:47:45,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[65.8344  ]
 [65.875114]
 [65.89938 ]
 [65.89064 ]
 [65.81035 ]], R is [[65.86725616]
 [65.94051361]
 [66.01423645]
 [66.08828735]
 [66.16178131]].
[2019-03-27 00:47:55,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:47:55,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2785
[2019-03-27 00:47:55,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.68333333333333, 80.83333333333333, 1.0, 2.0, 0.5521788851898377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771610.6828985944, 771610.682898595, 192175.1877840316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2328600.0000, 
sim time next is 2329200.0000, 
raw observation next is [28.6, 81.0, 1.0, 2.0, 0.5506626327980185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769491.1155753976, 769491.1155753976, 191914.4735542212], 
processed observation next is [1.0, 1.0, 0.5545023696682465, 0.81, 1.0, 1.0, 0.4586296780699018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2137475321042771, 0.2137475321042771, 0.2864395127674943], 
reward next is 0.7136, 
noisyNet noise sample is [array([1.5643462], dtype=float32), -0.33008528]. 
=============================================
[2019-03-27 00:48:01,413] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:01,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2186
[2019-03-27 00:48:01,426] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 79.0, 1.0, 2.0, 0.5718864987981999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799160.316084342, 799160.316084342, 195625.3288737926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2413800.0000, 
sim time next is 2414400.0000, 
raw observation next is [29.46666666666667, 79.33333333333333, 1.0, 2.0, 0.5719913985712006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799306.9593057355, 799306.9593057355, 195643.918306955], 
processed observation next is [1.0, 0.9565217391304348, 0.5955766192733019, 0.7933333333333333, 1.0, 1.0, 0.4843269862303622, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22202971091825988, 0.22202971091825988, 0.2920058482193358], 
reward next is 0.7080, 
noisyNet noise sample is [array([-0.14170279], dtype=float32), 0.7077516]. 
=============================================
[2019-03-27 00:48:03,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:03,757] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4765
[2019-03-27 00:48:03,764] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 77.33333333333334, 1.0, 2.0, 0.5780193806038082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807733.7294120664, 807733.729412067, 196722.4401446359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2410800.0000, 
sim time next is 2411400.0000, 
raw observation next is [29.9, 77.66666666666666, 1.0, 2.0, 0.5762160637791944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 805212.7898215837, 805212.7898215842, 196398.7064164983], 
processed observation next is [1.0, 0.9130434782608695, 0.6161137440758293, 0.7766666666666666, 1.0, 1.0, 0.48941694431228244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22367021939488435, 0.22367021939488452, 0.2931323976365646], 
reward next is 0.7069, 
noisyNet noise sample is [array([-0.6912664], dtype=float32), -0.6318919]. 
=============================================
[2019-03-27 00:48:04,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:04,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5429
[2019-03-27 00:48:04,275] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.56666666666667, 69.83333333333333, 1.0, 2.0, 0.5794348338234279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 809712.4608081976, 809712.4608081982, 196977.7283556188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2401800.0000, 
sim time next is 2402400.0000, 
raw observation next is [31.43333333333334, 70.66666666666667, 1.0, 2.0, 0.5799419195826917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810421.3417291143, 810421.3417291143, 197069.1712000144], 
processed observation next is [1.0, 0.8260869565217391, 0.6887835703001584, 0.7066666666666667, 1.0, 1.0, 0.49390592720806226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2251170393691984, 0.2251170393691984, 0.29413309134330506], 
reward next is 0.7059, 
noisyNet noise sample is [array([-0.7722079], dtype=float32), 0.011680076]. 
=============================================
[2019-03-27 00:48:10,026] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:10,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5192
[2019-03-27 00:48:10,045] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 93.0, 1.0, 2.0, 0.552380441864277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771892.4390403318, 771892.4390403318, 192209.9112043129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2498400.0000, 
sim time next is 2499000.0000, 
raw observation next is [26.88333333333333, 93.16666666666667, 1.0, 2.0, 0.5519965502658011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771355.7969124786, 771355.7969124786, 192143.8699131767], 
processed observation next is [1.0, 0.9565217391304348, 0.47314375987361756, 0.9316666666666668, 1.0, 1.0, 0.4602368075491579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21426549914235518, 0.21426549914235518, 0.286781895392801], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.29875925], dtype=float32), -0.62003046]. 
=============================================
[2019-03-27 00:48:10,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.567406]
 [70.591995]
 [70.765564]
 [70.8698  ]
 [70.81618 ]], R is [[70.55502319]
 [70.56259155]
 [70.57003021]
 [70.57723236]
 [70.58410645]].
[2019-03-27 00:48:19,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:19,886] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2285
[2019-03-27 00:48:19,890] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3877364187725297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583346.5155750271, 583346.5155750271, 173156.6542452343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2729400.0000, 
sim time next is 2730000.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.3900875902028152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585944.8756998703, 585944.8756998696, 173364.0270374702], 
processed observation next is [0.0, 0.6086956521739131, 0.2575039494470777, 0.98, 1.0, 1.0, 0.265165771328693, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1627624654721862, 0.162762465472186, 0.2587522791604033], 
reward next is 0.7412, 
noisyNet noise sample is [array([0.04494351], dtype=float32), 0.87078625]. 
=============================================
[2019-03-27 00:48:19,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.678345]
 [72.705246]
 [72.64243 ]
 [72.641884]
 [72.63597 ]], R is [[72.65535736]
 [72.67035675]
 [72.68544769]
 [72.70040131]
 [72.71529388]].
[2019-03-27 00:48:20,598] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:20,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4596
[2019-03-27 00:48:20,616] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.4404975202183125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632708.5283133048, 632708.5283133048, 177012.2143808495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2688600.0000, 
sim time next is 2689200.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4399813768221698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631967.518960236, 631967.5189602353, 176938.3436916701], 
processed observation next is [0.0, 0.13043478260869565, 0.3364928909952607, 0.94, 1.0, 1.0, 0.3252787672556263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17554653304450998, 0.1755465330445098, 0.26408708013682103], 
reward next is 0.7359, 
noisyNet noise sample is [array([0.8436099], dtype=float32), 2.2351177]. 
=============================================
[2019-03-27 00:48:24,437] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:48:24,448] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8423
[2019-03-27 00:48:24,456] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3533609519993398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 543689.7888835866, 543689.788883586, 170102.0400789446], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2754000.0000, 
sim time next is 2754600.0000, 
raw observation next is [22.0, 95.0, 1.0, 2.0, 0.3544060707907205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544588.1602606664, 544588.1602606664, 170155.7961132363], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.95, 1.0, 1.0, 0.22217598890448254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15127448896129622, 0.15127448896129622, 0.25396387479587507], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.05656666], dtype=float32), -0.1422063]. 
=============================================
[2019-03-27 00:48:28,575] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 00:48:28,577] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:48:28,578] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:48:28,579] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:48:28,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:48:28,579] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:48:28,580] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:48:28,581] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:48:28,582] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:48:28,582] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:48:28,583] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:48:28,589] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-27 00:48:28,611] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-27 00:48:28,611] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-27 00:48:28,645] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-27 00:48:28,663] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-27 00:48:45,769] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:48:45,770] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.32750828333334, 93.12756337666667, 1.0, 2.0, 0.3621673380851233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552976.6928245433, 552976.6928245439, 170753.6065157824]
[2019-03-27 00:48:45,771] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:48:45,773] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7976404690513027
[2019-03-27 00:49:03,841] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:49:03,842] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.21666666666667, 73.5, 1.0, 2.0, 0.585420344244173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830989.3213613497, 830989.3213613491, 199722.7370882335]
[2019-03-27 00:49:03,845] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:49:03,847] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.645712e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.7039846472685568
[2019-03-27 00:49:16,060] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:49:16,061] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.37532427, 94.11699894166668, 1.0, 2.0, 0.3349272963998193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523200.6882729252, 523200.6882729252, 168667.1258771397]
[2019-03-27 00:49:16,064] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:49:16,066] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3031909087529606
[2019-03-27 00:49:38,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:49:38,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.92726469333333, 68.88163800666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.090443283638558, 6.9112, 168.9119818420401, 1581002.633370539, 1453842.012631231, 311357.227207755]
[2019-03-27 00:49:38,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:49:38,645] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00414496e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00], sampled 0.03176745060662123
[2019-03-27 00:49:45,026] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:49:45,026] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.5, 77.66666666666667, 1.0, 2.0, 1.004425631324443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1403994.721485584, 1403994.721485584, 300281.9053895543]
[2019-03-27 00:49:45,026] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:49:45,028] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9465386e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.04277363511771737
[2019-03-27 00:49:51,016] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:49:51,018] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.41666666666666, 87.16666666666667, 1.0, 2.0, 0.6189670543785618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864977.9777798036, 864977.9777798041, 204332.6408443141]
[2019-03-27 00:49:51,019] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:49:51,020] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9388328914088686
[2019-03-27 00:49:52,603] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:49:52,605] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666667, 85.66666666666666, 1.0, 2.0, 0.5323590769359506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743904.9314034265, 743904.9314034265, 188820.8727985366]
[2019-03-27 00:49:52,606] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:49:52,608] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12567590201469592
[2019-03-27 00:50:13,118] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:50:13,119] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.6, 59.33333333333334, 1.0, 2.0, 0.8949554908300469, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005988051539939, 6.9112, 168.9112620453317, 2147975.625427707, 2080730.393895505, 432649.0104847441]
[2019-03-27 00:50:13,119] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:50:13,121] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2405266e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.0736911345712421
[2019-03-27 00:50:13,123] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2147975.625427707 W.
[2019-03-27 00:50:18,502] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.00096974894]
[2019-03-27 00:50:18,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.33333333333333, 74.5, 1.0, 2.0, 0.5485303656199151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766510.4291085254, 766510.4291085248, 191548.3699743114]
[2019-03-27 00:50:18,504] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:50:18,507] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4050507845839274
[2019-03-27 00:50:23,781] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:50:23,824] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:50:23,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:50:24,208] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:50:24,246] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:50:25,259] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 500000, evaluation results [500000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:50:27,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:27,956] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0669
[2019-03-27 00:50:27,961] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3516546787000042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541715.1016214251, 541715.1016214245, 169957.9511718156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2876400.0000, 
sim time next is 2877000.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.3578061486825639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551189.273692111, 551189.273692111, 170746.0739588929], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.22627246829224565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15310813158114195, 0.15310813158114195, 0.2548448865058103], 
reward next is 0.7452, 
noisyNet noise sample is [array([-1.4920673], dtype=float32), 0.6111116]. 
=============================================
[2019-03-27 00:50:27,971] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.85516]
 [70.7745 ]
 [70.73949]
 [70.69664]
 [70.69748]], R is [[70.89646912]
 [70.93383789]
 [70.97161865]
 [71.00888062]
 [71.04550934]].
[2019-03-27 00:50:31,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:31,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9006
[2019-03-27 00:50:31,365] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.0, 1.0, 2.0, 0.3199885840274344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506358.0131187652, 506358.0131187658, 167510.158817047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2920200.0000, 
sim time next is 2920800.0000, 
raw observation next is [20.66666666666667, 96.0, 1.0, 2.0, 0.3186198199692449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504778.9420151702, 504778.9420151702, 167401.0737023378], 
processed observation next is [1.0, 0.8260869565217391, 0.17851500789889443, 0.96, 1.0, 1.0, 0.17906002405933116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14021637278199173, 0.14021637278199173, 0.24985234880945942], 
reward next is 0.7501, 
noisyNet noise sample is [array([-1.0402902], dtype=float32), -0.91648]. 
=============================================
[2019-03-27 00:50:31,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:31,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6478
[2019-03-27 00:50:31,806] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.75, 95.5, 1.0, 2.0, 0.3147153254597316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498600.5081672747, 498600.5081672741, 166938.3623574729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2928600.0000, 
sim time next is 2929200.0000, 
raw observation next is [20.83333333333334, 95.0, 1.0, 2.0, 0.3159479759907824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500268.3602040446, 500268.3602040446, 167057.7114591704], 
processed observation next is [1.0, 0.9130434782608695, 0.1864139020537128, 0.95, 1.0, 1.0, 0.1758409349286535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13896343339001238, 0.13896343339001238, 0.24933986784950804], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.92690206], dtype=float32), 0.784549]. 
=============================================
[2019-03-27 00:50:38,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:38,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0193
[2019-03-27 00:50:38,123] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7587179448055117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1060370.914289656, 1060370.914289656, 234029.2175515963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3385200.0000, 
sim time next is 3385800.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7724873724178252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079624.607801491, 1079624.607801491, 237268.8230786504], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.7258884005034038, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.299895724389303, 0.299895724389303, 0.3541325717591797], 
reward next is 0.6459, 
noisyNet noise sample is [array([0.40859196], dtype=float32), 0.1426347]. 
=============================================
[2019-03-27 00:50:40,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9522582e-31 1.0000000e+00 0.0000000e+00 4.2827745e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:50:40,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4443
[2019-03-27 00:50:40,301] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.8214801701034867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1180295.802366947, 1180295.802366947, 253793.5278438263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3081600.0000, 
sim time next is 3082200.0000, 
raw observation next is [23.83333333333333, 95.0, 1.0, 2.0, 0.8370665503745157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1203420.733259079, 1203420.73325908, 257979.0417563959], 
processed observation next is [1.0, 0.6956521739130435, 0.32859399684044216, 0.95, 1.0, 1.0, 0.8036946390054406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33428353701641084, 0.3342835370164111, 0.3850433459050685], 
reward next is 0.6150, 
noisyNet noise sample is [array([0.8598307], dtype=float32), -1.1202086]. 
=============================================
[2019-03-27 00:50:47,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:47,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6365
[2019-03-27 00:50:47,531] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.5167947148369516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722148.2976788848, 722148.2976788848, 186269.8897154717], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3189600.0000, 
sim time next is 3190200.0000, 
raw observation next is [25.83333333333334, 94.00000000000001, 1.0, 2.0, 0.5155765045816408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720445.442047145, 720445.442047145, 186072.8021301565], 
processed observation next is [1.0, 0.9565217391304348, 0.42338072669826254, 0.9400000000000002, 1.0, 1.0, 0.4163572344357118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2001237339019847, 0.2001237339019847, 0.2777206001942634], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.06959147], dtype=float32), -0.77717453]. 
=============================================
[2019-03-27 00:50:51,706] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:51,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7855
[2019-03-27 00:50:51,718] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.5209711584512375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727986.2930018047, 727986.2930018047, 186947.8906946398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [29.66666666666667, 72.66666666666667, 1.0, 2.0, 0.5287594422156511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738873.1446716838, 738873.1446716838, 188225.5981751533], 
processed observation next is [0.0, 0.43478260869565216, 0.6050552922590839, 0.7266666666666667, 1.0, 1.0, 0.4322402918260856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20524254018657884, 0.20524254018657884, 0.2809337286196318], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.4936047], dtype=float32), 0.105811335]. 
=============================================
[2019-03-27 00:50:55,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:50:55,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2225
[2019-03-27 00:50:55,864] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.516806662117134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722164.9980060417, 722164.9980060423, 186271.7918337891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3319200.0000, 
sim time next is 3319800.0000, 
raw observation next is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.5209711584512375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727986.2930018047, 727986.2930018047, 186947.8906946398], 
processed observation next is [0.0, 0.43478260869565216, 0.5892575039494474, 0.7333333333333334, 1.0, 1.0, 0.4228568174111295, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20221841472272353, 0.20221841472272353, 0.2790267025293131], 
reward next is 0.7210, 
noisyNet noise sample is [array([-2.5924783], dtype=float32), 0.92843187]. 
=============================================
[2019-03-27 00:51:02,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6457768e-31 1.0000000e+00 0.0000000e+00 9.8465113e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 00:51:02,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7828
[2019-03-27 00:51:02,683] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5074331952403807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709062.5180404293, 709062.5180404286, 184768.9709951507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3460200.0000, 
sim time next is 3460800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5068390298243134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708231.9833506766, 708231.983350676, 184674.5551597565], 
processed observation next is [1.0, 0.043478260869565216, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.4058301564148354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19673110648629905, 0.1967311064862989, 0.275633664417547], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.40899727], dtype=float32), -0.08945456]. 
=============================================
[2019-03-27 00:51:04,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6513489e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:51:04,321] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7468
[2019-03-27 00:51:04,331] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2231116.907450256 W.
[2019-03-27 00:51:04,335] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.66666666666667, 71.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.006241701506255, 6.9112, 168.9071783790306, 2231116.907450256, 1454283.21061332, 311348.0615897746], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3486000.0000, 
sim time next is 3486600.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.7396185807149555, 1.0, 1.0, 0.7396185807149555, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2068317.66410907, 2068317.66410907, 391537.1125397339], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.6862874466445247, 1.0, 0.5, 0.6862874466445247, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5745326844747417, 0.5745326844747417, 0.5843837500593043], 
reward next is 0.4156, 
noisyNet noise sample is [array([-0.20586297], dtype=float32), 0.21494155]. 
=============================================
[2019-03-27 00:51:06,610] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:51:06,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2442
[2019-03-27 00:51:06,626] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5571405342094552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778546.5954261825, 778546.5954261832, 193033.5263727024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3524400.0000, 
sim time next is 3525000.0000, 
raw observation next is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.5581109252399513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779903.114912132, 779903.114912132, 193201.9338440468], 
processed observation next is [1.0, 0.8260869565217391, 0.6129541864139019, 0.7566666666666667, 1.0, 1.0, 0.46760352438548347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2166397541422589, 0.2166397541422589, 0.2883610952896221], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.5041172], dtype=float32), -0.16349481]. 
=============================================
[2019-03-27 00:51:06,642] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[62.874695]
 [63.09743 ]
 [63.02732 ]
 [62.791374]
 [62.732597]], R is [[63.04354477]
 [63.125     ]
 [63.20575714]
 [63.2856102 ]
 [63.36426163]].
[2019-03-27 00:51:17,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7277673e-28 1.0000000e+00 3.7892276e-35 7.4110669e-32 7.7713392e-36], sum to 1.0000
[2019-03-27 00:51:17,720] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0274
[2019-03-27 00:51:17,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1973451.100500579 W.
[2019-03-27 00:51:17,736] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 67.66666666666667, 1.0, 2.0, 0.7702593874822345, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.973159454451982, 6.9112, 168.9125490910939, 1973451.100500579, 1929495.037055113, 401745.7783189053], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3748800.0000, 
sim time next is 3749400.0000, 
raw observation next is [30.0, 66.5, 1.0, 2.0, 0.4545303435803575, 1.0, 1.0, 0.4545303435803575, 1.0, 2.0, 0.7758180408585219, 6.9112, 6.9112, 170.5573041426782, 1906473.594589637, 1906473.594589637, 382908.3210679595], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.665, 1.0, 1.0, 0.34280764286790055, 1.0, 0.5, 0.34280764286790055, 1.0, 1.0, 0.7266073669006363, 0.0, 0.0, 0.8375144448122397, 0.5295759984971213, 0.5295759984971213, 0.57150495681785], 
reward next is 0.4285, 
noisyNet noise sample is [array([-0.95890623], dtype=float32), -0.41937596]. 
=============================================
[2019-03-27 00:51:18,910] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:51:18,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6449
[2019-03-27 00:51:18,923] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.4515345951764086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645471.6100356221, 645471.6100356228, 178220.9800824272], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3721200.0000, 
sim time next is 3721800.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.4509556509691987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 644644.5115775216, 644644.5115775221, 178136.6356711813], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.74, 1.0, 1.0, 0.33850078430023944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17906791988264487, 0.17906791988264503, 0.26587557562862885], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.08656029], dtype=float32), 1.0520695]. 
=============================================
[2019-03-27 00:51:21,149] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 00:51:21,150] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:51:21,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:51:21,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:51:21,152] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:51:21,153] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:51:21,154] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:51:21,155] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:51:21,157] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:51:21,152] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:51:21,161] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:51:21,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-27 00:51:21,197] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-27 00:51:21,215] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-27 00:51:21,216] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-27 00:51:21,233] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-27 00:51:23,687] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:51:23,689] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.8, 96.0, 1.0, 2.0, 0.5326670631901701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793309.4376163457, 793309.4376163457, 194955.4486916788]
[2019-03-27 00:51:23,691] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:51:23,694] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.5298735e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6931857123699968
[2019-03-27 00:51:29,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:51:29,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.52945928666667, 78.39768975333334, 1.0, 2.0, 0.214682513978315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 358841.1565045965, 358841.1565045971, 156592.8354431126]
[2019-03-27 00:51:29,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:51:29,511] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.45925760023093976
[2019-03-27 00:51:52,247] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:51:52,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.03478063, 78.79486094, 1.0, 2.0, 0.3497903336119054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547332.0817042285, 547332.0817042291, 170630.1027289862]
[2019-03-27 00:51:52,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:51:52,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5104650738208713
[2019-03-27 00:51:57,355] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:51:57,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.66957586666666, 78.58057308333335, 1.0, 2.0, 0.7172714048062082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1002418.61647118, 1002418.61647118, 224628.2476681204]
[2019-03-27 00:51:57,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:51:57,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06566950208118194
[2019-03-27 00:52:24,649] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:52:24,650] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.2, 64.66666666666666, 1.0, 2.0, 0.8930045303884699, 1.0, 2.0, 0.8930045303884699, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2497675.523849052, 2497675.523849052, 468169.7256868795]
[2019-03-27 00:52:24,650] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:52:24,652] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0498201e-29 1.0000000e+00 5.6794089e-37 5.6604920e-34 1.5403695e-37], sampled 0.5198172562076333
[2019-03-27 00:52:24,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2497675.523849052 W.
[2019-03-27 00:52:26,471] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:52:26,472] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.36666666666667, 45.33333333333333, 1.0, 2.0, 0.7351912012316569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027474.442468206, 1027474.442468206, 228628.8587755238]
[2019-03-27 00:52:26,472] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:52:26,474] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5812645807441004
[2019-03-27 00:52:43,584] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:52:43,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.7, 91.33333333333334, 1.0, 2.0, 0.5830788218620951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814806.5871604229, 814806.5871604229, 197635.1711748453]
[2019-03-27 00:52:43,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:52:43,588] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43504721156710546
[2019-03-27 00:52:57,166] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:52:57,167] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.33467829333333, 93.23013567833334, 1.0, 2.0, 0.463754661385031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665717.3244962468, 665717.3244962468, 180377.7228790878]
[2019-03-27 00:52:57,167] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:52:57,170] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7296250129696402
[2019-03-27 00:53:11,514] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:53:11,516] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.43138648, 83.94832362333334, 1.0, 2.0, 0.5228543424865779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730618.6913865543, 730618.6913865537, 187254.5487606791]
[2019-03-27 00:53:11,518] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:53:11,522] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5754874502105114
[2019-03-27 00:53:14,614] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:53:15,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:53:15,570] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:53:15,593] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:53:15,644] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04436522]
[2019-03-27 00:53:15,644] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.53333333333333, 70.33333333333334, 1.0, 2.0, 0.3408091458304298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535413.4933574646, 535413.4933574651, 169705.0067497288]
[2019-03-27 00:53:15,644] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:53:15,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6699020379433085
[2019-03-27 00:53:15,678] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:53:16,694] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 525000, evaluation results [525000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:53:20,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:53:20,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5425
[2019-03-27 00:53:20,610] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5363314133705559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749457.7307579762, 749457.7307579762, 189484.2553356457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5360902168233459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749120.569155017, 749120.5691550176, 189443.8925984801], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4410725503895733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20808904698750474, 0.20808904698750488, 0.2827520785051942], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.8304578], dtype=float32), 0.10041405]. 
=============================================
[2019-03-27 00:53:29,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:53:29,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4867
[2019-03-27 00:53:29,053] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.66666666666667, 62.5, 1.0, 2.0, 0.615529330686184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860171.9717559528, 860171.9717559528, 203674.4557406329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3953400.0000, 
sim time next is 3954000.0000, 
raw observation next is [33.33333333333334, 65.0, 1.0, 2.0, 0.6086731510854085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850586.961389273, 850586.961389273, 202373.494847292], 
processed observation next is [0.0, 0.782608695652174, 0.7788309636650873, 0.65, 1.0, 1.0, 0.5285218687776007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23627415594146473, 0.23627415594146473, 0.30204999230939106], 
reward next is 0.6980, 
noisyNet noise sample is [array([-0.1552943], dtype=float32), -0.3355352]. 
=============================================
[2019-03-27 00:53:29,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.58184 ]
 [64.648636]
 [64.59731 ]
 [64.58733 ]
 [64.55639 ]], R is [[64.63362885]
 [64.68330383]
 [64.7374115 ]
 [64.78999329]
 [64.84107208]].
[2019-03-27 00:53:30,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3849497e-22 1.0000000e+00 4.0353213e-27 1.4490027e-24 7.4817554e-28], sum to 1.0000
[2019-03-27 00:53:30,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7817
[2019-03-27 00:53:30,575] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 84.0, 1.0, 2.0, 0.5484477407206233, 1.0, 1.0, 0.5484477407206233, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1533332.544809317, 1533332.544809317, 316001.133336123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4002600.0000, 
sim time next is 4003200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 1.014154376171154, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564308863, 1417602.724084382, 1417602.724084382, 303271.6671898391], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.84, 1.0, 1.0, 1.0170534652664507, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399447617018, 0.39377853446788386, 0.39377853446788386, 0.4526442793878196], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09029045], dtype=float32), -1.9383823]. 
=============================================
[2019-03-27 00:53:32,857] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1712803e-25 1.0000000e+00 3.9268996e-32 7.4118662e-30 3.3972189e-31], sum to 1.0000
[2019-03-27 00:53:32,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4942
[2019-03-27 00:53:32,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2445382.085635814 W.
[2019-03-27 00:53:32,883] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 75.83333333333334, 1.0, 2.0, 0.5828763145234007, 1.0, 2.0, 0.5828763145234007, 1.0, 2.0, 1.012263398184386, 6.911200000000001, 6.9112, 170.5573041426782, 2445382.085635814, 2445382.085635813, 477186.9347833343], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4035000.0000, 
sim time next is 4035600.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5447199753241531, 1.0, 2.0, 0.5447199753241531, 1.0, 2.0, 0.9459984554894884, 6.9112, 6.9112, 170.5573041426782, 2285155.742447306, 2285155.742447306, 447449.0263027181], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.4514698497881362, 1.0, 1.0, 0.4514698497881362, 1.0, 1.0, 0.93414445791401, 0.0, 0.0, 0.8375144448122397, 0.6347654840131406, 0.6347654840131406, 0.6678343676159971], 
reward next is 0.3322, 
noisyNet noise sample is [array([-0.7676561], dtype=float32), 1.0204594]. 
=============================================
[2019-03-27 00:53:44,358] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4972627e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:53:44,368] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9101
[2019-03-27 00:53:44,372] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.6011041619487852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 840005.536335655, 840005.5363356543, 200951.8184427393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5995710417833949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837862.2515906361, 837862.2515906361, 200666.1458993476], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5175554720281866, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23273951433073226, 0.23273951433073226, 0.29950171029753375], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.0899045], dtype=float32), 0.986117]. 
=============================================
[2019-03-27 00:53:49,034] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.8659185e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:53:49,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1191
[2019-03-27 00:53:49,048] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.0, 50.0, 1.0, 2.0, 0.5709728603557979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 797883.1084150588, 797883.1084150594, 195464.1266514715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4300200.0000, 
sim time next is 4300800.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.5697883846480478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796227.289987232, 796227.2899872327, 195253.9263856291], 
processed observation next is [1.0, 0.782608695652174, 0.9052132701421801, 0.5, 1.0, 1.0, 0.4816727525880094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22117424721867554, 0.22117424721867573, 0.29142377072481956], 
reward next is 0.7086, 
noisyNet noise sample is [array([-1.3235402], dtype=float32), 0.7960807]. 
=============================================
[2019-03-27 00:54:05,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:54:05,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0904
[2019-03-27 00:54:05,783] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.54855875032214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766550.1078980914, 766550.1078980914, 191553.7051416827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4574400.0000, 
sim time next is 4575000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5492822316496979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767561.4582137185, 767561.4582137185, 191677.4982692783], 
processed observation next is [0.0, 0.9565217391304348, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45696654415626253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21321151617047737, 0.21321151617047737, 0.2860858183123557], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.4797698], dtype=float32), 0.4384088]. 
=============================================
[2019-03-27 00:54:05,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[58.456703]
 [58.52939 ]
 [58.614155]
 [58.704926]
 [58.811127]], R is [[58.52007294]
 [58.64897156]
 [58.77679062]
 [58.90350342]
 [59.02907944]].
[2019-03-27 00:54:12,575] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8380616e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:54:12,586] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9239
[2019-03-27 00:54:12,593] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5171077194339162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722585.8265663391, 722585.8265663391, 186319.9044311771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4745400.0000, 
sim time next is 4746000.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.516402066171056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721599.4397520293, 721599.4397520293, 186205.7922090994], 
processed observation next is [1.0, 0.9565217391304348, 0.5102685624012641, 0.8066666666666668, 1.0, 1.0, 0.4173518869530795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20044428882000812, 0.20044428882000812, 0.2779190928494021], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.46977752], dtype=float32), 1.7945985]. 
=============================================
[2019-03-27 00:54:12,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[59.15754 ]
 [59.18223 ]
 [59.178886]
 [59.345932]
 [59.586544]], R is [[59.25021744]
 [59.37962723]
 [59.50749588]
 [59.63336182]
 [59.75770187]].
[2019-03-27 00:54:12,730] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:54:12,731] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:54:12,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:54:12,732] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:54:12,733] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:54:12,735] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:54:12,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:54:12,736] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:54:12,738] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:54:12,737] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:54:12,742] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:54:12,760] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-27 00:54:12,777] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-27 00:54:12,795] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-27 00:54:12,796] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-27 00:54:12,837] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-27 00:54:24,614] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05021299]
[2019-03-27 00:54:24,614] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.18810694833333, 46.22957612499999, 1.0, 2.0, 0.2807738243758175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 465269.80766472, 465269.8076647206, 163949.3328071453]
[2019-03-27 00:54:24,615] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:54:24,620] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18266922318205148
[2019-03-27 00:54:36,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05021299]
[2019-03-27 00:54:36,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.08347030333334, 51.41765997333333, 1.0, 2.0, 0.3479533977347283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542926.0293049713, 542926.0293049713, 170237.2343196602]
[2019-03-27 00:54:36,310] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:54:36,314] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5493240778594067
[2019-03-27 00:54:43,009] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05021299]
[2019-03-27 00:54:43,013] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.35, 95.0, 1.0, 2.0, 0.3772051679630284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 572902.9480273194, 572902.94802732, 172386.2797510883]
[2019-03-27 00:54:43,014] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:54:43,018] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33269727184722186
[2019-03-27 00:55:06,269] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05021299]
[2019-03-27 00:55:06,270] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.7, 70.66666666666667, 1.0, 2.0, 0.5679287518978273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793627.6520306073, 793627.6520306073, 194922.8084241151]
[2019-03-27 00:55:06,272] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:55:06,275] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.41651540261119446
[2019-03-27 00:55:52,235] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05021299]
[2019-03-27 00:55:52,237] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.3, 95.0, 1.0, 2.0, 0.4339660721759252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 637861.2179584355, 637861.2179584362, 177896.3554635614]
[2019-03-27 00:55:52,238] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:55:52,243] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9647355584862148
[2019-03-27 00:56:07,141] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:56:07,279] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:56:07,538] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:56:07,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:56:07,700] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:56:08,713] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 550000, evaluation results [550000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:56:12,715] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6958115e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:56:12,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7629
[2019-03-27 00:56:12,733] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6636247599018241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 927412.3136915882, 927412.3136915877, 213186.2250186434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4771800.0000, 
sim time next is 4772400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6051795553587257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845702.911507406, 845702.911507406, 201706.3067884783], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5243127172996694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2349174754187239, 0.2349174754187239, 0.30105418923653476], 
reward next is 0.6989, 
noisyNet noise sample is [array([1.4740276], dtype=float32), 0.6027078]. 
=============================================
[2019-03-27 00:56:16,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:56:16,284] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3933
[2019-03-27 00:56:16,292] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5084562045137168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710492.4996086985, 710492.4996086991, 184933.4620825247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4816800.0000, 
sim time next is 4817400.0000, 
raw observation next is [29.83333333333333, 70.0, 1.0, 2.0, 0.5105331950420744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713395.7618843167, 713395.7618843167, 185264.1819901212], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7, 1.0, 1.0, 0.4102809578820173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1981654894123102, 0.1981654894123102, 0.27651370446286744], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.22952805], dtype=float32), -0.47888926]. 
=============================================
[2019-03-27 00:56:17,439] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.8491170e-36 1.0000000e+00 0.0000000e+00 4.1371908e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 00:56:17,450] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9222
[2019-03-27 00:56:17,457] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.6804918828114739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 950994.5880500379, 950994.5880500379, 216689.0901414799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4858200.0000, 
sim time next is 4858800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7017733064338769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980749.3349556434, 980749.3349556434, 221226.7558547518], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.84, 1.0, 1.0, 0.6406907306432252, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27243037082101207, 0.27243037082101207, 0.3301891878429131], 
reward next is 0.6698, 
noisyNet noise sample is [array([-0.33645892], dtype=float32), -0.955979]. 
=============================================
[2019-03-27 00:56:18,444] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:56:18,453] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2482
[2019-03-27 00:56:18,458] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.8029879221334711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1122274.554489039, 1122274.554489039, 244644.3791342582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4867800.0000, 
sim time next is 4868400.0000, 
raw observation next is [28.33333333333334, 77.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.987081761574328, 6.9112, 168.9126192848853, 1507624.818489466, 1453791.792654888, 311349.3201276031], 
processed observation next is [1.0, 0.34782608695652173, 0.5418641390205374, 0.7733333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.007588176157432791, 0.0, 0.8294382892204908, 0.41878467180262946, 0.4038310535152467, 0.46470047780239265], 
reward next is 0.1559, 
noisyNet noise sample is [array([0.18364158], dtype=float32), -1.1678462]. 
=============================================
[2019-03-27 00:56:27,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:56:27,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1557
[2019-03-27 00:56:27,293] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4774453943239098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667270.4197075511, 667270.4197075511, 180153.3690864326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5016000.0000, 
sim time next is 5016600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4769108836161082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666526.0922993605, 666526.0922993605, 180073.6285332501], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3697721489350701, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1851461367498224, 0.1851461367498224, 0.26876660975111955], 
reward next is 0.7312, 
noisyNet noise sample is [array([-1.9907767], dtype=float32), 0.076987706]. 
=============================================
[2019-03-27 00:56:30,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:56:30,202] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0515
[2019-03-27 00:56:30,207] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4805817283252084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671654.9019920448, 671654.9019920448, 180625.1653267668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5114400.0000, 
sim time next is 5115000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4807540498853487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671896.143399213, 671896.143399213, 180651.216108518], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.374402469741384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1866378176108925, 0.1866378176108925, 0.2696286807589821], 
reward next is 0.7304, 
noisyNet noise sample is [array([1.6113214], dtype=float32), 0.6923781]. 
=============================================
[2019-03-27 00:56:30,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.608  ]
 [74.81343]
 [74.88198]
 [75.00568]
 [75.02043]], R is [[74.58201599]
 [74.56660461]
 [74.55114746]
 [74.5356369 ]
 [74.52013397]].
[2019-03-27 00:56:30,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:56:30,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0730
[2019-03-27 00:56:30,710] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.00000000000001, 1.0, 2.0, 0.517809345509258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723566.5849451869, 723566.5849451869, 186433.9142561213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5052000.0000, 
sim time next is 5052600.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5180843330622806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723950.9727686801, 723950.9727686801, 186478.4444092323], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.63, 1.0, 1.0, 0.4193787145328682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20109749243574446, 0.20109749243574446, 0.27832603643169], 
reward next is 0.7217, 
noisyNet noise sample is [array([0.9971099], dtype=float32), 0.4097815]. 
=============================================
[2019-03-27 00:56:36,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6224871e-27 1.0000000e+00 2.1492509e-34 2.7223053e-32 5.5957649e-33], sum to 1.0000
[2019-03-27 00:56:36,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4513
[2019-03-27 00:56:36,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3152154.215601694 W.
[2019-03-27 00:56:36,806] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.58333333333334, 59.16666666666666, 1.0, 2.0, 0.8610775410125504, 1.0, 2.0, 0.7511288100205378, 1.0, 1.0, 1.03, 7.005110436409119, 6.9112, 170.5573041426782, 3152154.215601694, 3084882.349969526, 577034.9510085251], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5485800.0000, 
sim time next is 5486400.0000, 
raw observation next is [35.7, 58.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.251493155923807, 6.9112, 170.5573041426782, 3153379.508733856, 2909613.678905067, 551856.4436762553], 
processed observation next is [1.0, 0.5217391304347826, 0.8909952606635072, 0.58, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.03402931559238072, 0.0, 0.8375144448122397, 0.8759387524260711, 0.8082260219180741, 0.8236663338451572], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5026536], dtype=float32), 0.68895334]. 
=============================================
[2019-03-27 00:56:38,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.2210136e-22 1.0000000e+00 2.1234424e-26 6.2174861e-23 3.8245773e-26], sum to 1.0000
[2019-03-27 00:56:38,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0728
[2019-03-27 00:56:38,207] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.9637880434295782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565103987, 1347155.112486669, 1347155.112486669, 288084.2290091156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5195400.0000, 
sim time next is 5196000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.951425839444459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1329864.757793406, 1329864.757793406, 284472.7960313506], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.9414769149933241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.369406877164835, 0.369406877164835, 0.42458626273335914], 
reward next is 0.5754, 
noisyNet noise sample is [array([0.12414333], dtype=float32), 2.6154702]. 
=============================================
[2019-03-27 00:56:38,231] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[35.52213 ]
 [35.169018]
 [34.999905]
 [35.045532]
 [36.408943]], R is [[35.44958115]
 [35.66510773]
 [35.30845642]
 [34.95537186]
 [34.6058197 ]].
[2019-03-27 00:56:38,298] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:56:38,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3975
[2019-03-27 00:56:38,312] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5217247307778852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729039.6691264116, 729039.669126411, 187069.6609698313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5175600.0000, 
sim time next is 5176200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5233596320074735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731325.008657745, 731325.0086577456, 187336.6337033267], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42573449639454636, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2031458357382625, 0.20314583573826264, 0.27960691597511445], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.6000264], dtype=float32), 1.337364]. 
=============================================
[2019-03-27 00:56:44,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0819413e-25 1.0000000e+00 6.3925978e-30 1.5544380e-27 2.4160199e-31], sum to 1.0000
[2019-03-27 00:56:44,704] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6288
[2019-03-27 00:56:44,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2821306.502373648 W.
[2019-03-27 00:56:44,720] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.4, 51.0, 1.0, 2.0, 1.008569759180381, 1.0, 2.0, 1.008569759180381, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2821306.502373648, 2821306.502373648, 534190.522804657], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5310000.0000, 
sim time next is 5310600.0000, 
raw observation next is [36.38333333333333, 51.16666666666666, 1.0, 2.0, 1.03189002128528, 1.0, 2.0, 1.03189002128528, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2886616.438374132, 2886616.438374132, 548559.5484095606], 
processed observation next is [1.0, 0.4782608695652174, 0.9233807266982622, 0.5116666666666666, 1.0, 1.0, 1.0384217123919035, 1.0, 1.0, 1.0384217123919035, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.8018378995483699, 0.8018378995483699, 0.8187455946411352], 
reward next is 0.1813, 
noisyNet noise sample is [array([-0.15942425], dtype=float32), -0.42935982]. 
=============================================
[2019-03-27 00:56:46,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7083753e-20 1.0000000e+00 1.0345802e-24 1.2847293e-25 8.2538658e-26], sum to 1.0000
[2019-03-27 00:56:46,952] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8659
[2019-03-27 00:56:46,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3369794.283089598 W.
[2019-03-27 00:56:46,968] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.9, 54.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.553253162810135, 6.9112, 170.5573041426782, 3369794.283089598, 2909865.485515855, 550120.5933535284], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5331600.0000, 
sim time next is 5332200.0000, 
raw observation next is [35.65, 55.16666666666666, 1.0, 2.0, 0.3267597885581237, 1.0, 2.0, 0.3267597885581237, 1.0, 1.0, 0.567473691612114, 6.9112, 6.9112, 170.5573041426782, 1370212.557546346, 1370212.557546346, 316053.773533994], 
processed observation next is [1.0, 0.7391304347826086, 0.8886255924170615, 0.5516666666666665, 1.0, 1.0, 0.18886721513026952, 1.0, 1.0, 0.18886721513026952, 1.0, 0.5, 0.47252889220989513, 0.0, 0.0, 0.8375144448122397, 0.38061459931842945, 0.38061459931842945, 0.4717220500507373], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5903338], dtype=float32), 1.7268897]. 
=============================================
[2019-03-27 00:56:51,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6866484e-15 1.0000000e+00 1.7655678e-18 8.8864564e-15 3.0557055e-18], sum to 1.0000
[2019-03-27 00:56:51,322] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2792
[2019-03-27 00:56:51,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3500265.869040178 W.
[2019-03-27 00:56:51,334] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.95, 57.0, 1.0, 2.0, 1.026748829806867, 1.0, 2.0, 0.833964454417696, 1.0, 2.0, 1.03, 7.005123510152987, 6.9112, 170.5573041426782, 3500265.869040178, 3432984.638153472, 644429.626689133], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5405400.0000, 
sim time next is 5406000.0000, 
raw observation next is [36.93333333333333, 56.0, 1.0, 2.0, 0.9494214637004362, 1.0, 2.0, 0.7953007713644805, 1.0, 2.0, 1.03, 7.00511740705343, 6.9112, 170.5573041426782, 3337772.063755096, 3270495.204766861, 611695.9996017803], 
processed observation next is [1.0, 0.5652173913043478, 0.9494470774091626, 0.56, 1.0, 1.0, 0.9390620044583569, 1.0, 1.0, 0.7533744233306994, 1.0, 1.0, 1.0365853658536586, 0.009391740705343033, 0.0, 0.8375144448122397, 0.9271589065986378, 0.908470890213017, 0.9129791038832542], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41026872], dtype=float32), 0.94036436]. 
=============================================
[2019-03-27 00:56:51,350] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[24.002771]
 [23.531351]
 [24.332546]
 [25.466887]
 [26.870695]], R is [[24.43952751]
 [24.19513321]
 [23.95318222]
 [23.71364975]
 [23.47651291]].
[2019-03-27 00:56:51,826] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5581398e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:56:51,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0378
[2019-03-27 00:56:51,844] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.8, 53.33333333333334, 1.0, 2.0, 0.5354331742055592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748202.109028389, 748202.1090283897, 189334.8043667382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5748600.0000, 
sim time next is 5749200.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5386016581491159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752631.2448169832, 752631.2448169838, 189866.068643478], 
processed observation next is [0.0, 0.5652173913043478, 0.8104265402843602, 0.53, 1.0, 1.0, 0.44409838331218776, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20906423467138424, 0.20906423467138438, 0.28338219200519105], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.31351042], dtype=float32), 0.40958488]. 
=============================================
[2019-03-27 00:56:56,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.8423638e-22 1.0000000e+00 1.3929049e-28 1.0769553e-24 4.3669047e-26], sum to 1.0000
[2019-03-27 00:56:56,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3664
[2019-03-27 00:56:56,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2777484.099467573 W.
[2019-03-27 00:56:56,623] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.83333333333334, 50.66666666666666, 1.0, 2.0, 0.6827150959362877, 1.0, 2.0, 0.6619475874824063, 1.0, 1.0, 1.03, 7.005096369296965, 6.9112, 170.5573041426782, 2777484.099467573, 2710222.310680056, 515924.1104216108], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5503800.0000, 
sim time next is 5504400.0000, 
raw observation next is [34.7, 51.0, 1.0, 2.0, 0.9905411239453595, 1.0, 2.0, 0.9905411239453595, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2770818.457447494, 2770818.457447493, 523290.8512616847], 
processed observation next is [1.0, 0.7391304347826086, 0.8436018957345973, 0.51, 1.0, 1.0, 0.9886037637895898, 1.0, 1.0, 0.9886037637895898, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7696717937354149, 0.7696717937354147, 0.7810311212860965], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17932682], dtype=float32), -1.8523786]. 
=============================================
[2019-03-27 00:56:58,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.424702e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 00:56:58,809] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5886
[2019-03-27 00:56:58,813] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.13333333333333, 85.0, 1.0, 2.0, 0.5625300381840287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786080.6597273023, 786080.6597273023, 193972.4697935905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5523600.0000, 
sim time next is 5524200.0000, 
raw observation next is [28.0, 85.5, 1.0, 2.0, 0.5598755479021689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782369.903309312, 782369.903309312, 193508.456325155], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.855, 1.0, 1.0, 0.4697295757857456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21732497314147553, 0.21732497314147553, 0.2888185915300821], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.3508006], dtype=float32), 0.8121408]. 
=============================================
[2019-03-27 00:57:01,014] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4588059e-22 1.0000000e+00 4.0104760e-27 1.1449314e-24 6.2955464e-25], sum to 1.0000
[2019-03-27 00:57:01,024] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9508
[2019-03-27 00:57:01,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2458257.347121953 W.
[2019-03-27 00:57:01,036] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.08333333333334, 69.16666666666667, 1.0, 2.0, 0.5859422199745288, 1.0, 2.0, 0.5859422199745288, 1.0, 1.0, 1.01758786204257, 6.9112, 6.9112, 170.5573041426782, 2458257.347121953, 2458257.347121953, 479662.5426111185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5565000.0000, 
sim time next is 5565600.0000, 
raw observation next is [31.3, 68.0, 1.0, 2.0, 0.8442458916323584, 1.0, 2.0, 0.8442458916323584, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2361203.408443033, 2361203.408443033, 441987.8256703334], 
processed observation next is [1.0, 0.43478260869565216, 0.6824644549763034, 0.68, 1.0, 1.0, 0.8123444477498294, 1.0, 1.0, 0.8123444477498294, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6558898356786204, 0.6558898356786204, 0.65968332189602], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0375534], dtype=float32), -0.32367304]. 
=============================================
[2019-03-27 00:57:04,710] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 00:57:04,711] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:57:04,712] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:57:04,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:57:04,713] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:57:04,713] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:57:04,714] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:57:04,715] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:57:04,716] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:57:04,717] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:57:04,718] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:57:04,740] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-27 00:57:04,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-27 00:57:04,776] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-27 00:57:04,777] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-27 00:57:04,812] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-27 00:57:11,738] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:57:11,741] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.36088029666667, 72.156601735, 1.0, 2.0, 0.2693865164790597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 445019.6311206885, 445019.6311206879, 162822.7589328457]
[2019-03-27 00:57:11,741] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:57:11,745] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2340091183797487
[2019-03-27 00:57:35,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:57:35,540] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.4, 78.33333333333334, 1.0, 2.0, 0.4974528270737252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695111.873042446, 695111.8730424466, 183197.6682008099]
[2019-03-27 00:57:35,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:57:35,544] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47007744269285623
[2019-03-27 00:57:54,069] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:57:54,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.33333333333333, 57.66666666666667, 1.0, 2.0, 0.568539845456947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794481.9178559047, 794481.9178559047, 195030.8973803966]
[2019-03-27 00:57:54,071] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:57:54,075] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7779983898320445
[2019-03-27 00:58:04,290] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:58:04,291] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.6, 74.5, 1.0, 2.0, 0.5886348347451222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822573.682632935, 822573.682632935, 198647.3102874189]
[2019-03-27 00:58:04,294] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:58:04,296] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2603866617666736
[2019-03-27 00:58:06,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:58:06,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.83333333333333, 72.33333333333334, 1.0, 2.0, 1.033192352182948, 1.0, 2.0, 1.033192352182948, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2890263.802837834, 2890263.802837834, 549378.083009223]
[2019-03-27 00:58:06,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:58:06,122] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.6508390e-25 1.0000000e+00 2.4354148e-31 1.8862652e-28 9.8810513e-30], sampled 0.4062336553974194
[2019-03-27 00:58:06,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2890263.802837834 W.
[2019-03-27 00:58:08,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:58:08,703] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.78333333333334, 48.66666666666667, 1.0, 2.0, 0.8871924876961432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1240029.530630362, 1240029.530630361, 266443.9242371136]
[2019-03-27 00:58:08,704] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:58:08,706] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.390881e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.8400548854695102
[2019-03-27 00:58:30,978] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.050894175]
[2019-03-27 00:58:30,978] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.67457218166667, 70.53945108166667, 1.0, 2.0, 0.7463840444048422, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.999726997782253, 6.9112, 168.9115295735508, 1940038.48570296, 1877234.91716635, 395267.1675831853]
[2019-03-27 00:58:30,981] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:58:30,984] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7450449e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.038689303248243934
[2019-03-27 00:58:30,985] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1940038.48570296 W.
[2019-03-27 00:58:59,182] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:58:59,516] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:58:59,680] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:58:59,828] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:58:59,833] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:59:00,847] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 575000, evaluation results [575000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:59:06,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:59:06,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3998
[2019-03-27 00:59:06,026] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 73.0, 1.0, 2.0, 0.5221660230982357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729656.5271813696, 729656.527181369, 187141.9440846388], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5732400.0000, 
sim time next is 5733000.0000, 
raw observation next is [29.25, 72.0, 1.0, 2.0, 0.5227318440542547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730447.4574323051, 730447.4574323044, 187234.2666590692], 
processed observation next is [0.0, 0.34782608695652173, 0.5853080568720379, 0.72, 1.0, 1.0, 0.42497812536657187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20290207150897363, 0.20290207150897344, 0.27945412934189434], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.36399016], dtype=float32), -0.68423736]. 
=============================================
[2019-03-27 00:59:06,043] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.64625 ]
 [70.609055]
 [70.56357 ]
 [70.470634]
 [70.45892 ]], R is [[70.70664215]
 [70.72025299]
 [70.73397827]
 [70.74689484]
 [70.75974274]].
[2019-03-27 00:59:13,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2973027e-17 1.0000000e+00 1.7705127e-24 1.1985189e-17 2.2871217e-21], sum to 1.0000
[2019-03-27 00:59:13,844] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2192
[2019-03-27 00:59:13,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2703078.319944326 W.
[2019-03-27 00:59:13,855] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333334, 70.66666666666667, 1.0, 2.0, 0.6472877108519784, 1.0, 2.0, 0.6442338949402517, 1.0, 1.0, 1.03, 7.005093576209164, 6.9112, 170.5573041426782, 2703078.319944326, 2635818.531959216, 505195.7918960738], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5916000.0000, 
sim time next is 5916600.0000, 
raw observation next is [31.7, 71.0, 1.0, 2.0, 0.6528700695695964, 1.0, 2.0, 0.6470250742990606, 1.0, 2.0, 1.03, 7.005094016299115, 6.9112, 170.5573041426782, 2714802.282381809, 2647542.179142366, 506855.4831494813], 
processed observation next is [1.0, 0.4782608695652174, 0.7014218009478673, 0.71, 1.0, 1.0, 0.5817711681561402, 1.0, 1.0, 0.5747290051795911, 1.0, 1.0, 1.0365853658536586, 0.009389401629911465, 0.0, 0.8375144448122397, 0.754111745106058, 0.7354283830951017, 0.7565007211186288], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.90118164], dtype=float32), -0.38908774]. 
=============================================
[2019-03-27 00:59:13,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3009291e-27 1.0000000e+00 2.9852832e-34 4.6296163e-31 4.3306090e-33], sum to 1.0000
[2019-03-27 00:59:13,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1833
[2019-03-27 00:59:13,973] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2013776.096405745 W.
[2019-03-27 00:59:13,980] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.71666666666667, 74.5, 1.0, 2.0, 0.7201331420160927, 1.0, 2.0, 0.7201331420160927, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2013776.096405745, 2013776.096405745, 382863.708203338], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6177000.0000, 
sim time next is 6177600.0000, 
raw observation next is [29.8, 74.0, 1.0, 2.0, 0.4900895094979384, 1.0, 2.0, 0.4900895094979384, 1.0, 1.0, 0.8511234029204757, 6.9112, 6.9112, 170.5573041426782, 2055765.396367466, 2055765.396367466, 408359.775370736], 
processed observation next is [1.0, 0.5217391304347826, 0.6113744075829385, 0.74, 1.0, 1.0, 0.38565001144329925, 1.0, 1.0, 0.38565001144329925, 1.0, 0.5, 0.8184431742932631, 0.0, 0.0, 0.8375144448122397, 0.5710459434354073, 0.5710459434354073, 0.6094922020458746], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.3067513], dtype=float32), -0.789374]. 
=============================================
[2019-03-27 00:59:31,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8059220e-27 1.0000000e+00 1.5813990e-36 3.9571594e-30 4.5429471e-31], sum to 1.0000
[2019-03-27 00:59:31,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9400
[2019-03-27 00:59:31,951] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.0, 1.0, 2.0, 0.7153428128753893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 999722.0550100794, 999722.0550100801, 224192.0248112483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6147600.0000, 
sim time next is 6148200.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.7062129601582489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 986956.7678765169, 986956.7678765175, 222192.2251174253], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.6460397110340348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2741546577434769, 0.2741546577434771, 0.3316301867424258], 
reward next is 0.6684, 
noisyNet noise sample is [array([1.0364406], dtype=float32), -1.5024967]. 
=============================================
[2019-03-27 00:59:39,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:59:39,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4305
[2019-03-27 00:59:39,556] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.96666666666667, 70.5, 1.0, 2.0, 0.5333704921153345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745318.7528925957, 745318.7528925957, 188990.0026327585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6342600.0000, 
sim time next is 6343200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.5352173682689337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747900.4405693329, 747900.4405693322, 189298.1262683131], 
processed observation next is [0.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.4400209256252213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20775012238037024, 0.20775012238037005, 0.28253451681837777], 
reward next is 0.7175, 
noisyNet noise sample is [array([-1.5549617], dtype=float32), -0.25552002]. 
=============================================
[2019-03-27 00:59:40,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 00:59:40,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2522
[2019-03-27 00:59:40,468] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333333, 74.83333333333333, 1.0, 2.0, 0.5215258195607643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728761.6222872569, 728761.6222872563, 187037.6384086809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6288600.0000, 
sim time next is 6289200.0000, 
raw observation next is [28.7, 76.0, 1.0, 2.0, 0.5235603367219033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731605.5632036064, 731605.5632036064, 187370.0243356544], 
processed observation next is [0.0, 0.8260869565217391, 0.5592417061611374, 0.76, 1.0, 1.0, 0.4259763093034979, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20322376755655733, 0.20322376755655733, 0.2796567527397827], 
reward next is 0.7203, 
noisyNet noise sample is [array([0.5412251], dtype=float32), -1.6978865]. 
=============================================
[2019-03-27 00:59:53,495] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7886352e-20 1.0000000e+00 1.6391674e-26 3.4874026e-22 6.0657215e-24], sum to 1.0000
[2019-03-27 00:59:53,505] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8555
[2019-03-27 00:59:53,512] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1871013.928588384 W.
[2019-03-27 00:59:53,517] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.28333333333333, 77.83333333333333, 1.0, 2.0, 0.4460836314757233, 1.0, 2.0, 0.4460836314757233, 1.0, 1.0, 0.7645127202668943, 6.9112, 6.9112, 170.5573041426782, 1871013.928588384, 1871013.928588384, 378238.4721176109], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6511800.0000, 
sim time next is 6512400.0000, 
raw observation next is [28.5, 76.0, 1.0, 2.0, 0.6338481005235648, 1.0, 2.0, 0.6338481005235648, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1772289.550120485, 1772289.550120485, 347086.1949566659], 
processed observation next is [1.0, 0.391304347826087, 0.5497630331753555, 0.76, 1.0, 1.0, 0.5588531331609214, 1.0, 1.0, 0.5588531331609214, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.49230265281124586, 0.49230265281124586, 0.5180390969502476], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2769464], dtype=float32), 0.5823057]. 
=============================================
[2019-03-27 00:59:55,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1192539e-27 1.0000000e+00 3.6456656e-34 1.1677406e-31 6.8797353e-32], sum to 1.0000
[2019-03-27 00:59:55,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3113
[2019-03-27 00:59:55,225] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1935189.086389594 W.
[2019-03-27 00:59:55,230] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.48333333333333, 61.66666666666667, 1.0, 2.0, 0.6920555201371151, 1.0, 1.0, 0.6920555201371151, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1935189.086389594, 1935189.086389594, 370733.407874325], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6538200.0000, 
sim time next is 6538800.0000, 
raw observation next is [30.36666666666667, 62.33333333333334, 1.0, 2.0, 0.6834827298834232, 1.0, 2.0, 0.6834827298834232, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1911195.68323565, 1911195.68323565, 367125.0306536956], 
processed observation next is [1.0, 0.6956521739130435, 0.6382306477093209, 0.6233333333333334, 1.0, 1.0, 0.6186538914258111, 1.0, 1.0, 0.6186538914258111, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5308876897876805, 0.5308876897876805, 0.5479478069458142], 
reward next is 0.4521, 
noisyNet noise sample is [array([-1.2754452], dtype=float32), -0.29179308]. 
=============================================
[2019-03-27 00:59:55,440] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2333825e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 00:59:55,451] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5562
[2019-03-27 00:59:55,455] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 71.0, 1.0, 2.0, 0.4895177550933122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684020.2914930462, 684020.2914930462, 181969.7258697279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6549600.0000, 
sim time next is 6550200.0000, 
raw observation next is [28.3, 71.5, 1.0, 2.0, 0.4871206774760796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680669.6979250896, 680669.6979250903, 181602.5562100193], 
processed observation next is [1.0, 0.8260869565217391, 0.5402843601895735, 0.715, 1.0, 1.0, 0.38207310539286704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18907491609030266, 0.18907491609030286, 0.2710485913582378], 
reward next is 0.7290, 
noisyNet noise sample is [array([0.17436159], dtype=float32), 1.0904517]. 
=============================================
[2019-03-27 00:59:56,839] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 00:59:56,841] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:59:56,842] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:59:56,842] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:59:56,843] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:59:56,844] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:59:56,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:59:56,844] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:59:56,846] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:59:56,845] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:59:56,847] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:59:56,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-27 00:59:56,863] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-27 00:59:56,881] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-27 00:59:56,917] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-27 00:59:56,918] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-27 00:59:58,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 00:59:58,922] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.9, 89.33333333333334, 1.0, 2.0, 0.3954059721869924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603299.6226488044, 603299.6226488044, 175177.8480270795]
[2019-03-27 00:59:58,924] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:59:58,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4504904345513574
[2019-03-27 01:00:49,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:00:49,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.5, 68.5, 1.0, 2.0, 1.012365866720954, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005993979588045, 6.9112, 168.9123930421009, 2312310.429104994, 2245060.541764551, 467119.4955938762]
[2019-03-27 01:00:49,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:00:49,876] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.054575e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.8604751278581658
[2019-03-27 01:00:49,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2312310.429104994 W.
[2019-03-27 01:01:09,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:01:09,113] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.85, 56.0, 1.0, 2.0, 0.5373586818714401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750893.7205659468, 750893.7205659474, 189656.3273564316]
[2019-03-27 01:01:09,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:01:09,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.518456684146902
[2019-03-27 01:01:22,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:01:22,230] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.97807888333333, 89.39497907333333, 1.0, 2.0, 0.9102835202181959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1272323.220478119, 1272323.22047812, 272782.2344371146]
[2019-03-27 01:01:22,232] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:01:22,234] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8560725e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.26481886717304715
[2019-03-27 01:01:27,473] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:01:27,473] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.85, 82.5, 1.0, 2.0, 0.5783635885648524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808214.9142437056, 808214.9142437056, 196783.5220466023]
[2019-03-27 01:01:27,474] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:01:27,476] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8878054980207706
[2019-03-27 01:01:33,149] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:01:33,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.5, 91.0, 1.0, 2.0, 0.5405557682737954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755362.8503467804, 755362.8503467811, 190193.382917863]
[2019-03-27 01:01:33,152] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:01:33,154] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6281752986649615
[2019-03-27 01:01:44,624] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:01:44,627] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.69303182, 91.00268346499999, 1.0, 2.0, 0.3191993418363134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503208.0526951316, 503208.0526951322, 167234.9970773523]
[2019-03-27 01:01:44,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:01:44,629] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5141830371483784
[2019-03-27 01:01:45,949] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.05547875]
[2019-03-27 01:01:45,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.17771338, 67.73255273, 1.0, 2.0, 0.4615357106395213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655368.4429515055, 655368.442951505, 179135.3840059731]
[2019-03-27 01:01:45,951] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:01:45,958] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05852384918423226
[2019-03-27 01:01:51,288] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:01:51,644] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:01:51,655] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:01:51,819] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:01:51,904] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:01:52,919] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 600000, evaluation results [600000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:01:59,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9342317e-34 1.0000000e+00 0.0000000e+00 4.8516334e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 01:01:59,296] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7537
[2019-03-27 01:01:59,302] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.0, 1.0, 2.0, 0.6337777590304938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885683.8858270902, 885683.8858270902, 207199.0750181284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6681600.0000, 
sim time next is 6682200.0000, 
raw observation next is [26.36666666666667, 89.0, 1.0, 2.0, 0.5896666440456897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824016.1191894843, 824016.1191894843, 198831.0154633349], 
processed observation next is [1.0, 0.34782608695652173, 0.4486571879936811, 0.89, 1.0, 1.0, 0.5056224627056503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22889336644152342, 0.22889336644152342, 0.2967627096467685], 
reward next is 0.7032, 
noisyNet noise sample is [array([-0.21749231], dtype=float32), 0.5232127]. 
=============================================
[2019-03-27 01:02:01,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5788826e-31 1.0000000e+00 0.0000000e+00 2.5950443e-34 1.1863925e-38], sum to 1.0000
[2019-03-27 01:02:01,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7081
[2019-03-27 01:02:01,518] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 64.0, 1.0, 2.0, 0.5619623146567498, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9441826601331204, 6.9112, 6.9112, 168.912910711399, 1571155.061463054, 1571155.061463054, 337074.1247863725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6699600.0000, 
sim time next is 6700200.0000, 
raw observation next is [29.95, 63.33333333333334, 1.0, 2.0, 0.9062482414027511, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564989949, 1266679.658289023, 1266679.658289024, 271659.195643292], 
processed observation next is [1.0, 0.5652173913043478, 0.6184834123222749, 0.6333333333333334, 1.0, 1.0, 0.8870460739792182, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399450961462, 0.3518554606358397, 0.35185546063584, 0.4054614860347642], 
reward next is 0.5945, 
noisyNet noise sample is [array([1.143974], dtype=float32), 0.3344158]. 
=============================================
[2019-03-27 01:02:04,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.08277780e-28 1.00000000e+00 3.31621990e-38 1.21090384e-32
 2.33470164e-32], sum to 1.0000
[2019-03-27 01:02:04,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6647
[2019-03-27 01:02:04,305] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 62.66666666666667, 1.0, 2.0, 0.892804598005513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1391396.86081757, 1391396.86081757, 287437.4119942768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6774000.0000, 
sim time next is 6774600.0000, 
raw observation next is [26.36666666666667, 61.33333333333333, 1.0, 2.0, 0.891837052721122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390055.033050526, 1390055.033050526, 287160.9570079824], 
processed observation next is [1.0, 0.391304347826087, 0.4486571879936811, 0.6133333333333333, 1.0, 1.0, 0.8696831960495446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38612639806959054, 0.38612639806959054, 0.4285984432954961], 
reward next is 0.5714, 
noisyNet noise sample is [array([2.2662532], dtype=float32), 0.27268884]. 
=============================================
[2019-03-27 01:02:11,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:02:11,601] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9716
[2019-03-27 01:02:11,610] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 57.33333333333333, 1.0, 2.0, 0.3606794020708926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552367.4256874664, 552367.4256874664, 170752.5849879545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6892800.0000, 
sim time next is 6893400.0000, 
raw observation next is [27.73333333333333, 58.16666666666667, 1.0, 2.0, 0.3623217060453695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554432.6237568004, 554432.6237568011, 170914.0386809656], 
processed observation next is [0.0, 0.782608695652174, 0.513428120063191, 0.5816666666666667, 1.0, 1.0, 0.23171289884984278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15400906215466678, 0.15400906215466698, 0.2550955801208442], 
reward next is 0.7449, 
noisyNet noise sample is [array([-1.3313185], dtype=float32), -1.5462562]. 
=============================================
[2019-03-27 01:02:12,529] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3353666e-26 1.0000000e+00 4.3232801e-33 3.9901628e-28 1.5609795e-28], sum to 1.0000
[2019-03-27 01:02:12,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6124
[2019-03-27 01:02:12,541] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 81.5, 1.0, 2.0, 0.4642418070896895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700839.9378212957, 700839.9378212957, 184602.5237585252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7233000.0000, 
sim time next is 7233600.0000, 
raw observation next is [24.2, 82.0, 1.0, 2.0, 0.3709714109903839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560684.6458244983, 560684.6458244983, 171234.8478279128], 
processed observation next is [1.0, 0.7391304347826086, 0.3459715639810427, 0.82, 1.0, 1.0, 0.24213423010889626, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15574573495124952, 0.15574573495124952, 0.25557439974315344], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.723397], dtype=float32), -0.8213582]. 
=============================================
[2019-03-27 01:02:13,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:02:13,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5884
[2019-03-27 01:02:13,717] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.33333333333334, 1.0, 2.0, 0.4156428271130365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611545.8753499115, 611545.8753499115, 175370.8192671171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6931200.0000, 
sim time next is 6931800.0000, 
raw observation next is [24.2, 88.0, 1.0, 2.0, 0.4163090947651873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612018.797796959, 612018.7977969585, 175401.2052912403], 
processed observation next is [0.0, 0.21739130434782608, 0.3459715639810427, 0.88, 1.0, 1.0, 0.29675794550022566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1700052216102664, 0.17000522161026624, 0.2617928437182691], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.2689046], dtype=float32), 0.40251645]. 
=============================================
[2019-03-27 01:02:17,741] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:02:17,751] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1220
[2019-03-27 01:02:17,760] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 76.0, 1.0, 2.0, 0.4453435043569308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 639089.9337824238, 639089.9337824245, 177636.9330750684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6994800.0000, 
sim time next is 6995400.0000, 
raw observation next is [26.55, 76.33333333333334, 1.0, 2.0, 0.4428623873691883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634940.0951931007, 634940.0951931007, 177204.2987184869], 
processed observation next is [0.0, 1.0, 0.4573459715639811, 0.7633333333333334, 1.0, 1.0, 0.3287498643002269, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1763722486647502, 0.1763722486647502, 0.26448402793804016], 
reward next is 0.7355, 
noisyNet noise sample is [array([1.0602756], dtype=float32), -0.39250883]. 
=============================================
[2019-03-27 01:02:18,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:02:18,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7293
[2019-03-27 01:02:18,152] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.41666666666666, 67.83333333333333, 1.0, 2.0, 0.4183159584869597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612532.4056348385, 612532.4056348385, 175380.4542522756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6990600.0000, 
sim time next is 6991200.0000, 
raw observation next is [27.3, 69.0, 1.0, 2.0, 0.4224405810370953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616696.7005535706, 616696.7005535706, 175725.2670841848], 
processed observation next is [0.0, 0.9565217391304348, 0.4928909952606636, 0.69, 1.0, 1.0, 0.3041452783579462, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17130463904265852, 0.17130463904265852, 0.2622765180360967], 
reward next is 0.7377, 
noisyNet noise sample is [array([-1.5756356], dtype=float32), 0.55689436]. 
=============================================
[2019-03-27 01:02:34,602] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:02:34,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3288
[2019-03-27 01:02:34,616] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 87.33333333333333, 1.0, 2.0, 0.3237276441616025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510450.7968378516, 510450.7968378516, 167787.0528761033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7282200.0000, 
sim time next is 7282800.0000, 
raw observation next is [22.0, 87.0, 1.0, 2.0, 0.3220762306654523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507762.1145266998, 507762.1145267004, 167580.1688882732], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.87, 1.0, 1.0, 0.1832243742957257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14104503181297218, 0.14104503181297234, 0.2501196550571242], 
reward next is 0.7499, 
noisyNet noise sample is [array([0.10249469], dtype=float32), 0.4683276]. 
=============================================
[2019-03-27 01:02:36,707] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6356466e-31 1.0000000e+00 0.0000000e+00 4.8409576e-33 3.9518243e-35], sum to 1.0000
[2019-03-27 01:02:36,716] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1496
[2019-03-27 01:02:36,722] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 57.5, 1.0, 2.0, 1.000270505240883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1526743.240083183, 1526743.240083183, 318064.6699848701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7308600.0000, 
sim time next is 7309200.0000, 
raw observation next is [27.93333333333333, 58.00000000000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.964758930878284, 6.9112, 168.9124963630703, 1622594.904888347, 1584598.459476157, 331221.0055080327], 
processed observation next is [1.0, 0.6086956521739131, 0.522906793048973, 0.5800000000000001, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.005355893087828356, 0.0, 0.8294376856181053, 0.45072080691342975, 0.44016623874337696, 0.49435970971348164], 
reward next is 0.2378, 
noisyNet noise sample is [array([-0.30595338], dtype=float32), -1.6410809]. 
=============================================
[2019-03-27 01:02:47,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:02:47,565] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9727
[2019-03-27 01:02:47,576] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 77.0, 1.0, 2.0, 0.4157413201553971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606491.3773013739, 606491.3773013739, 174739.69708899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7486200.0000, 
sim time next is 7486800.0000, 
raw observation next is [26.06666666666667, 76.66666666666667, 1.0, 2.0, 0.4157555384071969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 606176.3761989558, 606176.3761989564, 174699.6892515032], 
processed observation next is [0.0, 0.6521739130434783, 0.4344391785150081, 0.7666666666666667, 1.0, 1.0, 0.2960910101291529, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16838232672193218, 0.16838232672193232, 0.26074580485298987], 
reward next is 0.7393, 
noisyNet noise sample is [array([-1.049869], dtype=float32), -2.5053]. 
=============================================
[2019-03-27 01:02:48,847] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 01:02:48,849] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:02:48,850] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:02:48,851] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:02:48,851] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:02:48,852] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:02:48,853] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:02:48,853] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:02:48,851] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:02:48,858] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:02:48,860] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:02:48,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-27 01:02:48,875] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-27 01:02:48,876] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-27 01:02:48,876] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-27 01:02:48,945] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-27 01:03:00,044] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06015388]
[2019-03-27 01:03:00,045] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.6, 92.83333333333333, 1.0, 2.0, 0.220778831403438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367787.6481783985, 367787.6481783985, 157666.9052754362]
[2019-03-27 01:03:00,047] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:03:00,049] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9946774704711585
[2019-03-27 01:03:09,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06015388]
[2019-03-27 01:03:09,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.83231723, 97.7750312, 1.0, 2.0, 0.2650557628505338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 437608.7143643648, 437608.7143643648, 162381.3006347207]
[2019-03-27 01:03:09,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:03:09,768] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.500448505886997
[2019-03-27 01:03:48,532] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06015388]
[2019-03-27 01:03:48,533] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.0, 66.0, 1.0, 2.0, 0.5772562594078268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 806666.9264377396, 806666.9264377403, 196585.2604834078]
[2019-03-27 01:03:48,534] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:03:48,538] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8731433376517616
[2019-03-27 01:03:52,169] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06015388]
[2019-03-27 01:03:52,169] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.5, 43.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.576630672306759, 6.9112, 168.9093065948875, 1926147.500306038, 1454078.279831559, 311351.282296212]
[2019-03-27 01:03:52,170] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:03:52,173] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18337724515445242
[2019-03-27 01:03:52,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1926147.500306038 W.
[2019-03-27 01:04:08,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06015388]
[2019-03-27 01:04:08,760] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.369992255, 50.252170515, 1.0, 2.0, 0.8456506138234615, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984379560167, 6.9112, 168.9123159876082, 2078965.136460711, 2011722.090362695, 419446.3794746357]
[2019-03-27 01:04:08,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:04:08,765] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1873135548259629
[2019-03-27 01:04:08,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2078965.136460711 W.
[2019-03-27 01:04:42,191] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06015388]
[2019-03-27 01:04:42,192] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.08333333333334, 61.5, 1.0, 2.0, 0.3088884193182381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494597.3073083721, 494597.3073083721, 166706.1155368728]
[2019-03-27 01:04:42,196] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:04:42,200] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42655120501643373
[2019-03-27 01:04:43,602] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:04:43,771] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:04:43,839] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:04:43,933] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:04:43,968] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:04:44,984] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 625000, evaluation results [625000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:04:51,765] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1709080e-32 1.0000000e+00 0.0000000e+00 8.4560622e-38 1.4785072e-38], sum to 1.0000
[2019-03-27 01:04:51,772] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3439
[2019-03-27 01:04:51,777] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 92.33333333333333, 1.0, 2.0, 0.5029131377737938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723328.4567047626, 723328.4567047626, 186671.3806329673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7627800.0000, 
sim time next is 7628400.0000, 
raw observation next is [24.3, 92.0, 1.0, 2.0, 0.4815476531394288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690983.061003379, 690983.061003379, 183064.8518688829], 
processed observation next is [1.0, 0.30434782608695654, 0.3507109004739337, 0.92, 1.0, 1.0, 0.37535861824027567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1919397391676053, 0.1919397391676053, 0.2732311221923625], 
reward next is 0.7268, 
noisyNet noise sample is [array([-0.75873363], dtype=float32), -0.6860924]. 
=============================================
[2019-03-27 01:04:52,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:04:52,993] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:04:53,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-27 01:04:55,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0968797e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:04:55,423] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8367
[2019-03-27 01:04:55,429] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 92.0, 1.0, 2.0, 0.4764218507735118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665715.192146436, 665715.192146436, 179983.9668782419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7693200.0000, 
sim time next is 7693800.0000, 
raw observation next is [24.85, 92.5, 1.0, 2.0, 0.475874665225491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664950.3579151256, 664950.3579151256, 179902.1686926692], 
processed observation next is [1.0, 0.043478260869565216, 0.37677725118483424, 0.925, 1.0, 1.0, 0.3685236930427603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18470843275420154, 0.18470843275420154, 0.2685106995412973], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.7595409], dtype=float32), -1.0039583]. 
=============================================
[2019-03-27 01:05:02,481] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2331677e-30 1.0000000e+00 3.9749219e-37 5.2970638e-37 5.9599988e-34], sum to 1.0000
[2019-03-27 01:05:02,492] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4622
[2019-03-27 01:05:02,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1855377.346096263 W.
[2019-03-27 01:05:02,504] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 75.0, 1.0, 2.0, 0.4423588118455576, 1.0, 1.0, 0.4423588118455576, 1.0, 1.0, 0.7627659785481847, 6.911200000000001, 6.9112, 170.5573041426782, 1855377.346096263, 1855377.346096263, 376716.0407930803], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7812000.0000, 
sim time next is 7812600.0000, 
raw observation next is [29.1, 74.5, 1.0, 2.0, 0.7623557606850445, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.986854051578919, 6.9112, 168.9124548217408, 1962390.162908685, 1908718.734317181, 399456.8134132297], 
processed observation next is [1.0, 0.43478260869565216, 0.5781990521327015, 0.745, 1.0, 1.0, 0.7136816393795717, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0075654051578919026, 0.0, 0.8294374816311597, 0.5451083785857458, 0.5301996484214392, 0.5962041991242234], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3019965], dtype=float32), -0.7064965]. 
=============================================
[2019-03-27 01:05:03,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1814704e-22 1.0000000e+00 2.2550655e-30 1.5773448e-28 1.6585517e-27], sum to 1.0000
[2019-03-27 01:05:03,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6098
[2019-03-27 01:05:03,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2207282.64352141 W.
[2019-03-27 01:05:03,174] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.41666666666666, 68.33333333333334, 1.0, 2.0, 0.5261736780176381, 1.0, 1.0, 0.5261736780176381, 1.0, 2.0, 0.9084380281033396, 6.911200000000001, 6.9112, 170.5573041426782, 2207282.64352141, 2207282.64352141, 432694.2140027561], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7827000.0000, 
sim time next is 7827600.0000, 
raw observation next is [30.53333333333333, 68.66666666666667, 1.0, 2.0, 0.5016889288476623, 1.0, 2.0, 0.5016889288476623, 1.0, 2.0, 0.8691906480281221, 6.911199999999999, 6.9112, 170.5573041426782, 2104468.964174509, 2104468.96417451, 415939.9673970016], 
processed observation next is [1.0, 0.6086956521739131, 0.646129541864139, 0.6866666666666668, 1.0, 1.0, 0.3996252154791111, 1.0, 1.0, 0.3996252154791111, 1.0, 1.0, 0.8404764000342951, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5845747122706969, 0.5845747122706972, 0.620805921488062], 
reward next is 0.3792, 
noisyNet noise sample is [array([-0.08635134], dtype=float32), -1.7744551]. 
=============================================
[2019-03-27 01:05:06,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:05:06,888] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0727
[2019-03-27 01:05:06,894] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 82.33333333333334, 1.0, 2.0, 0.5236551086158481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731738.0398667025, 731738.0398667025, 187385.8173370538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7935600.0000, 
sim time next is 7936200.0000, 
raw observation next is [27.7, 83.0, 1.0, 2.0, 0.5248873486146212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733460.5250773531, 733460.5250773531, 187587.6723508576], 
processed observation next is [1.0, 0.8695652173913043, 0.5118483412322274, 0.83, 1.0, 1.0, 0.42757511881279653, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2037390347437092, 0.2037390347437092, 0.27998160052366805], 
reward next is 0.7200, 
noisyNet noise sample is [array([1.8851544], dtype=float32), -0.94307965]. 
=============================================
[2019-03-27 01:05:08,527] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:08,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:08,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-27 01:05:08,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:08,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:08,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-27 01:05:08,832] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:08,833] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:08,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-27 01:05:10,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,420] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-27 01:05:10,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,610] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-27 01:05:10,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,642] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,659] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,659] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-27 01:05:10,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,684] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-27 01:05:10,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-27 01:05:10,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,752] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-27 01:05:10,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-27 01:05:10,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-27 01:05:10,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:10,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:10,937] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-27 01:05:11,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:11,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:11,003] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-27 01:05:11,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:11,052] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:11,054] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-27 01:05:11,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:05:11,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:11,179] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-27 01:05:20,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:05:20,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8195
[2019-03-27 01:05:20,033] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 90.0, 1.0, 2.0, 0.2924557214353884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468358.5032334123, 468358.5032334116, 164834.0716436225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 248400.0000, 
sim time next is 249000.0000, 
raw observation next is [20.86666666666667, 90.16666666666667, 1.0, 2.0, 0.2918847654377892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467566.0319162112, 467566.0319162112, 164780.12513733], 
processed observation next is [0.0, 0.9130434782608695, 0.18799368088467638, 0.9016666666666667, 1.0, 1.0, 0.14684911498528816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12987945331005865, 0.12987945331005865, 0.24594048527959703], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.07207628], dtype=float32), -1.6180893]. 
=============================================
[2019-03-27 01:05:20,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.70009]
 [73.6174 ]
 [73.59915]
 [73.56954]
 [73.5538 ]], R is [[73.71200562]
 [73.72886658]
 [73.7454834 ]
 [73.76181793]
 [73.7778244 ]].
[2019-03-27 01:05:40,548] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 01:05:40,551] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:05:40,551] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:05:40,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:40,556] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:05:40,554] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:05:40,557] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:05:40,560] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:40,556] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:40,562] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:40,565] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:05:40,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-27 01:05:40,599] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-27 01:05:40,601] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-27 01:05:40,642] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-27 01:05:40,644] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-27 01:05:49,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.018372005]
[2019-03-27 01:05:49,947] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.06848794666667, 51.86042607666667, 1.0, 2.0, 0.4238959985478866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687753.5026075634, 687753.5026075628, 182832.5487639971]
[2019-03-27 01:05:49,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:05:49,952] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5061269560391274
[2019-03-27 01:06:59,634] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.018372005]
[2019-03-27 01:06:59,635] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.08333333333334, 77.66666666666667, 1.0, 2.0, 0.6716926139540381, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992472580679316, 6.9112, 168.9124070120309, 1835517.482565074, 1777860.104213259, 379145.6747810768]
[2019-03-27 01:06:59,638] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:06:59,642] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24957215836194524
[2019-03-27 01:06:59,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1835517.482565074 W.
[2019-03-27 01:07:03,663] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.018372005]
[2019-03-27 01:07:03,664] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.7, 86.0, 1.0, 2.0, 0.5123498811948195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715935.171580182, 715935.1715801826, 185553.5177551277]
[2019-03-27 01:07:03,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:07:03,672] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47026632900738097
[2019-03-27 01:07:20,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.018372005]
[2019-03-27 01:07:20,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.61287885, 62.38699347333334, 1.0, 2.0, 0.3863028637256017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588731.3639176141, 588731.3639176148, 173847.4344016132]
[2019-03-27 01:07:20,269] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:07:20,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6426662766640076
[2019-03-27 01:07:35,094] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:07:35,341] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:07:35,459] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:07:35,639] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:07:35,667] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:07:36,686] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 650000, evaluation results [650000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:07:38,094] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:07:38,104] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8867
[2019-03-27 01:07:38,110] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 53.0, 1.0, 2.0, 0.5511664748746561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 904785.9796404502, 904785.9796404509, 205523.3906306297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 489600.0000, 
sim time next is 490200.0000, 
raw observation next is [24.85, 53.16666666666667, 1.0, 2.0, 0.5712048465812091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937928.0739887205, 937928.0739887205, 209564.4470087265], 
processed observation next is [1.0, 0.6956521739130435, 0.37677725118483424, 0.5316666666666667, 1.0, 1.0, 0.48337933323037235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26053557610797795, 0.26053557610797795, 0.31278275672944256], 
reward next is 0.6872, 
noisyNet noise sample is [array([-0.603947], dtype=float32), 0.17215122]. 
=============================================
[2019-03-27 01:07:44,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:07:44,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7825
[2019-03-27 01:07:44,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.0, 82.5, 1.0, 2.0, 0.2264758970707983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 376717.4235899877, 376717.4235899884, 158308.8500992186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [18.86666666666667, 83.0, 1.0, 2.0, 0.2241016475739114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 372957.7030028827, 372957.7030028827, 158053.9721315993], 
processed observation next is [1.0, 0.9565217391304348, 0.09320695102685649, 0.83, 1.0, 1.0, 0.06518270792037517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1035993619452452, 0.1035993619452452, 0.23590145094268553], 
reward next is 0.7641, 
noisyNet noise sample is [array([-1.1637568], dtype=float32), -0.36770704]. 
=============================================
[2019-03-27 01:07:44,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.63516 ]
 [77.69636 ]
 [77.76995 ]
 [77.855675]
 [77.86272 ]], R is [[77.58037567]
 [77.56829071]
 [77.55601501]
 [77.54372406]
 [77.53147888]].
[2019-03-27 01:07:58,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:07:58,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4330
[2019-03-27 01:07:58,030] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 68.5, 1.0, 2.0, 0.3016818828715538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479914.674252031, 479914.6742520316, 165607.8170877082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 833400.0000, 
sim time next is 834000.0000, 
raw observation next is [24.2, 69.0, 1.0, 2.0, 0.3017484013843819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479727.3012637906, 479727.3012637906, 165589.387032358], 
processed observation next is [0.0, 0.6521739130434783, 0.3459715639810427, 0.69, 1.0, 1.0, 0.15873301371612278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13325758368438628, 0.13325758368438628, 0.24714833885426568], 
reward next is 0.7529, 
noisyNet noise sample is [array([0.10563594], dtype=float32), 0.047091793]. 
=============================================
[2019-03-27 01:07:58,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.375725]
 [75.40863 ]
 [75.45309 ]
 [75.50596 ]
 [75.47709 ]], R is [[75.35842133]
 [75.35765839]
 [75.35701752]
 [75.35674286]
 [75.35681915]].
[2019-03-27 01:07:58,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:07:58,409] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1009
[2019-03-27 01:07:58,412] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333334, 88.5, 1.0, 2.0, 0.3083062239039558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488098.2258334967, 488098.2258334967, 166156.5189735952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 859800.0000, 
sim time next is 860400.0000, 
raw observation next is [21.6, 89.0, 1.0, 2.0, 0.3090518464448617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488945.2291765588, 488945.2291765588, 166211.5719075229], 
processed observation next is [0.0, 1.0, 0.22274881516587688, 0.89, 1.0, 1.0, 0.16753234511429121, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13581811921571077, 0.13581811921571077, 0.24807697299630285], 
reward next is 0.7519, 
noisyNet noise sample is [array([-1.5333092], dtype=float32), -0.7293686]. 
=============================================
[2019-03-27 01:07:59,997] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:00,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1503
[2019-03-27 01:08:00,015] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6194185445970449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 953925.4080664881, 953925.4080664881, 215179.2465219662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 988200.0000, 
sim time next is 988800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6516838945191169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003603.168561075, 1003603.168561075, 222204.6266868375], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.5803420415892975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27877865793363193, 0.27877865793363193, 0.33164869654751866], 
reward next is 0.6684, 
noisyNet noise sample is [array([1.4921943], dtype=float32), 2.2873776]. 
=============================================
[2019-03-27 01:08:01,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:01,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6927
[2019-03-27 01:08:01,737] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.3573041311786649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549303.5648226578, 549303.5648226573, 170556.7196522344], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1015200.0000, 
sim time next is 1015800.0000, 
raw observation next is [21.71666666666667, 96.83333333333334, 1.0, 2.0, 0.3598122273471944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553296.8323661025, 553296.8323661019, 170896.1081969232], 
processed observation next is [1.0, 0.782608695652174, 0.22827804107424976, 0.9683333333333334, 1.0, 1.0, 0.22868943053878846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15369356454613958, 0.15369356454613942, 0.25506881820436295], 
reward next is 0.7449, 
noisyNet noise sample is [array([-1.5886497], dtype=float32), 0.18621327]. 
=============================================
[2019-03-27 01:08:06,778] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:06,789] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8772
[2019-03-27 01:08:06,795] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 95.0, 1.0, 2.0, 0.6881659048390057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1059848.747751766, 1059848.747751765, 230567.1253872909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.4730595273756967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729207.5030952801, 729207.5030952796, 187659.3506137628], 
processed observation next is [1.0, 0.5217391304347826, 0.23380726698262277, 0.95, 1.0, 1.0, 0.3651319606936104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20255763974868893, 0.20255763974868876, 0.2800885830056161], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.15119034], dtype=float32), 0.4533926]. 
=============================================
[2019-03-27 01:08:14,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:14,111] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3168
[2019-03-27 01:08:14,119] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 67.0, 1.0, 2.0, 0.7717465397206623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1190770.815690985, 1190770.815690985, 251603.0383437812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [25.66666666666666, 67.5, 1.0, 2.0, 0.4232138857200688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653470.7032023642, 653470.7032023648, 179974.1591324478], 
processed observation next is [1.0, 0.7391304347826086, 0.4154818325434437, 0.675, 1.0, 1.0, 0.3050769707470708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1815196397784345, 0.1815196397784347, 0.2686181479588773], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.43121967], dtype=float32), 1.2072543]. 
=============================================
[2019-03-27 01:08:14,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:14,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7215
[2019-03-27 01:08:14,777] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 82.0, 1.0, 2.0, 0.3103949106922713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492663.1602369188, 492663.1602369188, 166514.7629283872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1114200.0000, 
sim time next is 1114800.0000, 
raw observation next is [22.23333333333333, 82.66666666666667, 1.0, 2.0, 0.3105179619863199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 493141.9930866007, 493141.9930866014, 166554.6920804694], 
processed observation next is [1.0, 0.9130434782608695, 0.25276461295418634, 0.8266666666666667, 1.0, 1.0, 0.16929874938110834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1369838869685002, 0.1369838869685004, 0.248589092657417], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.21709855], dtype=float32), -0.32307836]. 
=============================================
[2019-03-27 01:08:17,687] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:17,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1588
[2019-03-27 01:08:17,700] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 84.83333333333333, 1.0, 2.0, 0.4717735108318624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659952.5064163224, 659952.5064163231, 179386.5780813171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1278600.0000, 
sim time next is 1279200.0000, 
raw observation next is [25.7, 85.66666666666667, 1.0, 2.0, 0.4705943119130958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659201.5933764144, 659201.5933764138, 179327.9456660535], 
processed observation next is [1.0, 0.8260869565217391, 0.4170616113744076, 0.8566666666666667, 1.0, 1.0, 0.3621618215820432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18311155371567067, 0.1831115537156705, 0.26765365024784105], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.50446314], dtype=float32), 1.6158302]. 
=============================================
[2019-03-27 01:08:23,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5546364e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:08:23,220] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1474
[2019-03-27 01:08:23,225] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1763254.504316075 W.
[2019-03-27 01:08:23,231] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.66666666666667, 72.33333333333333, 1.0, 2.0, 0.6306194302040915, 1.0, 1.0, 0.6306194302040915, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1763254.504316075, 1763254.504316075, 345823.8069477123], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [27.88333333333333, 71.66666666666667, 1.0, 2.0, 0.4210149363126188, 1.0, 2.0, 0.4210149363126188, 1.0, 1.0, 0.7076453458305514, 6.911200000000001, 6.9112, 170.5573041426782, 1765781.361585535, 1765781.361585534, 361241.3736298313], 
processed observation next is [1.0, 0.43478260869565216, 0.5205371248025275, 0.7166666666666667, 1.0, 1.0, 0.30242763411158896, 1.0, 1.0, 0.30242763411158896, 1.0, 0.5, 0.6434699339396969, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.49049482266264866, 0.4904948226626483, 0.5391662292982556], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09041449], dtype=float32), -0.89277506]. 
=============================================
[2019-03-27 01:08:26,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:26,421] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5679
[2019-03-27 01:08:26,429] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 95.0, 1.0, 2.0, 0.732035705460625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1085236.327081783, 1085236.327081784, 236145.6492130463], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [23.03333333333333, 95.0, 1.0, 2.0, 0.7341156852523142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1088893.433748887, 1088893.433748887, 236718.4646992754], 
processed observation next is [1.0, 0.34782608695652173, 0.29067930489731436, 0.95, 1.0, 1.0, 0.6796574521112219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3024703982635797, 0.3024703982635797, 0.3533111413422021], 
reward next is 0.6467, 
noisyNet noise sample is [array([-2.1602414], dtype=float32), -0.22805837]. 
=============================================
[2019-03-27 01:08:30,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:08:30,817] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8764
[2019-03-27 01:08:30,824] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 66.5, 1.0, 2.0, 0.3633739608108957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552737.9662072669, 552737.9662072662, 170668.3423413], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1503000.0000, 
sim time next is 1503600.0000, 
raw observation next is [26.66666666666667, 64.66666666666667, 1.0, 2.0, 0.3609115328024661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549785.6662363309, 549785.6662363316, 170443.7135454415], 
processed observation next is [0.0, 0.391304347826087, 0.4628751974723541, 0.6466666666666667, 1.0, 1.0, 0.2300138949427302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.152718240621203, 0.1527182406212032, 0.2543936023066291], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.11365913], dtype=float32), -0.9777014]. 
=============================================
[2019-03-27 01:08:32,702] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 01:08:32,703] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:08:32,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:08:32,705] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:08:32,708] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:08:32,706] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:08:32,709] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:08:32,710] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:08:32,711] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:08:32,709] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:08:32,715] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:08:32,739] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-27 01:08:32,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-27 01:08:32,757] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-27 01:08:32,799] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-27 01:08:32,815] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-27 01:09:49,863] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.012539185]
[2019-03-27 01:09:49,866] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.07189479, 86.88222275, 1.0, 2.0, 0.8498759438055208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1187842.996704384, 1187842.996704384, 256526.5772551582]
[2019-03-27 01:09:49,867] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:09:49,871] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21008391073651644
[2019-03-27 01:09:50,799] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.012539185]
[2019-03-27 01:09:50,800] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.18333333333333, 78.16666666666667, 1.0, 2.0, 0.5619285007923895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785239.759397992, 785239.759397992, 193867.0914209615]
[2019-03-27 01:09:50,801] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:09:50,804] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03287522185499925
[2019-03-27 01:09:57,772] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.012539185]
[2019-03-27 01:09:57,773] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.80217849333334, 84.35043207333334, 1.0, 2.0, 0.8162942721665265, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598254593684, 6.9112, 168.9123159907689, 2037878.126492792, 1970636.381224081, 411908.3716560958]
[2019-03-27 01:09:57,776] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:09:57,779] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6646644673322808
[2019-03-27 01:09:57,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2037878.126492792 W.
[2019-03-27 01:10:13,803] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.012539185]
[2019-03-27 01:10:13,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.10631095, 83.45583206500001, 1.0, 2.0, 0.4764643570745585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670831.7798985079, 670831.7798985079, 180642.6494342407]
[2019-03-27 01:10:13,806] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:10:13,811] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0015186446851296331
[2019-03-27 01:10:21,196] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.012539185]
[2019-03-27 01:10:21,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.14029122333334, 61.54884219666667, 1.0, 2.0, 0.3908644054790926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582923.2434837183, 582923.2434837183, 172962.7676231582]
[2019-03-27 01:10:21,199] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:10:21,201] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9705497320652394
[2019-03-27 01:10:26,123] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:10:26,368] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:10:26,415] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:10:26,504] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:10:26,520] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:10:27,538] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 675000, evaluation results [675000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:10:27,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:27,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4479
[2019-03-27 01:10:27,782] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 81.5, 1.0, 2.0, 0.3584798453612946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549739.9703249993, 549739.9703249993, 170553.3653356346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1540200.0000, 
sim time next is 1540800.0000, 
raw observation next is [23.6, 83.0, 1.0, 2.0, 0.3586410818911617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549815.2703252132, 549815.2703252139, 170554.578030716], 
processed observation next is [0.0, 0.8695652173913043, 0.3175355450236968, 0.83, 1.0, 1.0, 0.22727841191706225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1527264639792259, 0.1527264639792261, 0.2545590716876358], 
reward next is 0.7454, 
noisyNet noise sample is [array([1.8430744], dtype=float32), 0.6229915]. 
=============================================
[2019-03-27 01:10:29,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.707131e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:10:29,591] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7402
[2019-03-27 01:10:29,603] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1876695.224005529 W.
[2019-03-27 01:10:29,612] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 85.66666666666666, 1.0, 2.0, 0.6711554550671128, 1.0, 1.0, 0.6711554550671128, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1876695.224005529, 1876695.224005529, 362016.2114501753], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [27.0, 85.83333333333334, 1.0, 2.0, 0.4387878579314553, 1.0, 2.0, 0.4387878579314553, 1.0, 1.0, 0.749252692275563, 6.9112, 6.9112, 170.5573041426782, 1840386.897440519, 1840386.897440519, 373390.4175947412], 
processed observation next is [1.0, 0.6521739130434783, 0.4786729857819906, 0.8583333333333334, 1.0, 1.0, 0.3238407926885004, 1.0, 1.0, 0.3238407926885004, 1.0, 0.5, 0.6942106003360523, 0.0, 0.0, 0.8375144448122397, 0.5112185826223664, 0.5112185826223664, 0.5572991307384197], 
reward next is 0.4427, 
noisyNet noise sample is [array([0.8532453], dtype=float32), -1.0426191]. 
=============================================
[2019-03-27 01:10:30,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:30,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0294
[2019-03-27 01:10:30,139] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 77.33333333333334, 1.0, 2.0, 0.4147932769288407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607054.8536704936, 607054.8536704936, 174851.578997805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1447800.0000, 
sim time next is 1448400.0000, 
raw observation next is [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438], 
processed observation next is [0.0, 0.782608695652174, 0.41074249605055313, 0.7866666666666667, 1.0, 1.0, 0.2949261231269481, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16900591630277595, 0.16900591630277595, 0.26122583463797583], 
reward next is 0.7388, 
noisyNet noise sample is [array([-0.9668702], dtype=float32), -0.15029137]. 
=============================================
[2019-03-27 01:10:33,318] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:33,328] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6608
[2019-03-27 01:10:33,336] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.65, 52.33333333333334, 1.0, 2.0, 0.3756269328460859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 563145.4918114395, 563145.4918114389, 171297.9763785173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1519800.0000, 
sim time next is 1520400.0000, 
raw observation next is [29.5, 52.66666666666667, 1.0, 2.0, 0.372397961663648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 559498.6854023929, 559498.6854023923, 171020.9269812391], 
processed observation next is [0.0, 0.6086956521739131, 0.5971563981042655, 0.5266666666666667, 1.0, 1.0, 0.2438529658598169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15541630150066468, 0.15541630150066452, 0.2552551148973718], 
reward next is 0.7447, 
noisyNet noise sample is [array([-0.42142096], dtype=float32), -0.050451078]. 
=============================================
[2019-03-27 01:10:38,265] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:38,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3954
[2019-03-27 01:10:38,280] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 95.16666666666667, 1.0, 2.0, 0.4854643778384652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678354.5571582321, 678354.5571582327, 181350.1802974536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2011800.0000, 
sim time next is 2012400.0000, 
raw observation next is [25.0, 95.0, 1.0, 2.0, 0.4874043180397513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681066.1652401134, 681066.165240114, 181646.3651873764], 
processed observation next is [0.0, 0.30434782608695654, 0.38388625592417064, 0.95, 1.0, 1.0, 0.3824148410117486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1891850459000315, 0.18918504590003166, 0.2711139778916066], 
reward next is 0.7289, 
noisyNet noise sample is [array([-0.36142308], dtype=float32), 0.27884847]. 
=============================================
[2019-03-27 01:10:39,703] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:39,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0137
[2019-03-27 01:10:39,718] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.33333333333334, 1.0, 2.0, 0.4170438703713091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610836.2120430194, 610836.2120430188, 175224.0002879854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1635600.0000, 
sim time next is 1636200.0000, 
raw observation next is [23.1, 97.5, 1.0, 2.0, 0.4176619970886177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 611261.2224872009, 611261.2224872014, 175250.4030838056], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.975, 1.0, 1.0, 0.29838794829953935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16979478402422246, 0.16979478402422263, 0.2615677657967248], 
reward next is 0.7384, 
noisyNet noise sample is [array([1.0520741], dtype=float32), -2.2818654]. 
=============================================
[2019-03-27 01:10:42,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:42,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5664
[2019-03-27 01:10:42,053] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 99.0, 1.0, 2.0, 0.4302182426138396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622443.8504055929, 622443.8504055929, 176124.5201161314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1648200.0000, 
sim time next is 1648800.0000, 
raw observation next is [23.2, 99.0, 1.0, 2.0, 0.4300133032605817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622147.4625190102, 622147.4625190095, 176095.591419972], 
processed observation next is [1.0, 0.08695652173913043, 0.29857819905213273, 0.99, 1.0, 1.0, 0.31326904007299, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17281873958861393, 0.17281873958861374, 0.2628292409253314], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.61138356], dtype=float32), -0.1585481]. 
=============================================
[2019-03-27 01:10:43,509] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.8310599e-30 1.0000000e+00 0.0000000e+00 3.4012318e-35 1.4491833e-34], sum to 1.0000
[2019-03-27 01:10:43,517] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1595
[2019-03-27 01:10:43,523] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1721720.022521795 W.
[2019-03-27 01:10:43,528] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.86666666666667, 81.50000000000001, 1.0, 2.0, 0.6157767480721765, 1.0, 2.0, 0.6157767480721765, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1721720.022521795, 1721720.022521795, 340149.828620348], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1692600.0000, 
sim time next is 1693200.0000, 
raw observation next is [27.93333333333333, 81.0, 1.0, 2.0, 0.3605899306703886, 1.0, 2.0, 0.3605899306703886, 1.0, 1.0, 0.6182939324120945, 6.9112, 6.9112, 170.5573041426782, 1512173.67109241, 1512173.67109241, 331161.7482617911], 
processed observation next is [1.0, 0.6086956521739131, 0.522906793048973, 0.81, 1.0, 1.0, 0.22962642249444407, 1.0, 1.0, 0.22962642249444407, 1.0, 0.5, 0.5345047956245055, 0.0, 0.0, 0.8375144448122397, 0.4200482419701139, 0.4200482419701139, 0.49427126606237476], 
reward next is 0.5057, 
noisyNet noise sample is [array([-2.3851593], dtype=float32), -0.014562748]. 
=============================================
[2019-03-27 01:10:52,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:52,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7783
[2019-03-27 01:10:52,354] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 92.0, 1.0, 2.0, 0.3940268177230484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589394.8802071229, 589394.8802071229, 173605.6239739308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1839600.0000, 
sim time next is 1840200.0000, 
raw observation next is [23.31666666666667, 91.66666666666667, 1.0, 2.0, 0.4840337286073762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722343.5373102932, 722343.5373102926, 186838.6230784724], 
processed observation next is [1.0, 0.30434782608695654, 0.30410742496050575, 0.9166666666666667, 1.0, 1.0, 0.3783538898884051, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20065098258619254, 0.20065098258619238, 0.27886361653503344], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.9323806], dtype=float32), 0.86792684]. 
=============================================
[2019-03-27 01:10:55,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:10:55,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0673
[2019-03-27 01:10:55,705] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 87.83333333333334, 1.0, 2.0, 0.4656988414344334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659394.8663762542, 659394.8663762542, 179512.6298097052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1889400.0000, 
sim time next is 1890000.0000, 
raw observation next is [25.1, 88.0, 1.0, 2.0, 0.4644061241375815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658324.6432128618, 658324.6432128613, 179418.0188848631], 
processed observation next is [1.0, 0.9130434782608695, 0.38862559241706174, 0.88, 1.0, 1.0, 0.3547061736597368, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18286795644801718, 0.182867956448017, 0.26778808788785535], 
reward next is 0.7322, 
noisyNet noise sample is [array([-1.4336835], dtype=float32), 1.2379534]. 
=============================================
[2019-03-27 01:10:55,734] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[66.780174]
 [66.99257 ]
 [67.35028 ]
 [67.46247 ]
 [67.51873 ]], R is [[66.59241486]
 [66.65856171]
 [66.72399902]
 [66.78887939]
 [66.85337067]].
[2019-03-27 01:10:59,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9654846e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:10:59,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8743
[2019-03-27 01:10:59,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.051251240683174, 6.9112, 168.9120686360958, 1583186.047197754, 1483829.399126598, 316145.1216261043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1935000.0000, 
sim time next is 1935600.0000, 
raw observation next is [26.0, 80.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.213667205817211, 6.9112, 168.9109807955851, 1698466.682908667, 1483888.546857709, 316141.7367644325], 
processed observation next is [1.0, 0.391304347826087, 0.4312796208530806, 0.8066666666666665, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03024672058172113, 0.0, 0.829430243487923, 0.4717963008079631, 0.4121912630160303, 0.4718533384543769], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1513035], dtype=float32), -1.8696202]. 
=============================================
[2019-03-27 01:11:03,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:11:03,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2124
[2019-03-27 01:11:03,663] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.03333333333333, 80.33333333333333, 1.0, 2.0, 0.5604960811354082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783237.3559466929, 783237.3559466923, 193617.3095072526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2140800.0000, 
sim time next is 2141400.0000, 
raw observation next is [28.86666666666667, 81.16666666666667, 1.0, 2.0, 0.5592848233189696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781544.1212854272, 781544.1212854272, 193406.0303552382], 
processed observation next is [0.0, 0.782608695652174, 0.567140600315956, 0.8116666666666668, 1.0, 1.0, 0.4690178594204452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21709558924595201, 0.21709558924595201, 0.2886657169481167], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.8286226], dtype=float32), -1.8454504]. 
=============================================
[2019-03-27 01:11:08,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:11:08,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6729
[2019-03-27 01:11:08,865] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.71666666666667, 94.0, 1.0, 2.0, 0.5499899662106054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768550.7971480859, 768550.7971480859, 191799.1527939289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2505000.0000, 
sim time next is 2505600.0000, 
raw observation next is [26.7, 94.0, 1.0, 2.0, 0.5491106376544865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767321.5878844807, 767321.5878844807, 191648.483568887], 
processed observation next is [1.0, 0.0, 0.46445497630331756, 0.94, 1.0, 1.0, 0.4567598044029958, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21314488552346686, 0.21314488552346686, 0.2860425127893836], 
reward next is 0.7140, 
noisyNet noise sample is [array([1.0201485], dtype=float32), 1.5723028]. 
=============================================
[2019-03-27 01:11:09,416] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:11:09,423] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8873
[2019-03-27 01:11:09,429] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.2, 76.33333333333333, 1.0, 2.0, 0.5726510587551965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 800229.122817712, 800229.1228177126, 195761.9802924602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2126400.0000, 
sim time next is 2127000.0000, 
raw observation next is [30.25, 76.16666666666667, 1.0, 2.0, 0.5739163567625523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 801997.9322146008, 801997.9322146003, 195987.6671177533], 
processed observation next is [0.0, 0.6086956521739131, 0.6327014218009479, 0.7616666666666667, 1.0, 1.0, 0.4866462129669304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22277720339294468, 0.22277720339294452, 0.2925189061459004], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.5946512], dtype=float32), 1.5775349]. 
=============================================
[2019-03-27 01:11:09,449] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[70.25146]
 [70.2531 ]
 [70.2587 ]
 [70.26579]
 [70.28228]], R is [[70.2509613 ]
 [70.25627136]
 [70.26197052]
 [70.26795197]
 [70.27401733]].
[2019-03-27 01:11:11,168] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:11:11,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7916
[2019-03-27 01:11:11,184] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.18333333333334, 92.66666666666666, 1.0, 2.0, 0.5262457040990456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735359.3039055692, 735359.3039055692, 187810.3494373734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2155800.0000, 
sim time next is 2156400.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.5246303416972946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733101.2680343774, 733101.2680343774, 187545.0784434577], 
processed observation next is [0.0, 1.0, 0.4360189573459717, 0.93, 1.0, 1.0, 0.4272654719244513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20363924112066037, 0.20363924112066037, 0.2799180275275488], 
reward next is 0.7201, 
noisyNet noise sample is [array([-0.4711579], dtype=float32), 0.9031367]. 
=============================================
[2019-03-27 01:11:21,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:11:21,571] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3502
[2019-03-27 01:11:21,577] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.78333333333333, 77.83333333333333, 1.0, 2.0, 0.572678841892785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800267.9619169089, 800267.9619169089, 195766.490518366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2321400.0000, 
sim time next is 2322000.0000, 
raw observation next is [29.7, 78.0, 1.0, 2.0, 0.5704048126491973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797089.0150994578, 797089.0150994578, 195362.0085995898], 
processed observation next is [1.0, 0.9130434782608695, 0.6066350710900474, 0.78, 1.0, 1.0, 0.4824154369267437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22141361530540496, 0.22141361530540496, 0.29158508746207434], 
reward next is 0.7084, 
noisyNet noise sample is [array([1.9131901], dtype=float32), 0.95392525]. 
=============================================
[2019-03-27 01:11:21,607] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.40458 ]
 [63.326958]
 [63.43866 ]
 [63.361343]
 [63.525665]], R is [[63.65282059]
 [63.72410202]
 [63.79418564]
 [63.86336136]
 [63.93165207]].
[2019-03-27 01:11:23,668] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 01:11:23,670] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:11:23,671] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:11:23,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:11:23,672] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:11:23,674] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:11:23,674] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:11:23,675] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:11:23,676] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:11:23,677] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:11:23,677] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:11:23,694] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-27 01:11:23,715] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-27 01:11:23,715] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-27 01:11:23,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-27 01:11:23,750] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-27 01:11:26,825] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.002660991]
[2019-03-27 01:11:26,826] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.05, 67.16666666666667, 1.0, 2.0, 0.2286069549251045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 380481.2247078702, 380481.2247078709, 158454.6645095272]
[2019-03-27 01:11:26,828] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:11:26,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9644343001408107
[2019-03-27 01:11:30,952] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.002660991]
[2019-03-27 01:11:30,952] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.6, 85.0, 1.0, 2.0, 0.2397853028395814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 395649.4862059378, 395649.4862059372, 159880.4258597444]
[2019-03-27 01:11:30,956] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:11:30,958] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7620777589069798
[2019-03-27 01:12:49,095] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.002660991]
[2019-03-27 01:12:49,097] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.4, 73.33333333333334, 1.0, 2.0, 0.5562784091882665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777341.4235484758, 777341.4235484758, 192884.32023557]
[2019-03-27 01:12:49,099] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:12:49,103] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.015363375203156449
[2019-03-27 01:12:56,251] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.002660991]
[2019-03-27 01:12:56,252] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.33333333333334, 82.0, 1.0, 2.0, 0.5170421773306886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722494.2094832232, 722494.2094832226, 186308.9335406418]
[2019-03-27 01:12:56,252] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:12:56,255] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7312093343880531
[2019-03-27 01:13:01,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.002660991]
[2019-03-27 01:13:01,837] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.53333333333333, 77.0, 1.0, 2.0, 0.3747493844665999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 588698.4156158856, 588698.4156158856, 174175.6952414407]
[2019-03-27 01:13:01,838] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:13:01,842] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26621458279765775
[2019-03-27 01:13:18,473] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:13:18,559] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:13:18,925] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:13:19,006] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:13:19,032] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:13:20,047] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 700000, evaluation results [700000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:13:20,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6669021e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:13:20,761] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7601
[2019-03-27 01:13:20,767] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3551310431925994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 547062.4761622881, 547062.4761622875, 170401.0263663732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2774400.0000, 
sim time next is 2775000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3502518463429573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 539550.1134290005, 539550.1134289999, 169779.5061785043], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21717089920838228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1498750315080557, 0.14987503150805553, 0.25340224802761835], 
reward next is 0.7466, 
noisyNet noise sample is [array([-1.1208353], dtype=float32), 1.0668774]. 
=============================================
[2019-03-27 01:13:20,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[61.383514]
 [61.229733]
 [60.80368 ]
 [60.097507]
 [59.478794]], R is [[61.48650742]
 [61.61731339]
 [61.74726105]
 [61.86869049]
 [61.97472   ]].
[2019-03-27 01:13:23,455] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:23,464] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3792
[2019-03-27 01:13:23,468] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 78.0, 1.0, 2.0, 0.5741745914193467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802358.9288674291, 802358.9288674291, 196033.3951373182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
processed observation next is [1.0, 0.9565217391304348, 0.6074249605055293, 0.7833333333333333, 1.0, 1.0, 0.4869236659138897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22286662728617787, 0.22286662728617787, 0.29257959931056793], 
reward next is 0.7074, 
noisyNet noise sample is [array([-1.6340712], dtype=float32), 0.023651216]. 
=============================================
[2019-03-27 01:13:24,636] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0821381e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:13:24,646] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8405
[2019-03-27 01:13:24,654] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 84.5, 1.0, 2.0, 0.8090034999362989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1130686.541939168, 1130686.541939169, 246132.8662682403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2442600.0000, 
sim time next is 2443200.0000, 
raw observation next is [27.66666666666666, 84.66666666666667, 1.0, 2.0, 0.7717248340943025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1078558.346710952, 1078558.346710952, 237088.8691639467], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.8466666666666667, 1.0, 1.0, 0.7249696796316897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29959954075304224, 0.29959954075304224, 0.35386398382678613], 
reward next is 0.6461, 
noisyNet noise sample is [array([0.29036292], dtype=float32), -0.25404796]. 
=============================================
[2019-03-27 01:13:30,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.100456e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:13:30,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9339
[2019-03-27 01:13:30,078] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 93.5, 1.0, 2.0, 0.6729491025262744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 940448.8057733263, 940448.8057733256, 215115.0690299311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2532600.0000, 
sim time next is 2533200.0000, 
raw observation next is [26.46666666666667, 93.33333333333334, 1.0, 2.0, 0.7492603171921808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047146.582536944, 1047146.582536944, 231837.6183765737], 
processed observation next is [1.0, 0.30434782608695654, 0.45339652448657203, 0.9333333333333335, 1.0, 1.0, 0.6979039966170852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2908740507047067, 0.2908740507047067, 0.34602629608443836], 
reward next is 0.6540, 
noisyNet noise sample is [array([0.29258302], dtype=float32), -0.43049768]. 
=============================================
[2019-03-27 01:13:41,070] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:41,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1730
[2019-03-27 01:13:41,089] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4755042199235282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664432.5644943232, 664432.5644943238, 179847.056403567], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2700000.0000, 
sim time next is 2700600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4758230458241832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664878.2063941116, 664878.206394111, 179894.6935287791], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.3684615009929918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.184688390665031, 0.18468839066503084, 0.2684995425802673], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.8848152], dtype=float32), 0.35717037]. 
=============================================
[2019-03-27 01:13:43,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:43,534] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8006
[2019-03-27 01:13:43,538] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 95.0, 1.0, 2.0, 0.3544060707907205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544588.1602606664, 544588.1602606664, 170155.7961132363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [22.0, 96.0, 1.0, 2.0, 0.3579552349395392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548122.8818235274, 548122.8818235274, 170393.262155754], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.96, 1.0, 1.0, 0.22645209028860142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15225635606209095, 0.15225635606209095, 0.25431830172500597], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.00970231], dtype=float32), 0.21538347]. 
=============================================
[2019-03-27 01:13:49,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:49,779] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0706
[2019-03-27 01:13:49,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4098329272822472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603852.4830929274, 603852.4830929274, 174672.3354776844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2836200.0000, 
sim time next is 2836800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4113506259202543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606088.3237429457, 606088.3237429451, 174881.5591191078], 
processed observation next is [1.0, 0.8695652173913043, 0.3364928909952607, 0.89, 1.0, 1.0, 0.29078388665090876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1683578677063738, 0.16835786770637365, 0.2610172524165788], 
reward next is 0.7390, 
noisyNet noise sample is [array([0.33430085], dtype=float32), 0.52883834]. 
=============================================
[2019-03-27 01:13:50,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:50,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1157
[2019-03-27 01:13:50,765] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3625740804952334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558502.2154738517, 558502.2154738524, 171362.3075981955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2860200.0000, 
sim time next is 2860800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3608919693442823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555931.4741309017, 555931.4741309017, 171145.0564798836], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2299903245111835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15442540948080605, 0.15442540948080605, 0.25544038280579645], 
reward next is 0.7446, 
noisyNet noise sample is [array([-0.80295527], dtype=float32), -0.79140687]. 
=============================================
[2019-03-27 01:13:54,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:54,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1812
[2019-03-27 01:13:54,525] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.3374536420638731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522158.545047688, 522158.5450476874, 168445.3945084656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3051000.0000, 
sim time next is 3051600.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.3408426909329262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526683.8322111608, 526683.8322111614, 168784.6009892069], 
processed observation next is [1.0, 0.30434782608695654, 0.22590837282780438, 0.96, 1.0, 1.0, 0.2058345673890677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1463010645031002, 0.14630106450310038, 0.251917314909264], 
reward next is 0.7481, 
noisyNet noise sample is [array([1.0145012], dtype=float32), -0.68018585]. 
=============================================
[2019-03-27 01:13:56,194] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:56,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6389
[2019-03-27 01:13:56,210] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8235318903294749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1195045.921058673, 1195045.921058672, 255959.9240597442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3076200.0000, 
sim time next is 3076800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.8334669727279125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1209470.981654011, 1209470.981654011, 258589.5951404254], 
processed observation next is [1.0, 0.6086956521739131, 0.28909952606635075, 1.0, 1.0, 1.0, 0.7993577984673644, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3359641615705586, 0.3359641615705586, 0.3859546196125752], 
reward next is 0.6140, 
noisyNet noise sample is [array([0.36055297], dtype=float32), 1.5192124]. 
=============================================
[2019-03-27 01:13:58,312] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:58,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9958
[2019-03-27 01:13:58,325] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 98.0, 1.0, 2.0, 0.4701993980493784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747117.9361224582, 747117.9361224582, 189293.4138336418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2985600.0000, 
sim time next is 2986200.0000, 
raw observation next is [20.5, 97.0, 1.0, 2.0, 0.4848472576099283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769491.9380906202, 769491.9380906209, 191715.1566591969], 
processed observation next is [1.0, 0.5652173913043478, 0.1706161137440759, 0.97, 1.0, 1.0, 0.3793340453131666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21374776058072784, 0.21374776058072803, 0.286142024864473], 
reward next is 0.7139, 
noisyNet noise sample is [array([0.94600374], dtype=float32), 0.5192214]. 
=============================================
[2019-03-27 01:13:58,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:13:58,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4505
[2019-03-27 01:13:58,831] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.0, 1.0, 2.0, 0.3118763279392696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495249.3745411828, 495249.3745411834, 166709.224319104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.3094566324914813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491993.0004156154, 491993.0004156154, 166478.719867586], 
processed observation next is [1.0, 0.7391304347826086, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16802003914636301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13666472233767094, 0.13666472233767094, 0.24847570129490448], 
reward next is 0.7515, 
noisyNet noise sample is [array([2.3227925], dtype=float32), -0.0759954]. 
=============================================
[2019-03-27 01:14:01,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:14:01,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1687
[2019-03-27 01:14:01,917] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3338099701619507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518466.0327573584, 518466.0327573584, 168212.4646880443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3047400.0000, 
sim time next is 3048000.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3345901144555405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519680.4394432403, 519680.4394432397, 168308.181866882], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19830134271751865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1443556776231223, 0.14435567762312215, 0.25120624159236116], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.9758227], dtype=float32), -1.9179252]. 
=============================================
[2019-03-27 01:14:01,941] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[71.27679]
 [71.20999]
 [71.13612]
 [71.25841]
 [71.17046]], R is [[71.36745453]
 [71.40271759]
 [71.43703461]
 [71.46801758]
 [71.50273132]].
[2019-03-27 01:14:05,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:14:05,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7482
[2019-03-27 01:14:05,789] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666666, 99.0, 1.0, 2.0, 0.3432213617658628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532063.7906343807, 532063.79063438, 169267.1162802483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3127800.0000, 
sim time next is 3128400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.342328538041652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531439.1029385976, 531439.1029385976, 169237.7812907807], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.20762474462849634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14762197303849933, 0.14762197303849933, 0.25259370341907567], 
reward next is 0.7474, 
noisyNet noise sample is [array([-0.36738217], dtype=float32), 0.47718552]. 
=============================================
[2019-03-27 01:14:07,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2107732e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:14:07,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8240
[2019-03-27 01:14:07,274] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 80.33333333333334, 1.0, 2.0, 0.8477329814321128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1229115.629818144, 1229115.629818144, 262270.1081799117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3152400.0000, 
sim time next is 3153000.0000, 
raw observation next is [25.83333333333334, 79.66666666666667, 1.0, 2.0, 0.8513808529999223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231236.254246626, 1231236.254246626, 262814.8963577074], 
processed observation next is [1.0, 0.4782608695652174, 0.42338072669826254, 0.7966666666666667, 1.0, 1.0, 0.8209407867468943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3420100706240628, 0.3420100706240628, 0.3922610393398618], 
reward next is 0.6077, 
noisyNet noise sample is [array([0.877601], dtype=float32), -0.5118247]. 
=============================================
[2019-03-27 01:14:07,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[62.17742 ]
 [62.248734]
 [62.743095]
 [63.26778 ]
 [63.586308]], R is [[62.14263535]
 [62.12976074]
 [62.10946274]
 [62.10362244]
 [62.12136459]].
[2019-03-27 01:14:15,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2066588e-29 1.0000000e+00 2.4126498e-38 2.1256128e-35 3.5560397e-35], sum to 1.0000
[2019-03-27 01:14:15,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1911
[2019-03-27 01:14:15,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2272724.893112583 W.
[2019-03-27 01:14:15,309] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.16666666666666, 74.16666666666667, 1.0, 2.0, 0.9840839990074661, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.003336942012719, 6.9112, 168.9124088841813, 2272724.893112583, 2207359.987257702, 458538.5928465658], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3402600.0000, 
sim time next is 3403200.0000, 
raw observation next is [30.33333333333334, 73.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.202396767616563, 6.9112, 168.9113308985003, 2493280.692549616, 2286697.671549505, 475599.7007909541], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783573, 0.7333333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.029119676761656256, 0.0, 0.8294319626535146, 0.6925779701526711, 0.6351937976526403, 0.7098502996879912], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.47344014], dtype=float32), 0.78947634]. 
=============================================
[2019-03-27 01:14:16,093] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 01:14:16,095] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:14:16,096] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:14:16,097] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:14:16,098] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:14:16,098] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:14:16,099] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:14:16,100] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:14:16,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:14:16,105] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:14:16,101] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:14:16,124] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-27 01:14:16,144] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-27 01:14:16,145] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-27 01:14:16,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-27 01:14:16,202] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-27 01:14:22,006] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.011214492]
[2019-03-27 01:14:22,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.8, 86.0, 1.0, 2.0, 0.2733448314107534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442603.5072770095, 442603.5072770101, 163104.8118955987]
[2019-03-27 01:14:22,008] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:14:22,010] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4074715289026736
[2019-03-27 01:14:29,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.011214492]
[2019-03-27 01:14:29,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.73316233, 71.63101226, 1.0, 2.0, 0.3263023145566263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510582.0346087838, 510582.0346087844, 167706.0708207363]
[2019-03-27 01:14:29,371] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:14:29,375] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2005808713433016
[2019-03-27 01:14:38,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.011214492]
[2019-03-27 01:14:38,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.43865014666667, 89.38530317333334, 1.0, 2.0, 0.5971243068930685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834441.752696346, 834441.752696346, 200210.9917282904]
[2019-03-27 01:14:38,273] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:14:38,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3263910637563531
[2019-03-27 01:14:49,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.011214492]
[2019-03-27 01:14:49,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.45, 77.0, 1.0, 2.0, 0.5238877364036761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749992.8591422269, 749992.8591422262, 189708.3958125643]
[2019-03-27 01:14:49,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:14:49,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.00789352348620287
[2019-03-27 01:15:07,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.011214492]
[2019-03-27 01:15:07,226] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.12794255, 62.80025782, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.111421853018164, 6.9112, 168.9115747784679, 2425851.760547597, 2283808.648854482, 475694.4294335564]
[2019-03-27 01:15:07,226] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:15:07,229] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8200524e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.42264133638577517
[2019-03-27 01:15:07,229] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2425851.760547597 W.
[2019-03-27 01:16:10,735] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:16:11,190] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:16:11,412] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:16:11,556] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:16:11,612] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:16:12,631] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 725000, evaluation results [725000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:16:13,940] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:16:13,948] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3572
[2019-03-27 01:16:13,955] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.0, 1.0, 2.0, 0.5350461803463259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747661.1423160979, 747661.1423160972, 189270.173455255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3321000.0000, 
sim time next is 3321600.0000, 
raw observation next is [30.33333333333333, 71.33333333333333, 1.0, 2.0, 0.5411709263616752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756222.7671212461, 756222.7671212461, 190299.097676928], 
processed observation next is [0.0, 0.43478260869565216, 0.6366508688783569, 0.7133333333333333, 1.0, 1.0, 0.44719388718274117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21006187975590168, 0.21006187975590168, 0.28402850399541496], 
reward next is 0.7160, 
noisyNet noise sample is [array([-1.5974529], dtype=float32), 1.8273267]. 
=============================================
[2019-03-27 01:16:16,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:16:16,057] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8177
[2019-03-27 01:16:16,067] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 75.66666666666667, 1.0, 2.0, 0.6083247724871772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850099.9267960817, 850099.9267960817, 202306.4935603956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3341400.0000, 
sim time next is 3342000.0000, 
raw observation next is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5941244627154119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830248.030297597, 830248.030297597, 199656.6287523603], 
processed observation next is [0.0, 0.6956521739130435, 0.6524486571879939, 0.7633333333333334, 1.0, 1.0, 0.5109933285727853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2306244528604436, 0.2306244528604436, 0.2979949682871049], 
reward next is 0.7020, 
noisyNet noise sample is [array([1.0111382], dtype=float32), 0.39440086]. 
=============================================
[2019-03-27 01:16:16,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.58949 ]
 [71.68232 ]
 [71.622314]
 [71.61603 ]
 [71.58954 ]], R is [[71.64396667]
 [71.62557983]
 [71.61125183]
 [71.59705353]
 [71.58296967]].
[2019-03-27 01:16:16,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.10262235e-36 1.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00], sum to 1.0000
[2019-03-27 01:16:16,140] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5677
[2019-03-27 01:16:16,146] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 80.66666666666667, 1.0, 2.0, 0.7387330742704793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1032426.831788324, 1032426.831788325, 229424.6209387329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3466200.0000, 
sim time next is 3466800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7239280371469882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1011725.986990375, 1011725.986990374, 226091.4214379836], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6673831772855279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28103499638621526, 0.281034996386215, 0.3374498827432591], 
reward next is 0.6626, 
noisyNet noise sample is [array([0.2537305], dtype=float32), 1.5990105]. 
=============================================
[2019-03-27 01:16:17,833] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1412658e-24 1.0000000e+00 7.7271321e-32 7.3958599e-28 9.3223450e-30], sum to 1.0000
[2019-03-27 01:16:17,843] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2542
[2019-03-27 01:16:17,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2224231.415482284 W.
[2019-03-27 01:16:17,855] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.7953152368921741, 1.0, 1.0, 0.7953152368921741, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2224231.415482284, 2224231.415482283, 417599.179970051], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3402000.0000, 
sim time next is 3402600.0000, 
raw observation next is [30.16666666666666, 74.16666666666667, 1.0, 2.0, 0.9685791649579601, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.003182786727661, 6.9112, 168.9124093162649, 2251023.640089172, 2185768.096777639, 453885.51091225], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235385, 0.7416666666666667, 1.0, 1.0, 0.9621435722385062, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009198278672766059, 0.0, 0.8294372581784405, 0.6252843444692144, 0.6071578046604552, 0.6774410610630598], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26553646], dtype=float32), -0.68751925]. 
=============================================
[2019-03-27 01:16:20,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.480062e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:16:20,182] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0176
[2019-03-27 01:16:20,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2869289.906174036 W.
[2019-03-27 01:16:20,195] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666666, 67.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.735924641465632, 6.9112, 168.9084269072213, 2869289.906174036, 2284217.550523391, 474217.4305925946], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3429600.0000, 
sim time next is 3430200.0000, 
raw observation next is [31.33333333333334, 68.83333333333333, 1.0, 2.0, 0.9117637805046765, 1.0, 1.0, 0.9117637805046765, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2550231.456805768, 2550231.456805768, 477950.7949009421], 
processed observation next is [1.0, 0.6956521739130435, 0.6840442338072673, 0.6883333333333332, 1.0, 1.0, 0.8936913018128633, 1.0, 0.5, 0.8936913018128633, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7083976268904911, 0.7083976268904911, 0.7133593953745405], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18709438], dtype=float32), 0.44850346]. 
=============================================
[2019-03-27 01:16:22,344] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.528006e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:16:22,352] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6841
[2019-03-27 01:16:22,356] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5023997800195541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702026.7437834226, 702026.7437834232, 183972.8808807829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3462600.0000, 
sim time next is 3463200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5016900063401247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701034.6165113848, 701034.6165113841, 183861.1716263127], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.39962651366280083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19473183791982912, 0.19473183791982893, 0.27441965914375027], 
reward next is 0.7256, 
noisyNet noise sample is [array([-0.92581373], dtype=float32), 0.06464469]. 
=============================================
[2019-03-27 01:16:25,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:16:25,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1095
[2019-03-27 01:16:25,869] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 73.33333333333334, 1.0, 2.0, 0.5570663515143547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778442.8947538843, 778442.8947538843, 193020.7645969841], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3523200.0000, 
sim time next is 3523800.0000, 
raw observation next is [30.16666666666666, 74.16666666666666, 1.0, 2.0, 0.556690962470856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777918.1351099377, 777918.1351099372, 192955.5946542792], 
processed observation next is [1.0, 0.782608695652174, 0.6287519747235385, 0.7416666666666666, 1.0, 1.0, 0.4658927258685012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2160883708638716, 0.21608837086387145, 0.28799342485713314], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.3889886], dtype=float32), 0.7123886]. 
=============================================
[2019-03-27 01:16:38,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:16:38,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4738
[2019-03-27 01:16:38,026] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 78.16666666666667, 1.0, 2.0, 0.4854504547419786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678335.0957684433, 678335.0957684427, 181347.2992891828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3715800.0000, 
sim time next is 3716400.0000, 
raw observation next is [27.0, 77.33333333333334, 1.0, 2.0, 0.4822345228592408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673839.9483724871, 673839.9483724865, 180858.7175798029], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.7733333333333334, 1.0, 1.0, 0.37618617211956723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18717776343680198, 0.18717776343680181, 0.26993838444746704], 
reward next is 0.7301, 
noisyNet noise sample is [array([-0.27167648], dtype=float32), 0.099537596]. 
=============================================
[2019-03-27 01:16:49,856] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:16:49,865] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0639
[2019-03-27 01:16:49,870] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.576169738306765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805148.0293902752, 805148.0293902752, 196390.060684455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3902400.0000, 
sim time next is 3903000.0000, 
raw observation next is [27.83333333333334, 89.83333333333334, 1.0, 2.0, 0.576277442121096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805298.5933324341, 805298.5933324341, 196409.2361476552], 
processed observation next is [0.0, 0.17391304347826086, 0.5181674565560824, 0.8983333333333334, 1.0, 1.0, 0.48949089412180236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2236940537034539, 0.2236940537034539, 0.29314811365321675], 
reward next is 0.7069, 
noisyNet noise sample is [array([-0.4322111], dtype=float32), -0.07099334]. 
=============================================
[2019-03-27 01:16:49,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.179596]
 [70.346115]
 [70.54244 ]
 [70.70974 ]
 [70.82986 ]], R is [[70.06881714]
 [70.07500458]
 [70.08162689]
 [70.08857727]
 [70.09570312]].
[2019-03-27 01:16:53,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0653872e-31 1.0000000e+00 0.0000000e+00 8.4782200e-38 6.4329904e-36], sum to 1.0000
[2019-03-27 01:16:53,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3435
[2019-03-27 01:16:53,260] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 92.33333333333334, 1.0, 2.0, 0.8374828192902236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1170511.996205898, 1170511.996205898, 253321.216635226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4086600.0000, 
sim time next is 4087200.0000, 
raw observation next is [27.33333333333334, 90.66666666666667, 1.0, 2.0, 0.7806793902102795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1091079.614132834, 1091079.614132834, 239225.2127699922], 
processed observation next is [1.0, 0.30434782608695654, 0.4944707740916275, 0.9066666666666667, 1.0, 1.0, 0.7357583014581681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3030776705924539, 0.3030776705924539, 0.3570525563731227], 
reward next is 0.6429, 
noisyNet noise sample is [array([1.0220882], dtype=float32), -0.89834386]. 
=============================================
[2019-03-27 01:16:53,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4771509e-21 1.0000000e+00 2.1830630e-27 2.9161897e-23 7.5465190e-25], sum to 1.0000
[2019-03-27 01:16:53,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1542
[2019-03-27 01:16:53,621] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2176614.887170687 W.
[2019-03-27 01:16:53,627] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666666, 79.83333333333334, 1.0, 2.0, 0.5188704958035266, 1.0, 2.0, 0.5188704958035266, 1.0, 1.0, 0.9011064581156673, 6.9112, 6.9112, 170.5573041426782, 2176614.887170687, 2176614.887170687, 428441.1147005333], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4092600.0000, 
sim time next is 4093200.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5013982465696953, 1.0, 2.0, 0.5013982465696953, 1.0, 2.0, 0.8707629393576196, 6.9112, 6.9112, 170.5573041426782, 2103248.422233628, 2103248.422233628, 416114.8246544851], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.79, 1.0, 1.0, 0.3992749958671028, 1.0, 1.0, 0.3992749958671028, 1.0, 1.0, 0.8423938284849019, 0.0, 0.0, 0.8375144448122397, 0.5842356728426745, 0.5842356728426745, 0.6210669024693808], 
reward next is 0.3789, 
noisyNet noise sample is [array([-0.06769828], dtype=float32), 2.048392]. 
=============================================
[2019-03-27 01:16:53,981] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0377939e-26 1.0000000e+00 1.7714109e-32 8.6045716e-31 1.9086644e-31], sum to 1.0000
[2019-03-27 01:16:53,992] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4404
[2019-03-27 01:16:54,001] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1735743.309486838 W.
[2019-03-27 01:16:54,005] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4138587647506943, 1.0, 2.0, 0.4138587647506943, 1.0, 2.0, 0.7187358091870304, 6.9112, 6.9112, 170.5573041426782, 1735743.309486838, 1735743.309486838, 360693.4820828149], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3986400.0000, 
sim time next is 3987000.0000, 
raw observation next is [29.5, 81.5, 1.0, 2.0, 0.5932347524995831, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.911486594845339, 6.9112, 168.9129001399639, 1658656.020845072, 1658452.700665808, 363328.3540522438], 
processed observation next is [1.0, 0.13043478260869565, 0.5971563981042655, 0.815, 1.0, 1.0, 0.5099213885537146, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 2.8659484533921643e-05, 0.0, 0.8294396683475027, 0.4607377835680756, 0.46068130574050226, 0.5422811254511102], 
reward next is 0.4563, 
noisyNet noise sample is [array([0.01893793], dtype=float32), 0.110482775]. 
=============================================
[2019-03-27 01:16:54,022] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[39.04338 ]
 [38.117264]
 [37.156227]
 [38.375893]
 [37.99447 ]], R is [[40.20737839]
 [40.26695633]
 [40.28164673]
 [40.25662613]
 [40.28478622]].
[2019-03-27 01:17:01,929] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8908209e-23 1.0000000e+00 3.7989781e-30 3.9569218e-28 1.4075147e-26], sum to 1.0000
[2019-03-27 01:17:01,937] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9201
[2019-03-27 01:17:01,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2955695.338376746 W.
[2019-03-27 01:17:01,951] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.33333333333334, 66.5, 1.0, 2.0, 0.7675595783540041, 1.0, 2.0, 0.7043698286912646, 1.0, 2.0, 1.03, 7.005103059770689, 6.9112, 170.5573041426782, 2955695.338376746, 2888428.756930749, 543518.3889894654], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [34.66666666666667, 66.0, 1.0, 2.0, 1.006699934021896, 1.0, 2.0, 1.006699934021896, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2816070.081244535, 2816070.081244536, 533062.857556972], 
processed observation next is [1.0, 0.6086956521739131, 0.8420221169036337, 0.66, 1.0, 1.0, 1.008072209664935, 1.0, 1.0, 1.008072209664935, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.782241689234593, 0.7822416892345934, 0.7956162053089134], 
reward next is 0.2044, 
noisyNet noise sample is [array([0.68639284], dtype=float32), -1.6534142]. 
=============================================
[2019-03-27 01:17:02,294] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6503096e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:17:02,301] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3281
[2019-03-27 01:17:02,304] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6240140405236253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872033.8040864693, 872033.8040864693, 205306.2115416442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4127400.0000, 
sim time next is 4128000.0000, 
raw observation next is [31.66666666666666, 76.33333333333333, 1.0, 2.0, 0.6211045601145273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867966.2642322266, 867966.2642322266, 204744.6780877479], 
processed observation next is [1.0, 0.782608695652174, 0.6998420221169034, 0.7633333333333333, 1.0, 1.0, 0.5434994700175028, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2411017400645074, 0.2411017400645074, 0.30558907177275807], 
reward next is 0.6944, 
noisyNet noise sample is [array([-0.7318265], dtype=float32), 1.5961665]. 
=============================================
[2019-03-27 01:17:02,321] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[69.20073]
 [69.76544]
 [70.6261 ]
 [71.71011]
 [71.53671]], R is [[68.24196625]
 [68.25312042]
 [68.26352692]
 [68.27362823]
 [68.28504944]].
[2019-03-27 01:17:04,570] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6364729e-28 1.0000000e+00 7.9250984e-36 3.2898398e-28 1.6586560e-31], sum to 1.0000
[2019-03-27 01:17:04,577] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3516
[2019-03-27 01:17:04,581] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 88.16666666666667, 1.0, 2.0, 0.8064759847043237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1127152.139103602, 1127152.139103602, 245509.9468453412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [28.66666666666667, 87.33333333333334, 1.0, 2.0, 0.7760527835367242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084610.153922436, 1084610.153922437, 238121.4827969765], 
processed observation next is [1.0, 0.2608695652173913, 0.5576619273301741, 0.8733333333333334, 1.0, 1.0, 0.7301840765502702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3012805983117878, 0.30128059831178805, 0.3554051982044425], 
reward next is 0.6446, 
noisyNet noise sample is [array([1.070299], dtype=float32), 0.3714589]. 
=============================================
[2019-03-27 01:17:04,597] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[52.829758]
 [52.886494]
 [53.0107  ]
 [52.930424]
 [52.92506 ]], R is [[52.8480835 ]
 [52.95317078]
 [53.02719498]
 [53.10179138]
 [53.16760635]].
[2019-03-27 01:17:08,756] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 01:17:08,758] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:17:08,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:17:08,759] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:17:08,760] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:17:08,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:17:08,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:17:08,762] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:17:08,762] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:17:08,765] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:17:08,765] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:17:08,783] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-27 01:17:08,803] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-27 01:17:08,804] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-27 01:17:08,843] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-27 01:17:08,864] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-27 01:17:13,916] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:13,918] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.19684494, 87.00756155, 1.0, 2.0, 0.2219681932919918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 369712.1917940357, 369712.191794035, 157787.5294437255]
[2019-03-27 01:17:13,920] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:17:13,923] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21994539756461395
[2019-03-27 01:17:19,709] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:19,710] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.92056897666667, 51.21386822666666, 1.0, 2.0, 0.4618174228512561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751259.4022995488, 751259.4022995494, 189011.0405475118]
[2019-03-27 01:17:19,710] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:17:19,712] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3638582e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.483193560430792
[2019-03-27 01:17:26,257] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:26,258] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.58333333333333, 77.16666666666667, 1.0, 2.0, 0.6333834938920408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 992957.2007269862, 992957.2007269862, 219962.0367019851]
[2019-03-27 01:17:26,259] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:17:26,264] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7006070e-33 1.0000000e+00 0.0000000e+00 1.7918428e-37 4.4893707e-38], sampled 0.45270015206680125
[2019-03-27 01:17:32,370] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:32,371] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.9, 50.0, 1.0, 2.0, 0.3154438896230984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 499305.4494896589, 499305.4494896583, 166982.6883000706]
[2019-03-27 01:17:32,372] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:17:32,375] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9974273179898722
[2019-03-27 01:17:34,379] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:34,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.93333333333333, 84.66666666666666, 1.0, 2.0, 0.5090827636249359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711368.3166479467, 711368.3166479474, 185031.53148485]
[2019-03-27 01:17:34,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:17:34,387] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3550844e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4278148000384101
[2019-03-27 01:17:34,968] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:34,969] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.75, 76.0, 1.0, 2.0, 0.7156336489546339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1020297.379466703, 1020297.379466703, 226973.0338097693]
[2019-03-27 01:17:34,970] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:17:34,971] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1413114e-33 1.0000000e+00 0.0000000e+00 3.9002118e-37 9.6470375e-38], sampled 0.7032938146979668
[2019-03-27 01:17:47,085] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:47,086] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.91666666666667, 94.83333333333333, 1.0, 2.0, 0.4363039822492621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626099.986140532, 626099.986140532, 176339.7801899826]
[2019-03-27 01:17:47,087] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:17:47,090] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2465023e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6943616877361567
[2019-03-27 01:17:53,636] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:53,637] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.26304616833333, 100.0, 1.0, 2.0, 0.3232106654365874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509053.5773021967, 509053.5773021967, 167667.6281380171]
[2019-03-27 01:17:53,637] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:17:53,641] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.728503e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.30693428050919835
[2019-03-27 01:17:54,240] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:17:54,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.15, 89.0, 1.0, 2.0, 0.5142845730575798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718639.5393069873, 718639.5393069879, 185863.8784201521]
[2019-03-27 01:17:54,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:17:54,245] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0937171070922409
[2019-03-27 01:18:20,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:18:20,479] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 74.16666666666667, 1.0, 2.0, 0.7735599193817397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1081124.35731161, 1081124.35731161, 237525.1061156371]
[2019-03-27 01:18:20,482] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:18:20,484] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.7997951e-30 1.0000000e+00 1.4793034e-37 3.8377900e-33 1.1093534e-33], sampled 0.12563773444483006
[2019-03-27 01:18:39,082] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.005135146]
[2019-03-27 01:18:39,083] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.45377079, 71.17942535, 1.0, 2.0, 0.595814084664262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832610.0860854627, 832610.0860854627, 199968.4545666342]
[2019-03-27 01:18:39,085] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:18:39,087] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3102146e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.13587760842553154
[2019-03-27 01:19:03,154] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:19:03,513] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:19:03,652] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:19:03,920] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:19:03,944] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:19:04,960] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 750000, evaluation results [750000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:19:09,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6034614e-29 1.0000000e+00 0.0000000e+00 5.6104517e-32 4.8030969e-35], sum to 1.0000
[2019-03-27 01:19:09,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8815
[2019-03-27 01:19:09,674] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.83333333333334, 48.33333333333333, 1.0, 2.0, 0.4669901766384168, 1.0, 2.0, 0.4669901766384168, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1305457.428722132, 1305457.428722132, 290332.4396455654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4295400.0000, 
sim time next is 4296000.0000, 
raw observation next is [36.66666666666667, 48.66666666666666, 1.0, 2.0, 0.5654360869027473, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564251862, 790143.0887616877, 790143.0887616877, 194485.8428744563], 
processed observation next is [1.0, 0.7391304347826086, 0.9368088467614536, 0.4866666666666666, 1.0, 1.0, 0.4764290203647557, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399447337116, 0.21948419132269104, 0.21948419132269104, 0.29027737742456167], 
reward next is 0.7097, 
noisyNet noise sample is [array([-2.2761803], dtype=float32), -0.7096946]. 
=============================================
[2019-03-27 01:19:09,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[53.514275]
 [44.32559 ]
 [43.67735 ]
 [43.23104 ]
 [44.294464]], R is [[60.42009735]
 [60.38256454]
 [59.77873993]
 [59.18095398]
 [58.58914566]].
[2019-03-27 01:19:17,140] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:17,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8660
[2019-03-27 01:19:17,152] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5799923344610609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810491.8192723399, 810491.8192723405, 197077.8147772468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4425600.0000, 
sim time next is 4426200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5805682225253043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811296.8830547383, 811296.8830547389, 197181.7017340731], 
processed observation next is [0.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4946605090666316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22536024529298285, 0.225360245292983, 0.29430104736428825], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.39416093], dtype=float32), -0.89108807]. 
=============================================
[2019-03-27 01:19:17,418] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:17,427] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9343
[2019-03-27 01:19:17,431] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 79.0, 1.0, 2.0, 0.5995807247293435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837875.7882265762, 837875.7882265768, 200668.4283339775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [30.66666666666666, 79.0, 1.0, 2.0, 0.6048589406435843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845254.6929803054, 845254.6929803054, 201655.3184220949], 
processed observation next is [0.0, 0.34782608695652173, 0.6524486571879934, 0.79, 1.0, 1.0, 0.5239264345103425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23479297027230706, 0.23479297027230706, 0.3009780871971566], 
reward next is 0.6990, 
noisyNet noise sample is [array([-0.78767884], dtype=float32), -0.3182114]. 
=============================================
[2019-03-27 01:19:18,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1938325e-31 1.0000000e+00 0.0000000e+00 4.1897554e-33 8.3134836e-36], sum to 1.0000
[2019-03-27 01:19:18,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6215
[2019-03-27 01:19:18,721] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.6804918828114739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 950994.5880500379, 950994.5880500379, 216689.0901414799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4858200.0000, 
sim time next is 4858800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7017733064338769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980749.3349556434, 980749.3349556434, 221226.7558547518], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.84, 1.0, 1.0, 0.6406907306432252, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27243037082101207, 0.27243037082101207, 0.3301891878429131], 
reward next is 0.6698, 
noisyNet noise sample is [array([-0.8188884], dtype=float32), -0.14264719]. 
=============================================
[2019-03-27 01:19:20,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:20,128] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7551
[2019-03-27 01:19:20,135] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.507506380853706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709164.8182007936, 709164.8182007929, 184780.0542154979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5074416623186471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709074.3534734903, 709074.3534734909, 184769.774726508], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4065562196610205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19696509818708066, 0.1969650981870808, 0.27577578317389256], 
reward next is 0.7242, 
noisyNet noise sample is [array([-0.65496135], dtype=float32), -1.1215177]. 
=============================================
[2019-03-27 01:19:20,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.688095]
 [71.53929 ]
 [71.199   ]
 [70.38433 ]
 [69.07863 ]], R is [[71.59939575]
 [71.60761261]
 [71.61567688]
 [71.62363434]
 [71.63155365]].
[2019-03-27 01:19:22,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:22,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3570
[2019-03-27 01:19:22,597] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5025154840471426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702188.4758776496, 702188.4758776496, 183990.9712561955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5042153837952326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704564.6133413261, 704564.6133413261, 184258.9049253129], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4026691371026898, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19571239259481282, 0.19571239259481282, 0.27501329093330285], 
reward next is 0.7250, 
noisyNet noise sample is [array([-0.00696032], dtype=float32), -0.21389656]. 
=============================================
[2019-03-27 01:19:24,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:24,204] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4058
[2019-03-27 01:19:24,212] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5220185791478016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729450.423422317, 729450.423422317, 187117.5868678702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4568400.0000, 
sim time next is 4569000.0000, 
raw observation next is [28.0, 79.83333333333334, 1.0, 2.0, 0.5251265953594552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733794.9561776898, 733794.9561776898, 187626.4038743898], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.7983333333333335, 1.0, 1.0, 0.427863367902958, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2038319322715805, 0.2038319322715805, 0.28003940876774597], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.1896199], dtype=float32), 0.9534243]. 
=============================================
[2019-03-27 01:19:24,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.72977]
 [81.65288]
 [81.66813]
 [81.67007]
 [81.6716 ]], R is [[81.6072464 ]
 [81.51189423]
 [81.41747284]
 [81.32398224]
 [81.23140717]].
[2019-03-27 01:19:27,490] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8387417e-22 1.0000000e+00 1.1504223e-30 1.0461217e-30 2.1617416e-26], sum to 1.0000
[2019-03-27 01:19:27,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3596
[2019-03-27 01:19:27,504] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 79.0, 1.0, 2.0, 0.5097309338202364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712274.3428589283, 712274.3428589288, 185134.5623917884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4998000.0000, 
sim time next is 4998600.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5055472138692355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706426.262918072, 706426.2629180714, 184469.471636405], 
processed observation next is [1.0, 0.8695652173913043, 0.5023696682464456, 0.79, 1.0, 1.0, 0.4042737516496813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19622951747724224, 0.19622951747724204, 0.2753275696065746], 
reward next is 0.7247, 
noisyNet noise sample is [array([1.5601318], dtype=float32), 1.0424048]. 
=============================================
[2019-03-27 01:19:30,286] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:30,295] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6495
[2019-03-27 01:19:30,299] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.08333333333333, 94.00000000000001, 1.0, 2.0, 0.4527319079443171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648668.7607734681, 648668.7607734681, 178584.2729055045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4655400.0000, 
sim time next is 4656000.0000, 
raw observation next is [24.16666666666666, 94.0, 1.0, 2.0, 0.4558572356147115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650958.3237914713, 650958.3237914713, 178766.5752197482], 
processed observation next is [1.0, 0.9130434782608695, 0.34439178515007873, 0.94, 1.0, 1.0, 0.34440630796953187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18082175660874203, 0.18082175660874203, 0.26681578391007194], 
reward next is 0.7332, 
noisyNet noise sample is [array([0.52939814], dtype=float32), -0.5499527]. 
=============================================
[2019-03-27 01:19:30,310] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[67.899635]
 [67.85766 ]
 [67.64635 ]
 [67.87513 ]
 [68.19519 ]], R is [[68.12812042]
 [68.18029785]
 [68.23208618]
 [68.28286743]
 [68.33254242]].
[2019-03-27 01:19:30,526] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7364873e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:19:30,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6108
[2019-03-27 01:19:30,543] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2504530.947223508 W.
[2019-03-27 01:19:30,550] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8954412030290235, 1.0, 2.0, 0.8954412030290235, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2504530.947223508, 2504530.947223507, 469010.7915308224], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4640400.0000, 
sim time next is 4641000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.3956277264436011, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6862759200759443, 6.911199999999999, 6.9112, 168.9129565104283, 1105868.444562338, 1105868.444562339, 257573.595944821], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.2718406342693989, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.6174096586292004, -8.881784197001253e-17, 0.0, 0.8294399451522895, 0.3071856790450939, 0.30718567904509414, 0.3844382029027179], 
reward next is 0.6156, 
noisyNet noise sample is [array([-0.52283347], dtype=float32), 1.6931053]. 
=============================================
[2019-03-27 01:19:30,564] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[54.03186 ]
 [52.889935]
 [52.29937 ]
 [52.599976]
 [50.602943]], R is [[61.00094604]
 [60.69092178]
 [60.08401489]
 [59.48317719]
 [59.19750977]].
[2019-03-27 01:19:42,827] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.08003069e-30 1.00000000e+00 3.43298169e-37 1.02728334e-35
 9.53726537e-38], sum to 1.0000
[2019-03-27 01:19:42,835] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8523
[2019-03-27 01:19:42,840] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 85.66666666666667, 1.0, 2.0, 0.6910342311826162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965734.3298855813, 965734.3298855813, 218921.6578950011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4864800.0000, 
sim time next is 4865400.0000, 
raw observation next is [27.5, 84.0, 1.0, 2.0, 0.6866836397159045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959651.5431896322, 959651.5431896322, 217996.7302538599], 
processed observation next is [1.0, 0.30434782608695654, 0.5023696682464456, 0.84, 1.0, 1.0, 0.6225104092962704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26656987310823116, 0.26656987310823116, 0.3253682541102386], 
reward next is 0.6746, 
noisyNet noise sample is [array([0.47556132], dtype=float32), 1.2386229]. 
=============================================
[2019-03-27 01:19:44,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8348425e-26 1.0000000e+00 8.3341397e-32 1.8370157e-31 6.7819465e-31], sum to 1.0000
[2019-03-27 01:19:44,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0646
[2019-03-27 01:19:44,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2289898.072559787 W.
[2019-03-27 01:19:44,992] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 65.0, 1.0, 2.0, 0.8187740777913142, 1.0, 2.0, 0.8187740777913142, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2289898.072559787, 2289898.072559787, 429112.7642522829], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4886400.0000, 
sim time next is 4887000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.8486562630963616, 1.0, 2.0, 0.8486562630963616, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2373550.135943144, 2373550.135943144, 444251.9935057283], 
processed observation next is [1.0, 0.5652173913043478, 0.6919431279620853, 0.645, 1.0, 1.0, 0.8176581483088693, 1.0, 1.0, 0.8176581483088693, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6593194822064289, 0.6593194822064289, 0.6630626768742214], 
reward next is 0.3369, 
noisyNet noise sample is [array([0.6838781], dtype=float32), 0.51525444]. 
=============================================
[2019-03-27 01:19:45,009] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[44.020416]
 [44.89283 ]
 [45.322742]
 [46.067993]
 [46.184364]], R is [[44.01181412]
 [43.93122864]
 [43.49191666]
 [43.05699921]
 [42.62643051]].
[2019-03-27 01:19:49,974] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1989605e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:19:49,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9939
[2019-03-27 01:19:49,994] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5084905153081465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710540.4599146695, 710540.4599146689, 184936.5208529279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5101800.0000, 
sim time next is 5102400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5072402645211846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708792.8360748034, 708792.8360748028, 184737.793928313], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4063135717122706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1968868989096676, 0.19688689890966743, 0.27572805063927314], 
reward next is 0.7243, 
noisyNet noise sample is [array([-1.061806], dtype=float32), 0.6501926]. 
=============================================
[2019-03-27 01:19:52,404] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:19:52,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3045
[2019-03-27 01:19:52,425] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4774453943239098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667270.4197075511, 667270.4197075511, 180153.3690864326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5016000.0000, 
sim time next is 5016600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4769108836161082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666526.0922993605, 666526.0922993605, 180073.6285332501], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3697721489350701, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1851461367498224, 0.1851461367498224, 0.26876660975111955], 
reward next is 0.7312, 
noisyNet noise sample is [array([-1.0533644], dtype=float32), -0.27495596]. 
=============================================
[2019-03-27 01:20:01,134] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 01:20:01,137] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:20:01,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:20:01,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:20:01,141] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:20:01,143] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:20:01,144] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:20:01,142] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:20:01,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:20:01,149] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:20:01,151] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:20:01,167] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-27 01:20:01,187] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-27 01:20:01,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-27 01:20:01,204] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-27 01:20:01,229] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-27 01:20:29,912] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.027782794]
[2019-03-27 01:20:29,913] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.19360013666667, 85.27307199333333, 1.0, 2.0, 0.4986527137386264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 696789.0753177841, 696789.0753177847, 183384.5503829036]
[2019-03-27 01:20:29,917] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:20:29,921] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.868483156520289
[2019-03-27 01:20:51,138] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.027782794]
[2019-03-27 01:20:51,139] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.46666666666667, 57.0, 1.0, 2.0, 0.5382306182368137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752112.5774641241, 752112.5774641247, 189802.2540652197]
[2019-03-27 01:20:51,142] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:20:51,145] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16302685500357172
[2019-03-27 01:20:56,032] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.027782794]
[2019-03-27 01:20:56,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.9, 63.0, 1.0, 2.0, 0.554829925623451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775316.5791691511, 775316.5791691517, 192632.4859411352]
[2019-03-27 01:20:56,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:20:56,035] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.442557e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.6026489127657797
[2019-03-27 01:21:31,013] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.027782794]
[2019-03-27 01:21:31,013] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.53333333333333, 74.66666666666666, 1.0, 2.0, 0.5536442886361403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773659.1725635516, 773659.1725635516, 192427.2538485895]
[2019-03-27 01:21:31,015] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:21:31,018] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7075163758097791
[2019-03-27 01:21:35,584] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.027782794]
[2019-03-27 01:21:35,587] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.26900576, 82.51615051333333, 1.0, 2.0, 0.5765091388199106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805622.4925032012, 805622.4925032012, 196451.655980352]
[2019-03-27 01:21:35,589] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:21:35,592] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7891885180927356
[2019-03-27 01:21:55,740] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:21:56,038] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:21:56,299] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:21:56,519] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:21:56,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:21:57,638] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 775000, evaluation results [775000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:22:03,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0753937e-15 1.0000000e+00 7.3964578e-19 1.4855227e-16 1.8029824e-17], sum to 1.0000
[2019-03-27 01:22:03,886] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3380
[2019-03-27 01:22:03,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2653899.292394165 W.
[2019-03-27 01:22:03,898] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 73.66666666666667, 1.0, 2.0, 0.6325253251482028, 1.0, 2.0, 0.6325253251482028, 1.0, 2.0, 1.03, 6.988194311151082, 6.9112, 170.5573041426782, 2653899.292394165, 2598745.135388311, 500074.2501135534], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5301600.0000, 
sim time next is 5302200.0000, 
raw observation next is [32.16666666666667, 71.83333333333333, 1.0, 2.0, 0.6028845203468041, 1.0, 2.0, 0.6028845203468041, 1.0, 2.0, 1.03, 6.930323071451189, 6.9112, 170.5573041426782, 2529408.883095819, 2515710.24913852, 489027.8877887445], 
processed observation next is [1.0, 0.34782608695652173, 0.7235387045813588, 0.7183333333333333, 1.0, 1.0, 0.5215476148756675, 1.0, 1.0, 0.5215476148756675, 1.0, 1.0, 1.0365853658536586, 0.001912307145118941, 0.0, 0.8375144448122397, 0.7026135786377276, 0.6988084025384778, 0.729892369833947], 
reward next is 0.1745, 
noisyNet noise sample is [array([-0.13038844], dtype=float32), 0.30236742]. 
=============================================
[2019-03-27 01:22:07,583] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:22:07,600] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6689
[2019-03-27 01:22:07,605] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 81.0, 1.0, 2.0, 0.6137134874838422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857633.3956585486, 857633.3956585493, 203328.8708518688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5349600.0000, 
sim time next is 5350200.0000, 
raw observation next is [30.31666666666666, 81.5, 1.0, 2.0, 0.6153620116768689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859938.0570115658, 859938.0570115658, 203642.9743526781], 
processed observation next is [1.0, 0.9565217391304348, 0.6358609794628749, 0.815, 1.0, 1.0, 0.536580736960083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23887168250321272, 0.23887168250321272, 0.30394473783981807], 
reward next is 0.6961, 
noisyNet noise sample is [array([0.17619759], dtype=float32), 0.9097417]. 
=============================================
[2019-03-27 01:22:12,691] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4527667e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:22:12,701] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5566
[2019-03-27 01:22:12,706] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.88333333333333, 75.66666666666667, 1.0, 2.0, 0.5821301382215632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813480.3690571458, 813480.3690571458, 197465.3251334472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5422200.0000, 
sim time next is 5422800.0000, 
raw observation next is [30.86666666666667, 76.33333333333334, 1.0, 2.0, 0.5872359064941861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820618.0285986264, 820618.0285986264, 198393.2900724921], 
processed observation next is [1.0, 0.782608695652174, 0.6619273301737759, 0.7633333333333334, 1.0, 1.0, 0.5026938632460073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22794945238850733, 0.22794945238850733, 0.29610938816789867], 
reward next is 0.7039, 
noisyNet noise sample is [array([0.8141664], dtype=float32), -0.29607388]. 
=============================================
[2019-03-27 01:22:18,324] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.4470625e-30 1.0000000e+00 2.0089460e-35 4.3146428e-35 4.6826070e-35], sum to 1.0000
[2019-03-27 01:22:18,328] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4299
[2019-03-27 01:22:18,337] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 90.0, 1.0, 2.0, 0.5495751823582499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767970.9725694183, 767970.9725694183, 191727.8888013666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5529600.0000, 
sim time next is 5530200.0000, 
raw observation next is [27.11666666666667, 90.33333333333333, 1.0, 2.0, 0.5486145661652951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766628.1325044517, 766628.1325044523, 191563.3728262976], 
processed observation next is [1.0, 0.0, 0.4842022116903636, 0.9033333333333333, 1.0, 1.0, 0.4561621279099941, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21295225902901435, 0.21295225902901452, 0.2859154818302949], 
reward next is 0.7141, 
noisyNet noise sample is [array([-0.26207075], dtype=float32), 1.0413585]. 
=============================================
[2019-03-27 01:22:23,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:22:23,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9160
[2019-03-27 01:22:23,475] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 72.0, 1.0, 2.0, 0.5227318440542547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730447.4574323051, 730447.4574323044, 187234.2666590692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5733000.0000, 
sim time next is 5733600.0000, 
raw observation next is [29.4, 71.0, 1.0, 2.0, 0.5216146103174774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728885.7378993308, 728885.7378993302, 187051.8852675358], 
processed observation next is [0.0, 0.34782608695652173, 0.5924170616113744, 0.71, 1.0, 1.0, 0.42363206062346664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2024682605275919, 0.20246826052759173, 0.2791819183097549], 
reward next is 0.7208, 
noisyNet noise sample is [array([1.6581457], dtype=float32), 1.379358]. 
=============================================
[2019-03-27 01:22:25,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2121867e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:22:25,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4367
[2019-03-27 01:22:25,193] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 91.33333333333334, 1.0, 2.0, 0.5038736434684384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704086.9247482967, 704086.9247482962, 184204.8727349343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5629200.0000, 
sim time next is 5629800.0000, 
raw observation next is [25.61666666666667, 91.16666666666667, 1.0, 2.0, 0.5019505234115789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701398.7692300805, 701398.7692300805, 183901.9238461858], 
processed observation next is [0.0, 0.13043478260869565, 0.41311216429699865, 0.9116666666666667, 1.0, 1.0, 0.3999403896525046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19483299145280014, 0.19483299145280014, 0.2744804833525161], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.8873788], dtype=float32), 2.0284805]. 
=============================================
[2019-03-27 01:22:48,704] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9469114e-23 1.0000000e+00 7.7512895e-30 1.2885448e-22 5.9004374e-26], sum to 1.0000
[2019-03-27 01:22:48,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5916
[2019-03-27 01:22:48,721] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 92.0, 1.0, 2.0, 0.7420061951577213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1037003.46157578, 1037003.46157578, 230173.4945254287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6151200.0000, 
sim time next is 6151800.0000, 
raw observation next is [26.51666666666667, 92.0, 1.0, 2.0, 0.7281500722678417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1017629.321383568, 1017629.321383569, 227038.6975987976], 
processed observation next is [1.0, 0.17391304347826086, 0.45576619273301755, 0.92, 1.0, 1.0, 0.672469966587761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28267481149543555, 0.28267481149543583, 0.3388637277593994], 
reward next is 0.6611, 
noisyNet noise sample is [array([0.06475559], dtype=float32), -1.5262907]. 
=============================================
[2019-03-27 01:22:49,812] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.749474e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:22:49,819] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8530
[2019-03-27 01:22:49,825] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 90.16666666666667, 1.0, 2.0, 0.5391378269468952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753380.7421293079, 753380.7421293079, 189955.2490880575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6045000.0000, 
sim time next is 6045600.0000, 
raw observation next is [26.83333333333333, 90.33333333333334, 1.0, 2.0, 0.5390290820073553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753228.7301823727, 753228.7301823722, 189936.9319416075], 
processed observation next is [1.0, 1.0, 0.470774091627172, 0.9033333333333334, 1.0, 1.0, 0.44461335181609074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20923020282843688, 0.2092302028284367, 0.2834879581218022], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.2983587], dtype=float32), -0.018324062]. 
=============================================
[2019-03-27 01:22:52,898] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3207274e-15 1.0000000e+00 2.7345202e-20 7.1352097e-16 1.8464079e-17], sum to 1.0000
[2019-03-27 01:22:52,905] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0257
[2019-03-27 01:22:52,920] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2205482.893560492 W.
[2019-03-27 01:22:52,925] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.1, 65.0, 1.0, 2.0, 0.525745093525431, 1.0, 2.0, 0.525745093525431, 1.0, 1.0, 0.9093448364682698, 6.9112, 6.9112, 170.5573041426782, 2205482.893560492, 2205482.893560492, 432699.718701835], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6097200.0000, 
sim time next is 6097800.0000, 
raw observation next is [31.1, 65.0, 1.0, 2.0, 0.9619129680004852, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991822430608401, 6.9112, 168.9124765749983, 2241693.451491314, 2184497.286920758, 452336.0745596938], 
processed observation next is [1.0, 0.5652173913043478, 0.6729857819905214, 0.65, 1.0, 1.0, 0.9541120096391388, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008062243060840135, 0.0, 0.8294375884496127, 0.6226926254142539, 0.6068048019224328, 0.6751284694920803], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4654009], dtype=float32), 1.5757899]. 
=============================================
[2019-03-27 01:22:53,376] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 01:22:53,378] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:22:53,378] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:22:53,379] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:22:53,380] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:22:53,381] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:22:53,382] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:22:53,382] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:22:53,383] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:22:53,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:22:53,387] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:22:53,401] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-27 01:22:53,422] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-27 01:22:53,423] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-27 01:22:53,459] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-27 01:22:53,460] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-27 01:23:17,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.025757745]
[2019-03-27 01:23:17,704] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.45, 93.0, 1.0, 2.0, 0.4236743868923918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625057.6688467945, 625057.6688467945, 176706.6840701027]
[2019-03-27 01:23:17,704] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:23:17,708] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2385877e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7184048410186973
[2019-03-27 01:23:37,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.025757745]
[2019-03-27 01:23:37,173] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.78333333333333, 78.83333333333333, 1.0, 2.0, 0.8451013823446558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1181166.041098299, 1181166.041098299, 255283.3813575003]
[2019-03-27 01:23:37,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:23:37,178] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0202379e-25 1.0000000e+00 1.2209173e-33 2.5441812e-27 2.9621606e-29], sampled 0.6166658798184134
[2019-03-27 01:23:53,242] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.025757745]
[2019-03-27 01:23:53,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.06666666666667, 65.33333333333334, 1.0, 2.0, 0.5949660702647823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 831424.5792143772, 831424.5792143765, 199805.5660188013]
[2019-03-27 01:23:53,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:23:53,248] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.7949826e-33 1.0000000e+00 0.0000000e+00 1.3376545e-35 4.1857990e-38], sampled 0.25973569013760556
[2019-03-27 01:24:04,304] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.025757745]
[2019-03-27 01:24:04,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.0, 79.5, 1.0, 2.0, 0.6373484924466309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 890675.9624351875, 890675.9624351881, 207910.1252071437]
[2019-03-27 01:24:04,308] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:24:04,310] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.880001e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.26569666056591956
[2019-03-27 01:24:18,827] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.025757745]
[2019-03-27 01:24:18,827] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.291815765, 81.64143029499999, 1.0, 2.0, 0.5449066100737235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761444.8171081616, 761444.8171081616, 190930.9879808973]
[2019-03-27 01:24:18,828] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:24:18,830] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2140143e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4411201217182883
[2019-03-27 01:24:40,351] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.025757745]
[2019-03-27 01:24:40,354] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.30957972166667, 92.72189882333333, 1.0, 2.0, 0.2417458219260336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 403219.5091872543, 403219.5091872549, 159442.8745462383]
[2019-03-27 01:24:40,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:24:40,358] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.595678e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.04400183114309275
[2019-03-27 01:24:47,962] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:24:48,514] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:24:48,590] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:24:48,691] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:24:48,728] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:24:49,744] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 800000, evaluation results [800000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:24:55,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2453889e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:24:55,493] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5104
[2019-03-27 01:24:55,502] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 83.0, 1.0, 2.0, 0.5236461484442319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731725.5149117386, 731725.5149117386, 187383.8234797242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6330600.0000, 
sim time next is 6331200.0000, 
raw observation next is [27.6, 82.33333333333334, 1.0, 2.0, 0.5241016559454967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732362.2451954585, 732362.245195458, 187458.3822954582], 
processed observation next is [0.0, 0.2608695652173913, 0.5071090047393366, 0.8233333333333335, 1.0, 1.0, 0.4266285011391527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20343395699873848, 0.2034339569987383, 0.2797886302917287], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.3736294], dtype=float32), 0.4587543]. 
=============================================
[2019-03-27 01:25:23,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.9484227e-30 1.0000000e+00 0.0000000e+00 3.8535878e-34 1.7975425e-33], sum to 1.0000
[2019-03-27 01:25:23,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1578
[2019-03-27 01:25:23,367] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 92.0, 1.0, 2.0, 0.4950779244460796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691792.2405812937, 691792.2405812931, 182828.5435305255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6656400.0000, 
sim time next is 6657000.0000, 
raw observation next is [25.43333333333333, 92.33333333333334, 1.0, 2.0, 0.4948652071567946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691494.9054366194, 691494.9054366194, 182795.4937304375], 
processed observation next is [1.0, 0.043478260869565216, 0.40442338072669815, 0.9233333333333335, 1.0, 1.0, 0.3914038640443308, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19208191817683873, 0.19208191817683873, 0.272829095120056], 
reward next is 0.7272, 
noisyNet noise sample is [array([0.2519369], dtype=float32), 0.23078284]. 
=============================================
[2019-03-27 01:25:23,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.27942 ]
 [66.30231 ]
 [66.42349 ]
 [66.57934 ]
 [66.839935]], R is [[66.15529633]
 [66.22086334]
 [66.28565216]
 [66.34965515]
 [66.41287994]].
[2019-03-27 01:25:25,656] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.1965491e-26 1.0000000e+00 2.0347370e-35 1.5801616e-30 1.5352421e-30], sum to 1.0000
[2019-03-27 01:25:25,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4794
[2019-03-27 01:25:25,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1807316.22695818 W.
[2019-03-27 01:25:25,675] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.2, 72.0, 1.0, 2.0, 0.4309097422400245, 1.0, 2.0, 0.4309097422400245, 1.0, 2.0, 0.7379518397590011, 6.911200000000001, 6.9112, 170.5573041426782, 1807316.22695818, 1807316.226958179, 369062.7417911941], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6692400.0000, 
sim time next is 6693000.0000, 
raw observation next is [29.25, 71.33333333333333, 1.0, 2.0, 0.7142009405454313, 1.0, 2.0, 0.7142009405454313, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1997171.860309785, 1997171.860309785, 380257.6232719392], 
processed observation next is [1.0, 0.4782608695652174, 0.5853080568720379, 0.7133333333333333, 1.0, 1.0, 0.6556637837896762, 1.0, 1.0, 0.6556637837896762, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5547699611971625, 0.5547699611971625, 0.5675486914506556], 
reward next is 0.4325, 
noisyNet noise sample is [array([1.4918543], dtype=float32), 1.5990198]. 
=============================================
[2019-03-27 01:25:25,693] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[52.56936 ]
 [51.638756]
 [51.72294 ]
 [51.22994 ]
 [50.986687]], R is [[52.03194809]
 [51.96079254]
 [51.441185  ]
 [50.92677307]
 [50.41750717]].
[2019-03-27 01:25:39,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3967821e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:25:39,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9118
[2019-03-27 01:25:39,086] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 84.66666666666667, 1.0, 2.0, 0.4240072970887338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619883.8812531757, 619883.8812531751, 176057.0589635421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6916800.0000, 
sim time next is 6917400.0000, 
raw observation next is [24.75, 85.0, 1.0, 2.0, 0.4242072431295988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 620249.6057836277, 620249.6057836271, 176094.3472896515], 
processed observation next is [0.0, 0.043478260869565216, 0.3720379146919432, 0.85, 1.0, 1.0, 0.30627378690313106, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1722915571621188, 0.17229155716211864, 0.2628273840144052], 
reward next is 0.7372, 
noisyNet noise sample is [array([0.87540793], dtype=float32), 0.9756388]. 
=============================================
[2019-03-27 01:25:41,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9839629e-35 1.0000000e+00 0.0000000e+00 1.2833624e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 01:25:41,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8229
[2019-03-27 01:25:41,975] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 94.5, 1.0, 2.0, 0.5293945642474459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753184.2030502071, 753184.2030502065, 190045.4234577006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7101000.0000, 
sim time next is 7101600.0000, 
raw observation next is [24.1, 94.66666666666666, 1.0, 2.0, 0.5528481404135185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787519.6662528536, 787519.6662528529, 194206.4304038058], 
processed observation next is [1.0, 0.17391304347826086, 0.3412322274881518, 0.9466666666666665, 1.0, 1.0, 0.46126281977532346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2187554628480149, 0.2187554628480147, 0.2898603438862773], 
reward next is 0.7101, 
noisyNet noise sample is [array([1.1205987], dtype=float32), -0.777408]. 
=============================================
[2019-03-27 01:25:45,944] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 01:25:45,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:25:45,947] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:25:45,947] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:25:45,949] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:25:45,950] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:25:45,951] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:25:45,951] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:25:45,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:25:45,952] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:25:45,953] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:25:45,970] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-27 01:25:45,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-27 01:25:46,019] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-27 01:25:46,020] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-27 01:25:46,057] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-27 01:26:13,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06327156]
[2019-03-27 01:26:13,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.47380211833334, 94.48452358333334, 1.0, 2.0, 0.2910147275829052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 471561.1740145908, 471561.1740145914, 165046.311876149]
[2019-03-27 01:26:13,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:26:13,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.339487e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.2799223413098141
[2019-03-27 01:26:49,112] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06327156]
[2019-03-27 01:26:49,114] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.83333333333333, 84.83333333333333, 1.0, 2.0, 0.6182960333192415, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.960872862885538, 6.9112, 168.912557993867, 1728783.238472578, 1693543.683074503, 368009.2999262679]
[2019-03-27 01:26:49,114] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:26:49,120] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4805642e-27 1.0000000e+00 2.6683628e-36 3.4650572e-30 2.0987382e-31], sampled 0.6562934369424306
[2019-03-27 01:26:49,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1728783.238472578 W.
[2019-03-27 01:27:09,454] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06327156]
[2019-03-27 01:27:09,454] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.8, 75.0, 1.0, 2.0, 0.6031067284671029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 842805.1083154507, 842805.10831545, 201325.4946858428]
[2019-03-27 01:27:09,456] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:27:09,458] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2765978e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.946287166472235
[2019-03-27 01:27:26,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06327156]
[2019-03-27 01:27:26,847] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.13333333333333, 93.0, 1.0, 2.0, 0.5346565344543733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747116.4686878682, 747116.4686878676, 189203.4915525228]
[2019-03-27 01:27:26,849] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:27:26,855] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.5963391e-33 1.0000000e+00 0.0000000e+00 3.2337484e-36 2.0760223e-37], sampled 0.4309069910088046
[2019-03-27 01:27:40,863] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:27:40,919] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:27:40,984] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:27:41,068] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:27:41,236] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:27:42,255] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 825000, evaluation results [825000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:27:49,739] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2579990e-32 1.0000000e+00 0.0000000e+00 8.3768418e-34 2.3258532e-35], sum to 1.0000
[2019-03-27 01:27:49,749] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1328
[2019-03-27 01:27:49,755] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.78333333333333, 73.0, 1.0, 2.0, 0.7518275171007635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161936.169716239, 1161936.169716239, 246643.2278884322], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7293000.0000, 
sim time next is 7293600.0000, 
raw observation next is [25.0, 72.0, 1.0, 2.0, 0.7655216118082576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1181190.933044879, 1181190.933044879, 249976.0426289992], 
processed observation next is [1.0, 0.43478260869565216, 0.38388625592417064, 0.72, 1.0, 1.0, 0.7174959178412742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3281085925124664, 0.3281085925124664, 0.3730985710880585], 
reward next is 0.6269, 
noisyNet noise sample is [array([-0.8877703], dtype=float32), -0.36498612]. 
=============================================
[2019-03-27 01:27:51,969] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3457587e-27 1.0000000e+00 2.1672886e-35 3.0633928e-28 7.3619020e-31], sum to 1.0000
[2019-03-27 01:27:51,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4065
[2019-03-27 01:27:51,986] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 87.0, 1.0, 2.0, 0.8822851648551356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1305562.168814947, 1305562.168814947, 275467.7336765546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7227600.0000, 
sim time next is 7228200.0000, 
raw observation next is [24.16666666666667, 86.5, 1.0, 2.0, 0.9015621471986284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1335690.474549241, 1335690.474549242, 281281.110146294], 
processed observation next is [1.0, 0.6521739130434783, 0.34439178515007923, 0.865, 1.0, 1.0, 0.881400177347745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3710251318192336, 0.3710251318192339, 0.4198225524571552], 
reward next is 0.5802, 
noisyNet noise sample is [array([-0.5552003], dtype=float32), 0.7943901]. 
=============================================
[2019-03-27 01:28:12,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:12,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:12,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-27 01:28:15,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1468388e-34 1.0000000e+00 0.0000000e+00 7.9917407e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 01:28:15,791] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4178
[2019-03-27 01:28:15,797] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 95.0, 1.0, 2.0, 0.5300940610091275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763228.287883301, 763228.2878833016, 191303.9255806483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7612200.0000, 
sim time next is 7612800.0000, 
raw observation next is [23.76666666666667, 95.0, 1.0, 2.0, 0.5236451941342106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755012.724271016, 755012.724271016, 190340.6590097879], 
processed observation next is [1.0, 0.08695652173913043, 0.32543443917851517, 0.95, 1.0, 1.0, 0.4260785471496513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20972575674194888, 0.20972575674194888, 0.2840905358355043], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.06262865], dtype=float32), -1.0036476]. 
=============================================
[2019-03-27 01:28:20,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:28:20,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9172
[2019-03-27 01:28:20,911] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333334, 88.0, 1.0, 2.0, 0.4854030803664347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678268.8769390162, 678268.8769390162, 181340.2800587422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7683600.0000, 
sim time next is 7684200.0000, 
raw observation next is [25.6, 88.0, 1.0, 2.0, 0.4853846873764654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678243.1676352267, 678243.1676352262, 181337.402721698], 
processed observation next is [1.0, 0.9565217391304348, 0.4123222748815167, 0.88, 1.0, 1.0, 0.3799815510559824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1884008798986741, 0.18840087989867393, 0.2706528398831313], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.65163], dtype=float32), -0.1850596]. 
=============================================
[2019-03-27 01:28:29,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5437259e-28 1.0000000e+00 6.1496259e-37 6.0439655e-31 3.1484990e-32], sum to 1.0000
[2019-03-27 01:28:29,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6695
[2019-03-27 01:28:29,345] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2370407.689706975 W.
[2019-03-27 01:28:29,352] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.33333333333334, 1.0, 2.0, 0.8475337547906961, 1.0, 1.0, 0.8475337547906961, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2370407.689706975, 2370407.689706974, 443672.0161902425], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7832400.0000, 
sim time next is 7833000.0000, 
raw observation next is [31.0, 64.16666666666666, 1.0, 2.0, 0.8376699124205529, 1.0, 2.0, 0.8376699124205529, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2342794.341333556, 2342794.341333556, 438621.8014305285], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.6416666666666666, 1.0, 1.0, 0.8044215812295817, 1.0, 1.0, 0.8044215812295817, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6507762059259877, 0.6507762059259877, 0.6546594051201918], 
reward next is 0.3453, 
noisyNet noise sample is [array([0.75286865], dtype=float32), 1.0270519]. 
=============================================
[2019-03-27 01:28:29,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[57.4359  ]
 [56.927704]
 [55.889202]
 [53.991447]
 [53.94104 ]], R is [[58.25711823]
 [57.6745491 ]
 [57.09780502]
 [56.84777832]
 [56.46534729]].
[2019-03-27 01:28:29,747] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:29,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:29,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-27 01:28:29,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:29,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:29,974] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-27 01:28:30,167] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:30,168] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:30,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-27 01:28:32,102] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9633429e-36 1.0000000e+00 0.0000000e+00 2.9199872e-37 8.5385897e-38], sum to 1.0000
[2019-03-27 01:28:32,111] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5966
[2019-03-27 01:28:32,117] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 83.0, 1.0, 2.0, 0.6733956968784577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 941073.1995150559, 941073.1995150559, 215205.8595365897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7889400.0000, 
sim time next is 7890000.0000, 
raw observation next is [27.2, 82.66666666666667, 1.0, 2.0, 0.6842778206632252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 956287.858239193, 956287.8582391937, 217486.5422263585], 
processed observation next is [1.0, 0.30434782608695654, 0.4881516587677725, 0.8266666666666667, 1.0, 1.0, 0.6196118321243677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2656355161775536, 0.2656355161775538, 0.32460677944232613], 
reward next is 0.6754, 
noisyNet noise sample is [array([0.8682966], dtype=float32), -0.8025931]. 
=============================================
[2019-03-27 01:28:32,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.20602 ]
 [63.317356]
 [63.42661 ]
 [63.419796]
 [63.22298 ]], R is [[63.13835144]
 [63.18576813]
 [63.23633194]
 [63.29102325]
 [63.35200882]].
[2019-03-27 01:28:34,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:34,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:34,469] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-27 01:28:34,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:34,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:34,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-27 01:28:35,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:35,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:35,547] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-27 01:28:35,563] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:35,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:35,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-27 01:28:35,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:35,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:35,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-27 01:28:35,736] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:35,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:35,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-27 01:28:35,903] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:35,904] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:35,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-27 01:28:35,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:35,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:35,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-27 01:28:36,004] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:36,004] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:36,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-27 01:28:36,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:36,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:36,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-27 01:28:36,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:36,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:36,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-27 01:28:36,255] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:28:36,255] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:36,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-27 01:28:37,235] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 01:28:37,240] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:28:37,240] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:28:37,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:37,243] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:37,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-27 01:28:37,264] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:28:37,266] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:37,266] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:28:37,267] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:28:37,267] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:37,267] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:28:37,269] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-27 01:28:37,289] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-27 01:28:37,312] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-27 01:28:37,344] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-27 01:28:57,707] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06549001]
[2019-03-27 01:28:57,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.48333333333333, 52.5, 1.0, 2.0, 0.2743434316538517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447870.6410370889, 447870.6410370889, 163370.638656094]
[2019-03-27 01:28:57,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:28:57,714] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6230412867690762
[2019-03-27 01:29:14,719] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06549001]
[2019-03-27 01:29:14,721] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.9506331, 92.906318075, 1.0, 2.0, 0.5106264700930174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713526.1439537817, 713526.1439537811, 185275.9037232531]
[2019-03-27 01:29:14,723] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:29:14,728] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06964606209384039
[2019-03-27 01:29:19,856] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06549001]
[2019-03-27 01:29:19,857] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3516546787000042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541715.1016214251, 541715.1016214245, 169957.9511718156]
[2019-03-27 01:29:19,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:29:19,862] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9386456623540425
[2019-03-27 01:29:36,975] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06549001]
[2019-03-27 01:29:36,976] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.43333333333334, 66.0, 1.0, 2.0, 0.559784013557679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782241.9461475026, 782241.9461475026, 193492.8022147991]
[2019-03-27 01:29:36,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:29:36,980] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7284029271001468
[2019-03-27 01:30:23,508] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06549001]
[2019-03-27 01:30:23,510] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.8, 77.33333333333334, 1.0, 2.0, 0.7043771244206193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 984389.935543252, 984389.9355432515, 221798.276429499]
[2019-03-27 01:30:23,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:30:23,516] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4081758570127535
[2019-03-27 01:30:31,722] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:30:32,109] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:30:32,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:30:32,405] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:30:32,546] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:30:33,561] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 850000, evaluation results [850000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:30:33,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:33,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5732
[2019-03-27 01:30:33,841] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 94.0, 1.0, 2.0, 0.2888478642626525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464688.8910107907, 464688.8910107907, 164593.0614477661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 198000.0000, 
sim time next is 198600.0000, 
raw observation next is [20.23333333333333, 93.83333333333334, 1.0, 2.0, 0.2893638462053617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465394.6007317224, 465394.6007317224, 164641.3125937561], 
processed observation next is [0.0, 0.30434782608695654, 0.15797788309636643, 0.9383333333333335, 1.0, 1.0, 0.14381186289802614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.129276277981034, 0.129276277981034, 0.24573330237874044], 
reward next is 0.7543, 
noisyNet noise sample is [array([-1.1823474], dtype=float32), 0.58732206]. 
=============================================
[2019-03-27 01:30:42,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:42,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8462
[2019-03-27 01:30:42,252] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 86.66666666666666, 1.0, 2.0, 0.3209948008206268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506309.6188785122, 506309.6188785122, 167475.071336166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 222000.0000, 
sim time next is 222600.0000, 
raw observation next is [22.11666666666667, 86.33333333333334, 1.0, 2.0, 0.3211659357941711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506100.4631545788, 506100.4631545788, 167449.0561604514], 
processed observation next is [0.0, 0.5652173913043478, 0.24723538704581383, 0.8633333333333334, 1.0, 1.0, 0.1821276334869531, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.140583461987383, 0.140583461987383, 0.2499239644185842], 
reward next is 0.7501, 
noisyNet noise sample is [array([-0.521346], dtype=float32), -0.46447945]. 
=============================================
[2019-03-27 01:30:42,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:42,369] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5482
[2019-03-27 01:30:42,377] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 96.0, 1.0, 2.0, 0.7553360354431568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1134393.075448903, 1134393.075448903, 243666.2669851979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 145800.0000, 
sim time next is 146400.0000, 
raw observation next is [22.53333333333333, 96.0, 1.0, 2.0, 0.7250057222811997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1089465.522283992, 1089465.522283992, 236280.4847419269], 
processed observation next is [1.0, 0.6956521739130435, 0.26698262243285936, 0.96, 1.0, 1.0, 0.6686815931098792, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30262931174555335, 0.30262931174555335, 0.3526574399133237], 
reward next is 0.6473, 
noisyNet noise sample is [array([1.4299637], dtype=float32), -0.4025525]. 
=============================================
[2019-03-27 01:30:47,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:47,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4463
[2019-03-27 01:30:47,323] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 87.0, 1.0, 2.0, 0.3115171818356229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493323.9484018233, 493323.9484018227, 166543.1612208481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 230400.0000, 
sim time next is 231000.0000, 
raw observation next is [21.76666666666667, 87.0, 1.0, 2.0, 0.310256711402336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491622.1101814056, 491622.1101814062, 166423.4228649743], 
processed observation next is [0.0, 0.6956521739130435, 0.23064770932069528, 0.87, 1.0, 1.0, 0.16898398964136868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13656169727261266, 0.13656169727261283, 0.24839316845518553], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.63669884], dtype=float32), 1.5983927]. 
=============================================
[2019-03-27 01:30:47,350] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.698715]
 [83.55723 ]
 [83.5117  ]
 [83.44589 ]
 [83.38231 ]], R is [[83.65914154]
 [83.57397461]
 [83.48952484]
 [83.40581512]
 [83.32289886]].
[2019-03-27 01:30:47,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:47,702] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4699
[2019-03-27 01:30:47,707] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 89.0, 1.0, 2.0, 0.3006138163358041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 478712.2646796132, 478712.2646796139, 165529.8364474007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 241200.0000, 
sim time next is 241800.0000, 
raw observation next is [21.26666666666667, 89.0, 1.0, 2.0, 0.3001949018991961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478264.1305440522, 478264.1305440522, 165501.1985853083], 
processed observation next is [0.0, 0.8260869565217391, 0.2069510268562403, 0.89, 1.0, 1.0, 0.15686132758939292, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13285114737334783, 0.13285114737334783, 0.2470167143064303], 
reward next is 0.7530, 
noisyNet noise sample is [array([-0.7015617], dtype=float32), -0.13034998]. 
=============================================
[2019-03-27 01:30:50,037] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:50,044] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6296
[2019-03-27 01:30:50,051] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 92.0, 1.0, 2.0, 0.2706050916224741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 438197.227455016, 438197.227455016, 162818.2025584676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 280800.0000, 
sim time next is 281400.0000, 
raw observation next is [20.25, 91.33333333333334, 1.0, 2.0, 0.2725676136235184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440822.8523189685, 440822.8523189685, 162992.3844759037], 
processed observation next is [0.0, 0.2608695652173913, 0.1587677725118484, 0.9133333333333334, 1.0, 1.0, 0.12357543810062456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12245079231082459, 0.12245079231082459, 0.24327221563567716], 
reward next is 0.7567, 
noisyNet noise sample is [array([0.25848466], dtype=float32), 0.82668924]. 
=============================================
[2019-03-27 01:30:56,006] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:56,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0079
[2019-03-27 01:30:56,019] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.08333333333333, 79.66666666666667, 1.0, 2.0, 0.2615554802851839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427267.6970923222, 427267.6970923228, 162042.9994304557], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 375000.0000, 
sim time next is 375600.0000, 
raw observation next is [21.16666666666667, 79.33333333333334, 1.0, 2.0, 0.3018973468650727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492901.9234781688, 492901.9234781688, 166444.3170155201], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.7933333333333334, 1.0, 1.0, 0.1589124661024972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.136917200966158, 0.136917200966158, 0.2484243537545076], 
reward next is 0.7516, 
noisyNet noise sample is [array([1.6240494], dtype=float32), 0.22502874]. 
=============================================
[2019-03-27 01:30:57,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:30:57,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6064
[2019-03-27 01:30:57,274] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 73.0, 1.0, 2.0, 0.3208786493000252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518560.6218782662, 518560.6218782662, 168467.3340876571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 392400.0000, 
sim time next is 393000.0000, 
raw observation next is [22.73333333333333, 73.0, 1.0, 2.0, 0.4132083901791496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667442.1479354531, 667442.1479354531, 181047.0793080967], 
processed observation next is [1.0, 0.5652173913043478, 0.27646129541864134, 0.73, 1.0, 1.0, 0.29302215684234895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18540059664873698, 0.18540059664873698, 0.2702195213553682], 
reward next is 0.7298, 
noisyNet noise sample is [array([-0.49635532], dtype=float32), -1.151529]. 
=============================================
[2019-03-27 01:30:57,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.91831]
 [75.72858]
 [75.50807]
 [75.13549]
 [74.27319]], R is [[75.27841949]
 [75.27419281]
 [75.27159119]
 [75.27086639]
 [75.27327728]].
[2019-03-27 01:31:00,248] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:00,258] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5399
[2019-03-27 01:31:00,261] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 79.16666666666667, 1.0, 2.0, 0.2655559406353868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 437259.9974286218, 437259.9974286218, 162471.7820859479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 461400.0000, 
sim time next is 462000.0000, 
raw observation next is [20.66666666666667, 78.33333333333334, 1.0, 2.0, 0.3552044323731192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584708.8758904886, 584708.8758904886, 173216.9636957057], 
processed observation next is [1.0, 0.34782608695652173, 0.17851500789889443, 0.7833333333333334, 1.0, 1.0, 0.2231378703290593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1624191321918024, 0.1624191321918024, 0.2585327816353816], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.0903283], dtype=float32), -1.0656464]. 
=============================================
[2019-03-27 01:31:00,284] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.56673 ]
 [75.69908 ]
 [75.63546 ]
 [75.62336 ]
 [75.593544]], R is [[74.77445984]
 [74.78422546]
 [74.79634857]
 [74.80899048]
 [74.82158661]].
[2019-03-27 01:31:00,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:00,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9356
[2019-03-27 01:31:00,789] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 81.5, 1.0, 2.0, 0.2323528972158815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383386.1076044721, 383386.1076044721, 159184.1578109189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 455400.0000, 
sim time next is 456000.0000, 
raw observation next is [20.1, 81.33333333333334, 1.0, 2.0, 0.2356152286388764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388645.8454703358, 388645.8454703358, 159492.8250227705], 
processed observation next is [1.0, 0.2608695652173913, 0.15165876777251197, 0.8133333333333335, 1.0, 1.0, 0.07905449233599567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1079571792973155, 0.1079571792973155, 0.23804899257129924], 
reward next is 0.7620, 
noisyNet noise sample is [array([1.0957608], dtype=float32), 0.21063276]. 
=============================================
[2019-03-27 01:31:00,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.10892 ]
 [78.101974]
 [78.09934 ]
 [78.088585]
 [77.99991 ]], R is [[78.08615875]
 [78.06771088]
 [78.04966736]
 [78.03187561]
 [78.0142746 ]].
[2019-03-27 01:31:05,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:05,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5432
[2019-03-27 01:31:05,999] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.8, 81.0, 1.0, 2.0, 0.2318150839567941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 383785.0787003994, 383785.0787004, 159048.1367543245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 543000.0000, 
sim time next is 543600.0000, 
raw observation next is [20.0, 80.0, 1.0, 2.0, 0.2321895276978192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 384136.3550020184, 384136.3550020184, 159105.5177691892], 
processed observation next is [1.0, 0.30434782608695654, 0.1469194312796209, 0.8, 1.0, 1.0, 0.07492714180460143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10670454305611622, 0.10670454305611622, 0.23747092204356599], 
reward next is 0.7625, 
noisyNet noise sample is [array([-0.4743685], dtype=float32), -1.7077093]. 
=============================================
[2019-03-27 01:31:09,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:09,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8000
[2019-03-27 01:31:09,042] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666666, 79.66666666666667, 1.0, 2.0, 0.229532437258694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380902.3152695646, 380902.315269564, 158737.5024779522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 596400.0000, 
sim time next is 597000.0000, 
raw observation next is [19.53333333333333, 80.33333333333333, 1.0, 2.0, 0.2291182250388971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380366.530738741, 380366.5307387405, 158678.2271407442], 
processed observation next is [1.0, 0.9130434782608695, 0.12480252764612951, 0.8033333333333332, 1.0, 1.0, 0.07122677715529768, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10565736964965028, 0.10565736964965014, 0.23683317483693161], 
reward next is 0.7632, 
noisyNet noise sample is [array([1.1374136], dtype=float32), 0.86009276]. 
=============================================
[2019-03-27 01:31:09,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.51264 ]
 [70.47326 ]
 [70.42715 ]
 [70.362206]
 [70.36369 ]], R is [[70.51378632]
 [70.57172394]
 [70.62885284]
 [70.68519592]
 [70.74079132]].
[2019-03-27 01:31:10,184] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:10,196] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0124
[2019-03-27 01:31:10,202] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.28333333333333, 90.66666666666667, 1.0, 2.0, 0.2055258604055006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 155935.3899002669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 611400.0000, 
sim time next is 612000.0000, 
raw observation next is [17.2, 91.0, 1.0, 2.0, 0.2048236585511596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342264.3359998221, 342264.3359998221, 155835.6189262077], 
processed observation next is [1.0, 0.08695652173913043, 0.014218009478673018, 0.91, 1.0, 1.0, 0.04195621512187903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09507342666661726, 0.09507342666661726, 0.23259047600926525], 
reward next is 0.7674, 
noisyNet noise sample is [array([1.3151189], dtype=float32), -2.1421366]. 
=============================================
[2019-03-27 01:31:10,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.7251  ]
 [74.78945 ]
 [74.868225]
 [74.884705]
 [74.87941 ]], R is [[74.90652466]
 [74.92472076]
 [74.94256592]
 [74.96002197]
 [74.97711182]].
[2019-03-27 01:31:11,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6997243e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:31:11,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2974
[2019-03-27 01:31:11,761] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 64.83333333333333, 1.0, 2.0, 0.4858273453457059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797712.83561018, 797712.83561018, 193305.7828663378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 640200.0000, 
sim time next is 640800.0000, 
raw observation next is [23.0, 64.0, 1.0, 2.0, 0.4830647168335571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792847.8674232693, 792847.8674232693, 192819.1964247606], 
processed observation next is [1.0, 0.43478260869565216, 0.28909952606635075, 0.64, 1.0, 1.0, 0.3771864058235628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2202355187286859, 0.2202355187286859, 0.28778984541009045], 
reward next is 0.7122, 
noisyNet noise sample is [array([0.1691449], dtype=float32), -1.017539]. 
=============================================
[2019-03-27 01:31:12,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:12,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4221
[2019-03-27 01:31:12,083] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 76.33333333333334, 1.0, 2.0, 0.3101469496316649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490613.1999019503, 490613.1999019509, 166332.5888356419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [23.15, 77.16666666666666, 1.0, 2.0, 0.3103404121180225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491231.2380670637, 491231.2380670637, 166384.4380141593], 
processed observation next is [0.0, 0.7391304347826086, 0.2962085308056872, 0.7716666666666666, 1.0, 1.0, 0.16908483387713552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13645312168529547, 0.13645312168529547, 0.24833498211068553], 
reward next is 0.7517, 
noisyNet noise sample is [array([1.0615654], dtype=float32), 1.4030256]. 
=============================================
[2019-03-27 01:31:23,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:23,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0937
[2019-03-27 01:31:23,040] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 64.33333333333334, 1.0, 2.0, 0.2879641628469169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461770.693405755, 461770.6934057556, 164384.4956444142], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 829200.0000, 
sim time next is 829800.0000, 
raw observation next is [24.5, 65.0, 1.0, 2.0, 0.2900126695283051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464387.5928239186, 464387.5928239193, 164558.2732376559], 
processed observation next is [0.0, 0.6086956521739131, 0.3601895734597157, 0.65, 1.0, 1.0, 0.1445935777449459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1289965535621996, 0.1289965535621998, 0.24560936304127745], 
reward next is 0.7544, 
noisyNet noise sample is [array([1.3666203], dtype=float32), -0.8705097]. 
=============================================
[2019-03-27 01:31:27,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:31:27,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4719
[2019-03-27 01:31:27,951] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.16666666666667, 1.0, 2.0, 0.3411304233498152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528265.8485009397, 528265.8485009397, 168945.5289824788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 951000.0000, 
sim time next is 951600.0000, 
raw observation next is [21.8, 94.33333333333334, 1.0, 2.0, 0.340482990389693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 526970.7987906242, 526970.7987906237, 168833.0523317622], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.9433333333333335, 1.0, 1.0, 0.20540119324059394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14638077744184008, 0.14638077744183992, 0.25198963034591376], 
reward next is 0.7480, 
noisyNet noise sample is [array([-0.41497287], dtype=float32), -0.7536953]. 
=============================================
[2019-03-27 01:31:29,671] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 01:31:29,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:31:29,675] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:31:29,677] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:31:29,678] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:31:29,678] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:31:29,679] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:31:29,680] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:31:29,682] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:31:29,683] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:31:29,689] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:31:29,707] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-27 01:31:29,707] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-27 01:31:29,746] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-27 01:31:29,771] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-27 01:31:29,789] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-27 01:31:34,806] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08696199]
[2019-03-27 01:31:34,807] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.7, 80.0, 1.0, 2.0, 0.2603187166191264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 427142.9310148385, 427142.9310148379, 161945.1865026677]
[2019-03-27 01:31:34,808] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:31:34,810] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.559288958237084
[2019-03-27 01:32:00,224] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08696199]
[2019-03-27 01:32:00,224] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 95.0, 1.0, 2.0, 0.3827392337947604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578508.4508966112, 578508.4508966112, 172802.0628959095]
[2019-03-27 01:32:00,227] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:32:00,231] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4163684476835935
[2019-03-27 01:32:18,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08696199]
[2019-03-27 01:32:18,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.13333333333333, 69.33333333333333, 1.0, 2.0, 0.5471254308272379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764546.4818042632, 764546.4818042638, 191308.1347107677]
[2019-03-27 01:32:18,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:32:18,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9852941586380709
[2019-03-27 01:32:19,239] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08696199]
[2019-03-27 01:32:19,240] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 92.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.467326388646963, 6.9112, 168.90939050433, 1848552.030041358, 1454025.15877248, 311349.6983082551]
[2019-03-27 01:32:19,242] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:32:19,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1219965676431104
[2019-03-27 01:32:19,247] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1848552.030041358 W.
[2019-03-27 01:33:15,194] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08696199]
[2019-03-27 01:33:15,196] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.15205669, 85.29441233333333, 1.0, 2.0, 0.3286514578387168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519971.8135428379, 519971.8135428373, 168551.5560026165]
[2019-03-27 01:33:15,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:33:15,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6742977968232078
[2019-03-27 01:33:22,312] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:33:22,393] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:33:22,525] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:33:22,659] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:33:22,677] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:33:23,694] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 875000, evaluation results [875000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:33:25,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:25,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2182
[2019-03-27 01:33:25,707] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.66666666666667, 1.0, 2.0, 0.3269795633378633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507537.7101245375, 507537.7101245369, 167350.9904420424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 970800.0000, 
sim time next is 971400.0000, 
raw observation next is [21.9, 92.83333333333333, 1.0, 2.0, 0.3275763960988012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508178.4018330448, 508178.4018330454, 167391.3225322385], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9283333333333332, 1.0, 1.0, 0.18985107963710984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14116066717584577, 0.14116066717584594, 0.24983779482423657], 
reward next is 0.7502, 
noisyNet noise sample is [array([-1.531127], dtype=float32), 0.24037327]. 
=============================================
[2019-03-27 01:33:26,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:26,390] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9036
[2019-03-27 01:33:26,397] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 93.66666666666667, 1.0, 2.0, 0.5792133819971944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894249.3052030515, 894249.3052030515, 207135.8827228688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 985200.0000, 
sim time next is 985800.0000, 
raw observation next is [21.98333333333333, 93.83333333333334, 1.0, 2.0, 0.5864156505227034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 904401.9311373441, 904401.9311373441, 208482.0122765801], 
processed observation next is [1.0, 0.391304347826087, 0.24091627172195884, 0.9383333333333335, 1.0, 1.0, 0.5017056030394017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25122275864926225, 0.25122275864926225, 0.31116718250235836], 
reward next is 0.6888, 
noisyNet noise sample is [array([-0.92392236], dtype=float32), -0.32731813]. 
=============================================
[2019-03-27 01:33:27,128] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:27,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8966
[2019-03-27 01:33:27,147] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.83333333333334, 1.0, 2.0, 0.4068980605363464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623561.2886847962, 623561.2886847956, 177085.8042402147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1012200.0000, 
sim time next is 1012800.0000, 
raw observation next is [21.7, 97.66666666666667, 1.0, 2.0, 0.3574286508168133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548152.3398437325, 548152.3398437331, 170420.9135442379], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.9766666666666667, 1.0, 1.0, 0.22581765158652203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15226453884548127, 0.1522645388454814, 0.2543595724540864], 
reward next is 0.7456, 
noisyNet noise sample is [array([-0.33153626], dtype=float32), 0.71491957]. 
=============================================
[2019-03-27 01:33:36,141] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:36,148] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2820
[2019-03-27 01:33:36,152] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 88.0, 1.0, 2.0, 0.6708684931586075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1065116.051333728, 1065116.051333728, 229682.4849613019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1346400.0000, 
sim time next is 1347000.0000, 
raw observation next is [21.46666666666667, 88.16666666666667, 1.0, 2.0, 0.5704487401589996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906774.0291150539, 906774.0291150539, 207844.0228276239], 
processed observation next is [1.0, 0.6086956521739131, 0.21642969984202226, 0.8816666666666667, 1.0, 1.0, 0.4824683616373489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25188167475418166, 0.25188167475418166, 0.3102149594442148], 
reward next is 0.6898, 
noisyNet noise sample is [array([-0.68199956], dtype=float32), 1.0289394]. 
=============================================
[2019-03-27 01:33:36,184] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.079865]
 [70.23332 ]
 [70.605934]
 [71.14665 ]
 [71.50721 ]], R is [[70.59885406]
 [70.55005646]
 [70.49858856]
 [70.45064545]
 [70.4177475 ]].
[2019-03-27 01:33:46,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:46,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5968
[2019-03-27 01:33:46,500] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 95.0, 1.0, 2.0, 0.440180960308833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651081.9185339112, 651081.9185339105, 179298.5097571632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1324800.0000, 
sim time next is 1325400.0000, 
raw observation next is [23.08333333333334, 95.0, 1.0, 2.0, 0.4494937511500388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665620.2517701269, 665620.2517701269, 180780.0247312058], 
processed observation next is [1.0, 0.34782608695652173, 0.2930489731437602, 0.95, 1.0, 1.0, 0.3367394592169143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1848945143805908, 0.1848945143805908, 0.2698209324346355], 
reward next is 0.7302, 
noisyNet noise sample is [array([0.27220047], dtype=float32), -1.0128988]. 
=============================================
[2019-03-27 01:33:46,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:46,873] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7067
[2019-03-27 01:33:46,882] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 56.5, 1.0, 2.0, 0.3618000962645297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552033.1282474456, 552033.128247445, 170661.9828688587], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1525800.0000, 
sim time next is 1526400.0000, 
raw observation next is [28.0, 57.0, 1.0, 2.0, 0.3597016180012442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549558.4744311062, 549558.4744311069, 170475.6977692278], 
processed observation next is [0.0, 0.6956521739130435, 0.5260663507109005, 0.57, 1.0, 1.0, 0.2285561662665593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1526551317864184, 0.1526551317864186, 0.2544413399540713], 
reward next is 0.7456, 
noisyNet noise sample is [array([-0.64794725], dtype=float32), 0.7432917]. 
=============================================
[2019-03-27 01:33:48,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4130528e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:33:48,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2290
[2019-03-27 01:33:48,572] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 92.0, 1.0, 2.0, 0.6149520775824487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 981640.356770511, 981640.356770511, 217479.7012252361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1357200.0000, 
sim time next is 1357800.0000, 
raw observation next is [20.81666666666667, 92.16666666666667, 1.0, 2.0, 0.3646134778447146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581773.2967046147, 581773.2967046147, 173611.6546320042], 
processed observation next is [1.0, 0.7391304347826086, 0.18562401263823086, 0.9216666666666667, 1.0, 1.0, 0.2344740696924272, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16160369352905962, 0.16160369352905962, 0.2591218725850809], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.01913809], dtype=float32), 1.3868086]. 
=============================================
[2019-03-27 01:33:49,575] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:33:49,582] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4279
[2019-03-27 01:33:49,586] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 73.66666666666667, 1.0, 2.0, 0.4220774919393711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613484.5792079386, 613484.5792079386, 175339.262532732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1446000.0000, 
sim time next is 1446600.0000, 
raw observation next is [26.35, 74.83333333333333, 1.0, 2.0, 0.4201252165336955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612031.5313201292, 612031.5313201298, 175240.9820735452], 
processed observation next is [0.0, 0.7391304347826086, 0.4478672985781992, 0.7483333333333333, 1.0, 1.0, 0.30135568257071743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17000875870003587, 0.17000875870003604, 0.26155370458738086], 
reward next is 0.7384, 
noisyNet noise sample is [array([-1.2939185], dtype=float32), 0.96051925]. 
=============================================
[2019-03-27 01:34:16,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:34:16,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8504
[2019-03-27 01:34:16,168] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 93.66666666666667, 1.0, 2.0, 0.3460386756776163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535768.6817740325, 535768.6817740325, 169548.8740903079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1819200.0000, 
sim time next is 1819800.0000, 
raw observation next is [21.9, 93.5, 1.0, 2.0, 0.3469152533283838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536932.8936494275, 536932.8936494275, 169638.2785827089], 
processed observation next is [1.0, 0.043478260869565216, 0.23696682464454974, 0.935, 1.0, 1.0, 0.2131509076245588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14914802601372984, 0.14914802601372984, 0.25319146057120734], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.49891347], dtype=float32), 0.73656476]. 
=============================================
[2019-03-27 01:34:19,706] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 01:34:19,707] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:34:19,708] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:34:19,709] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:34:19,710] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:34:19,710] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:34:19,711] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:34:19,713] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:34:19,714] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:34:19,714] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:34:19,716] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:34:19,731] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-27 01:34:19,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-27 01:34:19,750] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-27 01:34:19,770] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-27 01:34:19,809] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-27 01:34:22,512] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06775696]
[2019-03-27 01:34:22,516] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.6, 96.0, 1.0, 2.0, 0.7789358315697448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167888.554337311, 1167888.554337311, 249431.4727295514]
[2019-03-27 01:34:22,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:34:22,520] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9698656797015516
[2019-03-27 01:34:39,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06775696]
[2019-03-27 01:34:39,417] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.93333333333333, 50.0, 1.0, 2.0, 0.2124022438718162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 355063.698687228, 355063.6986872287, 156372.4562640714]
[2019-03-27 01:34:39,419] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:34:39,422] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4780946055243134
[2019-03-27 01:35:53,554] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06775696]
[2019-03-27 01:35:53,555] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.24494366333333, 90.38636771, 1.0, 2.0, 0.6390639486880534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893074.2705058131, 893074.2705058131, 208248.8881479708]
[2019-03-27 01:35:53,557] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:35:53,559] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7967501402426155
[2019-03-27 01:36:11,069] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.06775696]
[2019-03-27 01:36:11,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.1757581, 96.9438855, 1.0, 2.0, 0.9171248723540124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1281891.301048703, 1281891.301048703, 274692.4962224824]
[2019-03-27 01:36:11,071] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:36:11,075] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3401955035193056
[2019-03-27 01:36:14,653] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:36:14,714] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:36:14,871] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:36:14,895] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:36:14,932] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:36:15,948] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 900000, evaluation results [900000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:36:19,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7399066e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:36:19,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3793
[2019-03-27 01:36:19,197] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 93.0, 1.0, 2.0, 0.4057690004842204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601360.3309604963, 601360.3309604963, 174543.9804971255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1969200.0000, 
sim time next is 1969800.0000, 
raw observation next is [23.2, 93.33333333333333, 1.0, 2.0, 0.4040301460663349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599988.9851594074, 599988.9851594067, 174452.5545965302], 
processed observation next is [1.0, 0.8260869565217391, 0.29857819905213273, 0.9333333333333332, 1.0, 1.0, 0.2819640314052228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16666360698872426, 0.16666360698872407, 0.2603769471590003], 
reward next is 0.7396, 
noisyNet noise sample is [array([0.5628109], dtype=float32), -1.0083355]. 
=============================================
[2019-03-27 01:36:31,606] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:36:31,613] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3006
[2019-03-27 01:36:31,622] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 84.5, 1.0, 2.0, 0.5519986809863414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771358.7754463197, 771358.7754463197, 192144.2854512214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2143800.0000, 
sim time next is 2144400.0000, 
raw observation next is [27.96666666666667, 85.33333333333334, 1.0, 2.0, 0.5507295009673426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769584.59041656, 769584.5904165606, 191926.0473097618], 
processed observation next is [0.0, 0.8260869565217391, 0.524486571879937, 0.8533333333333334, 1.0, 1.0, 0.4587102421293284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21377349733793333, 0.21377349733793352, 0.28645678702949523], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.9721766], dtype=float32), 0.490378]. 
=============================================
[2019-03-27 01:36:33,903] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4046424e-30 1.0000000e+00 0.0000000e+00 7.4969315e-34 1.4730768e-35], sum to 1.0000
[2019-03-27 01:36:33,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2711
[2019-03-27 01:36:33,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1973906.218985716 W.
[2019-03-27 01:36:33,920] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 70.0, 1.0, 2.0, 0.7705845911733576, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987179414220357, 6.9112, 168.9124434985569, 1973906.218985716, 1920003.971237208, 401398.0726246304], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [30.25, 69.5, 1.0, 2.0, 0.6969141809205139, 1.0, 1.0, 0.6969141809205139, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1948787.675495603, 1948787.675495604, 372805.5386277846], 
processed observation next is [1.0, 0.34782608695652173, 0.6327014218009479, 0.695, 1.0, 1.0, 0.634836362554836, 1.0, 0.5, 0.634836362554836, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5413299098598897, 0.54132990985989, 0.5564261770563949], 
reward next is 0.4436, 
noisyNet noise sample is [array([1.4902546], dtype=float32), -0.35958278]. 
=============================================
[2019-03-27 01:36:35,909] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9434508e-33 1.0000000e+00 0.0000000e+00 6.9423677e-34 4.1044038e-36], sum to 1.0000
[2019-03-27 01:36:35,919] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9206
[2019-03-27 01:36:35,927] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.8, 62.5, 1.0, 2.0, 0.5263078560193652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104298, 735446.1831224533, 735446.1831224533, 187824.6009689121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2395800.0000, 
sim time next is 2396400.0000, 
raw observation next is [32.66666666666666, 63.33333333333333, 1.0, 2.0, 0.5388995160824762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753047.6131070184, 753047.6131070184, 189918.3833308815], 
processed observation next is [1.0, 0.7391304347826086, 0.7472353870458132, 0.6333333333333333, 1.0, 1.0, 0.44445724829214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20917989252972732, 0.20917989252972732, 0.2834602736281814], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.1873852], dtype=float32), 0.5516399]. 
=============================================
[2019-03-27 01:36:44,322] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8464559e-26 1.0000000e+00 1.7337441e-34 2.4801003e-29 7.1314034e-30], sum to 1.0000
[2019-03-27 01:36:44,330] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7508
[2019-03-27 01:36:44,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2217340.587039844 W.
[2019-03-27 01:36:44,341] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.15, 66.5, 1.0, 2.0, 0.944513188928474, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994005536391795, 6.9112, 168.9123973765243, 2217340.587039844, 2158595.684034299, 447153.9051428576], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2368200.0000, 
sim time next is 2368800.0000, 
raw observation next is [31.3, 66.0, 1.0, 2.0, 0.7703076785097767, 1.0, 1.0, 0.7703076785097767, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2154224.954247677, 2154224.954247677, 405675.2865965037], 
processed observation next is [1.0, 0.43478260869565216, 0.6824644549763034, 0.66, 1.0, 1.0, 0.7232622632647911, 1.0, 0.5, 0.7232622632647911, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5983958206243547, 0.5983958206243547, 0.6054855023828414], 
reward next is 0.3945, 
noisyNet noise sample is [array([-1.3784921], dtype=float32), 0.5292075]. 
=============================================
[2019-03-27 01:36:57,672] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:36:57,684] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2464
[2019-03-27 01:36:57,693] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 92.5, 1.0, 2.0, 0.4865391851448324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679856.8996106591, 679856.8996106591, 181513.3265422555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2593800.0000, 
sim time next is 2594400.0000, 
raw observation next is [24.86666666666667, 92.66666666666667, 1.0, 2.0, 0.4839522424962031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676240.9306145759, 676240.9306145759, 181119.3198428449], 
processed observation next is [0.0, 0.0, 0.3775671406003162, 0.9266666666666667, 1.0, 1.0, 0.37825571385084716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1878447029484933, 0.1878447029484933, 0.27032734304902223], 
reward next is 0.7297, 
noisyNet noise sample is [array([1.4094049], dtype=float32), -0.96010756]. 
=============================================
[2019-03-27 01:37:01,519] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:37:01,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1499
[2019-03-27 01:37:01,534] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4091672451503332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602870.9496955557, 602870.9496955563, 174580.6887437804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4093354669916394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 603119.3580655652, 603119.3580655647, 174603.8807527346], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.2883559843272764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16753315501821256, 0.1675331550182124, 0.26060280709363376], 
reward next is 0.7394, 
noisyNet noise sample is [array([1.5019283], dtype=float32), 0.4010618]. 
=============================================
[2019-03-27 01:37:01,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.26145 ]
 [70.128525]
 [70.045456]
 [69.822105]
 [69.9159  ]], R is [[70.45845795]
 [70.49330902]
 [70.52775574]
 [70.56095886]
 [70.59368896]].
[2019-03-27 01:37:05,036] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:37:05,044] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9307
[2019-03-27 01:37:05,049] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 99.33333333333333, 1.0, 2.0, 0.4729707176279473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660891.3444894655, 660891.3444894662, 179469.3936297297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2698800.0000, 
sim time next is 2699400.0000, 
raw observation next is [24.0, 99.66666666666667, 1.0, 2.0, 0.4743382309712767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662802.794067383, 662802.7940673825, 179673.0016526292], 
processed observation next is [0.0, 0.21739130434782608, 0.3364928909952607, 0.9966666666666667, 1.0, 1.0, 0.3666725674352731, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18411188724093974, 0.18411188724093958, 0.2681686591830287], 
reward next is 0.7318, 
noisyNet noise sample is [array([2.179749], dtype=float32), -0.21812703]. 
=============================================
[2019-03-27 01:37:12,022] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 01:37:12,024] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:37:12,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:37:12,026] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:37:12,027] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:37:12,027] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:37:12,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:37:12,028] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:37:12,030] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:37:12,029] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:37:12,033] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:37:12,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-27 01:37:12,068] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-27 01:37:12,087] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-27 01:37:12,107] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-27 01:37:12,107] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-27 01:37:56,599] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04815377]
[2019-03-27 01:37:56,601] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 88.66666666666667, 1.0, 2.0, 0.5117395223431682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715081.9959154112, 715081.9959154117, 185455.2422750013]
[2019-03-27 01:37:56,601] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:37:56,604] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6291708191268363
[2019-03-27 01:38:02,457] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04815377]
[2019-03-27 01:38:02,458] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 81.16666666666666, 1.0, 2.0, 0.5106250491987964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713524.1577941555, 713524.1577941562, 185276.5599778855]
[2019-03-27 01:38:02,460] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:38:02,462] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3465182546916038
[2019-03-27 01:38:18,799] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04815377]
[2019-03-27 01:38:18,802] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.31666666666667, 59.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.976720515761585, 6.9112, 168.9124362827048, 1500269.125875663, 1453786.760111751, 311350.957023018]
[2019-03-27 01:38:18,804] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:38:18,806] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5037175339923291
[2019-03-27 01:38:56,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.04815377]
[2019-03-27 01:38:56,980] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.13817842, 81.60443914333334, 1.0, 2.0, 0.5685098252229192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890263.6802668078, 890263.6802668078, 206266.5915214384]
[2019-03-27 01:38:56,981] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:38:56,984] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4032107556164636
[2019-03-27 01:39:06,766] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:39:06,859] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:39:06,962] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:39:07,047] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:39:07,092] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:39:08,109] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 925000, evaluation results [925000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:39:14,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:14,332] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6675
[2019-03-27 01:39:14,336] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5296032489674175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740052.6665533786, 740052.6665533786, 188367.2146166768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3433200.0000, 
sim time next is 3433800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5342107793664643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746493.362137565, 746493.3621375657, 189132.3836550046], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.43880816791140276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20735926726043474, 0.20735926726043494, 0.282287139783589], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.76103204], dtype=float32), -0.16587327]. 
=============================================
[2019-03-27 01:39:14,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:14,984] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8978
[2019-03-27 01:39:14,990] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3028660219808765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482297.4615000679, 482297.4615000679, 165786.6792181012], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2943600.0000, 
sim time next is 2944200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3029009757181943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482353.0474773564, 482353.0474773558, 165790.674409861], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16012165749180035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.133986957632599, 0.13398695763259882, 0.24744876777591196], 
reward next is 0.7526, 
noisyNet noise sample is [array([2.0863302], dtype=float32), 0.3284586]. 
=============================================
[2019-03-27 01:39:15,648] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:15,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4753
[2019-03-27 01:39:15,662] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3022952447846516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481388.7555171007, 481388.7555171007, 165721.4112967301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2941200.0000, 
sim time next is 2941800.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3024168540965771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481582.4135163953, 481582.4135163959, 165735.312117779], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15953837842961094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13377289264344314, 0.1337728926434433, 0.24736613748922237], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.424312], dtype=float32), 2.2829893]. 
=============================================
[2019-03-27 01:39:19,485] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:19,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7776
[2019-03-27 01:39:19,498] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3036599564343593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483561.1139139077, 483561.1139139077, 165877.623578947], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3025800.0000, 
sim time next is 3026400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3028372869783388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482251.184449583, 482251.1844495836, 165783.3446633421], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16004492407028767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13395866234710638, 0.13395866234710654, 0.24743782785573448], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.1177662], dtype=float32), 0.3003769]. 
=============================================
[2019-03-27 01:39:19,767] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:19,775] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7279
[2019-03-27 01:39:19,781] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3014658689513921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480068.0505538484, 480068.0505538478, 165626.7477637717], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3030000.0000, 
sim time next is 3030600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3011493778918823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479564.4341337842, 479564.4341337842, 165590.718614072], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15801129866491842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13321234281494004, 0.13321234281494004, 0.2471503262896597], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.5651322], dtype=float32), -1.1355393]. 
=============================================
[2019-03-27 01:39:25,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:25,362] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8917
[2019-03-27 01:39:25,371] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4218956379822235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617734.3940748498, 617734.3940748498, 175876.4417005614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3300600.0000, 
sim time next is 3301200.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4211378517314665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616625.1432517284, 616625.1432517284, 175770.1027069799], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 1.0, 1.0, 0.30257572497767055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17128476201436899, 0.17128476201436899, 0.2623434368760894], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.33871284], dtype=float32), -0.9917224]. 
=============================================
[2019-03-27 01:39:26,010] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.166873e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:39:26,020] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6307
[2019-03-27 01:39:26,025] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.391406357752991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584038.9350054035, 584038.9350054035, 173073.3803550533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3115800.0000, 
sim time next is 3116400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3925464362790107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585739.7521701452, 585739.7521701445, 173228.1450865252], 
processed observation next is [1.0, 0.043478260869565216, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2681282364807358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1627054867139292, 0.162705486713929, 0.25854947027839587], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.0819147], dtype=float32), -1.3154979]. 
=============================================
[2019-03-27 01:39:30,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:30,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4516
[2019-03-27 01:39:30,360] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 94.0, 1.0, 2.0, 0.5123134853414256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715884.2964793191, 715884.2964793191, 185547.8558950655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3190800.0000, 
sim time next is 3191400.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5079358463610264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709765.1329730997, 709765.1329730991, 184848.7147648245], 
processed observation next is [1.0, 0.9565217391304348, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4071516221217185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19715698138141657, 0.19715698138141644, 0.2758936041266037], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.49803737], dtype=float32), 0.350385]. 
=============================================
[2019-03-27 01:39:31,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2476078e-29 1.0000000e+00 5.7573958e-36 1.2339446e-31 1.1661608e-30], sum to 1.0000
[2019-03-27 01:39:31,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5537
[2019-03-27 01:39:31,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2524472.528620838 W.
[2019-03-27 01:39:31,258] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 72.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.249628111074995, 6.9112, 168.9114433285571, 2524472.528620838, 2284382.129661372, 475369.4540864703], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3403800.0000, 
sim time next is 3404400.0000, 
raw observation next is [30.66666666666666, 71.66666666666667, 1.0, 2.0, 0.9762937826622031, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002678058004385, 6.9112, 168.9121372728504, 2261821.316760144, 2196923.948737564, 456216.6739832898], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879934, 0.7166666666666667, 1.0, 1.0, 0.9714382923641002, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00914780580043848, 0.0, 0.8294359223207334, 0.62828369910004, 0.6102566524271011, 0.6809204089302833], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5837995], dtype=float32), -0.5120938]. 
=============================================
[2019-03-27 01:39:31,733] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.977421e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:39:31,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6638
[2019-03-27 01:39:31,748] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4862856898744213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679502.5692020772, 679502.5692020766, 181475.2182603159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3199800.0000, 
sim time next is 3200400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4859197774397443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678991.1046314024, 678991.1046314024, 181419.4148364319], 
processed observation next is [0.0, 0.043478260869565216, 0.38388625592417064, 0.94, 1.0, 1.0, 0.38062623787921, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18860864017538956, 0.18860864017538956, 0.27077524602452524], 
reward next is 0.7292, 
noisyNet noise sample is [array([0.03640256], dtype=float32), 0.7918832]. 
=============================================
[2019-03-27 01:39:34,184] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:34,194] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8596
[2019-03-27 01:39:34,199] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5786776747525947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808653.99059273, 808653.99059273, 196841.6482538433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3249600.0000, 
sim time next is 3250200.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5808133630351888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811639.5778681749, 811639.5778681749, 197226.576163021], 
processed observation next is [0.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.4949558590785407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22545543829671524, 0.22545543829671524, 0.29436802412391194], 
reward next is 0.7056, 
noisyNet noise sample is [array([1.5515413], dtype=float32), -0.17242983]. 
=============================================
[2019-03-27 01:39:37,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:39:37,171] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4315
[2019-03-27 01:39:37,175] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 79.0, 1.0, 2.0, 0.534595523560122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747031.1835200633, 747031.1835200626, 189193.6894458492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3793200.0000, 
sim time next is 3793800.0000, 
raw observation next is [28.16666666666667, 79.0, 1.0, 2.0, 0.5283272826043334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738269.0474789258, 738269.0474789252, 188153.1214616688], 
processed observation next is [1.0, 0.9130434782608695, 0.5339652448657191, 0.79, 1.0, 1.0, 0.43171961759558236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20507473541081273, 0.20507473541081256, 0.2808255544204012], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.9179182], dtype=float32), 0.81877893]. 
=============================================
[2019-03-27 01:39:47,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.595417e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:39:47,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2621
[2019-03-27 01:39:47,566] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.6556928344327676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916322.7052083366, 916322.705208336, 211569.2633407649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [27.33333333333334, 79.0, 1.0, 2.0, 0.7490208551332743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046811.75168703, 1046811.75168703, 231779.4291621183], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.79, 1.0, 1.0, 0.6976154881123787, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2907810421352861, 0.2907810421352861, 0.3459394465106243], 
reward next is 0.6541, 
noisyNet noise sample is [array([-0.87600106], dtype=float32), 0.9329645]. 
=============================================
[2019-03-27 01:39:50,746] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1178153e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:39:50,754] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6655
[2019-03-27 01:39:50,761] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5512261777908358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770278.8939225607, 770278.8939225607, 192011.4572317818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5516599039562212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770885.1992047711, 770885.1992047705, 192085.9950600215], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4598312095858087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21413477755688085, 0.2141347775568807, 0.28669551501495744], 
reward next is 0.7133, 
noisyNet noise sample is [array([2.2393086], dtype=float32), 1.1339201]. 
=============================================
[2019-03-27 01:39:57,357] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1771926e-27 1.0000000e+00 1.2098025e-36 1.6039911e-32 1.4775645e-32], sum to 1.0000
[2019-03-27 01:39:57,365] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3967
[2019-03-27 01:39:57,375] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3295882.319507225 W.
[2019-03-27 01:39:57,380] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 71.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.506414680963704, 6.9112, 168.8981974579482, 3295882.319507225, 1454908.576656296, 307864.9356635855], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3658800.0000, 
sim time next is 3659400.0000, 
raw observation next is [29.0, 72.0, 1.0, 2.0, 0.6026929384351798, 1.0, 1.0, 0.6026929384351798, 1.0, 1.0, 1.03, 6.912396004054284, 6.9112, 170.5573041426782, 2528604.285685539, 2527747.539331121, 489645.1360118022], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.72, 1.0, 1.0, 0.5213167932953973, 1.0, 0.5, 0.5213167932953973, 1.0, 0.5, 1.0365853658536586, 0.00011960040542842165, 0.0, 0.8375144448122397, 0.7023900793570941, 0.7021520942586447, 0.7308136358385107], 
reward next is 0.2632, 
noisyNet noise sample is [array([0.30453375], dtype=float32), 1.4788047]. 
=============================================
[2019-03-27 01:39:59,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9077913e-23 1.0000000e+00 2.8124668e-29 1.6849564e-23 1.0626141e-26], sum to 1.0000
[2019-03-27 01:39:59,703] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2611
[2019-03-27 01:39:59,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2719805.576360418 W.
[2019-03-27 01:39:59,716] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 59.66666666666667, 1.0, 2.0, 0.9723243363885684, 1.0, 2.0, 0.9723243363885684, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2719805.576360418, 2719805.576360418, 512483.4118801155], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3678600.0000, 
sim time next is 3679200.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.940728681642997, 1.0, 2.0, 0.940728681642997, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2631332.49089774, 2631332.49089774, 494196.7976922882], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.59, 1.0, 1.0, 0.9285887730638518, 1.0, 1.0, 0.9285887730638518, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7309256919160388, 0.7309256919160388, 0.7376071607347585], 
reward next is 0.2624, 
noisyNet noise sample is [array([-0.70446616], dtype=float32), 0.774835]. 
=============================================
[2019-03-27 01:40:01,874] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.480042e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:40:01,883] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2585
[2019-03-27 01:40:01,889] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4900976722178233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684830.8911912265, 684830.8911912271, 182058.6909631451], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3714600.0000, 
sim time next is 3715200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4884502095470469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682528.0935166902, 682528.0935166902, 181805.8168284014], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.79, 1.0, 1.0, 0.38367495126150225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18959113708796949, 0.18959113708796949, 0.2713519654155245], 
reward next is 0.7286, 
noisyNet noise sample is [array([0.78911275], dtype=float32), 2.5878127]. 
=============================================
[2019-03-27 01:40:04,176] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 01:40:04,177] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:40:04,179] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:40:04,179] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:40:04,181] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:40:04,181] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:40:04,182] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:40:04,182] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:40:04,185] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:40:04,184] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:40:04,187] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:40:04,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-27 01:40:04,229] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-27 01:40:04,230] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-27 01:40:04,230] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-27 01:40:04,290] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-27 01:40:50,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.07399893]
[2019-03-27 01:40:50,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.84108101333334, 97.63293916666667, 1.0, 2.0, 0.3351844222747196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524679.1143761307, 524679.1143761307, 168809.8574073112]
[2019-03-27 01:40:50,759] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:40:50,761] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.7531156e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7694717772864264
[2019-03-27 01:40:54,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.07399893]
[2019-03-27 01:40:54,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.15, 61.33333333333334, 1.0, 2.0, 0.5270803541390477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736526.022139428, 736526.0221394274, 187947.1124369414]
[2019-03-27 01:40:54,133] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:40:54,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2787160213696003
[2019-03-27 01:41:05,840] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.07399893]
[2019-03-27 01:41:05,841] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 57.33333333333333, 1.0, 2.0, 0.5380066837759849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751799.545241202, 751799.545241202, 189764.923392691]
[2019-03-27 01:41:05,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:41:05,848] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6370458e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6534142752941056
[2019-03-27 01:41:58,750] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.07399893]
[2019-03-27 01:41:58,751] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.66666666666667, 68.33333333333334, 1.0, 2.0, 0.3252123957695934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512923.8026395476, 512923.8026395476, 167979.1986728037]
[2019-03-27 01:41:58,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:41:58,757] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.2991595e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.42111778495997343
[2019-03-27 01:41:59,039] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:41:59,184] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:41:59,346] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:41:59,482] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:41:59,577] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:42:00,594] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 950000, evaluation results [950000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:42:03,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:03,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2061
[2019-03-27 01:42:03,638] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5200806170125459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726741.4568506749, 726741.4568506742, 186801.982092354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3803400.0000, 
sim time next is 3804000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5185093241787437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724545.0414227098, 724545.0414227098, 186546.9073210047], 
processed observation next is [0.0, 0.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4198907520225828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20126251150630828, 0.20126251150630828, 0.27842821988209654], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.2775533], dtype=float32), -2.1620064]. 
=============================================
[2019-03-27 01:42:03,654] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.02154]
 [75.06559]
 [74.73671]
 [74.64668]
 [74.87684]], R is [[74.87701416]
 [74.8494339 ]
 [74.82189941]
 [74.79483032]
 [74.76828003]].
[2019-03-27 01:42:05,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:05,535] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8062
[2019-03-27 01:42:05,540] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5414278334244371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756581.892399559, 756581.8923995583, 190341.0291017367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3873600.0000, 
sim time next is 3874200.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5433314895713639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759242.9792850231, 759242.9792850225, 190663.0707257695], 
processed observation next is [0.0, 0.8695652173913043, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4497969753871854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21090082757917308, 0.21090082757917292, 0.2845717473518948], 
reward next is 0.7154, 
noisyNet noise sample is [array([0.53992814], dtype=float32), 0.2662228]. 
=============================================
[2019-03-27 01:42:07,459] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:07,466] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0993
[2019-03-27 01:42:07,470] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5793226602816446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809555.6477844748, 809555.6477844748, 196956.585201882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3891000.0000, 
sim time next is 3891600.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.5794535150899367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809738.5762904576, 809738.5762904576, 196980.0352166448], 
processed observation next is [0.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4933174880601647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2249273823029049, 0.2249273823029049, 0.29400005256215644], 
reward next is 0.7060, 
noisyNet noise sample is [array([0.21886475], dtype=float32), 0.26068205]. 
=============================================
[2019-03-27 01:42:07,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:07,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5685
[2019-03-27 01:42:07,714] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 69.0, 1.0, 2.0, 0.5548712087925728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775374.2891201531, 775374.2891201531, 192639.4390450597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3872400.0000, 
sim time next is 3873000.0000, 
raw observation next is [30.33333333333333, 69.5, 1.0, 2.0, 0.5482789331545527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766158.9533746095, 766158.95337461, 191505.6622400055], 
processed observation next is [0.0, 0.8260869565217391, 0.6366508688783569, 0.695, 1.0, 1.0, 0.45575775078861763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21282193149294706, 0.21282193149294723, 0.28582934662687387], 
reward next is 0.7142, 
noisyNet noise sample is [array([-2.1793554], dtype=float32), -0.38510963]. 
=============================================
[2019-03-27 01:42:07,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.17862 ]
 [82.04197 ]
 [81.913284]
 [81.84026 ]
 [81.8085  ]], R is [[82.18454742]
 [82.07518005]
 [81.96518707]
 [81.8546524 ]
 [81.74310303]].
[2019-03-27 01:42:09,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6208622e-33 1.0000000e+00 0.0000000e+00 3.7555704e-34 4.3916210e-37], sum to 1.0000
[2019-03-27 01:42:09,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1559
[2019-03-27 01:42:09,295] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.7885161822026348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1102038.019202122, 1102038.019202122, 241113.4513002959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4086000.0000, 
sim time next is 4086600.0000, 
raw observation next is [27.16666666666666, 92.33333333333334, 1.0, 2.0, 0.8374828192902236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1170511.996205898, 1170511.996205898, 253321.216635226], 
processed observation next is [1.0, 0.30434782608695654, 0.4865718799368086, 0.9233333333333335, 1.0, 1.0, 0.8041961678195465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.325142221168305, 0.325142221168305, 0.3780913681122776], 
reward next is 0.6219, 
noisyNet noise sample is [array([-0.14141001], dtype=float32), 1.4925402]. 
=============================================
[2019-03-27 01:42:12,979] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4494850e-29 1.0000000e+00 9.8503435e-37 1.7432711e-30 2.8886742e-32], sum to 1.0000
[2019-03-27 01:42:12,988] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8205
[2019-03-27 01:42:12,991] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.929289861679488, 6.9112, 168.9126525651582, 1466597.264269796, 1453763.716882355, 311355.5335554229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3998400.0000, 
sim time next is 3999000.0000, 
raw observation next is [29.83333333333333, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.011999370684645, 6.9112, 168.9122115008688, 1525314.127889714, 1453803.90048471, 311355.9980509268], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.010079937068464506, 0.0, 0.8294362868142855, 0.4236983688582539, 0.4038344168013083, 0.46471044485212953], 
reward next is 0.0313, 
noisyNet noise sample is [array([1.3928223], dtype=float32), 1.5714847]. 
=============================================
[2019-03-27 01:42:13,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[54.333096]
 [54.740982]
 [54.747223]
 [54.40779 ]
 [54.86919 ]], R is [[54.07688904]
 [53.98096085]
 [53.99352646]
 [54.02698135]
 [54.04793549]].
[2019-03-27 01:42:16,676] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1503310e-19 1.0000000e+00 6.5063713e-26 4.2387158e-21 1.7139499e-20], sum to 1.0000
[2019-03-27 01:42:16,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7173
[2019-03-27 01:42:16,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2324458.586707045 W.
[2019-03-27 01:42:16,698] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.33333333333334, 65.5, 1.0, 2.0, 0.5540800167804111, 1.0, 2.0, 0.5540800167804111, 1.0, 1.0, 0.9622537520859983, 6.9112, 6.9112, 170.5573041426782, 2324458.586707045, 2324458.586707045, 454557.3942471676], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4014600.0000, 
sim time next is 4015200.0000, 
raw observation next is [31.66666666666667, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 8.232305864446097, 6.9112, 168.9054866850932, 3225732.572628616, 2288536.050842061, 473081.1806765707], 
processed observation next is [1.0, 0.4782608695652174, 0.6998420221169038, 0.65, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.1321105864446097, 0.0, 0.8294032648901932, 0.8960368257301711, 0.6357044585672392, 0.7060913144426428], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3319646], dtype=float32), -2.4791303]. 
=============================================
[2019-03-27 01:42:20,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9841801e-33 1.0000000e+00 0.0000000e+00 1.9385465e-36 1.8279506e-37], sum to 1.0000
[2019-03-27 01:42:20,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3667
[2019-03-27 01:42:20,181] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6869576680131998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 960034.6752812007, 960034.6752812007, 218055.4227884784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4081200.0000, 
sim time next is 4081800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7025231281703276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 981797.7178059865, 981797.7178059865, 221390.9508297546], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.6415941303256959, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2727215882794407, 0.2727215882794407, 0.330434254969783], 
reward next is 0.6696, 
noisyNet noise sample is [array([-0.58704895], dtype=float32), 1.1315559]. 
=============================================
[2019-03-27 01:42:22,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1051560e-26 1.0000000e+00 2.2053450e-34 1.0140254e-29 2.5308771e-30], sum to 1.0000
[2019-03-27 01:42:22,546] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7504
[2019-03-27 01:42:22,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3409011.273871362 W.
[2019-03-27 01:42:22,557] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 67.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.607935741826958, 6.9112, 170.5573041426782, 3409011.273871362, 2909911.120602772, 549800.9871645594], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [34.33333333333333, 67.0, 1.0, 2.0, 0.8441705218408299, 1.0, 2.0, 0.7426753004346774, 1.0, 1.0, 1.03, 7.005109102622261, 6.9112, 170.5573041426782, 3116634.322669521, 3049363.412483135, 570734.4053233489], 
processed observation next is [1.0, 0.6956521739130435, 0.8262243285939966, 0.67, 1.0, 1.0, 0.8122536407720842, 1.0, 1.0, 0.6899702414875631, 1.0, 0.5, 1.0365853658536586, 0.009390910262226094, 0.0, 0.8375144448122397, 0.8657317562970892, 0.8470453923564263, 0.8518423960049984], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.96647304], dtype=float32), -0.86689067]. 
=============================================
[2019-03-27 01:42:28,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.478446e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 01:42:28,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7048
[2019-03-27 01:42:28,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5856808067897477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818444.0564191804, 818444.0564191804, 198108.2894874195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4421400.0000, 
sim time next is 4422000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5853686592339303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818007.6861553146, 818007.6861553151, 198051.4949600791], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5004441677517233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22722435726536516, 0.22722435726536533, 0.29559924620907324], 
reward next is 0.7044, 
noisyNet noise sample is [array([0.9129198], dtype=float32), 0.46093625]. 
=============================================
[2019-03-27 01:42:28,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[60.84142 ]
 [60.908264]
 [60.815483]
 [60.781914]
 [60.729958]], R is [[60.90381622]
 [60.9990921 ]
 [61.09326172]
 [61.18527985]
 [61.27519226]].
[2019-03-27 01:42:34,206] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:34,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0693
[2019-03-27 01:42:34,224] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.624147732083174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872220.7092239243, 872220.7092239243, 205331.2684898376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4319400.0000, 
sim time next is 4320000.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6231511112308057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870827.4017313664, 870827.4017313664, 205138.6838726971], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5459651942539827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2418965004809351, 0.2418965004809351, 0.3061771401085031], 
reward next is 0.6938, 
noisyNet noise sample is [array([1.0594302], dtype=float32), -1.4343597]. 
=============================================
[2019-03-27 01:42:34,235] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[65.809555]
 [65.803825]
 [65.77882 ]
 [65.76515 ]
 [65.76985 ]], R is [[63.47119141]
 [63.53001404]
 [63.58807755]
 [63.64558411]
 [63.70230484]].
[2019-03-27 01:42:39,450] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9783376e-31 1.0000000e+00 0.0000000e+00 4.5693560e-35 4.7491991e-37], sum to 1.0000
[2019-03-27 01:42:39,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4608
[2019-03-27 01:42:39,465] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 67.33333333333334, 1.0, 2.0, 0.4704478513701323, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657365.002808942, 657365.0028089426, 179097.3484902468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4900800.0000, 
sim time next is 4901400.0000, 
raw observation next is [29.5, 68.0, 1.0, 2.0, 0.4640638058844589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648441.7419336855, 648441.7419336861, 178159.288156335], 
processed observation next is [1.0, 0.7391304347826086, 0.5971563981042655, 0.68, 1.0, 1.0, 0.35429374202946856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1801227060926904, 0.18012270609269057, 0.2659093853079627], 
reward next is 0.7341, 
noisyNet noise sample is [array([1.0105861], dtype=float32), -0.4060392]. 
=============================================
[2019-03-27 01:42:41,637] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:41,646] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9176
[2019-03-27 01:42:41,654] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 66.83333333333333, 1.0, 2.0, 0.5300821452160126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740722.0960106722, 740722.0960106716, 188443.2998642867], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4461000.0000, 
sim time next is 4461600.0000, 
raw observation next is [30.66666666666667, 67.66666666666667, 1.0, 2.0, 0.5409105871556351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755858.8442419033, 755858.844241904, 190254.3010965207], 
processed observation next is [0.0, 0.6521739130434783, 0.6524486571879939, 0.6766666666666667, 1.0, 1.0, 0.446880225488717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20996079006719537, 0.20996079006719556, 0.28396164342764285], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.26450804], dtype=float32), -0.4012618]. 
=============================================
[2019-03-27 01:42:45,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:42:45,058] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3865
[2019-03-27 01:42:45,069] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5124042034942096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716011.1047529997, 716011.1047529991, 185561.8185028202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4491600.0000, 
sim time next is 4492200.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5105756537851628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713455.1117163413, 713455.111716342, 185269.0972298737], 
processed observation next is [0.0, 1.0, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.410332112994172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19818197547676147, 0.19818197547676167, 0.27652104064160254], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.4865797], dtype=float32), -0.06803879]. 
=============================================
[2019-03-27 01:42:56,836] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 01:42:56,838] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:42:56,839] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:42:56,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:42:56,841] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:42:56,842] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:42:56,843] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:42:56,843] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:42:56,841] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:42:56,847] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:42:56,846] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:42:56,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-27 01:42:56,891] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-27 01:42:56,909] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-27 01:42:56,912] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-27 01:42:56,946] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-27 01:43:03,137] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08101655]
[2019-03-27 01:43:03,141] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.24417301, 84.32769902, 1.0, 2.0, 0.3206718545718471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505155.2580692103, 505155.2580692109, 167373.3839649543]
[2019-03-27 01:43:03,143] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:43:03,150] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4651871e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.08593832926577705
[2019-03-27 01:43:47,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08101655]
[2019-03-27 01:43:47,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.88333333333333, 65.0, 1.0, 2.0, 0.9671082851086568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1351799.005228388, 1351799.005228387, 289066.6259758779]
[2019-03-27 01:43:47,149] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:43:47,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5074542e-32 1.0000000e+00 0.0000000e+00 3.6886075e-36 2.8603654e-37], sampled 0.34095603716197675
[2019-03-27 01:43:47,211] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08101655]
[2019-03-27 01:43:47,212] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.15912500833333, 91.63807265000001, 1.0, 2.0, 0.6924820477588642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 967758.6047663495, 967758.6047663488, 219237.8964388348]
[2019-03-27 01:43:47,214] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:43:47,217] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9978796e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6068330706395684
[2019-03-27 01:43:58,097] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.08101655]
[2019-03-27 01:43:58,099] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.35, 76.16666666666666, 1.0, 2.0, 0.7188801398074572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1004667.958510842, 1004667.958510842, 224974.3982421056]
[2019-03-27 01:43:58,100] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:43:58,102] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3212998e-33 1.0000000e+00 0.0000000e+00 4.4217110e-37 3.6958998e-38], sampled 0.8080645033864334
[2019-03-27 01:44:51,332] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:44:51,978] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:44:52,134] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:44:52,156] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:44:52,256] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:44:53,273] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 975000, evaluation results [975000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:44:57,017] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:44:57,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2279
[2019-03-27 01:44:57,033] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4914819092138431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686765.759974616, 686765.7599746166, 182272.0662400373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4830000.0000, 
sim time next is 4830600.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4914228084718356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686683.1496406853, 686683.1496406853, 182262.9594596369], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 1.0, 1.0, 0.3872563957491995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19074531934463482, 0.19074531934463482, 0.27203426785020435], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.21486989], dtype=float32), 1.8240073]. 
=============================================
[2019-03-27 01:44:58,184] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.71836419e-23 1.00000000e+00 1.00601577e-28 3.94106527e-25
 1.90919463e-25], sum to 1.0000
[2019-03-27 01:44:58,190] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6503
[2019-03-27 01:44:58,197] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2418771.254010146 W.
[2019-03-27 01:44:58,202] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.576539543585831, 1.0, 2.0, 0.576539543585831, 1.0, 1.0, 0.9958112708766493, 6.9112, 6.9112, 170.5573041426782, 2418771.254010146, 2418771.254010146, 470956.8988067256], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4793400.0000, 
sim time next is 4794000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8237052307102103, 1.0, 2.0, 0.8237052307102103, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2303701.93545475, 2303701.93545475, 431574.0366327861], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7875966635062774, 1.0, 1.0, 0.7875966635062774, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6399172042929862, 0.6399172042929862, 0.6441403531832628], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0606409], dtype=float32), 0.39995757]. 
=============================================
[2019-03-27 01:44:58,216] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[45.68893 ]
 [46.725147]
 [46.961033]
 [46.85167 ]
 [46.542564]], R is [[46.03130341]
 [45.86807251]
 [45.77523422]
 [45.737854  ]
 [45.68917847]].
[2019-03-27 01:45:01,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:45:01,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3233
[2019-03-27 01:45:01,844] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 89.0, 1.0, 2.0, 0.4876549488271359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681416.4922489817, 681416.4922489817, 181683.9927220132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5023800.0000, 
sim time next is 5024400.0000, 
raw observation next is [25.33333333333333, 89.0, 1.0, 2.0, 0.4839199932017846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676195.8533694813, 676195.8533694813, 181114.3606570241], 
processed observation next is [0.0, 0.13043478260869565, 0.3996840442338071, 0.89, 1.0, 1.0, 0.3782168592792585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1878321814915226, 0.1878321814915226, 0.27031994127914044], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.5756607], dtype=float32), 0.1921062]. 
=============================================
[2019-03-27 01:45:05,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3488112e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:45:05,689] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9517
[2019-03-27 01:45:05,695] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5224365732243104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730034.7143716051, 730034.7143716051, 187185.8046662234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5085600.0000, 
sim time next is 5086200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5223773964484538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729951.9943890804, 729951.994389081, 187176.1437091538], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42455108005837805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20276444288585568, 0.20276444288585585, 0.2793673786703788], 
reward next is 0.7206, 
noisyNet noise sample is [array([0.7471539], dtype=float32), 0.28321978]. 
=============================================
[2019-03-27 01:45:19,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.930956e-38], sum to 1.0000
[2019-03-27 01:45:19,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5844
[2019-03-27 01:45:19,628] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 64.5, 1.0, 2.0, 0.5153740946603607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720162.5068553776, 720162.506855377, 186040.0357613687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5135400.0000, 
sim time next is 5136000.0000, 
raw observation next is [30.66666666666666, 64.0, 1.0, 2.0, 0.5176329147832306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723319.9635912878, 723319.9635912871, 186405.0226921558], 
processed observation next is [0.0, 0.43478260869565216, 0.6524486571879934, 0.64, 1.0, 1.0, 0.4188348370882296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20092221210869107, 0.20092221210869088, 0.278216451779337], 
reward next is 0.7218, 
noisyNet noise sample is [array([-0.73021585], dtype=float32), 0.96257824]. 
=============================================
[2019-03-27 01:45:19,645] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.19408]
 [79.17064]
 [79.13046]
 [79.08002]
 [78.90913]], R is [[79.14988708]
 [79.08071899]
 [79.01292419]
 [78.94609833]
 [78.88020325]].
[2019-03-27 01:45:23,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.5025671e-19 1.0000000e+00 8.1823616e-25 2.2083226e-21 7.5399734e-21], sum to 1.0000
[2019-03-27 01:45:23,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0457
[2019-03-27 01:45:23,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1875728.083024098 W.
[2019-03-27 01:45:23,756] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.8, 78.00000000000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.505606232345007, 6.9112, 168.9097858285181, 1875728.083024098, 1454043.759541865, 311356.1422666765], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5382600.0000, 
sim time next is 5383200.0000, 
raw observation next is [31.0, 77.0, 1.0, 2.0, 0.5891737546884361, 0.0, 2.0, 0.0, 1.0, 1.0, 1.023199969155767, 6.911199999999999, 6.9112, 168.9124725185492, 1647292.856554819, 1647292.85655482, 360805.1132786371], 
processed observation next is [1.0, 0.30434782608695654, 0.6682464454976303, 0.77, 1.0, 1.0, 0.5050286201065496, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.028292645311911, -8.881784197001253e-17, 0.0, 0.8294375685305906, 0.4575813490430053, 0.45758134904300557, 0.5385150944457271], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7174221], dtype=float32), 0.12147871]. 
=============================================
[2019-03-27 01:45:26,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1770035e-24 1.0000000e+00 2.3393451e-33 7.3600638e-27 2.6928155e-26], sum to 1.0000
[2019-03-27 01:45:26,107] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5201
[2019-03-27 01:45:26,115] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.26666666666667, 82.0, 1.0, 2.0, 0.6158046336834332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860556.8500148747, 860556.8500148754, 203727.530860143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5433600.0000, 
sim time next is 5434200.0000, 
raw observation next is [30.2, 82.0, 1.0, 2.0, 0.6140203050687977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858062.3308292129, 858062.3308292129, 203387.1720910434], 
processed observation next is [1.0, 0.9130434782608695, 0.6303317535545023, 0.82, 1.0, 1.0, 0.534964222974455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23835064745255913, 0.23835064745255913, 0.30356294341946777], 
reward next is 0.6964, 
noisyNet noise sample is [array([-0.74516153], dtype=float32), 0.1162977]. 
=============================================
[2019-03-27 01:45:33,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2377579e-32 1.0000000e+00 0.0000000e+00 7.4764999e-37 4.7526522e-33], sum to 1.0000
[2019-03-27 01:45:33,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7595
[2019-03-27 01:45:33,058] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.98333333333333, 83.5, 1.0, 2.0, 0.6136009548963426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857476.0736089996, 857476.0736089996, 203307.4196781815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5352600.0000, 
sim time next is 5353200.0000, 
raw observation next is [29.9, 84.0, 1.0, 2.0, 0.6127049561961891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 856223.4563763249, 856223.4563763243, 203137.0524924579], 
processed observation next is [1.0, 1.0, 0.6161137440758293, 0.84, 1.0, 1.0, 0.5333794652966134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23783984899342359, 0.23783984899342342, 0.3031896305857581], 
reward next is 0.6968, 
noisyNet noise sample is [array([1.3055733], dtype=float32), 1.3151802]. 
=============================================
[2019-03-27 01:45:41,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7155131e-12 1.0000000e+00 1.5898903e-15 3.8593445e-13 6.3410999e-10], sum to 1.0000
[2019-03-27 01:45:41,607] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1289
[2019-03-27 01:45:41,614] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2201011.20287363 W.
[2019-03-27 01:45:41,617] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.51666666666667, 52.83333333333334, 1.0, 2.0, 0.932845882778815, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986254737969471, 6.9112, 168.9124473626092, 2201011.20287363, 2147764.949071982, 444051.2606804051], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5575800.0000, 
sim time next is 5576400.0000, 
raw observation next is [33.6, 52.0, 1.0, 2.0, 0.9001802039698571, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984473357307231, 6.9112, 168.9124592108864, 2155288.738648532, 2103306.250143442, 434886.8861237548], 
processed observation next is [1.0, 0.5652173913043478, 0.7914691943127963, 0.52, 1.0, 1.0, 0.879735185505852, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007327335730723128, 0.0, 0.829437503183874, 0.5986913162912588, 0.584251736150956, 0.6490849046623206], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.262185], dtype=float32), -0.21655218]. 
=============================================
[2019-03-27 01:45:42,304] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.2175455e-10 9.9999917e-01 1.7627384e-12 2.0543500e-09 8.6493105e-07], sum to 1.0000
[2019-03-27 01:45:42,314] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8195
[2019-03-27 01:45:42,320] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2918520.804910393 W.
[2019-03-27 01:45:42,324] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.73333333333333, 48.66666666666666, 1.0, 2.0, 0.7498621760224192, 1.0, 2.0, 0.6955211275254721, 1.0, 2.0, 1.03, 7.005101664071598, 6.9112, 170.5573041426782, 2918520.804910393, 2851255.223260428, 537539.652458259], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5499600.0000, 
sim time next is 5500200.0000, 
raw observation next is [35.61666666666667, 48.83333333333334, 1.0, 2.0, 0.749989204155392, 1.0, 2.0, 0.6955846415919585, 1.0, 2.0, 1.03, 7.005101674089328, 6.9112, 170.5573041426782, 2918787.631743343, 2851522.04291727, 537582.0686387301], 
processed observation next is [1.0, 0.6521739130434783, 0.887045813586098, 0.48833333333333345, 1.0, 1.0, 0.6987821736811952, 1.0, 1.0, 0.6332345079421187, 1.0, 1.0, 1.0365853658536586, 0.009390167408932814, 0.0, 0.8375144448122397, 0.8107743421509286, 0.7920894563659084, 0.8023612964757165], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9474207], dtype=float32), 1.7141078]. 
=============================================
[2019-03-27 01:45:43,587] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.6830920e-30 1.0000000e+00 4.8522414e-38 1.9723518e-36 5.8019418e-25], sum to 1.0000
[2019-03-27 01:45:43,597] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4330
[2019-03-27 01:45:43,603] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 83.0, 1.0, 2.0, 0.56496112017837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789479.121331899, 789479.121331899, 194399.8114578796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5521200.0000, 
sim time next is 5521800.0000, 
raw observation next is [28.55, 83.5, 1.0, 2.0, 0.563520643250032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787465.4464596465, 787465.4464596465, 194146.6071224975], 
processed observation next is [1.0, 0.9130434782608695, 0.552132701421801, 0.835, 1.0, 1.0, 0.4741212569277494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21874040179434623, 0.21874040179434623, 0.28977105540671266], 
reward next is 0.7102, 
noisyNet noise sample is [array([0.6528521], dtype=float32), -0.6206851]. 
=============================================
[2019-03-27 01:45:44,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3197865e-25 1.0000000e+00 5.1533298e-33 7.7079268e-27 5.5538111e-24], sum to 1.0000
[2019-03-27 01:45:44,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0336
[2019-03-27 01:45:44,404] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 93.5, 1.0, 2.0, 0.5424731547390964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758043.1275469636, 758043.1275469636, 190518.1862080742], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5535000.0000, 
sim time next is 5535600.0000, 
raw observation next is [26.43333333333333, 94.0, 1.0, 2.0, 0.5434098680027046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759352.5432354767, 759352.543235476, 190676.8088291652], 
processed observation next is [1.0, 0.043478260869565216, 0.4518167456556081, 0.94, 1.0, 1.0, 0.44989140723217425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21093126200985465, 0.21093126200985446, 0.28459225198382865], 
reward next is 0.7154, 
noisyNet noise sample is [array([1.1596587], dtype=float32), -0.025337722]. 
=============================================
[2019-03-27 01:45:48,019] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7589093e-14 1.0000000e+00 3.2794157e-20 7.2946181e-15 2.9414294e-11], sum to 1.0000
[2019-03-27 01:45:48,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1007
[2019-03-27 01:45:48,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1660204.700085991 W.
[2019-03-27 01:45:48,041] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.65, 90.0, 1.0, 2.0, 0.3958617962207523, 1.0, 2.0, 0.3958617962207523, 1.0, 2.0, 0.6802791036993229, 6.9112, 6.9112, 170.5573041426782, 1660204.700085991, 1660204.700085991, 349612.0578713801], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5797800.0000, 
sim time next is 5798400.0000, 
raw observation next is [26.6, 90.33333333333334, 1.0, 2.0, 0.3756273678003968, 1.0, 2.0, 0.3756273678003968, 1.0, 2.0, 0.6449124604033754, 6.9112, 6.9112, 170.5573041426782, 1575281.166723456, 1575281.166723456, 338840.0451298388], 
processed observation next is [1.0, 0.08695652173913043, 0.4597156398104266, 0.9033333333333334, 1.0, 1.0, 0.24774381662698408, 1.0, 1.0, 0.24774381662698408, 1.0, 1.0, 0.5669664151260675, 0.0, 0.0, 0.8375144448122397, 0.4375781018676267, 0.4375781018676267, 0.5057314106415505], 
reward next is 0.4943, 
noisyNet noise sample is [array([-0.09507962], dtype=float32), 0.49321318]. 
=============================================
[2019-03-27 01:45:49,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.015945e-29 1.000000e+00 0.000000e+00 3.033041e-30 7.325115e-25], sum to 1.0000
[2019-03-27 01:45:49,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2357
[2019-03-27 01:45:49,112] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 91.16666666666667, 1.0, 2.0, 0.5242929619366326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732629.6620206813, 732629.6620206819, 187489.5288003953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5616600.0000, 
sim time next is 5617200.0000, 
raw observation next is [26.2, 91.33333333333334, 1.0, 2.0, 0.5227985089389646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730540.6446770929, 730540.6446770923, 187244.9733231312], 
processed observation next is [0.0, 0.0, 0.44075829383886256, 0.9133333333333334, 1.0, 1.0, 0.42505844450477664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20292795685474802, 0.20292795685474788, 0.2794701094375093], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.32237932], dtype=float32), 0.34733346]. 
=============================================
[2019-03-27 01:45:49,535] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 01:45:49,536] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:45:49,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:45:49,537] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:45:49,539] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:45:49,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:45:49,542] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:45:49,546] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:45:49,548] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:45:49,544] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:45:49,555] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:45:50,199] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-27 01:45:50,387] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-27 01:45:50,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-27 01:45:50,649] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-27 01:45:50,650] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-27 01:45:53,212] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:45:53,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.5, 92.0, 1.0, 2.0, 0.2854282215026877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458581.6140000548, 458581.6140000548, 164172.945245895]
[2019-03-27 01:45:53,215] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:45:53,216] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2536005e-32 1.0000000e+00 0.0000000e+00 1.1132949e-35 1.5285498e-24], sampled 0.6189846598971633
[2019-03-27 01:45:56,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:45:56,206] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.2, 75.0, 1.0, 2.0, 0.4228065751600374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695236.5461233738, 695236.5461233745, 182903.5054878025]
[2019-03-27 01:45:56,207] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:45:56,213] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4691242e-30 1.0000000e+00 0.0000000e+00 2.1542345e-33 5.2352685e-23], sampled 0.9083997783586002
[2019-03-27 01:46:29,312] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:46:29,315] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.20941784666667, 86.39352524333334, 1.0, 2.0, 0.5445370711109431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760928.2436081513, 760928.2436081507, 190867.2385375744]
[2019-03-27 01:46:29,316] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:46:29,319] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4451782e-32 1.0000000e+00 0.0000000e+00 3.9972783e-35 3.0647199e-24], sampled 0.9938829153397282
[2019-03-27 01:46:34,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:46:34,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 90.5, 1.0, 2.0, 0.5203009157714125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727049.3995541059, 727049.3995541052, 186837.3332100168]
[2019-03-27 01:46:34,928] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:46:34,932] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6642962e-32 1.0000000e+00 0.0000000e+00 4.4347226e-35 2.8609189e-24], sampled 0.3586543506270725
[2019-03-27 01:46:43,696] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:46:43,697] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.68333333333333, 75.0, 1.0, 2.0, 0.8486603467502972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1186143.048622905, 1186143.048622905, 256207.0576975534]
[2019-03-27 01:46:43,698] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:46:43,700] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8210297e-25 1.0000000e+00 4.0440403e-36 1.1823737e-27 4.2187499e-19], sampled 0.28946355962368486
[2019-03-27 01:46:47,550] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:46:47,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.51563175666666, 63.05331397, 1.0, 2.0, 0.5552323038457844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775879.0658557764, 775879.0658557764, 192701.136402074]
[2019-03-27 01:46:47,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:46:47,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1606577e-30 1.0000000e+00 0.0000000e+00 2.0859812e-33 4.0350832e-23], sampled 0.6058927159495002
[2019-03-27 01:47:01,717] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:47:01,719] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.29802163, 81.13967633, 1.0, 2.0, 0.502714320855391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735440.9295293802, 735440.9295293802, 188172.0699346566]
[2019-03-27 01:47:01,720] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:47:01,724] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.8060600e-30 1.0000000e+00 0.0000000e+00 9.1148941e-33 1.3997858e-22], sampled 0.28575594475402866
[2019-03-27 01:47:03,212] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:47:03,213] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.98094894333334, 69.20590862, 1.0, 2.0, 1.026098673904774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1434309.951790748, 1434309.951790749, 306983.3141616493]
[2019-03-27 01:47:03,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:47:03,219] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8716312e-26 1.0000000e+00 2.1937372e-37 1.6257645e-28 9.4238664e-20], sampled 0.9851914348181837
[2019-03-27 01:47:09,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:47:09,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 78.0, 1.0, 2.0, 0.561201549979631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 784223.54219862, 784223.5421986195, 193739.9550933108]
[2019-03-27 01:47:09,183] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:47:09,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2596075e-29 1.0000000e+00 0.0000000e+00 8.2246107e-32 5.6567274e-22], sampled 0.20458673986384457
[2019-03-27 01:47:09,734] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:47:09,735] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.1, 49.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.38630818963777, 6.9112, 168.9102611344287, 2621041.338646539, 2283988.615455489, 475067.8357866183]
[2019-03-27 01:47:09,736] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:47:09,739] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.5746822e-19 1.0000000e+00 4.3924163e-26 2.2486430e-20 1.0391406e-13], sampled 0.8092081871231603
[2019-03-27 01:47:09,740] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2621041.338646539 W.
[2019-03-27 01:47:38,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.03650203]
[2019-03-27 01:47:38,528] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.95, 64.5, 1.0, 2.0, 0.4725253613019412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666870.0763498283, 666870.076349829, 180253.5851550041]
[2019-03-27 01:47:38,529] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:47:38,533] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5586574e-32 1.0000000e+00 0.0000000e+00 2.5465886e-35 2.3845766e-24], sampled 0.08437745537247643
[2019-03-27 01:47:43,422] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:47:44,984] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:47:45,073] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:47:45,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:47:45,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:47:46,503] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1000000, evaluation results [1000000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:47:55,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.60305079e-26 1.00000000e+00 1.19941620e-38 1.20681906e-23
 5.23356191e-19], sum to 1.0000
[2019-03-27 01:47:55,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2422
[2019-03-27 01:47:55,772] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.45, 86.16666666666667, 1.0, 2.0, 0.5434275459420019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759377.2549444822, 759377.2549444829, 190679.5765445616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5782200.0000, 
sim time next is 5782800.0000, 
raw observation next is [27.4, 86.33333333333334, 1.0, 2.0, 0.5415631753959474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756771.0843011299, 756771.0843011293, 190364.048004638], 
processed observation next is [0.0, 0.9565217391304348, 0.4976303317535545, 0.8633333333333334, 1.0, 1.0, 0.44766647638065954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2102141900836472, 0.21021419008364703, 0.2841254447830418], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.96772945], dtype=float32), -0.5707471]. 
=============================================
[2019-03-27 01:48:03,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8926567e-08 9.9980813e-01 1.6616094e-11 4.4998071e-08 1.9186184e-04], sum to 1.0000
[2019-03-27 01:48:03,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0199
[2019-03-27 01:48:03,758] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2455808.428706572 W.
[2019-03-27 01:48:03,763] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.43333333333333, 71.66666666666666, 1.0, 2.0, 0.8780386151837799, 1.0, 2.0, 0.8780386151837799, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2455808.428706572, 2455808.428706572, 459657.337981064], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5917800.0000, 
sim time next is 5918400.0000, 
raw observation next is [31.3, 72.0, 1.0, 2.0, 0.888394025265404, 1.0, 2.0, 0.888394025265404, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2484800.534970658, 2484800.534970658, 465204.0823552021], 
processed observation next is [1.0, 0.5217391304347826, 0.6824644549763034, 0.72, 1.0, 1.0, 0.8655349701992819, 1.0, 1.0, 0.8655349701992819, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6902223708251828, 0.6902223708251828, 0.694334451276421], 
reward next is 0.3057, 
noisyNet noise sample is [array([0.45557064], dtype=float32), 1.0278014]. 
=============================================
[2019-03-27 01:48:10,918] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1001919e-27 1.0000000e+00 0.0000000e+00 1.1471870e-32 3.0892094e-21], sum to 1.0000
[2019-03-27 01:48:10,929] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2771
[2019-03-27 01:48:10,932] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.93333333333333, 76.0, 1.0, 2.0, 0.5255182619685863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734342.4476368666, 734342.4476368666, 187691.5052014779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6114000.0000, 
sim time next is 6114600.0000, 
raw observation next is [28.75, 77.0, 1.0, 2.0, 0.5252405013827002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733954.1797674065, 733954.1797674065, 187645.8684115319], 
processed observation next is [1.0, 0.782608695652174, 0.561611374407583, 0.77, 1.0, 1.0, 0.42800060407554236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20387616104650183, 0.20387616104650183, 0.28006846031571925], 
reward next is 0.7199, 
noisyNet noise sample is [array([0.50029576], dtype=float32), 3.2395377]. 
=============================================
[2019-03-27 01:48:12,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3881615e-28 1.0000000e+00 0.0000000e+00 1.0001091e-33 9.0733297e-19], sum to 1.0000
[2019-03-27 01:48:12,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4978
[2019-03-27 01:48:12,106] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.52786251708802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737619.3722879834, 737619.3722879834, 188077.1312553247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6241200.0000, 
sim time next is 6241800.0000, 
raw observation next is [26.93333333333333, 89.16666666666667, 1.0, 2.0, 0.5286747282065754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738754.7266024591, 738754.7266024585, 188211.2551154973], 
processed observation next is [0.0, 0.21739130434782608, 0.4755134281200631, 0.8916666666666667, 1.0, 1.0, 0.43213822675491004, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20520964627846086, 0.2052096462784607, 0.2809123210679064], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.7444624], dtype=float32), 0.23942792]. 
=============================================
[2019-03-27 01:48:17,089] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9533806e-12 9.9998534e-01 2.1817850e-17 9.6777219e-13 1.4605440e-05], sum to 1.0000
[2019-03-27 01:48:17,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7712
[2019-03-27 01:48:17,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2048723.457422412 W.
[2019-03-27 01:48:17,108] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.55, 75.5, 1.0, 2.0, 0.8240432863789234, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.996261777578445, 6.9112, 168.9124505007125, 2048723.457422412, 1988377.88558458, 414233.6074859718], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6175800.0000, 
sim time next is 6176400.0000, 
raw observation next is [29.63333333333333, 75.0, 1.0, 2.0, 0.7214252731840122, 1.0, 1.0, 0.7214252731840122, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2017392.806587805, 2017392.806587805, 383432.7840008622], 
processed observation next is [1.0, 0.4782608695652174, 0.6034755134281199, 0.75, 1.0, 1.0, 0.6643677990168821, 1.0, 0.5, 0.6643677990168821, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5603868907188347, 0.5603868907188347, 0.5722877373147197], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9220652], dtype=float32), 0.1593391]. 
=============================================
[2019-03-27 01:48:22,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6472021e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3277363e-27], sum to 1.0000
[2019-03-27 01:48:22,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1754
[2019-03-27 01:48:22,883] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.76666666666667, 62.0, 1.0, 2.0, 0.5078006300671732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709576.1251180142, 709576.1251180142, 184827.4236625128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6273600.0000, 
sim time next is 6274200.0000, 
raw observation next is [30.73333333333333, 62.0, 1.0, 2.0, 0.5066521727252193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707970.7914016352, 707970.7914016346, 184645.0240866598], 
processed observation next is [0.0, 0.6086956521739131, 0.6556082148499209, 0.62, 1.0, 1.0, 0.4056050273797823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1966585531671209, 0.19665855316712072, 0.2755895881890445], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.56114215], dtype=float32), 0.33990586]. 
=============================================
[2019-03-27 01:48:25,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.2777673e-34 1.0000000e+00 0.0000000e+00 1.0235427e-37 1.4827262e-23], sum to 1.0000
[2019-03-27 01:48:25,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8105
[2019-03-27 01:48:25,357] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 63.0, 1.0, 2.0, 0.5053681343960709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706175.943087651, 706175.9430876516, 184441.5590048312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6280200.0000, 
sim time next is 6280800.0000, 
raw observation next is [30.46666666666667, 63.0, 1.0, 2.0, 0.5044110230729163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704838.0802898043, 704838.0802898037, 184290.20026298], 
processed observation next is [0.0, 0.6956521739130435, 0.6429699842022119, 0.63, 1.0, 1.0, 0.40290484707580276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19578835563605673, 0.19578835563605657, 0.2750600003925075], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.99192363], dtype=float32), 1.0282693]. 
=============================================
[2019-03-27 01:48:26,265] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3276904e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1340946e-25], sum to 1.0000
[2019-03-27 01:48:26,276] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3264
[2019-03-27 01:48:26,283] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 84.66666666666667, 1.0, 2.0, 0.5358956082893193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748848.5316518892, 748848.5316518886, 189411.0823294646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6298800.0000, 
sim time next is 6299400.0000, 
raw observation next is [27.51666666666667, 84.83333333333334, 1.0, 2.0, 0.5351049314570406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747743.2686537235, 747743.268653723, 189278.9555447607], 
processed observation next is [0.0, 0.9130434782608695, 0.5031595576619274, 0.8483333333333334, 1.0, 1.0, 0.4398854595867959, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2077064635149232, 0.20770646351492303, 0.2825059037981503], 
reward next is 0.7175, 
noisyNet noise sample is [array([2.1191926], dtype=float32), 0.8734834]. 
=============================================
[2019-03-27 01:48:42,704] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 01:48:42,706] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:48:42,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:48:42,706] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:48:42,707] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:48:42,708] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:48:42,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:48:42,710] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:48:42,711] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:48:42,713] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:48:42,712] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:48:42,732] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-27 01:48:42,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-27 01:48:42,769] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-27 01:48:42,788] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-27 01:48:42,809] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-27 01:49:00,049] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.034831185]
[2019-03-27 01:49:00,052] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.68200149, 84.49683445, 1.0, 2.0, 0.343242963974677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535008.9739351831, 535008.9739351824, 169580.9072951597]
[2019-03-27 01:49:00,054] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:49:00,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7474152e-33 1.0000000e+00 0.0000000e+00 1.3751060e-38 8.7162077e-22], sampled 0.2832834037149947
[2019-03-27 01:49:30,302] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.034831185]
[2019-03-27 01:49:30,303] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.85381162, 95.63578338, 1.0, 2.0, 0.4989812391416148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 697248.288778188, 697248.2887781875, 183438.5154120564]
[2019-03-27 01:49:30,304] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:49:30,307] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.7835029e-30 1.0000000e+00 0.0000000e+00 6.5158542e-35 1.8127252e-19], sampled 0.06430191653038397
[2019-03-27 01:49:35,172] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.034831185]
[2019-03-27 01:49:35,173] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.04237875, 76.55597132666668, 1.0, 2.0, 0.706406840162925, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975683508012, 6.9112, 168.9123160410752, 1884094.271408538, 1816857.394544538, 386031.9233734673]
[2019-03-27 01:49:35,174] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:49:35,180] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4741623e-15 9.9984264e-01 2.4904380e-24 5.4812511e-18 1.5732402e-04], sampled 0.7723415913578893
[2019-03-27 01:49:35,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1884094.271408538 W.
[2019-03-27 01:49:47,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.034831185]
[2019-03-27 01:49:47,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.8, 57.0, 1.0, 2.0, 0.6357861942065881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888491.7829124863, 888491.7829124863, 207603.1190621583]
[2019-03-27 01:49:47,117] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:49:47,119] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6616975e-29 1.0000000e+00 0.0000000e+00 3.9145417e-34 6.0053917e-19], sampled 0.053835309562562994
[2019-03-27 01:50:01,784] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.034831185]
[2019-03-27 01:50:01,784] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.5, 49.66666666666667, 1.0, 2.0, 1.03410616510367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9127796168073, 1445510.673065244, 1445510.673065245, 309494.7861449082]
[2019-03-27 01:50:01,785] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:50:01,786] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6231384e-21 1.0000000e+00 3.5739827e-32 1.0879887e-24 2.3377746e-11], sampled 0.9902951949053743
[2019-03-27 01:50:37,365] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.034831185]
[2019-03-27 01:50:37,366] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.79216215833333, 75.31240763, 1.0, 2.0, 0.6441404134154236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900171.4965716361, 900171.4965716361, 209251.9984923068]
[2019-03-27 01:50:37,367] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:50:37,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8008.1235 3007056444.4976 1740.0000
[2019-03-27 01:50:37,370] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.3337441e-26 1.0000000e+00 0.0000000e+00 3.1172389e-30 1.6138119e-16], sampled 0.9739874416525263
[2019-03-27 01:50:37,581] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8279.2966 2926421789.6385 1283.0000
[2019-03-27 01:50:37,633] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.4498 3163642710.7045 1804.0000
[2019-03-27 01:50:37,767] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8677.5195 2778868946.2948 893.0000
[2019-03-27 01:50:37,777] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8501.9505 2842121089.6715 1128.0000
[2019-03-27 01:50:38,793] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1025000, evaluation results [1025000.0, 7890.449811737718, 3163642710.704502, 1804.0, 8279.296597211523, 2926421789.638522, 1283.0, 8677.51945866099, 2778868946.294829, 893.0, 8008.123532127437, 3007056444.497596, 1740.0, 8501.950488996452, 2842121089.6715493, 1128.0]
[2019-03-27 01:50:45,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4761267e-17 9.9991763e-01 5.8696201e-27 2.7204546e-22 8.2357488e-05], sum to 1.0000
[2019-03-27 01:50:45,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4550
[2019-03-27 01:50:45,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1788461.472455094 W.
[2019-03-27 01:50:45,458] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.6, 67.0, 1.0, 2.0, 0.6396270630927249, 1.0, 2.0, 0.6396270630927249, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1788461.472455094, 1788461.472455094, 349342.2410922516], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6714000.0000, 
sim time next is 6714600.0000, 
raw observation next is [29.46666666666667, 67.0, 1.0, 2.0, 0.314959324088825, 1.0, 2.0, 0.314959324088825, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 880285.1514337303, 880285.1514337303, 252823.1838152735], 
processed observation next is [1.0, 0.7391304347826086, 0.5955766192733019, 0.67, 1.0, 1.0, 0.17464978805882528, 1.0, 1.0, 0.17464978805882528, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2445236531760362, 0.2445236531760362, 0.37734803554518437], 
reward next is 0.6227, 
noisyNet noise sample is [array([-0.7997776], dtype=float32), -0.14045396]. 
=============================================
[2019-03-27 01:50:46,881] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.7013682e-26 1.0000000e+00 1.7176780e-38 5.6958667e-33 2.8787766e-15], sum to 1.0000
[2019-03-27 01:50:46,890] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9929
[2019-03-27 01:50:46,895] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 67.0, 1.0, 2.0, 0.4170665138822869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611146.1835130262, 611146.1835130256, 175261.3542101542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6723600.0000, 
sim time next is 6724200.0000, 
raw observation next is [27.33333333333334, 67.0, 1.0, 2.0, 0.4122982921770527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606925.3413061538, 606925.3413061538, 174943.6983084941], 
processed observation next is [1.0, 0.8260869565217391, 0.4944707740916275, 0.67, 1.0, 1.0, 0.29192565322536473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16859037258504272, 0.16859037258504272, 0.2611099974753643], 
reward next is 0.7389, 
noisyNet noise sample is [array([-1.3983136], dtype=float32), 0.57688147]. 
=============================================
[2019-03-27 01:50:57,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2439518e-28], sum to 1.0000
[2019-03-27 01:50:57,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5731
[2019-03-27 01:50:57,534] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 39.33333333333333, 1.0, 2.0, 0.2861820342438035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461700.470572328, 461700.4705723287, 164388.0575221332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6871200.0000, 
sim time next is 6871800.0000, 
raw observation next is [29.31666666666667, 38.16666666666667, 1.0, 2.0, 0.2806612041765062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454205.2490424214, 454205.2490424214, 163873.6512266393], 
processed observation next is [0.0, 0.5217391304347826, 0.5884676145339655, 0.3816666666666667, 1.0, 1.0, 0.13332675201988697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12616812473400593, 0.12616812473400593, 0.24458753914423778], 
reward next is 0.7554, 
noisyNet noise sample is [array([-1.026084], dtype=float32), -0.16677094]. 
=============================================
[2019-03-27 01:51:02,770] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2288394e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6741128e-34], sum to 1.0000
[2019-03-27 01:51:02,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3284
[2019-03-27 01:51:02,784] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 57.0, 1.0, 2.0, 0.4769893388404018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666508.4050141387, 666508.4050141381, 180069.4859981938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6967800.0000, 
sim time next is 6968400.0000, 
raw observation next is [30.66666666666667, 58.66666666666666, 1.0, 2.0, 0.4781170296412782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668084.6493509321, 668084.6493509315, 180238.6457860113], 
processed observation next is [0.0, 0.6521739130434783, 0.6524486571879939, 0.5866666666666666, 1.0, 1.0, 0.37122533691720266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1855790692641478, 0.18557906926414763, 0.2690129041582258], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.65057385], dtype=float32), 1.1149257]. 
=============================================
[2019-03-27 01:51:17,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.9054674e-36 1.0000000e+00 0.0000000e+00 3.8074061e-38 8.1019713e-27], sum to 1.0000
[2019-03-27 01:51:17,791] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5635
[2019-03-27 01:51:17,794] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.21666666666667, 84.33333333333333, 1.0, 2.0, 0.2856919371293693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461171.9876179353, 461171.9876179353, 164351.2705929673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7411800.0000, 
sim time next is 7412400.0000, 
raw observation next is [21.3, 84.0, 1.0, 2.0, 0.2855715078731841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 460644.8240181939, 460644.8240181946, 164316.5141094386], 
processed observation next is [1.0, 0.8260869565217391, 0.2085308056872039, 0.84, 1.0, 1.0, 0.1392427805701013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1279568955606094, 0.1279568955606096, 0.24524852852155016], 
reward next is 0.7548, 
noisyNet noise sample is [array([-0.19567785], dtype=float32), -1.009826]. 
=============================================
[2019-03-27 01:51:19,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1765884e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5409129e-31], sum to 1.0000
[2019-03-27 01:51:19,582] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4205
[2019-03-27 01:51:19,588] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 64.0, 1.0, 2.0, 0.9353952005271736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1400284.95157299, 1400284.951572989, 293400.1639653633], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7318800.0000, 
sim time next is 7319400.0000, 
raw observation next is [27.31666666666666, 64.5, 1.0, 2.0, 0.4838834377651735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724033.3049543169, 724033.3049543169, 187037.529246509], 
processed observation next is [1.0, 0.7391304347826086, 0.493680884676145, 0.645, 1.0, 1.0, 0.3781728165845464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20112036248731027, 0.20112036248731027, 0.2791604914127], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.20720717], dtype=float32), 0.41850755]. 
=============================================
[2019-03-27 01:51:25,133] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0509434e-34], sum to 1.0000
[2019-03-27 01:51:25,140] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0841
[2019-03-27 01:51:25,149] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 73.5, 1.0, 2.0, 0.3878582133267932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583848.2853143984, 583848.285314399, 173211.2121714892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7331400.0000, 
sim time next is 7332000.0000, 
raw observation next is [25.53333333333333, 73.66666666666666, 1.0, 2.0, 0.3868699793266774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583089.1592438482, 583089.1592438476, 173164.1113373199], 
processed observation next is [1.0, 0.8695652173913043, 0.4091627172195892, 0.7366666666666666, 1.0, 1.0, 0.2612891317188884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16196921090106894, 0.16196921090106878, 0.2584538975183879], 
reward next is 0.7415, 
noisyNet noise sample is [array([-0.6973472], dtype=float32), 2.0586765]. 
=============================================
[2019-03-27 01:51:25,168] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[87.54588 ]
 [87.302795]
 [86.95244 ]
 [86.78693 ]
 [86.20183 ]], R is [[87.61151886]
 [87.47688293]
 [87.3434906 ]
 [87.21131134]
 [87.08042908]].
[2019-03-27 01:51:27,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2091421e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6760751e-27], sum to 1.0000
[2019-03-27 01:51:27,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7114
[2019-03-27 01:51:27,398] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 93.0, 1.0, 2.0, 0.6418571287601026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010958.854033334, 1010958.854033334, 222285.5119544567], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7384800.0000, 
sim time next is 7385400.0000, 
raw observation next is [21.25, 93.0, 1.0, 2.0, 0.605954555514079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954801.4699806212, 954801.4699806207, 214489.5681947949], 
processed observation next is [1.0, 0.4782608695652174, 0.20616113744075834, 0.93, 1.0, 1.0, 0.5252464524266012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2652226305501726, 0.2652226305501724, 0.3201336838728282], 
reward next is 0.6799, 
noisyNet noise sample is [array([-0.08482583], dtype=float32), -1.3416601]. 
=============================================
[2019-03-27 01:51:32,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:51:32,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:51:32,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-27 01:51:34,586] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 01:51:34,587] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:51:34,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:51:34,589] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:51:34,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:51:34,591] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:51:34,592] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:51:34,594] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:51:34,593] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:51:34,595] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:51:34,596] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:51:34,614] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-27 01:51:34,632] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-27 01:51:34,653] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-27 01:51:34,671] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-27 01:51:34,690] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-27 01:51:44,335] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:51:44,336] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.9, 63.5, 1.0, 2.0, 0.3093296929700757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 493812.0541545583, 493812.0541545589, 166637.4085864734]
[2019-03-27 01:51:44,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:51:44,344] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.133821e-34], sampled 0.6892674477350533
[2019-03-27 01:51:51,040] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:51:51,041] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.15, 96.0, 1.0, 2.0, 0.2938130628576818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 470426.6481691363, 470426.6481691369, 164977.2043162925]
[2019-03-27 01:51:51,043] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:51:51,047] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 6.15114e-33], sampled 0.6967328787495422
[2019-03-27 01:51:53,210] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:51:53,212] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.8839715, 60.66453822, 1.0, 2.0, 0.4909931822883677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686082.6233213138, 686082.6233213143, 182195.6509148051]
[2019-03-27 01:51:53,213] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:51:53,214] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.321229e-33], sampled 0.02611452994952601
[2019-03-27 01:51:58,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:51:58,219] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 80.0, 1.0, 2.0, 0.3576428750531925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548685.4184535787, 548685.4184535787, 170471.7032823074]
[2019-03-27 01:51:58,221] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:51:58,223] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.2760626e-35], sampled 0.5066325608511906
[2019-03-27 01:52:01,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:52:01,655] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.7, 82.0, 1.0, 2.0, 0.4640670605656019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664563.1564105334, 664563.1564105334, 180221.9078888934]
[2019-03-27 01:52:01,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:52:01,659] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4599605e-32], sampled 0.6299465921866704
[2019-03-27 01:52:05,031] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:52:05,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.8, 80.5, 1.0, 2.0, 0.4936839943426264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689843.8136891109, 689843.8136891109, 182611.7523798732]
[2019-03-27 01:52:05,034] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:52:05,041] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4334666e-33], sampled 0.8971803207284724
[2019-03-27 01:52:14,801] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:52:14,803] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.07846397, 92.47375049499999, 1.0, 2.0, 0.4500254931122123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645701.0130294311, 645701.0130294311, 178303.3756190913]
[2019-03-27 01:52:14,804] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:52:14,808] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3431782e-33], sampled 0.5138785590409588
[2019-03-27 01:52:21,538] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:52:21,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.68170362, 86.90150653666667, 1.0, 2.0, 0.4064034526224375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 608650.5474865542, 608650.5474865548, 175394.6661066201]
[2019-03-27 01:52:21,540] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:52:21,542] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5530246e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5321651e-32], sampled 0.4646445389200008
[2019-03-27 01:52:23,658] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.036005653]
[2019-03-27 01:52:23,662] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.9, 63.16666666666666, 1.0, 2.0, 0.5308976667931621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741862.0812024132, 741862.0812024138, 188578.0405494873]
[2019-03-27 01:52:23,663] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:52:23,666] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7234172e-34], sampled 0.27222818956641914
[2019-03-27 01:53:29,121] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:53:29,221] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:53:29,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:53:29,280] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:53:29,506] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:53:30,523] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1050000, evaluation results [1050000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:53:31,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5434624e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5637302e-31], sum to 1.0000
[2019-03-27 01:53:31,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8705
[2019-03-27 01:53:31,123] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 93.0, 1.0, 2.0, 0.4084714586318052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599274.7086393032, 599274.7086393038, 174167.2460016372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7516800.0000, 
sim time next is 7517400.0000, 
raw observation next is [23.58333333333334, 93.0, 1.0, 2.0, 0.4085312912341218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599467.876503546, 599467.8765035466, 174188.4731945665], 
processed observation next is [0.0, 0.0, 0.31674565560821516, 0.93, 1.0, 1.0, 0.2873870978724359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16651885458431834, 0.1665188545843185, 0.2599827958127858], 
reward next is 0.7400, 
noisyNet noise sample is [array([-0.52584976], dtype=float32), 0.44267312]. 
=============================================
[2019-03-27 01:53:33,456] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4947862e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4948949e-28], sum to 1.0000
[2019-03-27 01:53:33,464] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4146
[2019-03-27 01:53:33,469] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 84.0, 1.0, 2.0, 0.4237843468856708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613192.212126483, 613192.212126483, 175229.2722180709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7549200.0000, 
sim time next is 7549800.0000, 
raw observation next is [25.43333333333333, 83.16666666666667, 1.0, 2.0, 0.4283950986484798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617267.7866510127, 617267.7866510121, 175547.0529056282], 
processed observation next is [0.0, 0.391304347826087, 0.40442338072669815, 0.8316666666666667, 1.0, 1.0, 0.31131939596202385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17146327406972575, 0.17146327406972559, 0.2620105267248182], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.3736585], dtype=float32), -1.0910486]. 
=============================================
[2019-03-27 01:53:33,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3450770e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8266007e-27], sum to 1.0000
[2019-03-27 01:53:33,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7122
[2019-03-27 01:53:33,895] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 65.5, 1.0, 2.0, 0.4654160733662236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650501.3002128416, 650501.3002128416, 178376.8866243147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7560600.0000, 
sim time next is 7561200.0000, 
raw observation next is [29.0, 65.0, 1.0, 2.0, 0.4609718946588133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646762.4357191233, 646762.4357191239, 178049.3615695616], 
processed observation next is [0.0, 0.5217391304347826, 0.5734597156398105, 0.65, 1.0, 1.0, 0.3505685477817027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17965623214420093, 0.1796562321442011, 0.26574531577546506], 
reward next is 0.7343, 
noisyNet noise sample is [array([-1.7158253], dtype=float32), -0.38618803]. 
=============================================
[2019-03-27 01:53:38,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8849964e-16 1.0000000e+00 1.6566700e-24 3.8809091e-19 1.8966130e-08], sum to 1.0000
[2019-03-27 01:53:38,358] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5342
[2019-03-27 01:53:38,364] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.3868370682667023, 1.0, 1.0, 0.3868370682667023, 1.0, 2.0, 0.6718080586059169, 6.911199999999999, 6.9112, 170.5573041426782, 1622327.247253051, 1622327.247253051, 345721.7698446118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7821000.0000, 
sim time next is 7821600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.910215966959171, 6.9112, 168.9074164406991, 2162953.498473, 1454240.433432262, 311353.0962256903], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.09990159669591714, 0.0, 0.8294127408736642, 0.6008204162424999, 0.4039556759534061, 0.46470611376968696], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02733663], dtype=float32), 0.018886222]. 
=============================================
[2019-03-27 01:53:39,091] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1053813: loss 2.2959
[2019-03-27 01:53:39,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1053814: learning rate 0.0005
[2019-03-27 01:53:40,480] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1756448e-30 1.0000000e+00 0.0000000e+00 1.9239239e-35 1.9242321e-20], sum to 1.0000
[2019-03-27 01:53:40,491] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0915
[2019-03-27 01:53:40,495] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 88.0, 1.0, 2.0, 0.5205528639608469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727401.5832481319, 727401.5832481312, 186879.2071351952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7854000.0000, 
sim time next is 7854600.0000, 
raw observation next is [26.75, 88.0, 1.0, 2.0, 0.5193551821689449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725727.4146969969, 725727.4146969976, 186684.4148038175], 
processed observation next is [1.0, 0.9130434782608695, 0.4668246445497631, 0.88, 1.0, 1.0, 0.4209098580348734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2015909485269436, 0.2015909485269438, 0.2786334549310709], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.00583941], dtype=float32), -0.35308227]. 
=============================================
[2019-03-27 01:53:40,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.5012767e-31 1.0000000e+00 0.0000000e+00 1.3186970e-33 1.5786462e-19], sum to 1.0000
[2019-03-27 01:53:40,785] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1484
[2019-03-27 01:53:40,795] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2866967877716635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461796.446713019, 461796.446713019, 164395.6499334722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2863924508741391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461306.5985648271, 461306.5985648271, 164362.226335434], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.14023186852305916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1281407218235631, 0.1281407218235631, 0.24531675572452838], 
reward next is 0.7547, 
noisyNet noise sample is [array([1.1876363], dtype=float32), 0.20693313]. 
=============================================
[2019-03-27 01:53:41,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4194468e-32 1.0000000e+00 0.0000000e+00 9.9246044e-38 1.3350051e-18], sum to 1.0000
[2019-03-27 01:53:41,266] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8516
[2019-03-27 01:53:41,271] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 95.5, 1.0, 2.0, 0.2835464524042191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457260.8337850788, 457260.8337850795, 164087.344172261], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 192600.0000, 
sim time next is 193200.0000, 
raw observation next is [19.93333333333333, 95.33333333333333, 1.0, 2.0, 0.2837356703576741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457444.3041101692, 457444.3041101685, 164099.8781361738], 
processed observation next is [0.0, 0.21739130434782608, 0.14375987361769343, 0.9533333333333333, 1.0, 1.0, 0.13703092814177603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12706786225282476, 0.12706786225282457, 0.2449251912480206], 
reward next is 0.7551, 
noisyNet noise sample is [array([-1.3064833], dtype=float32), 0.80428517]. 
=============================================
[2019-03-27 01:53:46,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:46,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:46,766] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4437501e-25 1.0000000e+00 1.2362683e-37 2.6518651e-27 2.4340855e-17], sum to 1.0000
[2019-03-27 01:53:46,773] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8882
[2019-03-27 01:53:46,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-27 01:53:46,778] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 87.33333333333333, 1.0, 2.0, 0.5068338878458176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708224.7958070813, 708224.7958070813, 184673.6865660661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7779000.0000, 
sim time next is 7779600.0000, 
raw observation next is [26.4, 87.0, 1.0, 2.0, 0.5053899978036402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706206.5040660381, 706206.5040660386, 184444.8221595999], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.87, 1.0, 1.0, 0.404084334703181, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19616847335167725, 0.1961684733516774, 0.2752907793426864], 
reward next is 0.7247, 
noisyNet noise sample is [array([-0.80938536], dtype=float32), 0.47147313]. 
=============================================
[2019-03-27 01:53:46,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:46,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:46,879] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-27 01:53:47,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:47,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:47,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-27 01:53:52,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:52,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:52,778] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-27 01:53:54,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:54,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:54,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-27 01:53:54,844] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1061137: loss 0.0042
[2019-03-27 01:53:54,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1061137: learning rate 0.0005
[2019-03-27 01:53:54,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:54,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:54,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-27 01:53:55,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:55,382] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:55,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-27 01:53:55,468] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1061485: loss 1.0475
[2019-03-27 01:53:55,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1061485: learning rate 0.0005
[2019-03-27 01:53:55,512] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1061510: loss 0.7023
[2019-03-27 01:53:55,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1061510: learning rate 0.0005
[2019-03-27 01:53:55,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9703373e-13 9.5918959e-01 8.5636994e-20 3.4742844e-13 4.0810443e-02], sum to 1.0000
[2019-03-27 01:53:55,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8831
[2019-03-27 01:53:55,528] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1897030.039036958 W.
[2019-03-27 01:53:55,531] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.75, 63.5, 1.0, 2.0, 0.6826183107324646, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.93705628845204, 6.9112, 168.9128025205277, 1897030.039036958, 1878686.714827063, 388403.6393296625], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 52200.0000, 
sim time next is 52800.0000, 
raw observation next is [27.66666666666666, 64.0, 1.0, 2.0, 0.6697138978115018, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.935336331055479, 6.9112, 168.9127911192773, 1882312.940517838, 1865189.813324807, 386057.723821795], 
processed observation next is [1.0, 0.6086956521739131, 0.5102685624012636, 0.64, 1.0, 1.0, 0.6020649371222914, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.002413633105547941, 0.0, 0.8294391330060216, 0.5228647056993995, 0.518108281479113, 0.5762055579429777], 
reward next is 0.3031, 
noisyNet noise sample is [array([0.02218042], dtype=float32), 1.0643363]. 
=============================================
[2019-03-27 01:53:55,543] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1061533: loss 0.8236
[2019-03-27 01:53:55,545] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1061533: learning rate 0.0005
[2019-03-27 01:53:55,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:55,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:55,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-27 01:53:55,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:55,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:56,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-27 01:53:56,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-27 01:53:56,222] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:56,222] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-27 01:53:56,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:56,282] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,300] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-27 01:53:56,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:56,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,329] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-27 01:53:56,347] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:56,347] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-27 01:53:56,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:53:56,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:53:56,387] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-27 01:53:57,060] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.513983e-27 1.000000e+00 0.000000e+00 6.065763e-33 5.102446e-20], sum to 1.0000
[2019-03-27 01:53:57,062] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0502
[2019-03-27 01:53:57,069] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 85.66666666666667, 1.0, 2.0, 0.3690137845416782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 588413.6889443405, 588413.6889443405, 174182.9217423461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 22200.0000, 
sim time next is 22800.0000, 
raw observation next is [21.73333333333333, 85.33333333333334, 1.0, 2.0, 0.3612055407796835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 575655.3478477785, 575655.3478477779, 173093.5193641669], 
processed observation next is [1.0, 0.2608695652173913, 0.22906793048973137, 0.8533333333333334, 1.0, 1.0, 0.2303681214213054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15990426329104956, 0.15990426329104943, 0.25834853636442817], 
reward next is 0.7417, 
noisyNet noise sample is [array([1.7981149], dtype=float32), 0.2079088]. 
=============================================
[2019-03-27 01:53:57,901] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1062836: loss 0.8928
[2019-03-27 01:53:57,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1062836: learning rate 0.0005
[2019-03-27 01:54:02,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6629558e-34 1.0000000e+00 0.0000000e+00 2.1477593e-34 3.3705464e-33], sum to 1.0000
[2019-03-27 01:54:02,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5565
[2019-03-27 01:54:02,064] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 53.33333333333334, 1.0, 2.0, 0.5974845082175666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 982588.4422644093, 982588.4422644093, 215072.6719184397], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.5758176870902904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 947255.4411877558, 947255.4411877552, 210515.0070299353], 
processed observation next is [1.0, 0.6086956521739131, 0.3696682464454976, 0.53, 1.0, 1.0, 0.48893697239794015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26312651144104326, 0.2631265114410431, 0.3142015030297542], 
reward next is 0.6858, 
noisyNet noise sample is [array([0.04640959], dtype=float32), -0.29890352]. 
=============================================
[2019-03-27 01:54:02,253] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1064780: loss 1.0967
[2019-03-27 01:54:02,257] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1064781: learning rate 0.0005
[2019-03-27 01:54:03,019] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1065124: loss 1.1497
[2019-03-27 01:54:03,022] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1065125: learning rate 0.0005
[2019-03-27 01:54:04,154] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1065634: loss 1.0051
[2019-03-27 01:54:04,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1065634: learning rate 0.0005
[2019-03-27 01:54:05,723] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1066345: loss 1.1630
[2019-03-27 01:54:05,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1066345: learning rate 0.0005
[2019-03-27 01:54:06,165] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1066542: loss 1.1160
[2019-03-27 01:54:06,166] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1066542: learning rate 0.0005
[2019-03-27 01:54:06,465] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1066679: loss 0.0474
[2019-03-27 01:54:06,466] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1066679: learning rate 0.0005
[2019-03-27 01:54:06,727] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1066793: loss 0.0916
[2019-03-27 01:54:06,732] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1066793: learning rate 0.0005
[2019-03-27 01:54:06,849] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1066848: loss 0.0398
[2019-03-27 01:54:06,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1066851: learning rate 0.0005
[2019-03-27 01:54:06,869] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1066855: loss 0.0226
[2019-03-27 01:54:06,872] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1066855: learning rate 0.0005
[2019-03-27 01:54:06,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1066892: loss 0.0056
[2019-03-27 01:54:06,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1066892: learning rate 0.0005
[2019-03-27 01:54:07,019] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1066921: loss 0.0042
[2019-03-27 01:54:07,022] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1066922: learning rate 0.0005
[2019-03-27 01:54:08,036] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1067378: loss 1.3001
[2019-03-27 01:54:08,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1067378: learning rate 0.0005
[2019-03-27 01:54:08,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9760762e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8096544e-31], sum to 1.0000
[2019-03-27 01:54:08,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0505
[2019-03-27 01:54:08,527] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2863924508741391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461306.5985648271, 461306.5985648271, 164362.226335434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 185400.0000, 
sim time next is 186000.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2860759064107357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460796.9032282293, 460796.9032282293, 164327.4826521666], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.1398504896514888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12799913978561925, 0.12799913978561925, 0.24526489948084568], 
reward next is 0.7547, 
noisyNet noise sample is [array([1.2809609], dtype=float32), 2.299061]. 
=============================================
[2019-03-27 01:54:08,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.1092 ]
 [68.99529]
 [68.97297]
 [68.96424]
 [68.89811]], R is [[69.20344543]
 [69.26609802]
 [69.32807159]
 [69.38938141]
 [69.4500351 ]].
[2019-03-27 01:54:09,411] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1067992: loss 0.2566
[2019-03-27 01:54:09,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1067994: learning rate 0.0005
[2019-03-27 01:54:09,655] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1068102: loss 0.0106
[2019-03-27 01:54:09,657] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1068102: learning rate 0.0005
[2019-03-27 01:54:09,693] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1068117: loss 0.0057
[2019-03-27 01:54:09,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1068118: learning rate 0.0005
[2019-03-27 01:54:15,643] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1070778: loss 0.0115
[2019-03-27 01:54:15,646] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1070780: learning rate 0.0005
[2019-03-27 01:54:20,087] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1072769: loss 0.0029
[2019-03-27 01:54:20,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1072769: learning rate 0.0005
[2019-03-27 01:54:20,861] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1073108: loss 0.0012
[2019-03-27 01:54:20,863] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1073108: learning rate 0.0005
[2019-03-27 01:54:21,959] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1073598: loss 0.0055
[2019-03-27 01:54:21,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1073598: learning rate 0.0005
[2019-03-27 01:54:23,601] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1074339: loss 0.1035
[2019-03-27 01:54:23,603] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1074339: learning rate 0.0005
[2019-03-27 01:54:24,054] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1074524: loss 0.1080
[2019-03-27 01:54:24,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1074524: learning rate 0.0005
[2019-03-27 01:54:24,346] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1074655: loss 0.0441
[2019-03-27 01:54:24,347] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1074656: learning rate 0.0005
[2019-03-27 01:54:24,752] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1074838: loss 0.0032
[2019-03-27 01:54:24,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1074838: learning rate 0.0005
[2019-03-27 01:54:24,762] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1074842: loss 0.0029
[2019-03-27 01:54:24,765] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1074843: learning rate 0.0005
[2019-03-27 01:54:24,803] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1074861: loss 0.0022
[2019-03-27 01:54:24,804] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1074861: learning rate 0.0005
[2019-03-27 01:54:24,850] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1074883: loss 0.0032
[2019-03-27 01:54:24,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1074884: learning rate 0.0005
[2019-03-27 01:54:24,896] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1074900: loss 0.0033
[2019-03-27 01:54:24,901] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1074902: learning rate 0.0005
[2019-03-27 01:54:25,121] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:54:25,123] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:54:25,124] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:54:25,125] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:54:25,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:54:25,127] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:54:25,128] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:54:25,125] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:54:25,127] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:54:25,132] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:54:25,134] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:54:25,156] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-27 01:54:25,178] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-27 01:54:25,195] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-27 01:54:25,196] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-27 01:54:25,218] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-27 01:54:47,817] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.0067016236]
[2019-03-27 01:54:47,818] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 66.0, 1.0, 2.0, 0.2793798750508166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 453523.0597330888, 453523.0597330882, 163810.0089546775]
[2019-03-27 01:54:47,820] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:54:47,824] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5742822176104969
[2019-03-27 01:55:30,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.0067016236]
[2019-03-27 01:55:30,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.55, 66.66666666666667, 1.0, 2.0, 0.5762645179519909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805280.5260617153, 805280.5260617153, 196406.58498133]
[2019-03-27 01:55:30,847] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:55:30,848] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2437433e-35], sampled 0.4869084733022171
[2019-03-27 01:55:32,643] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), 0.0067016236]
[2019-03-27 01:55:32,645] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [38.3, 44.0, 1.0, 2.0, 0.9307823332934095, 1.0, 2.0, 0.9307823332934095, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 2603506.701475607, 2603506.701475607, 488283.1717183248]
[2019-03-27 01:55:32,645] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:55:32,649] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4315340e-24 1.0000000e+00 1.2151082e-35 1.8715024e-26 9.3915509e-16], sampled 0.40101892899335667
[2019-03-27 01:55:32,650] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2603506.701475607 W.
[2019-03-27 01:56:19,680] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:56:19,857] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2266 2779156391.9499 933.0000
[2019-03-27 01:56:20,057] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4478 2927275782.6669 1341.0000
[2019-03-27 01:56:20,175] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:56:20,194] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:56:21,208] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1075000, evaluation results [1075000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.447849345313, 2927275782.666934, 1341.0, 8661.22661193904, 2779156391.9498506, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:56:22,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1075454: loss 0.0268
[2019-03-27 01:56:22,218] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1075454: learning rate 0.0005
[2019-03-27 01:56:23,415] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1075991: loss 0.0988
[2019-03-27 01:56:23,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1075992: learning rate 0.0005
[2019-03-27 01:56:23,691] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1076112: loss 0.2268
[2019-03-27 01:56:23,693] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1076112: learning rate 0.0005
[2019-03-27 01:56:23,726] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1076128: loss 0.4406
[2019-03-27 01:56:23,729] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1076129: learning rate 0.0005
[2019-03-27 01:56:29,457] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2478602e-33], sum to 1.0000
[2019-03-27 01:56:29,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9875
[2019-03-27 01:56:29,472] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.26666666666667, 81.5, 1.0, 2.0, 0.2289954706381522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380504.1474334769, 380504.1474334762, 158613.0745081647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 598200.0000, 
sim time next is 598800.0000, 
raw observation next is [19.13333333333333, 82.0, 1.0, 2.0, 0.2283519834090758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 379638.9429128517, 379638.9429128511, 158517.9449274642], 
processed observation next is [1.0, 0.9565217391304348, 0.10584518167456543, 0.82, 1.0, 1.0, 0.07030359446876602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10545526192023658, 0.10545526192023642, 0.23659394765293165], 
reward next is 0.7634, 
noisyNet noise sample is [array([-0.91185766], dtype=float32), 1.5709327]. 
=============================================
[2019-03-27 01:56:29,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1078803: loss 0.3739
[2019-03-27 01:56:29,686] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1078804: learning rate 0.0005
[2019-03-27 01:56:33,684] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:56:33,692] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4224
[2019-03-27 01:56:33,696] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 65.33333333333333, 1.0, 2.0, 0.2477849490515559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 407927.7554038041, 407927.7554038034, 160681.1062061571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 672000.0000, 
sim time next is 672600.0000, 
raw observation next is [22.31666666666667, 66.66666666666667, 1.0, 2.0, 0.2479955038477095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408306.4379170497, 408306.4379170503, 160700.9683011164], 
processed observation next is [1.0, 0.782608695652174, 0.2567140600315958, 0.6666666666666667, 1.0, 1.0, 0.0939704865635054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11341845497695824, 0.11341845497695842, 0.2398521914942036], 
reward next is 0.7601, 
noisyNet noise sample is [array([-1.621754], dtype=float32), 0.6885906]. 
=============================================
[2019-03-27 01:56:34,089] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1080769: loss 0.1720
[2019-03-27 01:56:34,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1080769: learning rate 0.0005
[2019-03-27 01:56:34,855] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1081111: loss 0.0957
[2019-03-27 01:56:34,858] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1081111: learning rate 0.0005
[2019-03-27 01:56:35,998] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1081629: loss 0.4652
[2019-03-27 01:56:36,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1081629: learning rate 0.0005
[2019-03-27 01:56:37,808] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1082414: loss 0.3385
[2019-03-27 01:56:37,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1082415: learning rate 0.0005
[2019-03-27 01:56:38,029] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1082514: loss 0.0574
[2019-03-27 01:56:38,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1082514: learning rate 0.0005
[2019-03-27 01:56:38,351] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1082658: loss 0.2325
[2019-03-27 01:56:38,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1082659: learning rate 0.0005
[2019-03-27 01:56:38,592] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1082762: loss 0.1995
[2019-03-27 01:56:38,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1082762: learning rate 0.0005
[2019-03-27 01:56:38,663] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1082798: loss 0.0676
[2019-03-27 01:56:38,666] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1082798: learning rate 0.0005
[2019-03-27 01:56:38,696] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1082812: loss 0.1279
[2019-03-27 01:56:38,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1082812: learning rate 0.0005
[2019-03-27 01:56:38,812] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1082859: loss 0.5761
[2019-03-27 01:56:38,816] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1082860: learning rate 0.0005
[2019-03-27 01:56:38,901] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1082900: loss 0.0654
[2019-03-27 01:56:38,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1082900: learning rate 0.0005
[2019-03-27 01:56:40,585] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1083650: loss 0.0406
[2019-03-27 01:56:40,590] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1083651: learning rate 0.0005
[2019-03-27 01:56:40,827] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.789658e-37], sum to 1.0000
[2019-03-27 01:56:40,835] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7328
[2019-03-27 01:56:40,838] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.83333333333333, 1.0, 2.0, 0.260896546668012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426467.4504086699, 426467.4504086699, 161982.4881657572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 791400.0000, 
sim time next is 792000.0000, 
raw observation next is [19.4, 93.0, 1.0, 2.0, 0.2613516884841625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427089.8824543229, 427089.8824543229, 162026.0736546523], 
processed observation next is [0.0, 0.17391304347826086, 0.11848341232227487, 0.93, 1.0, 1.0, 0.1100622752821235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11863607845953414, 0.11863607845953414, 0.24182996067858553], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.97958326], dtype=float32), 0.1747335]. 
=============================================
[2019-03-27 01:56:40,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[76.99289]
 [76.92585]
 [76.87433]
 [76.73718]
 [76.61708]], R is [[77.23580933]
 [77.22168732]
 [77.20774841]
 [77.1939621 ]
 [77.18032074]].
[2019-03-27 01:56:41,181] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1083912: loss 0.0118
[2019-03-27 01:56:41,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1083912: learning rate 0.0005
[2019-03-27 01:56:41,592] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1084096: loss 0.0060
[2019-03-27 01:56:41,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1084097: learning rate 0.0005
[2019-03-27 01:56:41,674] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1084132: loss 0.0161
[2019-03-27 01:56:41,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1084132: learning rate 0.0005
[2019-03-27 01:56:43,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:56:43,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2816
[2019-03-27 01:56:43,752] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 78.66666666666667, 1.0, 2.0, 0.3034343301702617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482356.0714806924, 482356.0714806918, 165777.3045481032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.3019040529153987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480360.1082277217, 480360.1082277217, 165641.2060816276], 
processed observation next is [0.0, 0.782608695652174, 0.27330173775671435, 0.7883333333333334, 1.0, 1.0, 0.15892054568120323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13343336339658937, 0.13343336339658937, 0.24722568071884718], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.2550987], dtype=float32), 0.015899044]. 
=============================================
[2019-03-27 01:56:47,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:56:47,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6495
[2019-03-27 01:56:47,132] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3292132708040946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510337.8854934814, 510337.8854934814, 167546.445712559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 972600.0000, 
sim time next is 973200.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3282851924684677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508893.8120397911, 508893.8120397917, 167434.2684789953], 
processed observation next is [1.0, 0.2608695652173913, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19070505116682857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1413593922332753, 0.14135939223327548, 0.2499018932522318], 
reward next is 0.7501, 
noisyNet noise sample is [array([1.5473498], dtype=float32), 0.047648083]. 
=============================================
[2019-03-27 01:56:47,430] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1086706: loss 0.0151
[2019-03-27 01:56:47,433] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1086707: learning rate 0.0005
[2019-03-27 01:56:52,066] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1088785: loss 0.0106
[2019-03-27 01:56:52,069] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1088787: learning rate 0.0005
[2019-03-27 01:56:52,673] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1089065: loss 0.0475
[2019-03-27 01:56:52,674] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1089065: learning rate 0.0005
[2019-03-27 01:56:53,905] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1089610: loss 0.1037
[2019-03-27 01:56:53,910] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1089611: learning rate 0.0005
[2019-03-27 01:56:55,518] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1090336: loss 0.0201
[2019-03-27 01:56:55,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1090336: learning rate 0.0005
[2019-03-27 01:56:55,831] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1090475: loss 0.0362
[2019-03-27 01:56:55,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1090475: learning rate 0.0005
[2019-03-27 01:56:56,297] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1090683: loss 0.0629
[2019-03-27 01:56:56,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1090683: learning rate 0.0005
[2019-03-27 01:56:56,487] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1090772: loss 0.0250
[2019-03-27 01:56:56,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1090773: learning rate 0.0005
[2019-03-27 01:56:56,500] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1090777: loss 0.0363
[2019-03-27 01:56:56,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1090777: learning rate 0.0005
[2019-03-27 01:56:56,517] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1090784: loss 0.0141
[2019-03-27 01:56:56,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1090785: learning rate 0.0005
[2019-03-27 01:56:56,605] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1090819: loss 0.0041
[2019-03-27 01:56:56,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1090819: learning rate 0.0005
[2019-03-27 01:56:56,738] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1090881: loss 0.0034
[2019-03-27 01:56:56,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1090883: learning rate 0.0005
[2019-03-27 01:56:57,180] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1178947e-36 1.0000000e+00 0.0000000e+00 7.6647528e-38 3.6838461e-33], sum to 1.0000
[2019-03-27 01:56:57,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3720
[2019-03-27 01:56:57,191] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 91.5, 1.0, 2.0, 0.5153362958853872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733246.0981006918, 733246.0981006911, 187712.6272447009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1312200.0000, 
sim time next is 1312800.0000, 
raw observation next is [24.56666666666667, 91.33333333333334, 1.0, 2.0, 0.5004109943526287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712177.3340614865, 712177.3340614858, 185313.8003381039], 
processed observation next is [1.0, 0.17391304347826086, 0.3633491311216432, 0.9133333333333334, 1.0, 1.0, 0.3980855353646129, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1978270372393018, 0.19782703723930162, 0.27658776169866256], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.98192644], dtype=float32), -0.3404638]. 
=============================================
[2019-03-27 01:56:58,561] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1091698: loss 0.0150
[2019-03-27 01:56:58,562] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1091698: learning rate 0.0005
[2019-03-27 01:56:59,081] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1091928: loss 0.0238
[2019-03-27 01:56:59,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1091929: learning rate 0.0005
[2019-03-27 01:56:59,747] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1092226: loss 0.0660
[2019-03-27 01:56:59,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1092226: learning rate 0.0005
[2019-03-27 01:56:59,896] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1092297: loss 0.1517
[2019-03-27 01:56:59,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1092297: learning rate 0.0005
[2019-03-27 01:57:05,379] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1094751: loss 0.1182
[2019-03-27 01:57:05,384] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1094752: learning rate 0.0005
[2019-03-27 01:57:07,402] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:57:07,411] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9970
[2019-03-27 01:57:07,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 98.5, 1.0, 2.0, 0.3117303422212508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493617.1412787182, 493617.1412787182, 166563.9223591993], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1485000.0000, 
sim time next is 1485600.0000, 
raw observation next is [20.33333333333333, 98.66666666666667, 1.0, 2.0, 0.3104452244773311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492129.4430516197, 492129.4430516197, 166464.6233860319], 
processed observation next is [0.0, 0.17391304347826086, 0.16271721958925733, 0.9866666666666667, 1.0, 1.0, 0.16921111382810974, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13670262306989436, 0.13670262306989436, 0.24845466177019684], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.7433752], dtype=float32), 0.5170659]. 
=============================================
[2019-03-27 01:57:09,971] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1096788: loss 0.7317
[2019-03-27 01:57:09,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1096788: learning rate 0.0005
[2019-03-27 01:57:10,729] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1097124: loss 1.1867
[2019-03-27 01:57:10,736] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1097124: learning rate 0.0005
[2019-03-27 01:57:11,935] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1097665: loss 0.6632
[2019-03-27 01:57:11,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1097666: learning rate 0.0005
[2019-03-27 01:57:13,280] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1098263: loss 0.9508
[2019-03-27 01:57:13,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1098263: learning rate 0.0005
[2019-03-27 01:57:13,915] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1098549: loss 0.0530
[2019-03-27 01:57:13,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1098549: learning rate 0.0005
[2019-03-27 01:57:14,149] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1098651: loss 0.1072
[2019-03-27 01:57:14,151] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1098651: learning rate 0.0005
[2019-03-27 01:57:14,470] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1098799: loss 0.1853
[2019-03-27 01:57:14,473] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1098800: learning rate 0.0005
[2019-03-27 01:57:14,523] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1098821: loss 0.1782
[2019-03-27 01:57:14,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1098821: learning rate 0.0005
[2019-03-27 01:57:14,564] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1098840: loss 0.1584
[2019-03-27 01:57:14,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1098840: learning rate 0.0005
[2019-03-27 01:57:14,626] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1098869: loss 0.2067
[2019-03-27 01:57:14,630] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1098869: learning rate 0.0005
[2019-03-27 01:57:14,871] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1098973: loss 0.0480
[2019-03-27 01:57:14,873] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1098973: learning rate 0.0005
[2019-03-27 01:57:15,502] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3689604e-36], sum to 1.0000
[2019-03-27 01:57:15,504] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2065
[2019-03-27 01:57:15,510] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 94.66666666666666, 1.0, 2.0, 0.3253210251452163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512586.8352017128, 512586.8352017128, 167943.1068103118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1370400.0000, 
sim time next is 1371000.0000, 
raw observation next is [21.03333333333333, 94.83333333333333, 1.0, 2.0, 0.3246202013418588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511649.2636062397, 511649.2636062391, 167874.5646775391], 
processed observation next is [1.0, 0.8695652173913043, 0.19589257503949445, 0.9483333333333333, 1.0, 1.0, 0.1862893992070588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1421247954461777, 0.14212479544617754, 0.2505590517575211], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.8729156], dtype=float32), 0.20680502]. 
=============================================
[2019-03-27 01:57:15,539] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.97701 ]
 [75.925316]
 [75.87733 ]
 [75.780075]
 [75.81144 ]], R is [[76.00792694]
 [75.99718475]
 [75.98677063]
 [75.97662354]
 [75.96655273]].
[2019-03-27 01:57:16,773] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1099822: loss -47.5937
[2019-03-27 01:57:16,778] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1099822: learning rate 0.0005
[2019-03-27 01:57:16,790] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1099830: loss 0.0466
[2019-03-27 01:57:16,795] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1099833: learning rate 0.0005
[2019-03-27 01:57:17,165] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 01:57:17,168] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:57:17,170] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:57:17,171] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:17,171] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:57:17,172] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:17,175] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:57:17,176] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:57:17,176] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:17,178] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:17,177] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:17,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-27 01:57:17,221] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-27 01:57:17,242] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-27 01:57:17,264] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-27 01:57:17,264] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-27 01:57:25,891] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01183841]
[2019-03-27 01:57:25,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.56666666666667, 75.33333333333334, 1.0, 2.0, 0.2373339693286704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 392692.3431698664, 392692.3431698664, 159581.6865580959]
[2019-03-27 01:57:25,894] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:57:25,897] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8388948e-38], sampled 0.6954431006098464
[2019-03-27 01:57:34,116] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01183841]
[2019-03-27 01:57:34,117] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.08095348833333, 76.06471635666666, 1.0, 2.0, 0.458885023122568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 697669.9968980218, 697669.9968980211, 184311.3284562952]
[2019-03-27 01:57:34,118] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:57:34,121] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.228685e-37], sampled 0.20461589499132482
[2019-03-27 01:58:51,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01183841]
[2019-03-27 01:58:51,418] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 89.33333333333334, 1.0, 2.0, 0.5582917653434091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780155.9133225759, 780155.9133225753, 193231.5078245292]
[2019-03-27 01:58:51,419] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:58:51,421] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.186903e-37], sampled 0.10799998696531299
[2019-03-27 01:59:03,576] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01183841]
[2019-03-27 01:59:03,576] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.08761759166666, 78.31431967, 1.0, 2.0, 0.3185916436261385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503647.0690447468, 503647.0690447468, 167296.3607692483]
[2019-03-27 01:59:03,578] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:59:03,582] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9816627771752373
[2019-03-27 01:59:09,072] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 01:59:09,632] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 01:59:09,791] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 01:59:09,805] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:59:09,819] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:59:10,836] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1100000, evaluation results [1100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:59:10,996] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1100077: loss 0.0083
[2019-03-27 01:59:11,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1100080: learning rate 0.0005
[2019-03-27 01:59:11,101] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1100120: loss 0.0125
[2019-03-27 01:59:11,104] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1100121: learning rate 0.0005
[2019-03-27 01:59:14,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4432442e-38], sum to 1.0000
[2019-03-27 01:59:14,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7045
[2019-03-27 01:59:14,532] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.28333333333333, 82.66666666666667, 1.0, 2.0, 0.5062232049111284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707371.1733267285, 707371.1733267285, 184577.3008305534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1710600.0000, 
sim time next is 1711200.0000, 
raw observation next is [27.16666666666667, 83.33333333333334, 1.0, 2.0, 0.5072536953904286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708811.6099787472, 708811.6099787472, 184740.7165377566], 
processed observation next is [1.0, 0.8260869565217391, 0.4865718799368091, 0.8333333333333335, 1.0, 1.0, 0.4063297534824441, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19689211388298533, 0.19689211388298533, 0.2757324127429203], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.00945581], dtype=float32), -0.15520783]. 
=============================================
[2019-03-27 01:59:15,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:59:15,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2818
[2019-03-27 01:59:15,870] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 96.5, 1.0, 2.0, 0.333327726455469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520395.3048499784, 520395.3048499784, 168438.5593902614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [21.13333333333333, 96.66666666666666, 1.0, 2.0, 0.3322407999722555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 519339.8942545966, 519339.8942545972, 168372.2554917981], 
processed observation next is [0.0, 0.08695652173913043, 0.20063191153238533, 0.9666666666666666, 1.0, 1.0, 0.19547084334006684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14426108173738794, 0.1442610817373881, 0.2513018738683554], 
reward next is 0.7487, 
noisyNet noise sample is [array([-1.6184461], dtype=float32), -0.3540625]. 
=============================================
[2019-03-27 01:59:15,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.766937e-37 1.000000e+00 0.000000e+00 0.000000e+00 6.807926e-29], sum to 1.0000
[2019-03-27 01:59:15,894] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1268
[2019-03-27 01:59:15,902] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 94.0, 1.0, 2.0, 0.4768157272293644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666265.7373571822, 666265.7373571828, 180042.9938791809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [24.6, 94.0, 1.0, 2.0, 0.4742197296898489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662885.1596225568, 662885.1596225568, 179687.1728541443], 
processed observation next is [1.0, 0.08695652173913043, 0.36492890995260674, 0.94, 1.0, 1.0, 0.3665297948070469, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18413476656182132, 0.18413476656182132, 0.2681898102300661], 
reward next is 0.7318, 
noisyNet noise sample is [array([-0.00645081], dtype=float32), 1.4639765]. 
=============================================
[2019-03-27 01:59:16,094] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6966892e-38 1.0000000e+00 0.0000000e+00 2.4279390e-38 7.6751259e-34], sum to 1.0000
[2019-03-27 01:59:16,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5044
[2019-03-27 01:59:16,107] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 93.33333333333334, 1.0, 2.0, 0.5124576535202783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726592.295516293, 726592.295516293, 186915.6330045822], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1748400.0000, 
sim time next is 1749000.0000, 
raw observation next is [24.45, 93.16666666666666, 1.0, 2.0, 0.523230978863991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 188570.2929640439], 
processed observation next is [1.0, 0.21739130434782608, 0.3578199052132702, 0.9316666666666665, 1.0, 1.0, 0.425579492607218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20581770053148432, 0.2058177005314845, 0.28144819845379687], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.8759196], dtype=float32), 1.1249489]. 
=============================================
[2019-03-27 01:59:16,119] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.54425 ]
 [71.68039 ]
 [71.596695]
 [71.36687 ]
 [71.7697  ]], R is [[71.43195343]
 [71.43865967]
 [71.45109558]
 [71.4618454 ]
 [71.45642853]].
[2019-03-27 01:59:16,739] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1102644: loss 0.0181
[2019-03-27 01:59:16,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1102644: learning rate 0.0005
[2019-03-27 01:59:21,556] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1104805: loss 0.0253
[2019-03-27 01:59:21,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1104805: learning rate 0.0005
[2019-03-27 01:59:22,127] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1105062: loss 0.0032
[2019-03-27 01:59:22,128] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1105062: learning rate 0.0005
[2019-03-27 01:59:23,357] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1105607: loss 0.0648
[2019-03-27 01:59:23,360] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1105610: learning rate 0.0005
[2019-03-27 01:59:23,649] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4621916e-20 1.0000000e+00 3.8215652e-30 8.1949627e-22 9.8096004e-11], sum to 1.0000
[2019-03-27 01:59:23,658] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6502
[2019-03-27 01:59:23,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1735040.894198976 W.
[2019-03-27 01:59:23,678] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 82.66666666666667, 1.0, 2.0, 0.4136914211296287, 1.0, 2.0, 0.4136914211296287, 1.0, 1.0, 0.7092106720723799, 6.911199999999999, 6.9112, 170.5573041426782, 1735040.894198976, 1735040.894198977, 359246.5901672473], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1690800.0000, 
sim time next is 1691400.0000, 
raw observation next is [27.7, 82.33333333333334, 1.0, 2.0, 0.6223236236856077, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.946468880717275, 6.9112, 168.9127101825136, 1740053.791533663, 1715032.870180016, 369577.1307909046], 
processed observation next is [1.0, 0.5652173913043478, 0.5118483412322274, 0.8233333333333335, 1.0, 1.0, 0.5449682213079611, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0035268880717275407, 0.0, 0.8294387355694588, 0.4833482754260175, 0.4763980194944489, 0.5516076578968725], 
reward next is 0.2720, 
noisyNet noise sample is [array([0.00488977], dtype=float32), -1.5515218]. 
=============================================
[2019-03-27 01:59:24,753] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1106228: loss 0.0240
[2019-03-27 01:59:24,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1106230: learning rate 0.0005
[2019-03-27 01:59:25,263] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1106454: loss 0.0211
[2019-03-27 01:59:25,266] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1106454: learning rate 0.0005
[2019-03-27 01:59:25,582] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1106599: loss 0.0019
[2019-03-27 01:59:25,587] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1106601: learning rate 0.0005
[2019-03-27 01:59:25,868] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1106726: loss 0.0149
[2019-03-27 01:59:25,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1106726: learning rate 0.0005
[2019-03-27 01:59:25,935] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1106756: loss 0.0020
[2019-03-27 01:59:25,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1106757: learning rate 0.0005
[2019-03-27 01:59:25,976] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1106776: loss 0.0025
[2019-03-27 01:59:25,978] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1106777: learning rate 0.0005
[2019-03-27 01:59:26,091] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1106829: loss 0.0715
[2019-03-27 01:59:26,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1106830: learning rate 0.0005
[2019-03-27 01:59:26,338] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1106935: loss 0.0697
[2019-03-27 01:59:26,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1106937: learning rate 0.0005
[2019-03-27 01:59:28,657] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1107973: loss 1.2517
[2019-03-27 01:59:28,660] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1107973: learning rate 0.0005
[2019-03-27 01:59:28,722] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1107996: loss -108.0880
[2019-03-27 01:59:28,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1107997: learning rate 0.0005
[2019-03-27 01:59:29,149] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1108190: loss -4.1536
[2019-03-27 01:59:29,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1108190: learning rate 0.0005
[2019-03-27 01:59:29,248] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1108237: loss -107.6060
[2019-03-27 01:59:29,252] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1108238: learning rate 0.0005
[2019-03-27 01:59:29,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:59:29,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6469
[2019-03-27 01:59:29,543] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 94.0, 1.0, 2.0, 0.5065683184638683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707853.5785466613, 707853.5785466619, 184631.7498514981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1725600.0000, 
sim time next is 1726200.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5047705006445073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705340.5625705486, 705340.5625705493, 184347.0012442233], 
processed observation next is [1.0, 1.0, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4033379525837438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1959279340473746, 0.1959279340473748, 0.2751447779764527], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.76176184], dtype=float32), 0.1778867]. 
=============================================
[2019-03-27 01:59:33,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:59:33,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6374
[2019-03-27 01:59:33,252] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.78333333333333, 86.66666666666667, 1.0, 2.0, 0.5134105300132492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717417.7760401946, 717417.7760401939, 185724.0877086478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [26.9, 86.0, 1.0, 2.0, 0.515345161168602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720122.0626827726, 720122.062682772, 186035.3807480296], 
processed observation next is [0.0, 0.5652173913043478, 0.4739336492890995, 0.86, 1.0, 1.0, 0.41607850743205055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20003390630077017, 0.20003390630077, 0.2776647473851188], 
reward next is 0.7223, 
noisyNet noise sample is [array([2.016117], dtype=float32), 1.6764312]. 
=============================================
[2019-03-27 01:59:33,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.99511 ]
 [79.01661 ]
 [79.01011 ]
 [79.01578 ]
 [79.029884]], R is [[79.01662445]
 [78.9492569 ]
 [78.88286591]
 [78.81711578]
 [78.75216675]].
[2019-03-27 01:59:34,881] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1110748: loss 10.5977
[2019-03-27 01:59:34,883] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1110748: learning rate 0.0005
[2019-03-27 01:59:36,182] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.966904e-34], sum to 1.0000
[2019-03-27 01:59:36,191] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4356
[2019-03-27 01:59:36,198] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 92.5, 1.0, 2.0, 0.4532463085761164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646815.3388027243, 646815.3388027243, 178330.9582957907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1899000.0000, 
sim time next is 1899600.0000, 
raw observation next is [24.33333333333334, 92.66666666666667, 1.0, 2.0, 0.4540390997547379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647804.9490534874, 647804.9490534874, 178428.8729174199], 
processed observation next is [1.0, 1.0, 0.35229067930489766, 0.9266666666666667, 1.0, 1.0, 0.3422157828370336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17994581918152427, 0.17994581918152427, 0.26631175062301476], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.8151347], dtype=float32), 1.3187549]. 
=============================================
[2019-03-27 01:59:37,500] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5568207e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.9274333e-38], sum to 1.0000
[2019-03-27 01:59:37,510] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4813
[2019-03-27 01:59:37,521] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 90.0, 1.0, 2.0, 0.453565014078821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646514.4292675435, 646514.4292675435, 178281.3387856896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1893600.0000, 
sim time next is 1894200.0000, 
raw observation next is [24.65, 90.33333333333333, 1.0, 2.0, 0.4527578666226864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645650.7652913006, 645650.7652913006, 178200.1073501263], 
processed observation next is [1.0, 0.9565217391304348, 0.3672985781990521, 0.9033333333333333, 1.0, 1.0, 0.340672128461068, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17934743480313906, 0.17934743480313906, 0.2659703094778004], 
reward next is 0.7340, 
noisyNet noise sample is [array([1.0443277], dtype=float32), 0.5707287]. 
=============================================
[2019-03-27 01:59:39,543] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1112840: loss -92.0911
[2019-03-27 01:59:39,545] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1112840: learning rate 0.0005
[2019-03-27 01:59:40,117] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1113095: loss -99.8827
[2019-03-27 01:59:40,123] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1113095: learning rate 0.0005
[2019-03-27 01:59:41,284] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1113612: loss -162.3966
[2019-03-27 01:59:41,286] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1113613: learning rate 0.0005
[2019-03-27 01:59:41,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2151064e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0792851e-32], sum to 1.0000
[2019-03-27 01:59:41,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0261
[2019-03-27 01:59:41,545] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 96.5, 1.0, 2.0, 0.5889505493944133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823015.0414544895, 823015.0414544889, 198698.2665550767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2176200.0000, 
sim time next is 2176800.0000, 
raw observation next is [24.6, 96.66666666666666, 1.0, 2.0, 0.5738524123772334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801908.541795593, 801908.541795593, 195969.5670910848], 
processed observation next is [1.0, 0.17391304347826086, 0.36492890995260674, 0.9666666666666666, 1.0, 1.0, 0.4865691715388354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22275237272099804, 0.22275237272099804, 0.2924918911807236], 
reward next is 0.7075, 
noisyNet noise sample is [array([1.4577527], dtype=float32), -0.882065]. 
=============================================
[2019-03-27 01:59:42,887] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1114326: loss -31.4787
[2019-03-27 01:59:42,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1114327: learning rate 0.0005
[2019-03-27 01:59:43,251] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1114491: loss -135.7817
[2019-03-27 01:59:43,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1114491: learning rate 0.0005
[2019-03-27 01:59:43,413] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1114562: loss 13.2697
[2019-03-27 01:59:43,419] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1114562: learning rate 0.0005
[2019-03-27 01:59:43,790] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1114728: loss 19.2385
[2019-03-27 01:59:43,793] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1114728: learning rate 0.0005
[2019-03-27 01:59:43,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6479543e-26 1.0000000e+00 1.2424210e-34 2.1827038e-28 8.0799584e-21], sum to 1.0000
[2019-03-27 01:59:43,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6636
[2019-03-27 01:59:43,856] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1658076.942916365 W.
[2019-03-27 01:59:43,859] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.93333333333334, 76.66666666666667, 1.0, 2.0, 0.3953548426669251, 1.0, 1.0, 0.3953548426669251, 1.0, 2.0, 0.6630886334013008, 6.9112, 6.9112, 170.5573041426782, 1658076.942916365, 1658076.942916365, 346991.4363928224], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1953600.0000, 
sim time next is 1954200.0000, 
raw observation next is [26.76666666666667, 77.33333333333333, 1.0, 2.0, 0.3931980051692457, 1.0, 2.0, 0.3931980051692457, 1.0, 2.0, 0.6595625107289279, 6.9112, 6.9112, 170.5573041426782, 1649024.432137297, 1649024.432137297, 345867.3161169622], 
processed observation next is [1.0, 0.6086956521739131, 0.46761453396524505, 0.7733333333333333, 1.0, 1.0, 0.26891325924005505, 1.0, 1.0, 0.26891325924005505, 1.0, 1.0, 0.5848323301572291, 0.0, 0.0, 0.8375144448122397, 0.4580623422603603, 0.4580623422603603, 0.516219874801436], 
reward next is 0.4838, 
noisyNet noise sample is [array([2.35633], dtype=float32), -1.3486408]. 
=============================================
[2019-03-27 01:59:43,886] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1114767: loss 94.2603
[2019-03-27 01:59:43,887] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1114767: loss -66.4858
[2019-03-27 01:59:43,890] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1114768: learning rate 0.0005
[2019-03-27 01:59:43,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1114768: learning rate 0.0005
[2019-03-27 01:59:43,969] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1114806: loss -1.7446
[2019-03-27 01:59:43,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1114806: learning rate 0.0005
[2019-03-27 01:59:44,259] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1114930: loss -108.5338
[2019-03-27 01:59:44,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1114931: learning rate 0.0005
[2019-03-27 01:59:45,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:59:45,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7632
[2019-03-27 01:59:45,442] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.33333333333334, 1.0, 2.0, 0.3977087150440519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594618.8468944151, 594618.8468944144, 174077.3228205855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1974000.0000, 
sim time next is 1974600.0000, 
raw observation next is [22.85, 95.5, 1.0, 2.0, 0.3992332777631029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784818, 174110.2442602914], 
processed observation next is [1.0, 0.8695652173913043, 0.28199052132701435, 0.955, 1.0, 1.0, 0.2761846720037385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16539853427180032, 0.1653985342718005, 0.25986603620939014], 
reward next is 0.7401, 
noisyNet noise sample is [array([-0.24211676], dtype=float32), -0.7039297]. 
=============================================
[2019-03-27 01:59:46,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1115946: loss 5.0306
[2019-03-27 01:59:46,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1115946: learning rate 0.0005
[2019-03-27 01:59:47,130] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1116192: loss 5.0892
[2019-03-27 01:59:47,133] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1116194: learning rate 0.0005
[2019-03-27 01:59:47,142] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1116197: loss 5.1925
[2019-03-27 01:59:47,148] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1116198: learning rate 0.0005
[2019-03-27 01:59:47,630] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1116415: loss -135.4325
[2019-03-27 01:59:47,632] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1116415: learning rate 0.0005
[2019-03-27 01:59:52,591] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1118637: loss 7.8432
[2019-03-27 01:59:52,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1118639: learning rate 0.0005
[2019-03-27 01:59:53,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0083410e-34 1.0000000e+00 0.0000000e+00 1.5816335e-37 1.2481649e-33], sum to 1.0000
[2019-03-27 01:59:53,822] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2794
[2019-03-27 01:59:53,831] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1951691.701059716 W.
[2019-03-27 01:59:53,835] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 72.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.612612558762084, 6.9112, 168.9094685242679, 1951691.701059716, 1454095.766489364, 311349.2136883793], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2276400.0000, 
sim time next is 2277000.0000, 
raw observation next is [29.2, 72.0, 1.0, 2.0, 0.6935455950507654, 1.0, 1.0, 0.6935455950507654, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1939359.541631129, 1939359.541631128, 371369.0103131908], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.72, 1.0, 1.0, 0.6307778253623679, 1.0, 0.5, 0.6307778253623679, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5387109837864248, 0.5387109837864245, 0.5542821049450609], 
reward next is 0.4457, 
noisyNet noise sample is [array([-0.54299736], dtype=float32), 0.662269]. 
=============================================
[2019-03-27 01:59:53,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[55.001884]
 [60.424995]
 [61.22404 ]
 [60.84822 ]
 [61.1271  ]], R is [[48.23621368]
 [47.75385284]
 [47.92430496]
 [48.12539291]
 [48.31084824]].
[2019-03-27 01:59:53,994] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 01:59:54,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3914
[2019-03-27 01:59:54,004] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.25, 76.16666666666667, 1.0, 2.0, 0.5739163567625523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 801997.9322146008, 801997.9322146003, 195987.6671177533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2127000.0000, 
sim time next is 2127600.0000, 
raw observation next is [30.3, 76.0, 1.0, 2.0, 0.5748022414015747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 803236.3466301463, 803236.3466301457, 196145.977948463], 
processed observation next is [0.0, 0.6521739130434783, 0.6350710900473934, 0.76, 1.0, 1.0, 0.48771354385731885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22312120739726285, 0.22312120739726268, 0.29275519096785524], 
reward next is 0.7072, 
noisyNet noise sample is [array([0.58955085], dtype=float32), -1.3425214]. 
=============================================
[2019-03-27 01:59:57,519] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1120833: loss 7.1100
[2019-03-27 01:59:57,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1120834: learning rate 0.0005
[2019-03-27 01:59:57,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6659168e-29 1.0000000e+00 0.0000000e+00 1.0754576e-30 1.5479105e-28], sum to 1.0000
[2019-03-27 01:59:57,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4977
[2019-03-27 01:59:57,908] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.08333333333334, 82.83333333333334, 1.0, 2.0, 0.7613905261090055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064107.938572987, 1064107.938572987, 234654.9142125692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2188200.0000, 
sim time next is 2188800.0000, 
raw observation next is [28.3, 82.0, 1.0, 2.0, 0.7579716888774716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059327.439861486, 1059327.439861486, 233856.7730923202], 
processed observation next is [1.0, 0.34782608695652173, 0.5402843601895735, 0.82, 1.0, 1.0, 0.7083996251535802, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2942576221837461, 0.2942576221837461, 0.3490399598392839], 
reward next is 0.6510, 
noisyNet noise sample is [array([0.5955311], dtype=float32), -0.81527716]. 
=============================================
[2019-03-27 01:59:57,965] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1121029: loss 4.9138
[2019-03-27 01:59:57,969] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1121029: learning rate 0.0005
[2019-03-27 01:59:58,193] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.7645092e-30 1.0000000e+00 0.0000000e+00 3.2063735e-33 1.0213887e-24], sum to 1.0000
[2019-03-27 01:59:58,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5828
[2019-03-27 01:59:58,208] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 82.0, 1.0, 2.0, 0.6725323260219531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 939866.1013753957, 939866.1013753957, 215026.5012050357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2346600.0000, 
sim time next is 2347200.0000, 
raw observation next is [27.3, 82.0, 1.0, 2.0, 0.670485267407717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937004.0683421745, 937004.0683421738, 214601.8070888437], 
processed observation next is [1.0, 0.17391304347826086, 0.4928909952606636, 0.82, 1.0, 1.0, 0.6029942980815867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26027890787282626, 0.26027890787282604, 0.32030120461021444], 
reward next is 0.6797, 
noisyNet noise sample is [array([-0.8979008], dtype=float32), 1.6777742]. 
=============================================
[2019-03-27 01:59:59,043] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1121507: loss 5.5918
[2019-03-27 01:59:59,047] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1121507: learning rate 0.0005
[2019-03-27 02:00:00,623] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1122209: loss 6.2363
[2019-03-27 02:00:00,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1122210: learning rate 0.0005
[2019-03-27 02:00:00,866] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1122316: loss 6.8052
[2019-03-27 02:00:00,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1122316: learning rate 0.0005
[2019-03-27 02:00:01,144] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1122439: loss 8.4331
[2019-03-27 02:00:01,146] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1122439: learning rate 0.0005
[2019-03-27 02:00:01,457] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1122583: loss 7.8910
[2019-03-27 02:00:01,459] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1122583: learning rate 0.0005
[2019-03-27 02:00:01,620] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1122653: loss 9.2615
[2019-03-27 02:00:01,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1122653: learning rate 0.0005
[2019-03-27 02:00:01,651] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1122664: loss 9.1679
[2019-03-27 02:00:01,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1122664: learning rate 0.0005
[2019-03-27 02:00:01,685] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1122677: loss 9.2978
[2019-03-27 02:00:01,688] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1122678: learning rate 0.0005
[2019-03-27 02:00:02,005] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1122819: loss 9.5206
[2019-03-27 02:00:02,007] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1122819: learning rate 0.0005
[2019-03-27 02:00:04,905] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1124095: loss 0.2200
[2019-03-27 02:00:04,908] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1124096: learning rate 0.0005
[2019-03-27 02:00:05,147] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1124204: loss -56.7228
[2019-03-27 02:00:05,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1124205: learning rate 0.0005
[2019-03-27 02:00:05,638] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1124423: loss -100.0133
[2019-03-27 02:00:05,642] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1124424: learning rate 0.0005
[2019-03-27 02:00:05,680] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1124443: loss -138.7894
[2019-03-27 02:00:05,683] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1124443: learning rate 0.0005
[2019-03-27 02:00:06,925] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 02:00:06,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:00:06,929] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:00:06,931] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:00:06,931] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:00:06,933] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:00:06,935] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:00:06,934] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:00:06,939] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:00:06,936] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:00:06,942] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:00:06,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-27 02:00:06,973] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-27 02:00:06,975] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-27 02:00:06,991] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-27 02:00:07,008] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-27 02:00:29,868] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:00:29,868] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.45751714, 97.06342284, 1.0, 2.0, 0.4725952307327007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660366.5061411443, 660366.5061411436, 179414.0192372847]
[2019-03-27 02:00:29,869] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:00:29,872] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8127952e-32], sampled 0.07368908870873647
[2019-03-27 02:00:32,465] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:00:32,466] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.29642460833334, 84.59075941833333, 1.0, 2.0, 0.6654317160452095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 929938.6315076996, 929938.6315076996, 213562.5708709526]
[2019-03-27 02:00:32,466] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:00:32,469] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2293113e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4592381e-30], sampled 0.7764450064012582
[2019-03-27 02:00:42,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:00:42,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.64834156, 80.13728129, 1.0, 2.0, 0.5606909606387815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783509.7810495313, 783509.7810495313, 193647.6675307442]
[2019-03-27 02:00:42,925] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:00:42,930] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.546333e-33], sampled 0.2505714881346922
[2019-03-27 02:01:16,821] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:16,821] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.4473423, 66.51358745, 1.0, 2.0, 0.9493177981125768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1326916.381034995, 1326916.381034995, 283868.3574778857]
[2019-03-27 02:01:16,823] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:01:16,826] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.6890766e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1226639e-27], sampled 0.022346520005232517
[2019-03-27 02:01:21,573] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:21,573] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.96666666666667, 84.66666666666667, 1.0, 2.0, 0.5972269346386729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834585.2245331265, 834585.2245331265, 200229.5359872414]
[2019-03-27 02:01:21,576] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:01:21,580] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.132962e-31], sampled 0.2547115632608318
[2019-03-27 02:01:25,664] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:25,666] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 79.0, 1.0, 2.0, 0.564296428924405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788549.9341072537, 788549.9341072537, 194282.3491837651]
[2019-03-27 02:01:25,668] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:01:25,670] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8175343e-30], sampled 0.2958073913538487
[2019-03-27 02:01:35,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:35,074] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.91666666666667, 90.0, 1.0, 2.0, 0.6397206293654301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 893992.3499002282, 893992.3499002288, 208372.3977384839]
[2019-03-27 02:01:35,076] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:01:35,081] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.552183e-38 1.000000e+00 0.000000e+00 0.000000e+00 9.165431e-30], sampled 0.9420547512504465
[2019-03-27 02:01:35,962] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:35,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.52366278666667, 79.62861976666667, 1.0, 2.0, 0.8352817217168191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167433.930099447, 1167433.930099447, 252761.0390639144]
[2019-03-27 02:01:35,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:01:35,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1712396e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3346251e-29], sampled 0.7976845580488178
[2019-03-27 02:01:37,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:37,060] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.1, 54.33333333333334, 1.0, 2.0, 0.6978494865641238, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984955732748791, 6.9112, 168.9124552071705, 1872119.469901781, 1819794.769904132, 384952.3989308401]
[2019-03-27 02:01:37,061] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:01:37,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.2450374e-31 1.0000000e+00 0.0000000e+00 5.3567295e-35 2.4572535e-22], sampled 0.17417973961156497
[2019-03-27 02:01:37,066] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1872119.469901781 W.
[2019-03-27 02:01:47,845] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:47,847] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.33333333333334, 95.33333333333334, 1.0, 2.0, 0.4369592347870485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640473.5025719213, 640473.5025719213, 178112.2125608344]
[2019-03-27 02:01:47,848] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:01:47,852] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3837927e-32], sampled 0.6346478368646197
[2019-03-27 02:01:52,291] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.01846716]
[2019-03-27 02:01:52,292] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.19735333833333, 87.53053548333335, 1.0, 2.0, 0.2839394837474102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463088.7323511636, 463088.7323511636, 164396.2306594169]
[2019-03-27 02:01:52,293] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:01:52,296] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3645874e-33], sampled 0.48098321443512115
[2019-03-27 02:02:01,579] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:02:02,012] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:02:02,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4832 2779142794.6067 932.0000
[2019-03-27 02:02:02,231] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:02:02,295] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:02:03,312] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1125000, evaluation results [1125000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.483174570565, 2779142794.606698, 932.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:02:07,206] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1126741: loss 0.9753
[2019-03-27 02:02:07,210] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1126742: learning rate 0.0005
[2019-03-27 02:02:11,127] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5489767e-29 1.0000000e+00 0.0000000e+00 4.4312479e-31 7.2265886e-13], sum to 1.0000
[2019-03-27 02:02:11,134] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0106
[2019-03-27 02:02:11,140] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666666, 95.0, 1.0, 2.0, 0.5447611747908881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761241.5150704212, 761241.5150704217, 190906.4087500282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2512200.0000, 
sim time next is 2512800.0000, 
raw observation next is [26.4, 95.0, 1.0, 2.0, 0.5444001895171413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760736.8987499457, 760736.8987499464, 190845.0597908571], 
processed observation next is [1.0, 0.08695652173913043, 0.45023696682464454, 0.95, 1.0, 1.0, 0.45108456568330274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21131580520831825, 0.21131580520831844, 0.2848433728221748], 
reward next is 0.7152, 
noisyNet noise sample is [array([1.1017598], dtype=float32), -0.019096289]. 
=============================================
[2019-03-27 02:02:11,916] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1128849: loss -82.3285
[2019-03-27 02:02:11,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1128851: learning rate 0.0005
[2019-03-27 02:02:12,334] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1129036: loss 0.5310
[2019-03-27 02:02:12,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1129036: learning rate 0.0005
[2019-03-27 02:02:13,729] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1129654: loss 4.6065
[2019-03-27 02:02:13,733] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1129654: learning rate 0.0005
[2019-03-27 02:02:15,165] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1130294: loss 0.8462
[2019-03-27 02:02:15,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1130296: learning rate 0.0005
[2019-03-27 02:02:15,390] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.9645302e-28 1.0000000e+00 0.0000000e+00 4.0068393e-33 5.7462583e-18], sum to 1.0000
[2019-03-27 02:02:15,400] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2381
[2019-03-27 02:02:15,405] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 88.66666666666667, 1.0, 2.0, 0.5226035819356124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730268.1667612698, 730268.1667612704, 187212.9795218366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2584200.0000, 
sim time next is 2584800.0000, 
raw observation next is [26.4, 89.0, 1.0, 2.0, 0.521605518401556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728873.0288200259, 728873.0288200252, 187049.960397948], 
processed observation next is [1.0, 0.9565217391304348, 0.45023696682464454, 0.89, 1.0, 1.0, 0.42362110650789875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20246473022778497, 0.20246473022778477, 0.2791790453700716], 
reward next is 0.7208, 
noisyNet noise sample is [array([-0.6454932], dtype=float32), 0.7311758]. 
=============================================
[2019-03-27 02:02:15,488] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1130434: loss 0.7569
[2019-03-27 02:02:15,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1130436: learning rate 0.0005
[2019-03-27 02:02:15,598] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1130485: loss 0.6601
[2019-03-27 02:02:15,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1130486: learning rate 0.0005
[2019-03-27 02:02:16,052] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1130686: loss 0.9087
[2019-03-27 02:02:16,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1130686: learning rate 0.0005
[2019-03-27 02:02:16,073] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1130694: loss 1.2615
[2019-03-27 02:02:16,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1130696: learning rate 0.0005
[2019-03-27 02:02:16,112] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1130714: loss 0.3763
[2019-03-27 02:02:16,113] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1130714: learning rate 0.0005
[2019-03-27 02:02:16,142] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1130726: loss 0.3103
[2019-03-27 02:02:16,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1130727: learning rate 0.0005
[2019-03-27 02:02:16,529] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1130897: loss 0.5641
[2019-03-27 02:02:16,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1130898: learning rate 0.0005
[2019-03-27 02:02:18,779] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1131904: loss 13.1228
[2019-03-27 02:02:18,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1131906: learning rate 0.0005
[2019-03-27 02:02:19,168] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1132078: loss 0.0979
[2019-03-27 02:02:19,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1132078: learning rate 0.0005
[2019-03-27 02:02:19,765] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1132346: loss 0.1709
[2019-03-27 02:02:19,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1132346: learning rate 0.0005
[2019-03-27 02:02:19,798] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1132361: loss 0.1376
[2019-03-27 02:02:19,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1132361: learning rate 0.0005
[2019-03-27 02:02:24,751] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8933665e-34 1.0000000e+00 0.0000000e+00 4.4097705e-36 1.2382163e-24], sum to 1.0000
[2019-03-27 02:02:24,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2604
[2019-03-27 02:02:24,767] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4355793500451794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630808.7300177492, 630808.7300177492, 176962.4834249192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2710800.0000, 
sim time next is 2711400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4322396927415711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 627312.4138143113, 627312.4138143107, 176654.5404167373], 
processed observation next is [0.0, 0.391304347826087, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3159514370380375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17425344828175313, 0.17425344828175296, 0.26366349315930937], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.38106406], dtype=float32), -0.50469464]. 
=============================================
[2019-03-27 02:02:25,074] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1134694: loss 0.4091
[2019-03-27 02:02:25,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1134696: learning rate 0.0005
[2019-03-27 02:02:28,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6481607e-31 1.0000000e+00 0.0000000e+00 3.1914784e-30 2.1487356e-18], sum to 1.0000
[2019-03-27 02:02:28,445] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4575
[2019-03-27 02:02:28,451] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.7195063087567697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1059854.113421362, 1059854.113421363, 232299.514397812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2824800.0000, 
sim time next is 2825400.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.7039021614878351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1036894.220737903, 1036894.220737904, 228678.8326920875], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.89, 1.0, 1.0, 0.6432556162504037, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2880261724271953, 0.2880261724271955, 0.3413116905852052], 
reward next is 0.6587, 
noisyNet noise sample is [array([-1.0704958], dtype=float32), 0.26864642]. 
=============================================
[2019-03-27 02:02:29,699] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1136759: loss 0.1107
[2019-03-27 02:02:29,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1136760: learning rate 0.0005
[2019-03-27 02:02:30,269] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1137014: loss 0.1732
[2019-03-27 02:02:30,272] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1137014: learning rate 0.0005
[2019-03-27 02:02:31,597] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1137607: loss 0.1636
[2019-03-27 02:02:31,606] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1137610: learning rate 0.0005
[2019-03-27 02:02:33,122] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1138292: loss 0.4750
[2019-03-27 02:02:33,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1138292: learning rate 0.0005
[2019-03-27 02:02:33,451] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1138440: loss 0.0693
[2019-03-27 02:02:33,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1138440: learning rate 0.0005
[2019-03-27 02:02:33,555] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.8061318e-30 1.0000000e+00 0.0000000e+00 2.6392595e-35 3.0701525e-22], sum to 1.0000
[2019-03-27 02:02:33,562] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1030
[2019-03-27 02:02:33,570] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6990864653169488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076596.452656342, 1076596.452656341, 233145.4535431424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2910600.0000, 
sim time next is 2911200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6084503108895621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937004.2227734138, 937004.2227734138, 212865.1197646986], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.5282533866139303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2602789507703927, 0.2602789507703927, 0.3177091339771621], 
reward next is 0.6823, 
noisyNet noise sample is [array([-0.04646936], dtype=float32), -0.8813496]. 
=============================================
[2019-03-27 02:02:33,672] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1138534: loss 0.1325
[2019-03-27 02:02:33,678] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1138536: learning rate 0.0005
[2019-03-27 02:02:33,961] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1138666: loss 0.4287
[2019-03-27 02:02:33,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1138667: learning rate 0.0005
[2019-03-27 02:02:34,044] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1138702: loss 0.4659
[2019-03-27 02:02:34,047] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9159680e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2161726e-28], sum to 1.0000
[2019-03-27 02:02:34,051] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1138702: learning rate 0.0005
[2019-03-27 02:02:34,054] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6494
[2019-03-27 02:02:34,058] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 95.0, 1.0, 2.0, 0.3245970395610622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511336.1056912722, 511336.1056912722, 167844.7569561113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2919000.0000, 
sim time next is 2919600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3214245079321091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507786.9718983622, 507786.9718983629, 167602.9986496499], 
processed observation next is [1.0, 0.8260869565217391, 0.19431279620853087, 0.94, 1.0, 1.0, 0.18243916618326395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14105193663843396, 0.14105193663843416, 0.2501537293278357], 
reward next is 0.7498, 
noisyNet noise sample is [array([-1.9151425], dtype=float32), 0.6629445]. 
=============================================
[2019-03-27 02:02:34,093] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1138725: loss 0.4474
[2019-03-27 02:02:34,096] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1138727: learning rate 0.0005
[2019-03-27 02:02:34,106] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1138731: loss 0.3964
[2019-03-27 02:02:34,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1138731: learning rate 0.0005
[2019-03-27 02:02:34,584] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1138941: loss 0.1220
[2019-03-27 02:02:34,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1138942: learning rate 0.0005
[2019-03-27 02:02:36,906] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1139955: loss 5.4628
[2019-03-27 02:02:36,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1139956: learning rate 0.0005
[2019-03-27 02:02:37,239] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1632034e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0521615e-29], sum to 1.0000
[2019-03-27 02:02:37,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9978
[2019-03-27 02:02:37,253] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 99.0, 1.0, 2.0, 0.3101219421506055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493050.0560429331, 493050.0560429325, 166556.303467885], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2922600.0000, 
sim time next is 2923200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3084721257190241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491020.2512022325, 491020.2512022319, 166416.1234467001], 
processed observation next is [1.0, 0.8695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16683388640846278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13639451422284235, 0.1363945142228422, 0.24838227380104494], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.18284246], dtype=float32), 0.41926503]. 
=============================================
[2019-03-27 02:02:37,295] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1140132: loss 0.1541
[2019-03-27 02:02:37,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1140132: learning rate 0.0005
[2019-03-27 02:02:37,763] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1140337: loss 0.2607
[2019-03-27 02:02:37,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1140339: learning rate 0.0005
[2019-03-27 02:02:37,863] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1140381: loss 0.0963
[2019-03-27 02:02:37,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1140382: learning rate 0.0005
[2019-03-27 02:02:39,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.258056e-27], sum to 1.0000
[2019-03-27 02:02:39,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5732
[2019-03-27 02:02:39,496] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4913679868666576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686606.5206404879, 686606.5206404879, 182254.3936055178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3182400.0000, 
sim time next is 3183000.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.4893401349047127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683772.0167078648, 683772.0167078648, 181942.5830266677], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.9400000000000002, 1.0, 1.0, 0.38474715048760566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18993667130774022, 0.18993667130774022, 0.2715560940696533], 
reward next is 0.7284, 
noisyNet noise sample is [array([-0.50381535], dtype=float32), 1.4405221]. 
=============================================
[2019-03-27 02:02:39,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.85428]
 [83.02109]
 [82.65397]
 [82.32368]
 [81.96972]], R is [[83.74287415]
 [83.63343048]
 [83.52492523]
 [83.41764069]
 [83.31118774]].
[2019-03-27 02:02:42,502] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0919277e-29 1.0000000e+00 0.0000000e+00 4.9513902e-34 7.3472771e-18], sum to 1.0000
[2019-03-27 02:02:42,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4003
[2019-03-27 02:02:42,518] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 85.0, 1.0, 2.0, 0.6745084869485554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 989521.3541769555, 989521.3541769562, 221548.375659918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3148800.0000, 
sim time next is 3149400.0000, 
raw observation next is [24.83333333333334, 84.0, 1.0, 2.0, 0.6496299206387757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 952143.2767183082, 952143.2767183089, 216070.7810334703], 
processed observation next is [1.0, 0.43478260869565216, 0.3759873617693526, 0.84, 1.0, 1.0, 0.5778673742635851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2644842435328634, 0.2644842435328636, 0.3224937030350303], 
reward next is 0.6775, 
noisyNet noise sample is [array([1.5209141], dtype=float32), 0.16410626]. 
=============================================
[2019-03-27 02:02:42,941] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1142648: loss 0.0461
[2019-03-27 02:02:42,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1142648: learning rate 0.0005
[2019-03-27 02:02:47,451] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1144664: loss 0.0799
[2019-03-27 02:02:47,453] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1144665: learning rate 0.0005
[2019-03-27 02:02:48,048] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1144924: loss 0.1593
[2019-03-27 02:02:48,049] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1144924: learning rate 0.0005
[2019-03-27 02:02:49,541] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1145604: loss 0.1998
[2019-03-27 02:02:49,542] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1145604: learning rate 0.0005
[2019-03-27 02:02:50,982] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1146243: loss 7.2079
[2019-03-27 02:02:50,985] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1146243: learning rate 0.0005
[2019-03-27 02:02:51,453] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1146438: loss 2.7729
[2019-03-27 02:02:51,463] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1146440: learning rate 0.0005
[2019-03-27 02:02:51,679] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1146536: loss 1.4944
[2019-03-27 02:02:51,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1146537: learning rate 0.0005
[2019-03-27 02:02:51,844] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1146611: loss 2.3792
[2019-03-27 02:02:51,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1146612: learning rate 0.0005
[2019-03-27 02:02:51,907] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1146637: loss 2.0711
[2019-03-27 02:02:51,909] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1146637: learning rate 0.0005
[2019-03-27 02:02:52,059] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1146703: loss 2.4538
[2019-03-27 02:02:52,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1146703: learning rate 0.0005
[2019-03-27 02:02:52,111] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1146724: loss 2.9243
[2019-03-27 02:02:52,112] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1146724: learning rate 0.0005
[2019-03-27 02:02:52,524] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1146911: loss 18.8787
[2019-03-27 02:02:52,530] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1146911: learning rate 0.0005
[2019-03-27 02:02:52,545] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8424698e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4076233e-32], sum to 1.0000
[2019-03-27 02:02:52,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2942
[2019-03-27 02:02:52,558] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 66.33333333333334, 1.0, 2.0, 0.5679490367116142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 793656.0087628865, 793656.0087628871, 194927.5879631847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [32.0, 65.66666666666667, 1.0, 2.0, 0.5659146935608056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790812.1449620173, 790812.1449620173, 194568.3191767216], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6566666666666667, 1.0, 1.0, 0.4770056548925368, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21967004026722703, 0.21967004026722703, 0.2904004763831666], 
reward next is 0.7096, 
noisyNet noise sample is [array([1.6383686], dtype=float32), -0.49310002]. 
=============================================
[2019-03-27 02:02:55,618] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1148292: loss 13.2043
[2019-03-27 02:02:55,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1148293: learning rate 0.0005
[2019-03-27 02:02:55,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1148403: loss 12.0270
[2019-03-27 02:02:55,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1148403: learning rate 0.0005
[2019-03-27 02:02:55,895] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1148412: loss -10.7039
[2019-03-27 02:02:55,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1148413: learning rate 0.0005
[2019-03-27 02:02:55,924] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4828586e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3163320e-32], sum to 1.0000
[2019-03-27 02:02:55,934] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0723
[2019-03-27 02:02:55,945] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 85.66666666666667, 1.0, 2.0, 0.4645528505562885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652432.7671732802, 652432.7671732797, 178656.4304518052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3220800.0000, 
sim time next is 3221400.0000, 
raw observation next is [25.83333333333334, 84.83333333333333, 1.0, 2.0, 0.4667282380299969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654101.1035456455, 654101.1035456449, 178797.7069489905], 
processed observation next is [0.0, 0.2608695652173913, 0.42338072669826254, 0.8483333333333333, 1.0, 1.0, 0.35750390124096015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1816947509849015, 0.18169475098490137, 0.26686224917759777], 
reward next is 0.7331, 
noisyNet noise sample is [array([1.4583505], dtype=float32), 0.5230964]. 
=============================================
[2019-03-27 02:02:56,128] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1148517: loss 11.1661
[2019-03-27 02:02:56,131] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1148518: learning rate 0.0005
[2019-03-27 02:02:57,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.226663e-29], sum to 1.0000
[2019-03-27 02:02:57,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6118
[2019-03-27 02:02:57,344] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 71.33333333333333, 1.0, 2.0, 0.5411709263616752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756222.7671212461, 756222.7671212461, 190299.097676928], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3321600.0000, 
sim time next is 3322200.0000, 
raw observation next is [30.66666666666667, 70.66666666666667, 1.0, 2.0, 0.5483562756823022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766267.0699559365, 766267.0699559372, 191520.2590257182], 
processed observation next is [0.0, 0.43478260869565216, 0.6524486571879939, 0.7066666666666667, 1.0, 1.0, 0.45585093455699055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21285196387664904, 0.21285196387664923, 0.28585113287420627], 
reward next is 0.7141, 
noisyNet noise sample is [array([-0.25819826], dtype=float32), 0.21055447]. 
=============================================
[2019-03-27 02:02:58,714] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9437549e-36], sum to 1.0000
[2019-03-27 02:02:58,722] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6957
[2019-03-27 02:02:58,727] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 83.16666666666667, 1.0, 2.0, 0.5495049870031599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767872.8468131485, 767872.846813149, 191715.3097223832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3269400.0000, 
sim time next is 3270000.0000, 
raw observation next is [28.0, 82.33333333333334, 1.0, 2.0, 0.5391602199958581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753412.044838922, 753412.0448389213, 189958.9602197447], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8233333333333335, 1.0, 1.0, 0.44477134939260005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20928112356636722, 0.20928112356636702, 0.2835208361488727], 
reward next is 0.7165, 
noisyNet noise sample is [array([1.5834447], dtype=float32), -0.10048881]. 
=============================================
[2019-03-27 02:02:58,737] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[80.97545]
 [80.99588]
 [80.84569]
 [80.81885]
 [80.75213]], R is [[80.9634552 ]
 [80.86768341]
 [80.77393341]
 [80.68076324]
 [80.5881424 ]].
[2019-03-27 02:02:59,471] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 02:02:59,473] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:02:59,474] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:02:59,476] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:02:59,476] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:02:59,477] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:02:59,478] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:02:59,479] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:02:59,478] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:02:59,481] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:02:59,482] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:02:59,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-27 02:02:59,504] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-27 02:02:59,505] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-27 02:02:59,560] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-27 02:02:59,578] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-27 02:03:16,794] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.03957229]
[2019-03-27 02:03:16,796] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.9, 90.5, 1.0, 2.0, 0.3066146238890192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490495.8983857748, 490495.8983857754, 166404.1538865723]
[2019-03-27 02:03:16,797] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:03:16,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8226717e-33], sampled 0.3062519233257689
[2019-03-27 02:03:21,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.03957229]
[2019-03-27 02:03:21,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [17.65, 91.0, 1.0, 2.0, 0.2204601171362133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 367610.2543900423, 367610.2543900416, 157528.8484975846]
[2019-03-27 02:03:21,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:03:21,044] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.683168e-32], sampled 0.6067326643317911
[2019-03-27 02:03:48,072] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.03957229]
[2019-03-27 02:03:48,073] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.95, 69.5, 1.0, 2.0, 0.5384853883137946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752468.7141139146, 752468.7141139146, 189845.0189674479]
[2019-03-27 02:03:48,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:03:48,077] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2598023e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.8596391e-31], sampled 0.07311738786680066
[2019-03-27 02:03:51,950] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.03957229]
[2019-03-27 02:03:51,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.02808305, 66.14499061, 1.0, 2.0, 0.8551768852237892, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984974611716, 6.9112, 168.9123159837059, 2092298.407057543, 2025054.938812632, 421933.3049316308]
[2019-03-27 02:03:51,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:03:51,956] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.0296150e-27 1.0000000e+00 2.6134288e-37 6.2938736e-30 1.9698876e-19], sampled 0.8334426709195193
[2019-03-27 02:03:51,957] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2092298.407057543 W.
[2019-03-27 02:03:54,499] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.03957229]
[2019-03-27 02:03:54,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.81666666666667, 80.16666666666667, 1.0, 2.0, 0.5597853856676684, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9631708545066104, 6.911200000000001, 6.9112, 168.9129565028601, 1565064.235176599, 1565064.235176598, 340622.7036816226]
[2019-03-27 02:03:54,502] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:03:54,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4065105e-31 1.0000000e+00 0.0000000e+00 7.9957654e-35 6.8893268e-23], sampled 0.4142842921233645
[2019-03-27 02:04:45,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.03957229]
[2019-03-27 02:04:45,617] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.3, 57.0, 1.0, 2.0, 0.7744621026533514, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979933297198, 6.9112, 168.9123160134399, 1979332.750402489, 1912092.858614207, 401639.3864251882]
[2019-03-27 02:04:45,619] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:04:45,621] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9694724e-27 1.0000000e+00 6.7666494e-38 1.6460442e-30 2.8730769e-19], sampled 0.8187356752367753
[2019-03-27 02:04:45,623] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1979332.750402489 W.
[2019-03-27 02:04:54,436] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4832 2779142794.6067 932.0000
[2019-03-27 02:04:54,471] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:04:54,510] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:04:54,519] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.0108 3164052558.2216 1777.0000
[2019-03-27 02:04:54,572] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:04:55,590] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1150000, evaluation results [1150000.0, 7885.0108480144145, 3164052558.221575, 1777.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.483174570565, 2779142794.606698, 932.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:04:57,127] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1150688: loss 11.7920
[2019-03-27 02:04:57,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1150690: learning rate 0.0005
[2019-03-27 02:05:01,542] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1152663: loss 12.0678
[2019-03-27 02:05:01,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1152663: learning rate 0.0005
[2019-03-27 02:05:02,042] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1152885: loss 12.5356
[2019-03-27 02:05:02,044] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1152886: learning rate 0.0005
[2019-03-27 02:05:03,587] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1153572: loss 11.3987
[2019-03-27 02:05:03,591] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1153573: learning rate 0.0005
[2019-03-27 02:05:04,886] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1154124: loss 11.4011
[2019-03-27 02:05:04,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1154124: learning rate 0.0005
[2019-03-27 02:05:05,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7966231e-34 1.0000000e+00 0.0000000e+00 8.6880202e-38 9.1887153e-29], sum to 1.0000
[2019-03-27 02:05:05,104] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9535
[2019-03-27 02:05:05,109] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5166829660210659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721992.0912784077, 721992.0912784083, 186251.3456363576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3441600.0000, 
sim time next is 3442200.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.51689380709869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722286.8123248415, 722286.8123248421, 186285.4157394324], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41794434590203616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006352256457893, 0.20063522564578948, 0.27803793393945136], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.30116415], dtype=float32), -0.8208074]. 
=============================================
[2019-03-27 02:05:05,220] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1154271: loss 7.6047
[2019-03-27 02:05:05,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1154276: learning rate 0.0005
[2019-03-27 02:05:05,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7827574e-28 1.0000000e+00 0.0000000e+00 3.8239403e-31 1.8257354e-21], sum to 1.0000
[2019-03-27 02:05:05,341] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1972
[2019-03-27 02:05:05,346] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 87.33333333333334, 1.0, 2.0, 0.9440242841929092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1319512.732405236, 1319512.732405236, 282332.0423234189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [26.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8807175721614448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1230974.290583841, 1230974.290583841, 264685.8386431032], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271754, 0.8566666666666667, 1.0, 1.0, 0.856286231519813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34193730293995583, 0.34193730293995583, 0.3950534905120943], 
reward next is 0.6049, 
noisyNet noise sample is [array([1.3329998], dtype=float32), 1.8262312]. 
=============================================
[2019-03-27 02:05:05,695] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1154483: loss 7.6600
[2019-03-27 02:05:05,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1154483: learning rate 0.0005
[2019-03-27 02:05:05,731] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1154495: loss 7.9837
[2019-03-27 02:05:05,734] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1154495: learning rate 0.0005
[2019-03-27 02:05:05,791] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1154520: loss 8.1375
[2019-03-27 02:05:05,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1154520: learning rate 0.0005
[2019-03-27 02:05:05,858] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1154554: loss 8.6459
[2019-03-27 02:05:05,866] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1154556: learning rate 0.0005
[2019-03-27 02:05:05,899] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1154570: loss 8.0821
[2019-03-27 02:05:05,904] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1154570: learning rate 0.0005
[2019-03-27 02:05:06,508] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1154840: loss 8.6610
[2019-03-27 02:05:06,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1154841: learning rate 0.0005
[2019-03-27 02:05:09,778] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1156303: loss 0.2983
[2019-03-27 02:05:09,778] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1156303: learning rate 0.0005
[2019-03-27 02:05:10,167] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1156472: loss -83.0369
[2019-03-27 02:05:10,170] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1156473: learning rate 0.0005
[2019-03-27 02:05:10,416] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1156586: loss 0.8961
[2019-03-27 02:05:10,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1156587: learning rate 0.0005
[2019-03-27 02:05:10,637] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1156680: loss -22.0199
[2019-03-27 02:05:10,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1156680: learning rate 0.0005
[2019-03-27 02:05:15,364] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1158786: loss 17.2326
[2019-03-27 02:05:15,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1158787: learning rate 0.0005
[2019-03-27 02:05:15,785] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6835802e-23 1.0000000e+00 6.3183284e-33 2.6203013e-26 7.2133432e-16], sum to 1.0000
[2019-03-27 02:05:15,793] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5393
[2019-03-27 02:05:15,798] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5323908681859552, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9245868736884741, 6.911199999999999, 6.9112, 168.9129561860859, 1488420.147777343, 1488420.147777344, 326272.6955288192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4156200.0000, 
sim time next is 4156800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 1.00719651897865, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.91295651035, 1407870.460252417, 1407870.460252418, 301129.6874310865], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 1.0086705047935542, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451519049, 0.3910751278478936, 0.3910751278478939, 0.44944729467326344], 
reward next is 0.5506, 
noisyNet noise sample is [array([0.86821705], dtype=float32), 0.95347196]. 
=============================================
[2019-03-27 02:05:16,728] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:05:16,739] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6478
[2019-03-27 02:05:16,752] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.16666666666667, 1.0, 2.0, 0.5485338291444588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766515.270746748, 766515.2707467487, 191549.6587717301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [29.0, 77.33333333333334, 1.0, 2.0, 0.5458690742630228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762790.2339132758, 762790.2339132765, 191094.7013062788], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7733333333333334, 1.0, 1.0, 0.4528543063409913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21188617608702104, 0.21188617608702123, 0.2852159720989236], 
reward next is 0.7148, 
noisyNet noise sample is [array([0.67915], dtype=float32), 2.3719838]. 
=============================================
[2019-03-27 02:05:19,347] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1160557: loss 0.7616
[2019-03-27 02:05:19,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1160558: learning rate 0.0005
[2019-03-27 02:05:20,234] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1160956: loss 2.1458
[2019-03-27 02:05:20,236] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1160956: learning rate 0.0005
[2019-03-27 02:05:21,793] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1161652: loss -19.4281
[2019-03-27 02:05:21,794] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1161652: learning rate 0.0005
[2019-03-27 02:05:22,136] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9159593e-31 1.0000000e+00 0.0000000e+00 5.8862383e-38 2.0351469e-29], sum to 1.0000
[2019-03-27 02:05:22,147] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1144
[2019-03-27 02:05:22,153] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6911327214580079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 987819.162025164, 987819.1620251647, 221865.750396678], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3729600.0000, 
sim time next is 3730200.0000, 
raw observation next is [26.83333333333333, 74.83333333333334, 1.0, 2.0, 0.6414056247883516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 917541.7052253456, 917541.7052253456, 211475.3498743518], 
processed observation next is [1.0, 0.17391304347826086, 0.470774091627172, 0.7483333333333334, 1.0, 1.0, 0.5679585840823513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2548726958959293, 0.2548726958959293, 0.315634850558734], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.33989608], dtype=float32), -0.7814627]. 
=============================================
[2019-03-27 02:05:23,020] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1162199: loss -13.1118
[2019-03-27 02:05:23,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1162199: learning rate 0.0005
[2019-03-27 02:05:23,465] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1162395: loss -14.3559
[2019-03-27 02:05:23,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1162395: learning rate 0.0005
[2019-03-27 02:05:23,725] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1162513: loss 20.4266
[2019-03-27 02:05:23,729] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1162513: learning rate 0.0005
[2019-03-27 02:05:23,747] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1162521: loss -0.4594
[2019-03-27 02:05:23,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1162523: learning rate 0.0005
[2019-03-27 02:05:23,758] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1162528: loss -18.5828
[2019-03-27 02:05:23,760] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1162528: learning rate 0.0005
[2019-03-27 02:05:23,788] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1162539: loss -7.4556
[2019-03-27 02:05:23,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1162539: learning rate 0.0005
[2019-03-27 02:05:23,941] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1162611: loss 41.7025
[2019-03-27 02:05:23,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1162611: learning rate 0.0005
[2019-03-27 02:05:24,611] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1162905: loss -169.5803
[2019-03-27 02:05:24,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1162905: learning rate 0.0005
[2019-03-27 02:05:27,857] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1164318: loss 4.4090
[2019-03-27 02:05:27,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1164320: learning rate 0.0005
[2019-03-27 02:05:28,139] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1164440: loss 0.0935
[2019-03-27 02:05:28,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1164443: learning rate 0.0005
[2019-03-27 02:05:28,381] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1164550: loss 0.0687
[2019-03-27 02:05:28,385] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1164550: learning rate 0.0005
[2019-03-27 02:05:28,597] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1164647: loss 0.1205
[2019-03-27 02:05:28,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1164649: learning rate 0.0005
[2019-03-27 02:05:30,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3305098e-37], sum to 1.0000
[2019-03-27 02:05:30,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2307
[2019-03-27 02:05:30,635] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3853800.0000, 
sim time next is 3854400.0000, 
raw observation next is [35.0, 57.33333333333333, 1.0, 2.0, 0.6093395870336394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851518.6419786054, 851518.6419786054, 202499.3785488599], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5733333333333333, 1.0, 1.0, 0.5293248036549872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23653295610516817, 0.23653295610516817, 0.3022378784311342], 
reward next is 0.6978, 
noisyNet noise sample is [array([-0.5462736], dtype=float32), 0.025923697]. 
=============================================
[2019-03-27 02:05:31,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2639705e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2087505e-31], sum to 1.0000
[2019-03-27 02:05:31,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8595
[2019-03-27 02:05:31,722] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 91.0, 1.0, 2.0, 0.577627009638801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807185.2154711109, 807185.2154711109, 196651.3604342198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3894000.0000, 
sim time next is 3894600.0000, 
raw observation next is [27.58333333333334, 91.5, 1.0, 2.0, 0.5767443311795232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 805951.2785154509, 805951.2785154515, 196492.9050597016], 
processed observation next is [0.0, 0.043478260869565216, 0.506319115323855, 0.915, 1.0, 1.0, 0.49005341105966643, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2238753551431808, 0.22387535514318097, 0.29327299262642026], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.54143083], dtype=float32), 1.9403166]. 
=============================================
[2019-03-27 02:05:33,294] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1166749: loss 0.0726
[2019-03-27 02:05:33,297] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1166750: learning rate 0.0005
[2019-03-27 02:05:34,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.263988e-37], sum to 1.0000
[2019-03-27 02:05:34,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7459
[2019-03-27 02:05:34,471] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.598966487685373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837017.0929512661, 837017.0929512654, 200553.6879196656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3918600.0000, 
sim time next is 3919200.0000, 
raw observation next is [31.33333333333334, 73.66666666666666, 1.0, 2.0, 0.6007832373284243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 839556.8869685862, 839556.8869685855, 200892.2111110543], 
processed observation next is [0.0, 0.34782608695652173, 0.6840442338072673, 0.7366666666666666, 1.0, 1.0, 0.519015948588463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23321024638016286, 0.23321024638016266, 0.2998391210612751], 
reward next is 0.7002, 
noisyNet noise sample is [array([0.35324904], dtype=float32), 0.26979053]. 
=============================================
[2019-03-27 02:05:37,401] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1168585: loss 0.1178
[2019-03-27 02:05:37,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1168586: learning rate 0.0005
[2019-03-27 02:05:38,071] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1168885: loss 0.1982
[2019-03-27 02:05:38,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1168886: learning rate 0.0005
[2019-03-27 02:05:39,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0246921e-28 1.0000000e+00 0.0000000e+00 3.3418503e-36 6.5323068e-27], sum to 1.0000
[2019-03-27 02:05:39,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3786
[2019-03-27 02:05:39,468] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5983498642354573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836155.0622102371, 836155.0622102377, 200439.0969074108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4236600.0000, 
sim time next is 4237200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5980822105192695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835780.8863707257, 835780.8863707257, 200389.3920534862], 
processed observation next is [1.0, 0.043478260869565216, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5157616994208065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23216135732520157, 0.23216135732520157, 0.29908864485594955], 
reward next is 0.7009, 
noisyNet noise sample is [array([-0.18656272], dtype=float32), -1.8611746]. 
=============================================
[2019-03-27 02:05:39,751] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1169629: loss 0.1608
[2019-03-27 02:05:39,752] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1169629: learning rate 0.0005
[2019-03-27 02:05:40,845] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1170106: loss 0.0817
[2019-03-27 02:05:40,848] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1170106: learning rate 0.0005
[2019-03-27 02:05:41,427] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1170365: loss 0.0470
[2019-03-27 02:05:41,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1170365: learning rate 0.0005
[2019-03-27 02:05:41,531] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1170409: loss 0.0008
[2019-03-27 02:05:41,534] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1170409: learning rate 0.0005
[2019-03-27 02:05:41,580] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1170436: loss 0.0196
[2019-03-27 02:05:41,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1170437: learning rate 0.0005
[2019-03-27 02:05:41,620] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1170449: loss 0.0005
[2019-03-27 02:05:41,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1170449: learning rate 0.0005
[2019-03-27 02:05:41,849] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1170548: loss 0.0068
[2019-03-27 02:05:41,853] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1170548: learning rate 0.0005
[2019-03-27 02:05:41,923] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1170582: loss 0.0155
[2019-03-27 02:05:41,926] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1170583: learning rate 0.0005
[2019-03-27 02:05:42,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.328613e-36], sum to 1.0000
[2019-03-27 02:05:42,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6383
[2019-03-27 02:05:42,589] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 86.5, 1.0, 2.0, 0.5403003727860822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755005.838478563, 755005.8384785623, 190151.2808299438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4055400.0000, 
sim time next is 4056000.0000, 
raw observation next is [27.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5401544423213469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754801.8454693147, 754801.8454693141, 190126.5668811943], 
processed observation next is [1.0, 0.9565217391304348, 0.4944707740916275, 0.8733333333333333, 1.0, 1.0, 0.4459692076160806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20966717929703188, 0.2096671792970317, 0.2837709953450661], 
reward next is 0.7162, 
noisyNet noise sample is [array([-0.5604837], dtype=float32), 0.3110957]. 
=============================================
[2019-03-27 02:05:42,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.39963]
 [79.32346]
 [79.25126]
 [79.21065]
 [79.20405]], R is [[79.39216614]
 [79.31443787]
 [79.23712921]
 [79.16021729]
 [79.08384705]].
[2019-03-27 02:05:42,608] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1170887: loss 0.0043
[2019-03-27 02:05:42,613] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1170887: learning rate 0.0005
[2019-03-27 02:05:42,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5559857e-11 9.9871635e-01 7.4631099e-16 3.2761942e-12 1.2836184e-03], sum to 1.0000
[2019-03-27 02:05:42,703] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9290
[2019-03-27 02:05:42,713] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3210620.627215636 W.
[2019-03-27 02:05:42,720] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 50.0, 1.0, 2.0, 0.8889057652913162, 1.0, 2.0, 0.7650429221599205, 1.0, 2.0, 1.03, 7.005112631929873, 6.9112, 170.5573041426782, 3210620.627215636, 3143347.188842644, 587638.5156567041], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4288200.0000, 
sim time next is 4288800.0000, 
raw observation next is [38.0, 49.00000000000001, 1.0, 2.0, 0.8856853021867638, 1.0, 2.0, 0.7634326906076445, 1.0, 2.0, 1.03, 7.005112377839421, 6.9112, 170.5573041426782, 3203854.373748922, 3136581.117391258, 586396.3613968741], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49000000000000005, 1.0, 1.0, 0.8622714484177877, 1.0, 1.0, 0.7149791453104151, 1.0, 1.0, 1.0365853658536586, 0.009391237783942064, 0.0, 0.8375144448122397, 0.8899595482635894, 0.8712725326086828, 0.8752184498460807], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.8162167], dtype=float32), 0.49585244]. 
=============================================
[2019-03-27 02:05:45,745] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1172277: loss 6.3297
[2019-03-27 02:05:45,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1172277: learning rate 0.0005
[2019-03-27 02:05:46,681] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1172693: loss -0.9108
[2019-03-27 02:05:46,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1172694: learning rate 0.0005
[2019-03-27 02:05:46,731] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1172718: loss -34.6459
[2019-03-27 02:05:46,732] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1172718: learning rate 0.0005
[2019-03-27 02:05:47,035] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1172849: loss 0.7307
[2019-03-27 02:05:47,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1172850: learning rate 0.0005
[2019-03-27 02:05:50,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7582286e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.8486666e-36], sum to 1.0000
[2019-03-27 02:05:50,104] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7006
[2019-03-27 02:05:50,108] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6077134616791527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 849245.3125225074, 849245.3125225074, 202192.033350096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4226400.0000, 
sim time next is 4227000.0000, 
raw observation next is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.6070549292664559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848324.6830787209, 848324.6830787209, 202067.8425434175], 
processed observation next is [1.0, 0.9565217391304348, 0.7077409162717223, 0.7166666666666667, 1.0, 1.0, 0.526572203935489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23564574529964472, 0.23564574529964472, 0.30159379484092164], 
reward next is 0.6984, 
noisyNet noise sample is [array([0.80673975], dtype=float32), -1.1856385]. 
=============================================
[2019-03-27 02:05:50,120] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[60.272873]
 [60.082897]
 [60.02771 ]
 [59.80587 ]
 [59.609325]], R is [[60.31639481]
 [60.41144943]
 [60.50465393]
 [60.59598541]
 [60.68544006]].
[2019-03-27 02:05:51,450] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1174817: loss 34.0417
[2019-03-27 02:05:51,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1174818: learning rate 0.0005
[2019-03-27 02:05:51,855] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 02:05:51,858] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:05:51,858] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:05:51,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:05:51,861] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:05:51,862] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:05:51,861] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:05:51,863] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:05:51,864] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:05:51,866] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:05:51,865] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:05:51,885] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-27 02:05:51,885] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-27 02:05:51,902] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-27 02:05:51,947] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-27 02:05:51,965] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-27 02:06:23,463] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.07418288]
[2019-03-27 02:06:23,464] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.15396445333334, 71.65308664, 1.0, 2.0, 0.5196384777469539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726123.4165943611, 726123.4165943618, 186729.9609326195]
[2019-03-27 02:06:23,465] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:06:23,467] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07388372659205089
[2019-03-27 02:06:49,706] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.07418288]
[2019-03-27 02:06:49,707] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 71.66666666666667, 1.0, 2.0, 0.6163266092105485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861286.581036685, 861286.581036685, 203827.0705384729]
[2019-03-27 02:06:49,708] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:06:49,711] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5656683267710043
[2019-03-27 02:07:15,859] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.07418288]
[2019-03-27 02:07:15,861] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.39981934666667, 86.5112263, 1.0, 2.0, 0.5904743458270812, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9843082581871416, 6.9112, 6.9112, 168.9129117969175, 1650932.052732307, 1650932.052732307, 352347.1975095068]
[2019-03-27 02:07:15,863] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:07:15,867] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2965096e-21 1.0000000e+00 2.7132352e-31 2.9156196e-25 1.6259224e-17], sampled 0.05512215150486888
[2019-03-27 02:07:16,625] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0370729], dtype=float32), -0.07418288]
[2019-03-27 02:07:16,628] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.75, 94.0, 1.0, 2.0, 0.4992781503528899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697663.3120294423, 697663.3120294423, 183481.7483296216]
[2019-03-27 02:07:16,629] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:07:16,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14918640624635338
[2019-03-27 02:07:47,087] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.0911 3163989000.4795 1777.0000
[2019-03-27 02:07:47,088] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:07:47,142] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:07:47,191] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:07:47,216] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.1510 2779145736.7934 930.0000
[2019-03-27 02:07:48,229] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1175000, evaluation results [1175000.0, 7885.091086252912, 3163989000.479539, 1777.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8662.15095059898, 2779145736.793425, 930.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:07:51,693] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1176541: loss 110.3355
[2019-03-27 02:07:51,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1176542: learning rate 0.0005
[2019-03-27 02:07:52,253] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1176792: loss 63.2015
[2019-03-27 02:07:52,256] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1176792: learning rate 0.0005
[2019-03-27 02:07:54,402] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1177756: loss 1.6675
[2019-03-27 02:07:54,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1177756: learning rate 0.0005
[2019-03-27 02:07:55,289] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1178151: loss 4.4632
[2019-03-27 02:07:55,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1178151: learning rate 0.0005
[2019-03-27 02:07:55,775] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1178374: loss -0.8571
[2019-03-27 02:07:55,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1178374: learning rate 0.0005
[2019-03-27 02:07:55,792] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1178379: loss -1.1057
[2019-03-27 02:07:55,794] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1178379: learning rate 0.0005
[2019-03-27 02:07:55,930] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1178439: loss -21.4093
[2019-03-27 02:07:55,935] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1178441: learning rate 0.0005
[2019-03-27 02:07:55,983] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1178464: loss -19.8289
[2019-03-27 02:07:55,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1178464: learning rate 0.0005
[2019-03-27 02:07:56,289] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1178597: loss 78.0586
[2019-03-27 02:07:56,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1178597: learning rate 0.0005
[2019-03-27 02:07:56,405] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1178649: loss 22.8859
[2019-03-27 02:07:56,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1178650: learning rate 0.0005
[2019-03-27 02:07:57,118] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1178966: loss 0.2714
[2019-03-27 02:07:57,121] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1178966: learning rate 0.0005
[2019-03-27 02:08:00,154] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1180291: loss 374.1910
[2019-03-27 02:08:00,155] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1180291: learning rate 0.0005
[2019-03-27 02:08:00,730] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1180545: loss 4.3516
[2019-03-27 02:08:00,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1180547: learning rate 0.0005
[2019-03-27 02:08:00,786] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1180572: loss 1.0594
[2019-03-27 02:08:00,795] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1180575: learning rate 0.0005
[2019-03-27 02:08:01,221] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1180768: loss 1.8786
[2019-03-27 02:08:01,224] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1180769: learning rate 0.0005
[2019-03-27 02:08:02,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2375640e-35 1.0000000e+00 0.0000000e+00 1.3819752e-36 1.4302345e-36], sum to 1.0000
[2019-03-27 02:08:02,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1694
[2019-03-27 02:08:02,386] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7433204213897285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1038841.079339711, 1038841.079339712, 230473.8937004936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6741954722451413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 942191.3845995613, 942191.3845995606, 215374.1392244629], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.6074644243917365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26171982905543373, 0.2617198290554335, 0.3214539391409894], 
reward next is 0.6785, 
noisyNet noise sample is [array([1.0910909], dtype=float32), 0.021025911]. 
=============================================
[2019-03-27 02:08:03,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:08:03,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1551
[2019-03-27 02:08:03,952] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162255930472541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721352.7595897908, 721352.7595897908, 186176.7387330674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4490400.0000, 
sim time next is 4491000.0000, 
raw observation next is [26.5, 86.5, 1.0, 2.0, 0.5142657200147461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718613.1859502372, 718613.1859502365, 185860.829198575], 
processed observation next is [0.0, 1.0, 0.4549763033175356, 0.865, 1.0, 1.0, 0.4147779759213808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1996147738750659, 0.1996147738750657, 0.2774042226844403], 
reward next is 0.7226, 
noisyNet noise sample is [array([1.1975913], dtype=float32), -1.8777895]. 
=============================================
[2019-03-27 02:08:03,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.73    ]
 [79.59623 ]
 [79.443855]
 [79.15445 ]
 [79.01986 ]], R is [[79.77592468]
 [79.70028687]
 [79.62490082]
 [79.54956055]
 [79.47366333]].
[2019-03-27 02:08:05,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3303612e-23 1.0000000e+00 5.2208825e-33 2.5871921e-22 3.7948568e-19], sum to 1.0000
[2019-03-27 02:08:05,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2700
[2019-03-27 02:08:05,271] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 70.0, 1.0, 2.0, 0.5168445327871081, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722217.9349581592, 722217.9349581586, 186280.3508713248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4728000.0000, 
sim time next is 4728600.0000, 
raw observation next is [30.5, 70.0, 1.0, 2.0, 0.5069605332519705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708401.8228894348, 708401.8228894342, 184697.0564013602], 
processed observation next is [1.0, 0.7391304347826086, 0.6445497630331753, 0.7, 1.0, 1.0, 0.40597654608671147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1967782841359541, 0.19677828413595394, 0.2756672483602391], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.87279874], dtype=float32), 0.9547765]. 
=============================================
[2019-03-27 02:08:05,559] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1182702: loss 3.0809
[2019-03-27 02:08:05,562] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1182704: learning rate 0.0005
[2019-03-27 02:08:09,457] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1184444: loss 0.0862
[2019-03-27 02:08:09,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1184444: learning rate 0.0005
[2019-03-27 02:08:10,192] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1184768: loss 0.0933
[2019-03-27 02:08:10,193] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1184768: learning rate 0.0005
[2019-03-27 02:08:12,155] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1185646: loss 0.0818
[2019-03-27 02:08:12,157] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1185647: learning rate 0.0005
[2019-03-27 02:08:13,110] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1186072: loss 0.0466
[2019-03-27 02:08:13,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1186073: learning rate 0.0005
[2019-03-27 02:08:13,678] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1186325: loss 0.0688
[2019-03-27 02:08:13,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1186325: learning rate 0.0005
[2019-03-27 02:08:13,887] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1186399: loss 0.4207
[2019-03-27 02:08:13,888] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1186399: learning rate 0.0005
[2019-03-27 02:08:13,924] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1186413: loss 0.1392
[2019-03-27 02:08:13,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1186413: learning rate 0.0005
[2019-03-27 02:08:14,054] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1186472: loss 0.2588
[2019-03-27 02:08:14,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1186473: learning rate 0.0005
[2019-03-27 02:08:14,447] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1186646: loss 0.5960
[2019-03-27 02:08:14,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1186647: learning rate 0.0005
[2019-03-27 02:08:14,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1188647e-09 9.7576761e-01 1.0812997e-13 1.4626625e-07 2.4232207e-02], sum to 1.0000
[2019-03-27 02:08:14,479] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7399
[2019-03-27 02:08:14,491] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2396552.054932247 W.
[2019-03-27 02:08:14,495] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 0.5712484377708563, 1.0, 2.0, 0.5712484377708563, 1.0, 2.0, 0.99206962166282, 6.911199999999999, 6.9112, 170.5573041426782, 2396552.054932247, 2396552.054932248, 467911.7280979324], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4726200.0000, 
sim time next is 4726800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.8493053740661922, 1.0, 2.0, 0.8493053740661922, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2375367.316592513, 2375367.316592513, 444590.2172391167], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.8184402097183039, 1.0, 1.0, 0.8184402097183039, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6598242546090314, 0.6598242546090314, 0.6635674884165921], 
reward next is 0.3364, 
noisyNet noise sample is [array([0.743425], dtype=float32), 0.48887962]. 
=============================================
[2019-03-27 02:08:14,587] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1186710: loss 0.0325
[2019-03-27 02:08:14,589] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1186710: learning rate 0.0005
[2019-03-27 02:08:15,172] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1186969: loss 0.4926
[2019-03-27 02:08:15,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1186970: learning rate 0.0005
[2019-03-27 02:08:16,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:08:16,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8452
[2019-03-27 02:08:16,529] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 78.33333333333333, 1.0, 2.0, 0.4889720010529854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683257.4450206095, 683257.4450206102, 181885.9344141895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4837200.0000, 
sim time next is 4837800.0000, 
raw observation next is [27.08333333333333, 78.66666666666667, 1.0, 2.0, 0.4882767536349127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682285.6398334254, 682285.6398334247, 181779.3175101378], 
processed observation next is [1.0, 1.0, 0.4826224328593995, 0.7866666666666667, 1.0, 1.0, 0.3834659682348346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18952378884261817, 0.18952378884261797, 0.2713124141942355], 
reward next is 0.7287, 
noisyNet noise sample is [array([0.2781435], dtype=float32), 0.21439989]. 
=============================================
[2019-03-27 02:08:18,087] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1188263: loss 0.0723
[2019-03-27 02:08:18,089] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1188263: learning rate 0.0005
[2019-03-27 02:08:18,704] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1188538: loss 173.6978
[2019-03-27 02:08:18,706] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1188539: learning rate 0.0005
[2019-03-27 02:08:18,880] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1188616: loss 201.3465
[2019-03-27 02:08:18,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1188617: learning rate 0.0005
[2019-03-27 02:08:19,397] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1188841: loss 180.5265
[2019-03-27 02:08:19,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1188845: learning rate 0.0005
[2019-03-27 02:08:22,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0706953e-13 9.9999845e-01 1.6016454e-19 3.0841882e-10 1.5414921e-06], sum to 1.0000
[2019-03-27 02:08:22,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0079
[2019-03-27 02:08:22,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2109008.031865223 W.
[2019-03-27 02:08:22,154] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7541549143494152, 1.0, 2.0, 0.7541549143494152, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2109008.031865223, 2109008.031865224, 398168.255545913], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4789200.0000, 
sim time next is 4789800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4914520794058372, 1.0, 2.0, 0.4914520794058372, 1.0, 1.0, 0.8504419123906382, 6.911199999999999, 6.9112, 170.5573041426782, 2061486.431409194, 2061486.431409194, 408746.0373100652], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3872916619347436, 1.0, 1.0, 0.3872916619347436, 1.0, 0.5, 0.8176120882812661, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5726351198358872, 0.5726351198358872, 0.6100687124030824], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07813276], dtype=float32), 1.3525627]. 
=============================================
[2019-03-27 02:08:23,405] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1190628: loss 297.2607
[2019-03-27 02:08:23,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1190629: learning rate 0.0005
[2019-03-27 02:08:24,902] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.352508e-34 0.000000e+00], sum to 1.0000
[2019-03-27 02:08:24,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4931
[2019-03-27 02:08:24,916] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.83333333333334, 1.0, 2.0, 0.4903505526481806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685184.3639620292, 685184.3639620292, 182097.7699068118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4914600.0000, 
sim time next is 4915200.0000, 
raw observation next is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.8066666666666668, 1.0, 1.0, 0.3924437600539641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1924170449651395, 0.1924170449651395, 0.27302871357898206], 
reward next is 0.7270, 
noisyNet noise sample is [array([0.34559605], dtype=float32), -1.243799]. 
=============================================
[2019-03-27 02:08:27,303] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1192373: loss 117.0959
[2019-03-27 02:08:27,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1192374: learning rate 0.0005
[2019-03-27 02:08:27,953] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1192659: loss 76.4509
[2019-03-27 02:08:27,956] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1192659: learning rate 0.0005
[2019-03-27 02:08:28,324] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1990736e-13 2.8658790e-12 8.4952852e-22 2.2656068e-11 1.0000000e+00], sum to 1.0000
[2019-03-27 02:08:28,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3154
[2019-03-27 02:08:28,337] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.58333333333333, 64.66666666666667, 1.0, 2.0, 0.5499447445363397, 1.0, 2.0, 0.5499447445363397, 1.0, 2.0, 0.9550721517534749, 6.9112, 6.9112, 170.5573041426782, 2307094.412804534, 2307094.412804534, 451401.6294023165], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4891800.0000, 
sim time next is 4892400.0000, 
raw observation next is [31.5, 65.0, 1.0, 2.0, 0.5541630145441088, 1.0, 2.0, 0.5541630145441088, 1.0, 2.0, 0.9623978917537613, 6.911200000000001, 6.9112, 170.5573041426782, 2324807.100113358, 2324807.100113357, 454621.0360776267], 
processed observation next is [1.0, 0.6521739130434783, 0.6919431279620853, 0.65, 1.0, 1.0, 0.4628470054748298, 1.0, 1.0, 0.4628470054748298, 1.0, 1.0, 0.9541437704314162, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6457797500314884, 0.645779750031488, 0.6785388598173533], 
reward next is 0.3215, 
noisyNet noise sample is [array([-0.62380403], dtype=float32), 0.69456685]. 
=============================================
[2019-03-27 02:08:30,056] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1193579: loss 58.3330
[2019-03-27 02:08:30,061] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1193580: learning rate 0.0005
[2019-03-27 02:08:31,270] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1194117: loss 67.4411
[2019-03-27 02:08:31,275] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1194119: learning rate 0.0005
[2019-03-27 02:08:31,934] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1194417: loss 37.9777
[2019-03-27 02:08:31,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1194417: learning rate 0.0005
[2019-03-27 02:08:31,971] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1194432: loss 37.9228
[2019-03-27 02:08:31,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1194432: learning rate 0.0005
[2019-03-27 02:08:32,116] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1194496: loss 44.9340
[2019-03-27 02:08:32,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1194496: learning rate 0.0005
[2019-03-27 02:08:32,237] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1194549: loss 75.4020
[2019-03-27 02:08:32,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1194550: learning rate 0.0005
[2019-03-27 02:08:32,362] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1194604: loss 36.7568
[2019-03-27 02:08:32,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1194606: learning rate 0.0005
[2019-03-27 02:08:32,795] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1194796: loss 51.7854
[2019-03-27 02:08:32,796] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1194796: learning rate 0.0005
[2019-03-27 02:08:33,063] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1194917: loss 80.8491
[2019-03-27 02:08:33,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1194917: learning rate 0.0005
[2019-03-27 02:08:36,057] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1196256: loss 1.8153
[2019-03-27 02:08:36,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1196256: learning rate 0.0005
[2019-03-27 02:08:36,698] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1196538: loss 0.9493
[2019-03-27 02:08:36,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1196539: learning rate 0.0005
[2019-03-27 02:08:36,862] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1196613: loss 1.0194
[2019-03-27 02:08:36,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1196613: learning rate 0.0005
[2019-03-27 02:08:37,305] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1196804: loss 0.2685
[2019-03-27 02:08:37,307] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1196806: learning rate 0.0005
[2019-03-27 02:08:40,506] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5978700e-36 1.0000000e+00 0.0000000e+00 1.3620806e-30 2.8637310e-35], sum to 1.0000
[2019-03-27 02:08:40,515] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9021
[2019-03-27 02:08:40,519] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162498980852747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721386.7339989959, 721386.7339989964, 186180.6622535018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5098800.0000, 
sim time next is 5099400.0000, 
raw observation next is [26.5, 86.5, 1.0, 2.0, 0.5146236257301708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719113.4775577422, 719113.4775577415, 185918.4154986019], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.865, 1.0, 1.0, 0.41520918762671183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1997537437660395, 0.1997537437660393, 0.277490172385973], 
reward next is 0.7225, 
noisyNet noise sample is [array([0.61785746], dtype=float32), 0.3854312]. 
=============================================
[2019-03-27 02:08:41,517] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1198682: loss 0.2109
[2019-03-27 02:08:41,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1198682: learning rate 0.0005
[2019-03-27 02:08:42,007] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2328604e-36 1.0000000e+00 0.0000000e+00 5.3585157e-32 1.8402865e-36], sum to 1.0000
[2019-03-27 02:08:42,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0166
[2019-03-27 02:08:42,019] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 69.0, 1.0, 2.0, 0.6177085002161556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863218.4915319238, 863218.4915319243, 204092.6684706311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5338800.0000, 
sim time next is 5339400.0000, 
raw observation next is [32.75, 70.5, 1.0, 2.0, 0.6219128087265359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869096.218187149, 869096.218187149, 204900.415946984], 
processed observation next is [1.0, 0.8260869565217391, 0.7511848341232228, 0.705, 1.0, 1.0, 0.5444732635259468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24141561616309695, 0.24141561616309695, 0.3058215163387821], 
reward next is 0.6942, 
noisyNet noise sample is [array([0.8637332], dtype=float32), 0.9070783]. 
=============================================
[2019-03-27 02:08:44,335] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7464604e-34 3.9712978e-38], sum to 1.0000
[2019-03-27 02:08:44,345] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9743
[2019-03-27 02:08:44,353] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5407264224986249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755601.404290123, 755601.404290123, 190223.2730833222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5161200.0000, 
sim time next is 5161800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5406710671499779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755524.0242093195, 755524.0242093201, 190213.936129436], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4465916471686481, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20986778450258875, 0.20986778450258892, 0.28390139720811347], 
reward next is 0.7161, 
noisyNet noise sample is [array([0.2953768], dtype=float32), -0.9210433]. 
=============================================
[2019-03-27 02:08:44,462] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 02:08:44,464] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:08:44,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:08:44,466] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:08:44,468] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:08:44,470] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:08:44,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:08:44,475] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:08:44,476] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:08:44,467] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:08:44,476] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:08:44,501] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-27 02:08:44,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-27 02:08:44,518] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-27 02:08:44,556] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-27 02:08:44,557] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-27 02:08:46,652] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:08:46,653] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.54505642333334, 87.21905446333334, 1.0, 2.0, 0.4842959215230457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678079.4863688071, 678079.4863688071, 181346.841024922]
[2019-03-27 02:08:46,656] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:08:46,661] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.840118e-36 0.000000e+00], sampled 0.7063370433464734
[2019-03-27 02:09:10,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:09:10,705] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.13333333333334, 86.0, 1.0, 2.0, 0.8258029945387197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1225044.500419657, 1225044.500419656, 260161.9662070169]
[2019-03-27 02:09:10,706] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:09:10,709] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6159277e-33 1.0000000e+00 0.0000000e+00 1.8738498e-29 1.3571727e-32], sampled 0.795179984805709
[2019-03-27 02:09:17,238] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:09:17,239] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.1, 83.0, 1.0, 2.0, 0.5417498416376717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757032.0214540289, 757032.0214540289, 190396.1012867672]
[2019-03-27 02:09:17,241] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:09:17,243] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.273221e-37 0.000000e+00], sampled 0.00524990007661752
[2019-03-27 02:09:19,834] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:09:19,835] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.8, 76.0, 1.0, 2.0, 0.3440691208925932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538817.7175293899, 538817.7175293899, 169945.5626328367]
[2019-03-27 02:09:19,837] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:09:19,841] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.29606972531315945
[2019-03-27 02:09:22,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:09:22,730] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.31666666666667, 70.83333333333333, 1.0, 2.0, 0.7829648808636912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1094275.467849884, 1094275.467849883, 239768.7751734418]
[2019-03-27 02:09:22,730] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:09:22,734] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00958905e-32 1.00000000e+00 0.00000000e+00 6.17593432e-29
 4.46196467e-32], sampled 0.02971477624077601
[2019-03-27 02:09:59,294] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:09:59,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.8500373, 92.18774136, 1.0, 2.0, 0.6065663692823344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847641.6758975792, 847641.6758975792, 201976.8017158596]
[2019-03-27 02:09:59,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:09:59,298] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.480698e-36 0.000000e+00], sampled 0.16236158344230955
[2019-03-27 02:10:14,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:10:14,836] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.3, 85.83333333333334, 1.0, 2.0, 0.5321450761777206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743605.7875573632, 743605.7875573632, 188785.7250457925]
[2019-03-27 02:10:14,838] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:10:14,841] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2622986e-35 0.0000000e+00], sampled 0.4693814299966952
[2019-03-27 02:10:29,378] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:10:29,378] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.58006259333333, 95.01221419333334, 1.0, 2.0, 0.2847340959938893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462445.5423862468, 462445.5423862468, 164404.6808452514]
[2019-03-27 02:10:29,379] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:10:29,380] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6904356555418705
[2019-03-27 02:10:32,932] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.048062], dtype=float32), -0.04263108]
[2019-03-27 02:10:32,934] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.95137052, 91.68462012500001, 1.0, 2.0, 0.4988895392547442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697120.1104743284, 697120.1104743284, 183420.7545489881]
[2019-03-27 02:10:32,936] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:10:32,940] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9617348e-38 0.0000000e+00], sampled 0.18370024805156315
[2019-03-27 02:10:38,633] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.3373 2927903971.8801 1363.0000
[2019-03-27 02:10:38,823] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7873.7989 3164332320.5455 1843.0000
[2019-03-27 02:10:39,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.2164 2779326485.5244 930.0000
[2019-03-27 02:10:39,349] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0432 3007656547.9164 1771.0000
[2019-03-27 02:10:39,535] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.2665 2842217628.4307 1143.0000
[2019-03-27 02:10:40,551] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1200000, evaluation results [1200000.0, 7873.798917120696, 3164332320.5455246, 1843.0, 8249.337305837787, 2927903971.8800573, 1363.0, 8664.216373601446, 2779326485.5243645, 930.0, 7999.043237261197, 3007656547.916405, 1771.0, 8495.26647822116, 2842217628.430676, 1143.0]
[2019-03-27 02:10:41,590] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1200470: loss 0.1556
[2019-03-27 02:10:41,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1200470: learning rate 0.0005
[2019-03-27 02:10:42,148] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1200716: loss 0.2400
[2019-03-27 02:10:42,153] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1200716: learning rate 0.0005
[2019-03-27 02:10:42,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3238603e-21 1.0000000e+00 9.2643165e-29 1.4794475e-18 2.2907402e-18], sum to 1.0000
[2019-03-27 02:10:42,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6195
[2019-03-27 02:10:42,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1702428.66006339 W.
[2019-03-27 02:10:42,205] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.4059217522745727, 1.0, 1.0, 0.4059217522745727, 1.0, 1.0, 0.6927382427246395, 6.9112, 6.9112, 170.5573041426782, 1702428.66006339, 1702428.66006339, 354435.8400159031], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5192400.0000, 
sim time next is 5193000.0000, 
raw observation next is [26.5, 86.5, 1.0, 2.0, 0.3586940990072854, 1.0, 2.0, 0.3586940990072854, 1.0, 2.0, 0.6101828481071936, 6.911199999999999, 6.9112, 170.5573041426782, 1504217.71283254, 1504217.712832541, 329616.1813901248], 
processed observation next is [1.0, 0.08695652173913043, 0.4549763033175356, 0.865, 1.0, 1.0, 0.22734228796058478, 1.0, 1.0, 0.22734228796058478, 1.0, 1.0, 0.5246132293990166, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4178382535645945, 0.4178382535645947, 0.4919644498360071], 
reward next is 0.5080, 
noisyNet noise sample is [array([-0.61829954], dtype=float32), 1.2254243]. 
=============================================
[2019-03-27 02:10:42,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[38.270367]
 [50.539597]
 [56.70368 ]
 [55.931057]
 [55.845856]], R is [[34.24726105]
 [34.37578201]
 [34.03202438]
 [34.41399765]
 [34.79199982]].
[2019-03-27 02:10:44,100] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1201587: loss 2.5219
[2019-03-27 02:10:44,103] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1201587: learning rate 0.0005
[2019-03-27 02:10:44,632] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3423825e-23 1.0000000e+00 7.4539037e-33 6.8618954e-22 3.9661523e-22], sum to 1.0000
[2019-03-27 02:10:44,640] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8876
[2019-03-27 02:10:44,650] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1888625.464085743 W.
[2019-03-27 02:10:44,655] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.36666666666667, 81.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.523775644784932, 6.9112, 168.9093130523271, 1888625.464085743, 1454052.592402264, 311356.4664007935], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5298600.0000, 
sim time next is 5299200.0000, 
raw observation next is [30.5, 81.0, 1.0, 2.0, 0.5513741271154579, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9575545165893609, 6.9112, 6.9112, 168.9124576815749, 1541530.720689799, 1541530.720689799, 337436.7257952494], 
processed observation next is [1.0, 0.34782608695652173, 0.6445497630331753, 0.81, 1.0, 1.0, 0.4594869001391058, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9482372153528791, 0.0, 0.0, 0.8294374956742544, 0.4282029779693886, 0.4282029779693886, 0.503636904172014], 
reward next is 0.4964, 
noisyNet noise sample is [array([-0.5238705], dtype=float32), -1.9454782]. 
=============================================
[2019-03-27 02:10:45,289] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1202111: loss 2.0374
[2019-03-27 02:10:45,292] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1202113: learning rate 0.0005
[2019-03-27 02:10:46,082] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1202463: loss 6.3656
[2019-03-27 02:10:46,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1202463: learning rate 0.0005
[2019-03-27 02:10:46,123] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1202480: loss 3.1699
[2019-03-27 02:10:46,126] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1202480: learning rate 0.0005
[2019-03-27 02:10:46,180] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1202504: loss 9.5988
[2019-03-27 02:10:46,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1202504: learning rate 0.0005
[2019-03-27 02:10:46,278] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1202550: loss 2.6396
[2019-03-27 02:10:46,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1202550: learning rate 0.0005
[2019-03-27 02:10:46,394] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1202599: loss 2.1335
[2019-03-27 02:10:46,395] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1202600: learning rate 0.0005
[2019-03-27 02:10:46,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7708533e-33 1.0000000e+00 0.0000000e+00 1.8352324e-28 1.7214804e-33], sum to 1.0000
[2019-03-27 02:10:46,421] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7599
[2019-03-27 02:10:46,428] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 79.5, 1.0, 2.0, 0.5479810284523111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765742.5145672448, 765742.5145672441, 191454.9592761847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5257800.0000, 
sim time next is 5258400.0000, 
raw observation next is [28.66666666666667, 79.66666666666666, 1.0, 2.0, 0.5469552025755323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764308.521315797, 764308.5213157964, 191279.8565869479], 
processed observation next is [1.0, 0.8695652173913043, 0.5576619273301741, 0.7966666666666665, 1.0, 1.0, 0.45416289466931603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21230792258772138, 0.21230792258772122, 0.28549232326410134], 
reward next is 0.7145, 
noisyNet noise sample is [array([1.0208817], dtype=float32), -0.19909324]. 
=============================================
[2019-03-27 02:10:46,817] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1202789: loss 0.6106
[2019-03-27 02:10:46,819] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1202789: learning rate 0.0005
[2019-03-27 02:10:46,950] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1202850: loss 1.4262
[2019-03-27 02:10:46,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1202850: learning rate 0.0005
[2019-03-27 02:10:49,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5770174e-07 9.9919027e-01 3.6269213e-11 3.7530265e-06 8.0555509e-04], sum to 1.0000
[2019-03-27 02:10:49,691] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0355
[2019-03-27 02:10:49,700] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2744449.209693441 W.
[2019-03-27 02:10:49,706] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.13333333333333, 66.66666666666667, 1.0, 2.0, 0.666986205111933, 1.0, 2.0, 0.6540831420702291, 1.0, 1.0, 1.03, 7.005095129193382, 6.9112, 170.5573041426782, 2744449.209693441, 2677188.309242563, 511104.7571156467], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5304000.0000, 
sim time next is 5304600.0000, 
raw observation next is [33.45, 65.0, 1.0, 2.0, 0.9665533369276813, 1.0, 2.0, 0.9665533369276813, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2703645.350867784, 2703645.350867785, 509109.6547239842], 
processed observation next is [1.0, 0.391304347826087, 0.7843601895734599, 0.65, 1.0, 1.0, 0.9597028155755196, 1.0, 1.0, 0.9597028155755196, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7510125974632733, 0.7510125974632736, 0.7598651563044541], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7399941], dtype=float32), 0.38037324]. 
=============================================
[2019-03-27 02:10:49,878] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1204157: loss 3.0453
[2019-03-27 02:10:49,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1204157: learning rate 0.0005
[2019-03-27 02:10:50,575] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1204467: loss 4.9327
[2019-03-27 02:10:50,577] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1204467: learning rate 0.0005
[2019-03-27 02:10:50,986] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1204652: loss 5.9181
[2019-03-27 02:10:50,988] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1204652: learning rate 0.0005
[2019-03-27 02:10:51,511] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1204879: loss 5.0971
[2019-03-27 02:10:51,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1204880: learning rate 0.0005
[2019-03-27 02:10:55,478] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1206660: loss 2.5826
[2019-03-27 02:10:55,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1206660: learning rate 0.0005
[2019-03-27 02:10:59,242] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1208340: loss 3.8828
[2019-03-27 02:10:59,245] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1208340: learning rate 0.0005
[2019-03-27 02:11:00,031] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1208692: loss 2.7725
[2019-03-27 02:11:00,034] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1208693: learning rate 0.0005
[2019-03-27 02:11:02,034] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1209580: loss 2.6628
[2019-03-27 02:11:02,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1209580: learning rate 0.0005
[2019-03-27 02:11:03,103] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1210061: loss 5.9562
[2019-03-27 02:11:03,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1210062: learning rate 0.0005
[2019-03-27 02:11:04,005] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1210470: loss 5.2823
[2019-03-27 02:11:04,009] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1210471: learning rate 0.0005
[2019-03-27 02:11:04,062] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1210493: loss 2.5676
[2019-03-27 02:11:04,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1210493: learning rate 0.0005
[2019-03-27 02:11:04,081] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:11:04,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8903
[2019-03-27 02:11:04,097] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.93333333333334, 89.33333333333334, 1.0, 2.0, 0.4997674207621288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698347.215813829, 698347.215813829, 183559.6003165944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5635200.0000, 
sim time next is 5635800.0000, 
raw observation next is [26.1, 88.5, 1.0, 2.0, 0.5009055488701574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699938.0967232737, 699938.0967232744, 183738.0839278941], 
processed observation next is [0.0, 0.21739130434782608, 0.4360189573459717, 0.885, 1.0, 1.0, 0.3986813841809125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19442724908979828, 0.19442724908979844, 0.27423594616103597], 
reward next is 0.7258, 
noisyNet noise sample is [array([0.00035257], dtype=float32), -1.1638772]. 
=============================================
[2019-03-27 02:11:04,153] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:11:04,165] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1985
[2019-03-27 02:11:04,169] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.5461227528156555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763144.8483131959, 763144.8483131966, 191137.5523005457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5781000.0000, 
sim time next is 5781600.0000, 
raw observation next is [27.5, 86.0, 1.0, 2.0, 0.5443056818944126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760604.7878911024, 760604.7878911024, 190828.5858248018], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.86, 1.0, 1.0, 0.45097070107760556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21127910774752845, 0.21127910774752845, 0.284818784813137], 
reward next is 0.7152, 
noisyNet noise sample is [array([-1.1898501], dtype=float32), 0.86495876]. 
=============================================
[2019-03-27 02:11:04,209] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1210556: loss 4.4114
[2019-03-27 02:11:04,210] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1210556: learning rate 0.0005
[2019-03-27 02:11:04,243] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1210569: loss 4.0651
[2019-03-27 02:11:04,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1210569: learning rate 0.0005
[2019-03-27 02:11:04,448] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1210659: loss 1.8540
[2019-03-27 02:11:04,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1210659: learning rate 0.0005
[2019-03-27 02:11:04,914] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1210867: loss 4.4012
[2019-03-27 02:11:04,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1210867: learning rate 0.0005
[2019-03-27 02:11:04,926] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1210871: loss 3.7639
[2019-03-27 02:11:04,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1210873: learning rate 0.0005
[2019-03-27 02:11:07,714] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1212115: loss 5.2913
[2019-03-27 02:11:07,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1212117: learning rate 0.0005
[2019-03-27 02:11:08,120] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1212301: loss 4.6930
[2019-03-27 02:11:08,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1212302: learning rate 0.0005
[2019-03-27 02:11:08,601] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1212515: loss 2.2642
[2019-03-27 02:11:08,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1212515: learning rate 0.0005
[2019-03-27 02:11:09,229] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1212794: loss 3.7074
[2019-03-27 02:11:09,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1212794: learning rate 0.0005
[2019-03-27 02:11:11,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:11:11,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8336
[2019-03-27 02:11:11,888] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666666, 89.66666666666667, 1.0, 2.0, 0.5205903980642519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727454.0499943167, 727454.0499943167, 186885.3165544713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6223200.0000, 
sim time next is 6223800.0000, 
raw observation next is [26.53333333333333, 89.83333333333333, 1.0, 2.0, 0.520294911051758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727041.005907795, 727041.0059077957, 186837.2215072147], 
processed observation next is [0.0, 0.0, 0.45655608214849913, 0.8983333333333333, 1.0, 1.0, 0.4220420615081421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20195583497438752, 0.2019558349743877, 0.2788615246376339], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.18619287], dtype=float32), 1.1245434]. 
=============================================
[2019-03-27 02:11:13,349] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1214628: loss 1.0397
[2019-03-27 02:11:13,352] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1214630: learning rate 0.0005
[2019-03-27 02:11:16,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:11:16,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3565
[2019-03-27 02:11:16,700] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 86.66666666666667, 1.0, 2.0, 0.5397328615130403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754212.5270855926, 754212.527085592, 190055.2146745392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5784000.0000, 
sim time next is 5784600.0000, 
raw observation next is [27.25, 86.83333333333333, 1.0, 2.0, 0.5387466443216276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752833.9174064758, 752833.917406475, 189889.2241655562], 
processed observation next is [0.0, 0.9565217391304348, 0.490521327014218, 0.8683333333333333, 1.0, 1.0, 0.44427306544774403, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20912053261290994, 0.20912053261290975, 0.28341675248590475], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.8263387], dtype=float32), 0.48000997]. 
=============================================
[2019-03-27 02:11:17,128] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1216319: loss 0.9675
[2019-03-27 02:11:17,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1216319: learning rate 0.0005
[2019-03-27 02:11:18,207] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1216774: loss 5.6687
[2019-03-27 02:11:18,211] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1216775: learning rate 0.0005
[2019-03-27 02:11:20,151] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1217644: loss 4.8982
[2019-03-27 02:11:20,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1217645: learning rate 0.0005
[2019-03-27 02:11:20,957] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1218001: loss 4.1164
[2019-03-27 02:11:20,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1218002: learning rate 0.0005
[2019-03-27 02:11:22,018] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1218472: loss 0.0545
[2019-03-27 02:11:22,019] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1218472: learning rate 0.0005
[2019-03-27 02:11:22,117] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1218516: loss 1.8432
[2019-03-27 02:11:22,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1218516: learning rate 0.0005
[2019-03-27 02:11:22,295] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1218595: loss 0.6117
[2019-03-27 02:11:22,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1218595: learning rate 0.0005
[2019-03-27 02:11:22,321] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1218606: loss 0.5465
[2019-03-27 02:11:22,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1218606: learning rate 0.0005
[2019-03-27 02:11:22,665] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1218760: loss 0.1302
[2019-03-27 02:11:22,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1218760: learning rate 0.0005
[2019-03-27 02:11:23,054] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1218931: loss 0.3425
[2019-03-27 02:11:23,057] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1218932: learning rate 0.0005
[2019-03-27 02:11:23,071] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1218936: loss 0.1157
[2019-03-27 02:11:23,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1218936: learning rate 0.0005
[2019-03-27 02:11:25,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5336027e-07 9.3458796e-01 9.1559251e-12 2.8503630e-06 6.5408945e-02], sum to 1.0000
[2019-03-27 02:11:25,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0819
[2019-03-27 02:11:25,570] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1220055: loss 493.4857
[2019-03-27 02:11:25,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1220056: learning rate 0.0005
[2019-03-27 02:11:25,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2673798.595438523 W.
[2019-03-27 02:11:25,586] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.26666666666667, 73.33333333333334, 1.0, 2.0, 0.9558945264342966, 1.0, 1.0, 0.9558945264342966, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2673798.595438523, 2673798.595438523, 502907.7874754378], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5911800.0000, 
sim time next is 5912400.0000, 
raw observation next is [31.43333333333334, 72.66666666666667, 1.0, 2.0, 0.5635547493481208, 1.0, 2.0, 0.5635547493481208, 1.0, 1.0, 0.9787082292141696, 6.9112, 6.9112, 170.5573041426782, 2364244.278241113, 2364244.278241113, 461878.2661652059], 
processed observation next is [1.0, 0.43478260869565216, 0.6887835703001584, 0.7266666666666667, 1.0, 1.0, 0.4741623486121937, 1.0, 1.0, 0.4741623486121937, 1.0, 0.5, 0.9740344258709385, 0.0, 0.0, 0.8375144448122397, 0.6567345217336424, 0.6567345217336424, 0.6893705465152327], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.10014635], dtype=float32), -0.03498441]. 
=============================================
[2019-03-27 02:11:26,194] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1220334: loss 4.8691
[2019-03-27 02:11:26,198] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1220335: learning rate 0.0005
[2019-03-27 02:11:26,728] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1220571: loss 4.4297
[2019-03-27 02:11:26,730] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1220572: learning rate 0.0005
[2019-03-27 02:11:27,269] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1220805: loss 6.5213
[2019-03-27 02:11:27,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1220806: learning rate 0.0005
[2019-03-27 02:11:30,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7283356e-27 1.0000000e+00 5.5113113e-37 2.5347027e-23 1.8952975e-26], sum to 1.0000
[2019-03-27 02:11:30,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6477
[2019-03-27 02:11:30,146] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 85.0, 1.0, 2.0, 0.7155424474625988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1000001.184209395, 1000001.184209395, 224236.5561045737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6073200.0000, 
sim time next is 6073800.0000, 
raw observation next is [27.93333333333333, 84.33333333333334, 1.0, 2.0, 0.6946400925051264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970775.8972965084, 970775.8972965084, 219693.3994104363], 
processed observation next is [1.0, 0.30434782608695654, 0.522906793048973, 0.8433333333333334, 1.0, 1.0, 0.6320964969941282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2696599714712523, 0.2696599714712523, 0.32790059613497957], 
reward next is 0.6721, 
noisyNet noise sample is [array([-0.04158074], dtype=float32), -1.773531]. 
=============================================
[2019-03-27 02:11:31,278] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1222606: loss 0.3751
[2019-03-27 02:11:31,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1222606: learning rate 0.0005
[2019-03-27 02:11:33,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0751227e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 02:11:33,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6545
[2019-03-27 02:11:33,569] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 78.0, 1.0, 2.0, 0.5386367175919351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752680.2535426113, 752680.2535426107, 189871.5060304962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6030000.0000, 
sim time next is 6030600.0000, 
raw observation next is [28.73333333333333, 79.0, 1.0, 2.0, 0.5401148189327852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754746.4568066583, 754746.4568066589, 190120.2302943042], 
processed observation next is [1.0, 0.8260869565217391, 0.560821484992101, 0.79, 1.0, 1.0, 0.4459214685937171, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2096517935574051, 0.20965179355740524, 0.28376153775269286], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.3597818], dtype=float32), -0.39376852]. 
=============================================
[2019-03-27 02:11:34,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4790709e-22 1.0000000e+00 4.4878776e-32 9.6393594e-21 4.6391205e-19], sum to 1.0000
[2019-03-27 02:11:35,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7377
[2019-03-27 02:11:35,007] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 85.66666666666667, 1.0, 2.0, 0.7740402501575597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1081796.00769756, 1081796.00769756, 237638.9870007362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6072600.0000, 
sim time next is 6073200.0000, 
raw observation next is [27.8, 85.0, 1.0, 2.0, 0.7155424474625988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1000001.184209395, 1000001.184209395, 224236.5561045737], 
processed observation next is [1.0, 0.30434782608695654, 0.5165876777251186, 0.85, 1.0, 1.0, 0.657280057183854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27777810672483194, 0.27777810672483194, 0.3346814270217518], 
reward next is 0.6653, 
noisyNet noise sample is [array([-1.2323358], dtype=float32), -1.4810377]. 
=============================================
[2019-03-27 02:11:35,073] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1224295: loss 0.3310
[2019-03-27 02:11:35,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1224295: learning rate 0.0005
[2019-03-27 02:11:36,198] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1224790: loss 3.6955
[2019-03-27 02:11:36,201] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1224790: learning rate 0.0005
[2019-03-27 02:11:36,670] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 02:11:36,672] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:11:36,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:11:36,673] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:11:36,675] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:11:36,676] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:11:36,678] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:11:36,678] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:11:36,679] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:11:36,679] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:11:36,680] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:11:36,699] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-27 02:11:36,720] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-27 02:11:36,721] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-27 02:11:36,721] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-27 02:11:36,780] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-27 02:11:38,839] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:11:38,840] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.96666666666667, 85.66666666666667, 1.0, 2.0, 0.3557073192929176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549347.9638717785, 549347.9638717785, 170629.8965344804]
[2019-03-27 02:11:38,843] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:11:38,844] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32844567299409066
[2019-03-27 02:12:12,889] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:12:12,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.33333333333333, 90.50000000000001, 1.0, 2.0, 0.3959051066532551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593443.2477506171, 593443.2477506171, 174012.8888063339]
[2019-03-27 02:12:12,895] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:12:12,898] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4210003e-36 0.0000000e+00], sampled 0.2337937508501512
[2019-03-27 02:12:28,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:12:28,753] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.66666666666666, 80.33333333333334, 1.0, 2.0, 0.6953607343293676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 971783.472008448, 971783.4720084474, 219843.3529516213]
[2019-03-27 02:12:28,753] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:12:28,756] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.3996725e-34 1.0000000e+00 0.0000000e+00 2.3672583e-30 2.6336819e-32], sampled 0.5448198047559709
[2019-03-27 02:12:54,358] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:12:54,360] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.91666666666666, 79.83333333333334, 1.0, 2.0, 0.8176909683108255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1142834.93147943, 1142834.93147943, 248301.077997393]
[2019-03-27 02:12:54,364] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:12:54,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0918165e-30 1.0000000e+00 0.0000000e+00 5.0058189e-27 3.3140365e-28], sampled 0.7643115768668397
[2019-03-27 02:13:01,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:13:01,870] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.28032949, 79.07362008999999, 1.0, 2.0, 0.5448298000934083, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9461891847940785, 6.911199999999999, 6.9112, 168.9126116531438, 1523220.974433948, 1523220.974433949, 333539.9102154026]
[2019-03-27 02:13:01,872] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:13:01,874] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1411496e-19 1.0000000e+00 1.3507547e-29 4.9590887e-17 3.6836568e-14], sampled 0.5019465374992084
[2019-03-27 02:13:12,202] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:13:12,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.26666666666667, 73.16666666666667, 1.0, 2.0, 0.8518499342277718, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98298978744752, 6.9112, 168.9124715212557, 2087641.880337884, 2036711.88054135, 421883.6195262977]
[2019-03-27 02:13:12,205] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:13:12,208] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.5396777e-12 8.5562831e-01 1.2683839e-20 1.5746648e-09 1.4437172e-01], sampled 0.5246028385504614
[2019-03-27 02:13:12,208] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2087641.880337884 W.
[2019-03-27 02:13:12,937] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.057890624]
[2019-03-27 02:13:12,938] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.65, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.716085756030097, 6.9112, 168.908545302957, 2025144.808632423, 1454146.063450743, 311350.4159104481]
[2019-03-27 02:13:12,939] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:13:12,944] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9327594e-21 1.0000000e+00 3.1157512e-31 1.5321155e-18 1.9144480e-15], sampled 0.7317215416335208
[2019-03-27 02:13:12,945] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2025144.808632423 W.
[2019-03-27 02:13:31,659] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8629.0055 2836511894.6618 812.0000
[2019-03-27 02:13:31,668] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8758.4948 2777351056.7627 698.0000
[2019-03-27 02:13:31,811] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8146.1579 3152468541.3642 1217.0000
[2019-03-27 02:13:31,890] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8371.5106 2923591164.7681 1056.0000
[2019-03-27 02:13:31,953] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8244.3500 2996287545.3435 1139.0000
[2019-03-27 02:13:32,972] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1225000, evaluation results [1225000.0, 8146.157872599684, 3152468541.3642216, 1217.0, 8371.51060527936, 2923591164.768051, 1056.0, 8758.494847144744, 2777351056.762681, 698.0, 8244.349971194395, 2996287545.343464, 1139.0, 8629.00550034206, 2836511894.6618247, 812.0]
[2019-03-27 02:13:33,901] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4566481e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 02:13:33,913] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0084
[2019-03-27 02:13:33,921] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.61666666666667, 68.66666666666667, 1.0, 2.0, 0.5432324178786426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759104.4885729792, 759104.4885729786, 190647.2397184563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6261000.0000, 
sim time next is 6261600.0000, 
raw observation next is [30.63333333333333, 68.33333333333334, 1.0, 2.0, 0.5423444730103086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757863.2456525544, 757863.2456525537, 190496.8040392407], 
processed observation next is [0.0, 0.4782608695652174, 0.6508688783570299, 0.6833333333333335, 1.0, 1.0, 0.44860779880760077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2105175682368207, 0.2105175682368205, 0.2843235881182697], 
reward next is 0.7157, 
noisyNet noise sample is [array([-1.6460449], dtype=float32), -0.8027994]. 
=============================================
[2019-03-27 02:13:34,340] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1225612: loss 0.4201
[2019-03-27 02:13:34,344] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1225613: learning rate 0.0005
[2019-03-27 02:13:35,102] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1225954: loss 0.6968
[2019-03-27 02:13:35,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1225954: learning rate 0.0005
[2019-03-27 02:13:36,119] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1226414: loss 1.1477
[2019-03-27 02:13:36,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1226414: learning rate 0.0005
[2019-03-27 02:13:36,426] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1226550: loss 0.5132
[2019-03-27 02:13:36,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1226550: learning rate 0.0005
[2019-03-27 02:13:36,499] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1226584: loss 0.7263
[2019-03-27 02:13:36,502] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1226584: learning rate 0.0005
[2019-03-27 02:13:36,509] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1226587: loss 0.5202
[2019-03-27 02:13:36,513] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1226587: learning rate 0.0005
[2019-03-27 02:13:36,776] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1226704: loss 0.4705
[2019-03-27 02:13:36,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1226705: learning rate 0.0005
[2019-03-27 02:13:37,277] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1226926: loss 0.4963
[2019-03-27 02:13:37,278] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1226927: learning rate 0.0005
[2019-03-27 02:13:37,584] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1227064: loss 0.5897
[2019-03-27 02:13:37,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1227065: learning rate 0.0005
[2019-03-27 02:13:39,863] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1228091: loss 0.8722
[2019-03-27 02:13:39,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1228092: learning rate 0.0005
[2019-03-27 02:13:40,268] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1228274: loss 6.8791
[2019-03-27 02:13:40,271] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1228274: learning rate 0.0005
[2019-03-27 02:13:40,977] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1228587: loss 18.4501
[2019-03-27 02:13:40,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1228587: learning rate 0.0005
[2019-03-27 02:13:41,494] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1228818: loss 13.4355
[2019-03-27 02:13:41,498] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1228818: learning rate 0.0005
[2019-03-27 02:13:41,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5764577e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 02:13:41,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1156
[2019-03-27 02:13:41,835] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 82.0, 1.0, 2.0, 0.5428322196696868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758545.0581727669, 758545.0581727669, 190579.4464858848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6251400.0000, 
sim time next is 6252000.0000, 
raw observation next is [28.5, 81.0, 1.0, 2.0, 0.5423620644222276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757887.8363718415, 757887.8363718421, 190499.8630487495], 
processed observation next is [0.0, 0.34782608695652173, 0.5497630331753555, 0.81, 1.0, 1.0, 0.4486289932797922, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2105243989921782, 0.21052439899217834, 0.28432815380410376], 
reward next is 0.7157, 
noisyNet noise sample is [array([-0.1054455], dtype=float32), -0.5071559]. 
=============================================
[2019-03-27 02:13:41,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.608055]
 [73.61513 ]
 [73.62403 ]
 [73.61923 ]
 [73.438774]], R is [[73.61374664]
 [73.59316254]
 [73.57281494]
 [73.5528717 ]
 [73.53324127]].
[2019-03-27 02:13:43,376] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:13:43,385] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4051
[2019-03-27 02:13:43,390] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.0, 1.0, 2.0, 0.5277896534109376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737517.5194075225, 737517.5194075225, 188064.6001796792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6306600.0000, 
sim time next is 6307200.0000, 
raw observation next is [27.3, 85.0, 1.0, 2.0, 0.5281840973461742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738068.8950873374, 738068.8950873368, 188129.6472705961], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.85, 1.0, 1.0, 0.4315471052363545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20501913752426038, 0.2050191375242602, 0.2807905183143225], 
reward next is 0.7192, 
noisyNet noise sample is [array([-1.8186826], dtype=float32), 0.21381384]. 
=============================================
[2019-03-27 02:13:45,519] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1230617: loss 1.4064
[2019-03-27 02:13:45,523] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1230618: learning rate 0.0005
[2019-03-27 02:13:48,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5120574e-27 1.0000000e+00 0.0000000e+00 2.5472526e-25 2.2447560e-24], sum to 1.0000
[2019-03-27 02:13:48,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5667
[2019-03-27 02:13:48,659] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 85.0, 1.0, 2.0, 0.6246366859131165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872904.2828208901, 872904.2828208901, 205417.8219956658], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6595200.0000, 
sim time next is 6595800.0000, 
raw observation next is [27.16666666666666, 84.00000000000001, 1.0, 2.0, 0.7127405912244973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 996083.6338513892, 996083.6338513892, 223617.9648897907], 
processed observation next is [1.0, 0.34782608695652173, 0.4865718799368086, 0.8400000000000002, 1.0, 1.0, 0.6539043267765028, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27668989829205254, 0.27668989829205254, 0.3337581565519264], 
reward next is 0.6662, 
noisyNet noise sample is [array([-0.37207833], dtype=float32), -0.9844806]. 
=============================================
[2019-03-27 02:13:49,360] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1232338: loss 1.0497
[2019-03-27 02:13:49,367] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1232338: learning rate 0.0005
[2019-03-27 02:13:50,245] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1232734: loss 4.6640
[2019-03-27 02:13:50,247] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1232734: learning rate 0.0005
[2019-03-27 02:13:52,179] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1233596: loss 12.9802
[2019-03-27 02:13:52,181] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1233596: learning rate 0.0005
[2019-03-27 02:13:52,809] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5406161e-24 1.0000000e+00 5.7054043e-36 3.0711045e-20 6.0796067e-21], sum to 1.0000
[2019-03-27 02:13:52,819] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6362
[2019-03-27 02:13:52,824] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 92.33333333333334, 1.0, 2.0, 0.6725292212962413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 939861.7605912875, 939861.7605912869, 215026.8114320446], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6499200.0000, 
sim time next is 6499800.0000, 
raw observation next is [26.3, 92.0, 1.0, 2.0, 0.6646483518587758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928843.403911114, 928843.403911114, 213399.1069552598], 
processed observation next is [1.0, 0.21739130434782608, 0.4454976303317536, 0.92, 1.0, 1.0, 0.5959618697093684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25801205664197613, 0.25801205664197613, 0.31850612978396986], 
reward next is 0.6815, 
noisyNet noise sample is [array([0.41924125], dtype=float32), 1.881535]. 
=============================================
[2019-03-27 02:13:53,030] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1233979: loss 3.3231
[2019-03-27 02:13:53,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1233979: learning rate 0.0005
[2019-03-27 02:13:54,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1234413: loss 250.3197
[2019-03-27 02:13:54,024] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1234414: learning rate 0.0005
[2019-03-27 02:13:54,205] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1234496: loss 36.7073
[2019-03-27 02:13:54,210] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1234497: learning rate 0.0005
[2019-03-27 02:13:54,496] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1234625: loss 7.3308
[2019-03-27 02:13:54,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1234625: learning rate 0.0005
[2019-03-27 02:13:54,520] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1234635: loss 8.5289
[2019-03-27 02:13:54,522] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1234635: learning rate 0.0005
[2019-03-27 02:13:54,658] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1234698: loss 5.8931
[2019-03-27 02:13:54,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1234700: learning rate 0.0005
[2019-03-27 02:13:55,044] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1234872: loss 11.6719
[2019-03-27 02:13:55,046] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1234872: learning rate 0.0005
[2019-03-27 02:13:55,380] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1235019: loss 8.7544
[2019-03-27 02:13:55,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1235020: learning rate 0.0005
[2019-03-27 02:13:57,860] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1236126: loss 0.4788
[2019-03-27 02:13:57,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1236130: learning rate 0.0005
[2019-03-27 02:13:58,224] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1236288: loss 0.0293
[2019-03-27 02:13:58,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1236289: learning rate 0.0005
[2019-03-27 02:13:58,885] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1236589: loss 0.0306
[2019-03-27 02:13:58,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1236589: learning rate 0.0005
[2019-03-27 02:13:59,206] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3955705e-10 9.7505093e-01 3.8909713e-19 4.6970783e-08 2.4949057e-02], sum to 1.0000
[2019-03-27 02:13:59,223] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6701
[2019-03-27 02:13:59,231] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2086330.270996278 W.
[2019-03-27 02:13:59,235] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.73333333333333, 56.0, 1.0, 2.0, 0.4973690131511028, 1.0, 1.0, 0.4973690131511028, 1.0, 1.0, 0.8455260414377175, 6.9112, 6.9112, 170.5573041426782, 2086330.270996278, 2086330.270996278, 410011.6365824464], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6526200.0000, 
sim time next is 6526800.0000, 
raw observation next is [31.8, 56.0, 1.0, 2.0, 0.6136344914744108, 1.0, 2.0, 0.6136344914744108, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1715725.449328427, 1715725.449328428, 339335.2612068074], 
processed observation next is [1.0, 0.5652173913043478, 0.7061611374407584, 0.56, 1.0, 1.0, 0.5344993873185673, 1.0, 1.0, 0.5344993873185673, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4765904025912297, 0.47659040259123, 0.5064705391146379], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.23869799], dtype=float32), -2.514291]. 
=============================================
[2019-03-27 02:13:59,294] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1236764: loss 0.0165
[2019-03-27 02:13:59,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1236765: learning rate 0.0005
[2019-03-27 02:14:01,543] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3912214e-28 1.0000000e+00 0.0000000e+00 3.1827037e-28 3.0560194e-26], sum to 1.0000
[2019-03-27 02:14:01,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6513
[2019-03-27 02:14:01,562] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 89.66666666666667, 1.0, 2.0, 0.5165169162891372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721759.9812020903, 721759.9812020903, 186224.2413698407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6572400.0000, 
sim time next is 6573000.0000, 
raw observation next is [26.25, 89.83333333333333, 1.0, 2.0, 0.5150644695549017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719729.702926985, 719729.7029269844, 185989.9050478236], 
processed observation next is [1.0, 0.043478260869565216, 0.4431279620853081, 0.8983333333333333, 1.0, 1.0, 0.41574032476494177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19992491747971805, 0.19992491747971788, 0.27759687320570686], 
reward next is 0.7224, 
noisyNet noise sample is [array([0.0791306], dtype=float32), -0.113574974]. 
=============================================
[2019-03-27 02:14:01,575] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[59.916145]
 [60.1404  ]
 [60.368187]
 [60.580444]
 [60.875854]], R is [[59.76916504]
 [59.89352798]
 [60.01639175]
 [60.13779831]
 [60.2578125 ]].
[2019-03-27 02:14:02,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1235802e-31 1.0000000e+00 0.0000000e+00 1.0079210e-28 4.1233916e-29], sum to 1.0000
[2019-03-27 02:14:02,253] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2963
[2019-03-27 02:14:02,263] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 92.83333333333333, 1.0, 2.0, 0.5478756560373194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765595.215277056, 765595.2152770566, 191434.3419718563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6583800.0000, 
sim time next is 6584400.0000, 
raw observation next is [25.7, 93.0, 1.0, 2.0, 0.5747600634431401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 803177.384302047, 803177.3843020463, 196133.1431390801], 
processed observation next is [1.0, 0.21739130434782608, 0.4170616113744076, 0.93, 1.0, 1.0, 0.48766272703992786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22310482897279083, 0.22310482897279063, 0.29273603453594044], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.12407683], dtype=float32), 0.25201023]. 
=============================================
[2019-03-27 02:14:03,389] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1238599: loss 0.2508
[2019-03-27 02:14:03,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1238600: learning rate 0.0005
[2019-03-27 02:14:03,880] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1701523e-38 1.0000000e+00 0.0000000e+00 3.9163339e-34 0.0000000e+00], sum to 1.0000
[2019-03-27 02:14:03,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8820
[2019-03-27 02:14:03,893] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.88333333333333, 79.66666666666667, 1.0, 2.0, 0.3714902160695102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587046.9243801547, 587046.924380154, 174058.8901870213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6763800.0000, 
sim time next is 6764400.0000, 
raw observation next is [23.0, 79.0, 1.0, 2.0, 0.3478090090862956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549331.0909467237, 549331.0909467237, 170876.5057150823], 
processed observation next is [1.0, 0.30434782608695654, 0.28909952606635075, 0.79, 1.0, 1.0, 0.21422772179071756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15259196970742325, 0.15259196970742325, 0.2550395607687796], 
reward next is 0.7450, 
noisyNet noise sample is [array([-0.17044963], dtype=float32), -0.3689116]. 
=============================================
[2019-03-27 02:14:07,339] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1240379: loss 0.0318
[2019-03-27 02:14:07,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1240379: learning rate 0.0005
[2019-03-27 02:14:07,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.1916414e-33 1.0000000e+00 0.0000000e+00 1.5103187e-27 3.7514591e-35], sum to 1.0000
[2019-03-27 02:14:07,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5194
[2019-03-27 02:14:07,496] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 95.0, 1.0, 2.0, 0.5763933873543017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805460.6783436633, 805460.6783436626, 196423.7977255318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6669000.0000, 
sim time next is 6669600.0000, 
raw observation next is [24.66666666666666, 95.0, 1.0, 2.0, 0.559798263962831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782261.8669904968, 782261.8669904968, 193489.9327497745], 
processed observation next is [1.0, 0.17391304347826086, 0.36808846761453373, 0.95, 1.0, 1.0, 0.46963646260582054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21729496305291576, 0.21729496305291576, 0.2887909444026485], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.69248945], dtype=float32), -0.37512988]. 
=============================================
[2019-03-27 02:14:08,119] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1240728: loss 0.0500
[2019-03-27 02:14:08,122] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1240728: learning rate 0.0005
[2019-03-27 02:14:09,977] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1241557: loss 0.0594
[2019-03-27 02:14:09,978] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1241557: learning rate 0.0005
[2019-03-27 02:14:10,685] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1241870: loss 0.8025
[2019-03-27 02:14:10,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1241872: learning rate 0.0005
[2019-03-27 02:14:10,970] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7115242e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 02:14:10,977] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8521
[2019-03-27 02:14:10,983] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333333, 66.66666666666667, 1.0, 2.0, 0.8505409820210375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326035.591225285, 1326035.591225284, 274888.0411811259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6772200.0000, 
sim time next is 6772800.0000, 
raw observation next is [25.66666666666667, 65.33333333333334, 1.0, 2.0, 0.8753163340902249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1364327.620822406, 1364327.620822406, 282170.6310863165], 
processed observation next is [1.0, 0.391304347826087, 0.4154818325434442, 0.6533333333333334, 1.0, 1.0, 0.8497787157713553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3789798946728905, 0.3789798946728905, 0.42115019565121864], 
reward next is 0.5788, 
noisyNet noise sample is [array([1.8389145], dtype=float32), -0.15046188]. 
=============================================
[2019-03-27 02:14:12,140] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1242518: loss 0.0196
[2019-03-27 02:14:12,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1242520: learning rate 0.0005
[2019-03-27 02:14:12,189] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1242539: loss 0.0098
[2019-03-27 02:14:12,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1242540: learning rate 0.0005
[2019-03-27 02:14:12,573] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1242714: loss 0.0436
[2019-03-27 02:14:12,575] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1242714: learning rate 0.0005
[2019-03-27 02:14:12,631] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1242729: loss 0.0914
[2019-03-27 02:14:12,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1242730: learning rate 0.0005
[2019-03-27 02:14:12,799] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1242804: loss 0.0547
[2019-03-27 02:14:12,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1242805: learning rate 0.0005
[2019-03-27 02:14:13,158] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1242966: loss 0.0062
[2019-03-27 02:14:13,161] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1242967: learning rate 0.0005
[2019-03-27 02:14:13,169] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:14:13,182] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7783
[2019-03-27 02:14:13,186] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 83.0, 1.0, 2.0, 0.3408964043393971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530618.8737611003, 530618.8737611009, 169209.3866058711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6841800.0000, 
sim time next is 6842400.0000, 
raw observation next is [23.03333333333333, 83.33333333333334, 1.0, 2.0, 0.3414941418514039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531150.203740674, 531150.2037406745, 169241.6249318058], 
processed observation next is [0.0, 0.17391304347826086, 0.29067930489731436, 0.8333333333333335, 1.0, 1.0, 0.20661944801373966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14754172326129833, 0.14754172326129847, 0.2525994401967251], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.08561929], dtype=float32), 0.24287455]. 
=============================================
[2019-03-27 02:14:13,467] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1243098: loss 0.0066
[2019-03-27 02:14:13,468] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1243098: learning rate 0.0005
[2019-03-27 02:14:15,763] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1244126: loss 0.8169
[2019-03-27 02:14:15,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1244126: learning rate 0.0005
[2019-03-27 02:14:16,115] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1244286: loss 20.8599
[2019-03-27 02:14:16,119] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1244286: learning rate 0.0005
[2019-03-27 02:14:16,905] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1244641: loss -161.4960
[2019-03-27 02:14:16,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1244642: learning rate 0.0005
[2019-03-27 02:14:17,299] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1244818: loss -67.0248
[2019-03-27 02:14:17,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1244821: learning rate 0.0005
[2019-03-27 02:14:21,324] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1246615: loss -61.5253
[2019-03-27 02:14:21,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1246616: learning rate 0.0005
[2019-03-27 02:14:22,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:14:22,608] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4885
[2019-03-27 02:14:22,614] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333333, 52.33333333333334, 1.0, 2.0, 0.471694100601146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659106.9488606076, 659106.9488606076, 179279.9763761303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6958200.0000, 
sim time next is 6958800.0000, 
raw observation next is [32.0, 52.0, 1.0, 2.0, 0.4749011103075036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663589.5628702575, 663589.5628702568, 179757.0766193429], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.52, 1.0, 1.0, 0.36735073531024526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18433043413062708, 0.18433043413062689, 0.2682941442079745], 
reward next is 0.7317, 
noisyNet noise sample is [array([-2.0518703], dtype=float32), -1.7689663]. 
=============================================
[2019-03-27 02:14:25,380] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1248424: loss 124.1910
[2019-03-27 02:14:25,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1248426: learning rate 0.0005
[2019-03-27 02:14:26,220] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1248803: loss -178.3426
[2019-03-27 02:14:26,221] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1248804: learning rate 0.0005
[2019-03-27 02:14:27,913] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1249553: loss -291.2698
[2019-03-27 02:14:27,922] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1249553: learning rate 0.0005
[2019-03-27 02:14:28,478] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1249804: loss -571.5887
[2019-03-27 02:14:28,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1249805: learning rate 0.0005
[2019-03-27 02:14:28,923] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 02:14:28,926] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:14:28,927] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:14:28,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:14:28,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:14:28,931] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:14:28,932] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:14:28,933] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:14:28,929] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:14:28,938] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:14:28,942] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:14:28,955] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-27 02:14:28,974] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-27 02:14:28,992] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-27 02:14:28,993] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-27 02:14:29,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-27 02:14:50,093] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.061572507]
[2019-03-27 02:14:50,095] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.33333333333334, 98.0, 1.0, 2.0, 0.3065308649251152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487080.4324494764, 487080.4324494764, 166115.9018761173]
[2019-03-27 02:14:50,096] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:14:50,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8341700869529164
[2019-03-27 02:15:06,175] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.061572507]
[2019-03-27 02:15:06,178] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.92023327, 100.0, 1.0, 2.0, 0.5375721636233121, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9254773761002358, 6.911200000000001, 6.9112, 168.9125795505974, 1502915.898622363, 1502915.898622363, 327669.1825684546]
[2019-03-27 02:15:06,178] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:15:06,182] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9133908184366952
[2019-03-27 02:15:10,196] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.061572507]
[2019-03-27 02:15:10,199] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.45303836666667, 94.34510528833333, 1.0, 2.0, 0.270422112665234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 440165.4680580654, 440165.4680580648, 162909.541793054]
[2019-03-27 02:15:10,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:15:10,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9252308804563882
[2019-03-27 02:15:36,429] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.061572507]
[2019-03-27 02:15:36,430] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.33333333333334, 82.33333333333334, 1.0, 2.0, 0.8620881488115991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1204921.269443467, 1204921.269443467, 259722.8437797384]
[2019-03-27 02:15:36,431] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:15:36,433] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8616602073892458
[2019-03-27 02:15:59,257] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00752734], dtype=float32), -0.061572507]
[2019-03-27 02:15:59,258] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 84.0, 1.0, 2.0, 0.5348330402060532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747363.2004427084, 747363.2004427084, 189233.4468731701]
[2019-03-27 02:15:59,258] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:15:59,263] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1125094563954312
[2019-03-27 02:16:23,715] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:16:23,732] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:16:23,943] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:16:24,113] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:16:24,116] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:16:25,135] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1250000, evaluation results [1250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:16:26,108] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1250439: loss -231.8408
[2019-03-27 02:16:26,110] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1250439: learning rate 0.0005
[2019-03-27 02:16:26,204] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1250478: loss 28.6280
[2019-03-27 02:16:26,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1250478: learning rate 0.0005
[2019-03-27 02:16:26,536] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1250627: loss -24.1272
[2019-03-27 02:16:26,537] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1250627: learning rate 0.0005
[2019-03-27 02:16:26,633] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1250670: loss -151.6315
[2019-03-27 02:16:26,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1250670: learning rate 0.0005
[2019-03-27 02:16:26,701] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1250700: loss -87.0523
[2019-03-27 02:16:26,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1250701: learning rate 0.0005
[2019-03-27 02:16:27,154] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1250900: loss -522.1765
[2019-03-27 02:16:27,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1250901: learning rate 0.0005
[2019-03-27 02:16:27,558] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1251084: loss -523.9752
[2019-03-27 02:16:27,561] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1251084: learning rate 0.0005
[2019-03-27 02:16:29,391] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1251909: loss -287.2554
[2019-03-27 02:16:29,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1251909: learning rate 0.0005
[2019-03-27 02:16:30,413] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1252367: loss 0.1210
[2019-03-27 02:16:30,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1252368: learning rate 0.0005
[2019-03-27 02:16:31,026] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1252634: loss 0.4199
[2019-03-27 02:16:31,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1252634: learning rate 0.0005
[2019-03-27 02:16:31,444] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1252830: loss 0.4251
[2019-03-27 02:16:31,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1252831: learning rate 0.0005
[2019-03-27 02:16:35,247] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1254525: loss 0.3778
[2019-03-27 02:16:35,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1254525: learning rate 0.0005
[2019-03-27 02:16:38,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2358275e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 02:16:38,589] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4314
[2019-03-27 02:16:38,599] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.93333333333333, 68.66666666666667, 1.0, 2.0, 0.901314276857696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1376320.31286933, 1376320.31286933, 286640.7263224876], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7296000.0000, 
sim time next is 7296600.0000, 
raw observation next is [26.16666666666666, 67.83333333333333, 1.0, 2.0, 0.9211127486650742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1402809.416674592, 1402809.416674592, 292211.7475226044], 
processed observation next is [1.0, 0.43478260869565216, 0.4391785150078987, 0.6783333333333332, 1.0, 1.0, 0.9049551188735834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3896692824096089, 0.3896692824096089, 0.43613693660090214], 
reward next is 0.5639, 
noisyNet noise sample is [array([-0.34598368], dtype=float32), 0.07637196]. 
=============================================
[2019-03-27 02:16:39,598] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1256390: loss 0.0240
[2019-03-27 02:16:39,601] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1256392: learning rate 0.0005
[2019-03-27 02:16:40,410] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1256753: loss 0.0203
[2019-03-27 02:16:40,412] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1256753: learning rate 0.0005
[2019-03-27 02:16:42,133] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1257524: loss 0.9526
[2019-03-27 02:16:42,136] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1257525: learning rate 0.0005
[2019-03-27 02:16:42,793] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1257816: loss 0.3297
[2019-03-27 02:16:42,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1257816: learning rate 0.0005
[2019-03-27 02:16:43,408] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4145473e-33 1.0000000e+00 0.0000000e+00 2.8553279e-30 2.5023627e-32], sum to 1.0000
[2019-03-27 02:16:43,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4163
[2019-03-27 02:16:43,428] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 60.33333333333333, 1.0, 2.0, 0.8099630238555156, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564583266, 1215471.077139755, 1215471.077139756, 257724.2888958898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7312200.0000, 
sim time next is 7312800.0000, 
raw observation next is [27.73333333333333, 60.66666666666667, 1.0, 2.0, 0.9443266797806517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510418, 1425410.2792913, 1425410.279291301, 297759.3580693092], 
processed observation next is [1.0, 0.6521739130434783, 0.513428120063191, 0.6066666666666667, 1.0, 1.0, 0.9329237105790984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439945152239, 0.3959472998031389, 0.3959472998031392, 0.44441695234225254], 
reward next is 0.5556, 
noisyNet noise sample is [array([-0.26993486], dtype=float32), -0.1693099]. 
=============================================
[2019-03-27 02:16:44,284] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1258488: loss 0.0302
[2019-03-27 02:16:44,286] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1258488: learning rate 0.0005
[2019-03-27 02:16:44,315] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1258498: loss 0.0366
[2019-03-27 02:16:44,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1258499: learning rate 0.0005
[2019-03-27 02:16:44,793] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1258712: loss 0.0545
[2019-03-27 02:16:44,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1258713: learning rate 0.0005
[2019-03-27 02:16:44,837] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1258731: loss 0.1369
[2019-03-27 02:16:44,842] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1258731: learning rate 0.0005
[2019-03-27 02:16:44,860] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1258742: loss 0.1840
[2019-03-27 02:16:44,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1258742: learning rate 0.0005
[2019-03-27 02:16:45,478] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1259011: loss 0.1060
[2019-03-27 02:16:45,484] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1259013: learning rate 0.0005
[2019-03-27 02:16:45,952] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1259222: loss 0.1892
[2019-03-27 02:16:45,955] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1259223: learning rate 0.0005
[2019-03-27 02:16:48,135] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1260212: loss -176.4383
[2019-03-27 02:16:48,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1260212: learning rate 0.0005
[2019-03-27 02:16:48,887] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1260545: loss -64.5289
[2019-03-27 02:16:48,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1260545: learning rate 0.0005
[2019-03-27 02:16:48,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:16:48,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:16:48,986] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-27 02:16:49,240] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1260704: loss -164.6143
[2019-03-27 02:16:49,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1260705: learning rate 0.0005
[2019-03-27 02:16:52,843] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1262424: loss -62.5246
[2019-03-27 02:16:52,845] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1262424: learning rate 0.0005
[2019-03-27 02:16:56,155] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:16:56,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1673
[2019-03-27 02:16:56,171] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 62.33333333333333, 1.0, 2.0, 0.4392080589186769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628318.064137571, 628318.0641375717, 176505.2226159334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7564200.0000, 
sim time next is 7564800.0000, 
raw observation next is [29.0, 61.66666666666667, 1.0, 2.0, 0.4348498349056556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625057.65067362, 625057.65067362, 176266.2403404828], 
processed observation next is [0.0, 0.5652173913043478, 0.5734597156398105, 0.6166666666666667, 1.0, 1.0, 0.31909618663332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17362712518711668, 0.17362712518711668, 0.26308394080669073], 
reward next is 0.7369, 
noisyNet noise sample is [array([-1.6292819], dtype=float32), -1.0075709]. 
=============================================
[2019-03-27 02:16:56,948] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1264249: loss -59.2075
[2019-03-27 02:16:56,949] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1264249: learning rate 0.0005
[2019-03-27 02:16:57,790] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1264627: loss 106.7604
[2019-03-27 02:16:57,793] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1264627: learning rate 0.0005
[2019-03-27 02:16:59,477] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1265378: loss 94.3096
[2019-03-27 02:16:59,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1265380: learning rate 0.0005
[2019-03-27 02:17:00,187] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1265701: loss 19.6533
[2019-03-27 02:17:00,190] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1265702: learning rate 0.0005
[2019-03-27 02:17:01,381] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1266231: loss -26.9606
[2019-03-27 02:17:01,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1266231: learning rate 0.0005
[2019-03-27 02:17:01,595] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1266325: loss -42.3836
[2019-03-27 02:17:01,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1266325: learning rate 0.0005
[2019-03-27 02:17:02,007] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1266511: loss -36.4516
[2019-03-27 02:17:02,011] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1266511: learning rate 0.0005
[2019-03-27 02:17:02,080] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1266540: loss -41.4127
[2019-03-27 02:17:02,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1266541: learning rate 0.0005
[2019-03-27 02:17:02,099] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1266547: loss 46.8104
[2019-03-27 02:17:02,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1266548: learning rate 0.0005
[2019-03-27 02:17:02,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7239808e-22 1.0000000e+00 1.6449950e-31 1.1202468e-21 2.4536168e-24], sum to 1.0000
[2019-03-27 02:17:02,448] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0279
[2019-03-27 02:17:02,455] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1999433.049289413 W.
[2019-03-27 02:17:02,460] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.86666666666667, 59.0, 1.0, 2.0, 0.4766725345084866, 1.0, 2.0, 0.4766725345084866, 1.0, 1.0, 0.8060014248636443, 6.911200000000001, 6.9112, 170.5573041426782, 1999433.049289413, 1999433.049289412, 395582.8743695608], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7654200.0000, 
sim time next is 7654800.0000, 
raw observation next is [30.73333333333333, 60.0, 1.0, 2.0, 0.844461197158053, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.970056958211561, 6.9112, 168.9125483212916, 2077300.400266011, 2035545.349314602, 420317.4703789029], 
processed observation next is [1.0, 0.6086956521739131, 0.6556082148499209, 0.6, 1.0, 1.0, 0.8126038519976542, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005885695821156123, 0.0, 0.8294379407567585, 0.5770278889627808, 0.5654292636985006, 0.6273395080282133], 
reward next is 0.0784, 
noisyNet noise sample is [array([-0.13202149], dtype=float32), 0.24448727]. 
=============================================
[2019-03-27 02:17:02,549] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1266744: loss -80.8978
[2019-03-27 02:17:02,551] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1266745: learning rate 0.0005
[2019-03-27 02:17:03,007] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1266952: loss 1.7763
[2019-03-27 02:17:03,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1266953: learning rate 0.0005
[2019-03-27 02:17:07,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:07,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:07,147] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-27 02:17:07,836] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:07,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:07,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-27 02:17:08,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:08,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:08,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-27 02:17:10,638] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:10,639] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:10,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-27 02:17:14,143] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:14,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:14,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-27 02:17:14,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:14,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:15,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-27 02:17:16,262] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:16,262] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:16,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-27 02:17:16,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:16,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:16,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-27 02:17:17,445] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:17,446] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:17,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-27 02:17:17,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:17,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:17,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-27 02:17:17,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:17,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:17,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:17,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:17,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:17,903] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:17,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-27 02:17:17,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-27 02:17:17,975] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-27 02:17:18,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:18,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-27 02:17:18,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:17:18,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,239] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-27 02:17:18,968] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 02:17:18,971] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:17:18,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,972] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:17:18,976] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:17:18,973] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:17:18,977] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,978] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,973] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:17:18,981] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:17:18,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-27 02:17:19,005] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-27 02:17:19,020] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-27 02:17:19,035] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-27 02:17:19,036] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-27 02:17:28,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06926639], dtype=float32), -0.068303935]
[2019-03-27 02:17:28,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.81666666666667, 60.66666666666666, 1.0, 2.0, 0.7596594214233054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1242601.624228484, 1242601.624228484, 254190.4673776444]
[2019-03-27 02:17:28,347] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:17:28,350] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7270330703201376
[2019-03-27 02:17:39,055] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06926639], dtype=float32), -0.068303935]
[2019-03-27 02:17:39,056] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.26666666666667, 65.0, 1.0, 2.0, 0.2257507855190631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 375964.5903255193, 375964.5903255193, 158138.104492881]
[2019-03-27 02:17:39,057] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:17:39,061] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8532082354136169
[2019-03-27 02:17:54,745] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06926639], dtype=float32), -0.068303935]
[2019-03-27 02:17:54,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.63364427, 77.19503499, 1.0, 2.0, 0.5959439763603622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832791.6721498248, 832791.6721498248, 199993.0952446892]
[2019-03-27 02:17:54,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:17:54,752] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7094642901142806
[2019-03-27 02:18:36,836] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06926639], dtype=float32), -0.068303935]
[2019-03-27 02:18:36,837] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.464458945, 85.230549225, 1.0, 2.0, 0.8995098805875882, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987744035901, 6.9112, 168.9123159664486, 2154350.473517708, 2087105.040562285, 433956.9425149758]
[2019-03-27 02:18:36,838] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:18:36,843] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.20966238e-36 1.00000000e+00 0.00000000e+00 1.00419195e-33
 0.00000000e+00], sampled 0.13321577978533472
[2019-03-27 02:18:36,846] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2154350.473517708 W.
[2019-03-27 02:18:57,690] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06926639], dtype=float32), -0.068303935]
[2019-03-27 02:18:57,691] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.99956182666667, 82.34403213666667, 1.0, 2.0, 0.3331384897244562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522427.6382520947, 522427.6382520947, 168655.1218822314]
[2019-03-27 02:18:57,691] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:18:57,694] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7896242562196691
[2019-03-27 02:19:02,823] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06926639], dtype=float32), -0.068303935]
[2019-03-27 02:19:02,824] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.43333333333333, 77.33333333333333, 1.0, 2.0, 0.5910274552104863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825918.4928744863, 825918.4928744863, 199086.5178748696]
[2019-03-27 02:19:02,826] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:19:02,831] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24946616381849818
[2019-03-27 02:19:14,082] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:19:14,382] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:19:14,497] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:19:14,498] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:19:14,684] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:19:15,699] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1275000, evaluation results [1275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:19:16,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:16,680] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1932
[2019-03-27 02:19:16,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 85.33333333333334, 1.0, 2.0, 0.2839427060194353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 456876.1405156444, 456876.1405156438, 164060.34452536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 15600.0000, 
sim time next is 16200.0000, 
raw observation next is [21.3, 85.5, 1.0, 2.0, 0.2843686877177798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457095.9576430458, 457095.9576430458, 164073.2743378893], 
processed observation next is [1.0, 0.17391304347826086, 0.2085308056872039, 0.855, 1.0, 1.0, 0.13779359965997565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1269710993452905, 0.1269710993452905, 0.24488548408640196], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.2781975], dtype=float32), 1.8200049]. 
=============================================
[2019-03-27 02:19:18,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:18,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9585
[2019-03-27 02:19:18,102] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333333, 89.33333333333334, 1.0, 2.0, 0.4399588967573597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680774.4790538431, 680774.4790538425, 182658.1238073049], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 94800.0000, 
sim time next is 95400.0000, 
raw observation next is [22.45, 89.5, 1.0, 2.0, 0.3884612252746842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 600477.9889225508, 600477.9889225514, 175081.0517185793], 
processed observation next is [1.0, 0.08695652173913043, 0.26303317535545023, 0.895, 1.0, 1.0, 0.2632062955116677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16679944136737523, 0.16679944136737537, 0.26131500256504375], 
reward next is 0.7387, 
noisyNet noise sample is [array([-0.3221114], dtype=float32), -0.44165492]. 
=============================================
[2019-03-27 02:19:23,787] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:23,798] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2438
[2019-03-27 02:19:23,803] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:23,807] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333334, 86.0, 1.0, 2.0, 0.2809594953350604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 452218.6943056459, 452218.6943056459, 163747.0519832825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [21.1, 86.0, 1.0, 2.0, 0.2798996984578882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 450793.4470761004, 450793.4470761004, 163652.5046664113], 
processed observation next is [0.0, 0.9130434782608695, 0.1990521327014219, 0.86, 1.0, 1.0, 0.1324092752504677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12522040196558346, 0.12522040196558346, 0.24425746965136014], 
reward next is 0.7557, 
noisyNet noise sample is [array([-1.4814063], dtype=float32), 0.17863208]. 
=============================================
[2019-03-27 02:19:23,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5084
[2019-03-27 02:19:23,818] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333333, 95.83333333333333, 1.0, 2.0, 0.2835703086503259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457537.0063997328, 457537.0063997328, 164105.5407879313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [19.86666666666667, 95.66666666666666, 1.0, 2.0, 0.283503165940434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457312.1752977503, 457312.1752977503, 164090.6126821286], 
processed observation next is [0.0, 0.21739130434782608, 0.14060031595576644, 0.9566666666666666, 1.0, 1.0, 0.1367508023378723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12703115980493063, 0.12703115980493063, 0.24491136221213222], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.24985644], dtype=float32), -0.6290915]. 
=============================================
[2019-03-27 02:19:23,836] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.380646]
 [79.3574  ]
 [79.17583 ]
 [79.138275]
 [78.954025]], R is [[79.35661316]
 [79.31811523]
 [79.27998352]
 [79.24219513]
 [79.20472717]].
[2019-03-27 02:19:25,564] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:25,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1545
[2019-03-27 02:19:25,575] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 83.5, 1.0, 2.0, 0.2573115908106323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 422468.2373203978, 422468.2373203984, 161640.2069155045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [20.13333333333333, 84.0, 1.0, 2.0, 0.2577771184945483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 423251.0919288568, 423251.0919288562, 161687.2231921373], 
processed observation next is [1.0, 0.9130434782608695, 0.15323854660347538, 0.84, 1.0, 1.0, 0.10575556445126302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11756974775801578, 0.11756974775801561, 0.24132421371960788], 
reward next is 0.7587, 
noisyNet noise sample is [array([-0.32730013], dtype=float32), 1.1446949]. 
=============================================
[2019-03-27 02:19:28,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:28,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7698
[2019-03-27 02:19:28,603] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.88333333333334, 91.66666666666667, 1.0, 2.0, 0.301606091918902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481326.6202784381, 481326.6202784388, 165731.2482351321], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 213000.0000, 
sim time next is 213600.0000, 
raw observation next is [20.96666666666667, 91.33333333333334, 1.0, 2.0, 0.3037413313631027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484300.2789703728, 484300.2789703728, 165939.3758367369], 
processed observation next is [0.0, 0.4782608695652174, 0.1927330173775673, 0.9133333333333334, 1.0, 1.0, 0.1611341341724129, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.134527855269548, 0.134527855269548, 0.24767071020408493], 
reward next is 0.7523, 
noisyNet noise sample is [array([0.6094582], dtype=float32), 0.9120426]. 
=============================================
[2019-03-27 02:19:32,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:32,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0859
[2019-03-27 02:19:32,317] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 78.66666666666667, 1.0, 2.0, 0.3030353406860182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481738.2384693777, 481738.2384693783, 165733.1128130777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 319200.0000, 
sim time next is 319800.0000, 
raw observation next is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.3013051075397071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479424.4883262644, 479424.4883262644, 165574.4798935821], 
processed observation next is [0.0, 0.6956521739130435, 0.27330173775671435, 0.7883333333333334, 1.0, 1.0, 0.15819892474663502, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1331734689795179, 0.1331734689795179, 0.2471260893934061], 
reward next is 0.7529, 
noisyNet noise sample is [array([-0.8965796], dtype=float32), -0.29498178]. 
=============================================
[2019-03-27 02:19:37,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:37,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8744
[2019-03-27 02:19:37,652] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 55.33333333333333, 1.0, 2.0, 0.3501702907954479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 573934.1410301038, 573934.1410301044, 172526.7757412604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 564000.0000, 
sim time next is 564600.0000, 
raw observation next is [24.5, 55.66666666666667, 1.0, 2.0, 0.3498141191536414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573622.1155521977, 573622.1155521977, 172483.1554737045], 
processed observation next is [1.0, 0.5217391304347826, 0.3601895734597157, 0.5566666666666668, 1.0, 1.0, 0.21664351705258, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15933947654227712, 0.15933947654227712, 0.2574375454831411], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.8334233], dtype=float32), -0.64370745]. 
=============================================
[2019-03-27 02:19:39,648] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4278498e-32 5.3962255e-37], sum to 1.0000
[2019-03-27 02:19:39,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8992
[2019-03-27 02:19:39,659] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 56.33333333333334, 1.0, 2.0, 0.3917393684684735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642896.085983045, 642896.085983045, 178270.7484363686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [24.35, 56.0, 1.0, 2.0, 0.338113927322048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555031.0528448228, 555031.0528448233, 170949.8978471664], 
processed observation next is [1.0, 0.5217391304347826, 0.35308056872037924, 0.56, 1.0, 1.0, 0.2025469003880096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1541752924568952, 0.15417529245689537, 0.25514910126442747], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.01344116], dtype=float32), 0.60019547]. 
=============================================
[2019-03-27 02:19:46,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:46,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7368
[2019-03-27 02:19:46,520] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.5, 87.66666666666667, 1.0, 2.0, 0.2270708616949041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377424.1968030478, 377424.1968030478, 158417.41702413], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 526800.0000, 
sim time next is 527400.0000, 
raw observation next is [18.45, 88.0, 1.0, 2.0, 0.2204669521490859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 366481.11293434, 366481.1129343406, 157820.9812425564], 
processed observation next is [1.0, 0.08695652173913043, 0.07345971563981045, 0.88, 1.0, 1.0, 0.06080355680612758, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10180030914842779, 0.10180030914842794, 0.23555370334709907], 
reward next is 0.7644, 
noisyNet noise sample is [array([0.16490686], dtype=float32), -1.2032434]. 
=============================================
[2019-03-27 02:19:50,839] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:50,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2851
[2019-03-27 02:19:50,856] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.0, 82.5, 1.0, 2.0, 0.2264758970707983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 376717.4235899877, 376717.4235899884, 158308.8500992186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [18.86666666666667, 83.0, 1.0, 2.0, 0.2241016475739114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 372957.7030028827, 372957.7030028827, 158053.9721315993], 
processed observation next is [1.0, 0.9565217391304348, 0.09320695102685649, 0.83, 1.0, 1.0, 0.06518270792037517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1035993619452452, 0.1035993619452452, 0.23590145094268553], 
reward next is 0.7641, 
noisyNet noise sample is [array([0.00314244], dtype=float32), 1.191682]. 
=============================================
[2019-03-27 02:19:50,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.59483]
 [84.55105]
 [84.50121]
 [84.46545]
 [84.37421]], R is [[84.56894684]
 [84.48697662]
 [84.40550995]
 [84.32472229]
 [84.24466705]].
[2019-03-27 02:19:52,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:52,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2187
[2019-03-27 02:19:52,065] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.96666666666667, 92.0, 1.0, 2.0, 0.2007718088497139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 335655.8152497928, 335655.8152497928, 155379.4642532544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 620400.0000, 
sim time next is 621000.0000, 
raw observation next is [16.95, 92.0, 1.0, 2.0, 0.2012261717516125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 336434.2421623027, 336434.2421623032, 155399.7682012333], 
processed observation next is [1.0, 0.17391304347826086, 0.002369668246445531, 0.92, 1.0, 1.0, 0.03762189367664156, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.0934539561561952, 0.09345395615619533, 0.23193995253915417], 
reward next is 0.7681, 
noisyNet noise sample is [array([0.20479941], dtype=float32), 1.7170873]. 
=============================================
[2019-03-27 02:19:52,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.37513]
 [75.39559]
 [75.33492]
 [75.16517]
 [75.11405]], R is [[75.44797516]
 [75.461586  ]
 [75.47444153]
 [75.48766327]
 [75.50069427]].
[2019-03-27 02:19:57,854] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:19:57,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2101
[2019-03-27 02:19:57,871] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 92.5, 1.0, 2.0, 0.2181934735618407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 363567.6476597872, 363567.6476597878, 157414.8046532437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 703800.0000, 
sim time next is 704400.0000, 
raw observation next is [17.6, 92.33333333333333, 1.0, 2.0, 0.2166460440961672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 361031.072193291, 361031.0721932903, 157267.4669381341], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9233333333333333, 1.0, 1.0, 0.05620005312791227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10028640894258083, 0.10028640894258063, 0.23472756259423], 
reward next is 0.7653, 
noisyNet noise sample is [array([-0.27562594], dtype=float32), -1.7293469]. 
=============================================
[2019-03-27 02:20:05,500] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:20:05,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9039
[2019-03-27 02:20:05,518] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 78.66666666666667, 1.0, 2.0, 0.3034343301702617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482356.0714806924, 482356.0714806918, 165777.3045481032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.3019040529153987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480360.1082277217, 480360.1082277217, 165641.2060816276], 
processed observation next is [0.0, 0.782608695652174, 0.27330173775671435, 0.7883333333333334, 1.0, 1.0, 0.15892054568120323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13343336339658937, 0.13343336339658937, 0.24722568071884718], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.43265983], dtype=float32), -0.058790933]. 
=============================================
[2019-03-27 02:20:09,162] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2118096e-24 1.0000000e+00 1.3664689e-36 8.4727276e-22 9.0295176e-18], sum to 1.0000
[2019-03-27 02:20:09,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7573
[2019-03-27 02:20:09,181] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 64.0, 1.0, 2.0, 0.5417478025342233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826096.3513937802, 826096.3513937802, 198781.4912092043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1168800.0000, 
sim time next is 1169400.0000, 
raw observation next is [26.88333333333333, 63.5, 1.0, 2.0, 0.5262258908200603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801873.4452989831, 801873.4452989837, 195883.366619986], 
processed observation next is [1.0, 0.5217391304347826, 0.47314375987361756, 0.635, 1.0, 1.0, 0.4291878202651329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22274262369416198, 0.22274262369416215, 0.2923632337611731], 
reward next is 0.7076, 
noisyNet noise sample is [array([0.01884833], dtype=float32), -0.23820584]. 
=============================================
[2019-03-27 02:20:10,342] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:20:10,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9307
[2019-03-27 02:20:10,358] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 67.33333333333334, 1.0, 2.0, 0.3072081468250611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485678.3826556922, 485678.3826556922, 165965.2089310022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 913200.0000, 
sim time next is 913800.0000, 
raw observation next is [24.86666666666667, 66.66666666666666, 1.0, 2.0, 0.3079135301212406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486547.4680176543, 486547.4680176543, 166023.0520876434], 
processed observation next is [0.0, 0.5652173913043478, 0.3775671406003162, 0.6666666666666665, 1.0, 1.0, 0.1661608796641453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13515207444934843, 0.13515207444934843, 0.24779560013081103], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.47285712], dtype=float32), -1.3244718]. 
=============================================
[2019-03-27 02:20:11,879] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 02:20:11,881] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:20:11,882] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:20:11,883] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:20:11,885] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:20:11,885] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:20:11,887] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:20:11,889] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:20:11,886] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:20:11,890] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:20:11,892] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:20:11,917] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-27 02:20:11,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-27 02:20:11,940] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-27 02:20:11,957] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-27 02:20:11,958] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-27 02:20:22,416] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:20:22,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.81241062666667, 68.29866753333334, 1.0, 2.0, 0.2985058685456595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488752.6143892111, 488752.6143892116, 166084.9118007685]
[2019-03-27 02:20:22,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:20:22,424] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22373466312183632
[2019-03-27 02:20:29,196] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:20:29,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.15, 88.5, 1.0, 2.0, 0.2980085863276811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476556.6058885139, 476556.6058885139, 165402.6710470118]
[2019-03-27 02:20:29,198] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:20:29,202] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6745937769916908
[2019-03-27 02:20:37,751] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:20:37,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.31475216666666, 82.10685457333332, 1.0, 2.0, 0.5329657240113476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744752.9416573644, 744752.9416573644, 188920.7835923051]
[2019-03-27 02:20:37,755] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:20:37,758] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3110706e-35 0.0000000e+00], sampled 0.0386593863019139
[2019-03-27 02:21:15,622] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:21:15,623] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.66666666666666, 79.0, 1.0, 2.0, 0.5354309014307969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748198.9319862404, 748198.9319862397, 189334.0538590824]
[2019-03-27 02:21:15,625] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:21:15,627] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24087834398662555
[2019-03-27 02:21:27,474] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:21:27,475] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.856710975, 90.45133704333334, 1.0, 2.0, 0.7430254325765718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1038428.610745066, 1038428.610745065, 230409.3242142193]
[2019-03-27 02:21:27,476] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:21:27,482] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.0944305e-33 1.0000000e+00 0.0000000e+00 3.7035236e-29 1.9843790e-31], sampled 0.07102951020657255
[2019-03-27 02:21:38,831] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:21:38,834] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.6, 82.5, 1.0, 2.0, 0.5658303215798423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790694.1992320444, 790694.1992320444, 194552.3026521728]
[2019-03-27 02:21:38,836] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:21:38,841] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2576907e-38 0.0000000e+00], sampled 0.061577346312558956
[2019-03-27 02:21:47,949] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:21:47,950] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.31666666666667, 95.16666666666667, 1.0, 2.0, 0.4344784065116183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637743.4956103576, 637743.4956103576, 177863.9116589875]
[2019-03-27 02:21:47,951] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:21:47,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7955885167848062
[2019-03-27 02:21:51,753] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:21:51,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.83333333333333, 83.83333333333333, 1.0, 2.0, 0.5800573191191805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810582.6646645654, 810582.6646645659, 197089.0336454794]
[2019-03-27 02:21:51,757] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:21:51,760] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0485602e-35 1.0000000e+00 0.0000000e+00 4.8354419e-32 4.1574635e-35], sampled 0.6586210574024104
[2019-03-27 02:21:53,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.06934934]
[2019-03-27 02:21:53,107] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 80.0, 1.0, 2.0, 0.5695099131353062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 795838.0056688567, 795838.0056688561, 195202.6185813354]
[2019-03-27 02:21:53,108] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:21:53,112] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7942915e-37 1.0000000e+00 0.0000000e+00 1.0596028e-33 7.9841407e-38], sampled 0.40993208777241885
[2019-03-27 02:22:03,130] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8275.9292 2926934482.6160 1296.0000
[2019-03-27 02:22:03,212] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7910.2443 3162632666.3000 1740.0000
[2019-03-27 02:22:03,347] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.8995 3007351774.5010 1750.0000
[2019-03-27 02:22:03,355] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8686.4226 2778734355.2489 872.0000
[2019-03-27 02:22:03,458] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8509.3577 2841294812.2268 1109.0000
[2019-03-27 02:22:04,475] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1300000, evaluation results [1300000.0, 7910.244348130063, 3162632666.2999654, 1740.0, 8275.929218291107, 2926934482.6160054, 1296.0, 8686.422614907131, 2778734355.248881, 872.0, 8005.899525186868, 3007351774.500956, 1750.0, 8509.357705149874, 2841294812.226752, 1109.0]
[2019-03-27 02:22:04,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:22:04,899] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4749
[2019-03-27 02:22:04,907] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333334, 89.0, 1.0, 2.0, 0.3451865020357726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534531.4758566526, 534531.4758566519, 169450.6587296302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1549200.0000, 
sim time next is 1549800.0000, 
raw observation next is [22.35, 89.5, 1.0, 2.0, 0.3446836717122636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534035.2358530081, 534035.2358530075, 169418.3342282879], 
processed observation next is [0.0, 0.9565217391304348, 0.25829383886255936, 0.895, 1.0, 1.0, 0.2104622550750164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14834312107028003, 0.14834312107027986, 0.2528631854153551], 
reward next is 0.7471, 
noisyNet noise sample is [array([1.5510917], dtype=float32), -0.012095204]. 
=============================================
[2019-03-27 02:22:16,736] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8511954e-35 1.0000000e+00 0.0000000e+00 1.2194477e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 02:22:16,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8868
[2019-03-27 02:22:16,752] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 91.0, 1.0, 2.0, 0.2789237662727559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 449301.7335277609, 449301.7335277603, 163553.2581702353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1144800.0000, 
sim time next is 1145400.0000, 
raw observation next is [20.63333333333334, 90.33333333333334, 1.0, 2.0, 0.3657571851556497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588589.6711988831, 588589.6711988825, 174144.7103898676], 
processed observation next is [1.0, 0.2608695652173913, 0.17693522906793085, 0.9033333333333334, 1.0, 1.0, 0.2358520303080117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16349713088857865, 0.16349713088857848, 0.25991747819383226], 
reward next is 0.7401, 
noisyNet noise sample is [array([1.2596475], dtype=float32), 0.82217246]. 
=============================================
[2019-03-27 02:22:24,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.0182805e-37 1.0000000e+00 0.0000000e+00 4.3586177e-33 9.0834582e-38], sum to 1.0000
[2019-03-27 02:22:24,807] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3874
[2019-03-27 02:22:24,814] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 78.66666666666666, 1.0, 2.0, 0.4607359116030669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 643790.2275964209, 643790.2275964203, 177674.0386452523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1273200.0000, 
sim time next is 1273800.0000, 
raw observation next is [27.05, 79.33333333333334, 1.0, 2.0, 0.4675282410209003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 653284.1258420117, 653284.125842011, 178665.6062563016], 
processed observation next is [1.0, 0.7391304347826086, 0.4810426540284361, 0.7933333333333334, 1.0, 1.0, 0.3584677602661449, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18146781273389212, 0.18146781273389193, 0.26666508396462923], 
reward next is 0.7333, 
noisyNet noise sample is [array([2.293369], dtype=float32), 0.55409104]. 
=============================================
[2019-03-27 02:22:25,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7245484e-24 1.0000000e+00 7.6886821e-37 1.2518161e-20 7.1029555e-21], sum to 1.0000
[2019-03-27 02:22:25,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6437
[2019-03-27 02:22:25,375] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.00000000000001, 1.0, 2.0, 0.4584754386161387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650208.60854035, 650208.60854035, 178580.432649693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1296600.0000, 
sim time next is 1297200.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.459051136245247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651035.2717632691, 651035.2717632685, 178666.2408256308], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3482543810183699, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18084313104535252, 0.18084313104535235, 0.26666603108303105], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.34222394], dtype=float32), -0.042244982]. 
=============================================
[2019-03-27 02:22:28,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:22:28,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0082
[2019-03-27 02:22:28,604] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 83.5, 1.0, 2.0, 0.3583791891732259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549517.5405913909, 549517.5405913909, 170532.6455839205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1541400.0000, 
sim time next is 1542000.0000, 
raw observation next is [23.43333333333334, 84.0, 1.0, 2.0, 0.3581823112842414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549437.9069466967, 549437.9069466974, 170532.5589414631], 
processed observation next is [0.0, 0.8695652173913043, 0.30963665086887876, 0.84, 1.0, 1.0, 0.22672567624607395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15262164081852686, 0.15262164081852705, 0.25452620737531806], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.9285235], dtype=float32), -1.042924]. 
=============================================
[2019-03-27 02:22:28,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.237885]
 [80.24153 ]
 [80.137825]
 [80.11853 ]
 [80.10639 ]], R is [[80.1750946 ]
 [80.11882019]
 [80.0630722 ]
 [80.00788879]
 [79.95337677]].
[2019-03-27 02:22:36,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:22:36,977] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8084
[2019-03-27 02:22:36,983] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 97.33333333333334, 1.0, 2.0, 0.323469055500303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508083.6761096269, 508083.6761096276, 167562.6779779367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [20.8, 97.5, 1.0, 2.0, 0.3218962846542214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506212.7897439256, 506212.7897439249, 167434.2136142681], 
processed observation next is [0.0, 0.13043478260869565, 0.1848341232227489, 0.975, 1.0, 1.0, 0.18300757187255587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1406146638177571, 0.14061466381775692, 0.24990181136457928], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.08241206], dtype=float32), -1.2246199]. 
=============================================
[2019-03-27 02:22:52,401] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:22:52,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9481
[2019-03-27 02:22:52,414] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 97.5, 1.0, 2.0, 0.4504408295010301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643562.3911438324, 643562.3911438324, 178017.7064078728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1985400.0000, 
sim time next is 1986000.0000, 
raw observation next is [23.76666666666667, 97.33333333333333, 1.0, 2.0, 0.4512865427725447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643499.0266313003, 643499.0266313003, 177979.0370970285], 
processed observation next is [1.0, 1.0, 0.32543443917851517, 0.9733333333333333, 1.0, 1.0, 0.33889944912354786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17874972961980565, 0.17874972961980565, 0.2656403538761619], 
reward next is 0.7344, 
noisyNet noise sample is [array([0.7794598], dtype=float32), 0.27886018]. 
=============================================
[2019-03-27 02:22:52,434] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.95515]
 [73.93984]
 [73.93411]
 [73.93879]
 [73.83471]], R is [[73.98142242]
 [73.97590637]
 [73.97054291]
 [73.9656601 ]
 [73.96121216]].
[2019-03-27 02:22:56,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:22:56,410] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5833
[2019-03-27 02:22:56,419] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 86.5, 1.0, 2.0, 0.4971257563367059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694654.6938360535, 694654.6938360529, 183146.9945599539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2050200.0000, 
sim time next is 2050800.0000, 
raw observation next is [26.23333333333333, 86.66666666666667, 1.0, 2.0, 0.4959851917591979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 693060.4151499391, 693060.4151499397, 182969.4248038846], 
processed observation next is [0.0, 0.7391304347826086, 0.44233807266982617, 0.8666666666666667, 1.0, 1.0, 0.392753243083371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1925167819860942, 0.19251678198609437, 0.27308869373714123], 
reward next is 0.7269, 
noisyNet noise sample is [array([0.90267634], dtype=float32), 0.60423094]. 
=============================================
[2019-03-27 02:23:00,648] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 02:23:00,650] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:23:00,651] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:23:00,652] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:23:00,653] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:23:00,654] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:23:00,656] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:23:00,656] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:23:00,658] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:23:00,659] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:23:00,662] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:23:00,682] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-27 02:23:00,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-27 02:23:00,705] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-27 02:23:00,741] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-27 02:23:00,760] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-27 02:23:21,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:23:21,584] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.96666666666667, 95.0, 1.0, 2.0, 0.321592243169331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507438.3278033506, 507438.32780335, 167564.5567187917]
[2019-03-27 02:23:21,584] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:23:21,586] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23795159085380535
[2019-03-27 02:23:34,486] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:23:34,487] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.77753360666667, 87.78954965666667, 1.0, 2.0, 0.6263479539423346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875296.6954872793, 875296.6954872793, 205757.9650226582]
[2019-03-27 02:23:34,490] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:23:34,492] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10990804730257775
[2019-03-27 02:23:47,032] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:23:47,036] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.93281128, 93.178546525, 1.0, 2.0, 0.3535182433923977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548520.6213832466, 548520.6213832466, 170626.8996471346]
[2019-03-27 02:23:47,037] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:23:47,040] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10589820108562098
[2019-03-27 02:23:57,952] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:23:57,953] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.0, 56.0, 1.0, 2.0, 0.5944829802785813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830749.229837461, 830749.2298374603, 199723.4140715139]
[2019-03-27 02:23:57,954] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:23:57,956] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12706943456237552
[2019-03-27 02:23:58,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:23:58,311] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333333, 71.66666666666667, 1.0, 2.0, 0.6070122641293909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 848265.0371713643, 848265.037171365, 202059.8077364328]
[2019-03-27 02:23:58,311] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:23:58,313] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7513143129255698
[2019-03-27 02:24:10,867] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:24:10,869] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.90000000000001, 47.0, 1.0, 2.0, 0.669346844018579, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005973369574648, 6.9112, 168.9123160559816, 1832235.048614313, 1764999.813322123, 378142.6319065233]
[2019-03-27 02:24:10,873] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:24:10,875] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.7346560e-31 1.0000000e+00 0.0000000e+00 1.1629507e-27 5.8010775e-33], sampled 0.6219235493853179
[2019-03-27 02:24:10,876] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1832235.048614313 W.
[2019-03-27 02:24:11,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:24:11,323] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 0.8430611160315993, 1.0, 1.0, 0.8430611160315993, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2357886.678790007, 2357886.678790007, 441376.3467314463]
[2019-03-27 02:24:11,325] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:24:11,328] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2022236e-24 1.0000000e+00 9.9591596e-37 3.0929119e-21 7.5111719e-23], sampled 0.5128071326735137
[2019-03-27 02:24:11,332] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2357886.678790007 W.
[2019-03-27 02:24:25,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:24:25,420] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.60885321333333, 92.7922299, 1.0, 2.0, 0.4933589141363015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728858.8862831432, 728858.8862831426, 187496.7078321495]
[2019-03-27 02:24:25,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:24:25,428] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1389214666367794
[2019-03-27 02:24:33,471] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:24:33,472] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.09530161, 54.45932734666667, 1.0, 2.0, 0.4455687057821872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649183.5397049574, 649183.539704958, 178893.3654646772]
[2019-03-27 02:24:33,473] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:24:33,476] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05786737070534731
[2019-03-27 02:24:40,934] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.0404017]
[2019-03-27 02:24:40,935] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.88333333333334, 78.16666666666667, 1.0, 2.0, 0.4264226533309549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 620671.1091066434, 620671.1091066428, 176057.0423806288]
[2019-03-27 02:24:40,937] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:24:40,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24308964526933596
[2019-03-27 02:24:55,530] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:24:55,779] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.0190 3164057175.9822 1789.0000
[2019-03-27 02:24:56,230] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.5016 2927486968.6228 1339.0000
[2019-03-27 02:24:56,331] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.3076 2779202076.8639 928.0000
[2019-03-27 02:24:56,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3827 2842477500.9200 1133.0000
[2019-03-27 02:24:57,443] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1325000, evaluation results [1325000.0, 7883.019038674673, 3164057175.982237, 1789.0, 8257.501562266125, 2927486968.622825, 1339.0, 8663.307627499345, 2779202076.863919, 928.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.382681559728, 2842477500.91997, 1133.0]
[2019-03-27 02:25:13,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.554181e-34 1.000000e+00 0.000000e+00 9.499346e-33 0.000000e+00], sum to 1.0000
[2019-03-27 02:25:13,550] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9545
[2019-03-27 02:25:13,557] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.56666666666667, 75.33333333333333, 1.0, 2.0, 0.5865709141981166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 819688.3932872053, 819688.3932872047, 198270.7730353634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2407200.0000, 
sim time next is 2407800.0000, 
raw observation next is [30.48333333333333, 75.66666666666667, 1.0, 2.0, 0.5849447147882951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817415.0284312845, 817415.0284312845, 197974.7118572583], 
processed observation next is [1.0, 0.8695652173913043, 0.6437598736176934, 0.7566666666666667, 1.0, 1.0, 0.4999333913111989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22705973011980124, 0.22705973011980124, 0.2954846445630721], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.60834605], dtype=float32), -0.6237001]. 
=============================================
[2019-03-27 02:25:17,400] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:25:17,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6654
[2019-03-27 02:25:17,413] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.05, 67.0, 1.0, 2.0, 0.5658571838627734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790731.7506965585, 790731.7506965585, 194558.9448403714], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2399400.0000, 
sim time next is 2400000.0000, 
raw observation next is [31.93333333333333, 67.66666666666667, 1.0, 2.0, 0.5691840836718579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795382.5183233231, 795382.5183233238, 195146.5599630201], 
processed observation next is [1.0, 0.782608695652174, 0.7124802527646128, 0.6766666666666667, 1.0, 1.0, 0.4809446791227204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22093958842314532, 0.2209395884231455, 0.2912635223328658], 
reward next is 0.7087, 
noisyNet noise sample is [array([-2.2942572], dtype=float32), 0.095717356]. 
=============================================
[2019-03-27 02:25:17,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[57.348087]
 [56.459553]
 [55.756435]
 [55.492676]
 [55.60638 ]], R is [[57.69805908]
 [57.83069229]
 [57.96227646]
 [58.09288406]
 [58.224617  ]].
[2019-03-27 02:25:28,788] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:25:28,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4067
[2019-03-27 02:25:28,802] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.8244816380047731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1152330.970824007, 1152330.970824007, 250008.5680721605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2440800.0000, 
sim time next is 2441400.0000, 
raw observation next is [27.61666666666667, 84.16666666666667, 1.0, 2.0, 0.8077665982720743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1128956.89362321, 1128956.89362321, 245825.962736549], 
processed observation next is [1.0, 0.2608695652173913, 0.5078988941548186, 0.8416666666666667, 1.0, 1.0, 0.7683934918940655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3135991371175583, 0.3135991371175583, 0.36690442199484924], 
reward next is 0.6331, 
noisyNet noise sample is [array([0.59326434], dtype=float32), -0.5997352]. 
=============================================
[2019-03-27 02:25:31,785] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3562025e-13 1.0000000e+00 5.9437562e-22 7.5734904e-12 3.8918532e-10], sum to 1.0000
[2019-03-27 02:25:31,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1893
[2019-03-27 02:25:31,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1888038.56568049 W.
[2019-03-27 02:25:31,817] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 87.0, 1.0, 2.0, 0.45013904038265, 1.0, 1.0, 0.45013904038265, 1.0, 2.0, 0.772484979207247, 6.9112, 6.9112, 170.5573041426782, 1888038.56568049, 1888038.56568049, 380887.5686493419], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2451600.0000, 
sim time next is 2452200.0000, 
raw observation next is [26.9, 87.16666666666667, 1.0, 2.0, 0.7103634930262291, 1.0, 2.0, 0.7103634930262291, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1986430.979080065, 1986430.979080065, 378588.2732140051], 
processed observation next is [1.0, 0.391304347826087, 0.4739336492890995, 0.8716666666666667, 1.0, 1.0, 0.6510403530436496, 1.0, 1.0, 0.6510403530436496, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5517863830777958, 0.5517863830777958, 0.5650571242000076], 
reward next is 0.4349, 
noisyNet noise sample is [array([0.29656553], dtype=float32), -1.5695007]. 
=============================================
[2019-03-27 02:25:43,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:25:43,902] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9176
[2019-03-27 02:25:43,905] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3482468249197515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536465.1345835674, 536465.1345835674, 169526.656154694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2770200.0000, 
sim time next is 2770800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3477371748223802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535680.1998126784, 535680.1998126784, 169462.5135409374], 
processed observation next is [1.0, 0.043478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21414117448479542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14880005550352177, 0.14880005550352177, 0.25292912468796624], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.11876272], dtype=float32), 0.33472177]. 
=============================================
[2019-03-27 02:25:45,326] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:25:45,334] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9125
[2019-03-27 02:25:45,338] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3099701987121949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490351.5515316635, 490351.5515316641, 166313.7923316537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2948400.0000, 
sim time next is 2949000.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.3110690049642909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491912.914583405, 491912.914583405, 166424.9596614277], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.169962656583483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13664247627316806, 0.13664247627316806, 0.2483954621812354], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.81764036], dtype=float32), -1.1038836]. 
=============================================
[2019-03-27 02:25:45,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.26434 ]
 [70.243576]
 [70.40026 ]
 [70.54004 ]
 [70.61006 ]], R is [[70.28601837]
 [70.33493042]
 [70.38373566]
 [70.43188477]
 [70.48008728]].
[2019-03-27 02:25:49,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:25:49,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4597
[2019-03-27 02:25:49,267] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3608919693442823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555931.4741309017, 555931.4741309017, 171145.0564798836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2860800.0000, 
sim time next is 2861400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3551094340499869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547029.2468366164, 547029.246836617, 170398.2610608056], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.22302341451805652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15195256856572678, 0.15195256856572695, 0.25432576277732183], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.6212242], dtype=float32), -0.020183675]. 
=============================================
[2019-03-27 02:25:53,684] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 02:25:53,687] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:25:53,687] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:25:53,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:25:53,690] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:25:53,692] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:25:53,696] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:25:53,695] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:25:53,697] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:25:53,697] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:25:53,698] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:25:53,717] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-27 02:25:53,736] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-27 02:25:53,737] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-27 02:25:53,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-27 02:25:53,798] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-27 02:25:59,566] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.031994868]
[2019-03-27 02:25:59,568] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.73333333333333, 61.66666666666667, 1.0, 2.0, 0.2190088450354377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 364803.9372716596, 364803.937271659, 157521.1722152987]
[2019-03-27 02:25:59,571] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:25:59,573] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06063618625503131
[2019-03-27 02:26:10,263] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.031994868]
[2019-03-27 02:26:10,264] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.25, 87.33333333333334, 1.0, 2.0, 0.2940174415135339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470800.6567662177, 470800.6567662177, 165003.8930041764]
[2019-03-27 02:26:10,266] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:26:10,268] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6437267539458503
[2019-03-27 02:26:10,779] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.031994868]
[2019-03-27 02:26:10,781] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.16666666666666, 91.16666666666667, 1.0, 2.0, 0.2763779219209359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 447623.8562903187, 447623.8562903194, 163433.2696983895]
[2019-03-27 02:26:10,783] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:26:10,786] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6151855285661558
[2019-03-27 02:26:57,406] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.031994868]
[2019-03-27 02:26:57,407] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.83333333333333, 63.33333333333333, 1.0, 2.0, 0.5586214734096806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780616.8160840336, 780616.8160840336, 193290.0267907087]
[2019-03-27 02:26:57,410] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:26:57,411] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04018102279788183
[2019-03-27 02:27:23,122] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.031994868]
[2019-03-27 02:27:23,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.93392225, 81.88092595, 1.0, 2.0, 0.543472033875394, 0.0, 2.0, 0.0, 1.0, 2.0, 0.928926685616406, 6.911200000000001, 6.9112, 168.9128917396723, 1519422.24824647, 1519422.248246469, 329727.4619000961]
[2019-03-27 02:27:23,128] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:27:23,133] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2638513e-33 1.0000000e+00 0.0000000e+00 1.4637636e-32 6.3952611e-36], sampled 0.6151022738341294
[2019-03-27 02:27:48,825] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.2706 2927629861.6535 1340.0000
[2019-03-27 02:27:48,854] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7482 3007616673.1229 1767.0000
[2019-03-27 02:27:48,911] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7871.8284 3164094730.7630 1834.0000
[2019-03-27 02:27:48,914] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8669.5650 2779207059.9945 916.0000
[2019-03-27 02:27:49,008] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.4222 2842427469.5382 1139.0000
[2019-03-27 02:27:50,023] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1350000, evaluation results [1350000.0, 7871.828370555531, 3164094730.762967, 1834.0, 8258.270574692679, 2927629861.65349, 1340.0, 8669.565018344529, 2779207059.994456, 916.0, 7998.748246968197, 3007616673.1228576, 1767.0, 8496.422209203045, 2842427469.53822, 1139.0]
[2019-03-27 02:28:01,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:28:01,579] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3239
[2019-03-27 02:28:01,587] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3038629939412646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483883.6642718208, 483883.6642718214, 165900.8632374746], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3030254175235287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482550.5332816411, 482550.5332816406, 165804.8657590378], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16027158737774538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13404181480045585, 0.13404181480045574, 0.2474699488940863], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.55401146], dtype=float32), -1.071519]. 
=============================================
[2019-03-27 02:28:07,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:28:07,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6810
[2019-03-27 02:28:07,642] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5521228668724585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771532.3748875097, 771532.3748875103, 192166.4793978358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3693600.0000, 
sim time next is 3694200.0000, 
raw observation next is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.5563359687187139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777421.8864615781, 777421.8864615781, 192893.8610922055], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7566666666666667, 1.0, 1.0, 0.46546502255266736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21595052401710502, 0.21595052401710502, 0.287901285212247], 
reward next is 0.7121, 
noisyNet noise sample is [array([-2.8405614], dtype=float32), -0.51688457]. 
=============================================
[2019-03-27 02:28:12,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:28:12,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7419
[2019-03-27 02:28:12,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 91.33333333333334, 1.0, 2.0, 0.4741103120876279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662484.2189085392, 662484.2189085392, 179638.8861037538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3205200.0000, 
sim time next is 3205800.0000, 
raw observation next is [25.0, 91.0, 1.0, 2.0, 0.4726573727799099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660809.7376230506, 660809.7376230506, 179468.8078354376], 
processed observation next is [0.0, 0.08695652173913043, 0.38388625592417064, 0.91, 1.0, 1.0, 0.3646474370842288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1835582604508474, 0.1835582604508474, 0.2678638922916979], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.85286623], dtype=float32), -0.12589791]. 
=============================================
[2019-03-27 02:28:14,586] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:28:14,599] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6450
[2019-03-27 02:28:14,605] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 68.0, 1.0, 2.0, 0.5704442115171784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 797144.0921052421, 797144.0921052414, 195369.4869310706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3325200.0000, 
sim time next is 3325800.0000, 
raw observation next is [31.83333333333333, 67.5, 1.0, 2.0, 0.5728011104296235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800438.8857474062, 800438.8857474062, 195788.7925139981], 
processed observation next is [0.0, 0.4782608695652174, 0.7077409162717218, 0.675, 1.0, 1.0, 0.4853025426862933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22234413492983504, 0.22234413492983504, 0.29222207837910164], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.04065452], dtype=float32), -0.9226048]. 
=============================================
[2019-03-27 02:28:15,113] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3453274e-12 9.9998975e-01 3.9461306e-22 8.4745266e-12 1.0230278e-05], sum to 1.0000
[2019-03-27 02:28:15,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1203
[2019-03-27 02:28:15,129] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2272492.247517125 W.
[2019-03-27 02:28:15,135] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5417040790404464, 1.0, 2.0, 0.5417040790404464, 1.0, 1.0, 0.9407608410168316, 6.911200000000001, 6.9112, 170.5573041426782, 2272492.247517125, 2272492.247517124, 445183.8127206988], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3499200.0000, 
sim time next is 3499800.0000, 
raw observation next is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.8684171020381105, 1.0, 2.0, 0.8684171020381105, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2428871.621114033, 2428871.621114033, 454557.7051628443], 
processed observation next is [1.0, 0.5217391304347826, 0.7235387045813588, 0.6633333333333334, 1.0, 1.0, 0.8414663879977236, 1.0, 1.0, 0.8414663879977236, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6746865614205647, 0.6746865614205647, 0.6784443360639467], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5824443], dtype=float32), -0.019412752]. 
=============================================
[2019-03-27 02:28:26,267] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:28:26,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8558
[2019-03-27 02:28:26,282] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5074331952403807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709062.5180404293, 709062.5180404286, 184768.9709951507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3460200.0000, 
sim time next is 3460800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5068390298243134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708231.9833506766, 708231.983350676, 184674.5551597565], 
processed observation next is [1.0, 0.043478260869565216, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.4058301564148354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19673110648629905, 0.1967311064862989, 0.275633664417547], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.6348593], dtype=float32), -0.81090856]. 
=============================================
[2019-03-27 02:28:27,459] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:28:27,466] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0414
[2019-03-27 02:28:27,470] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5176435178990764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723334.7850147382, 723334.7850147382, 186406.6696250263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3539400.0000, 
sim time next is 3540000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5187864221581748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724932.3796360098, 724932.3796360098, 186591.8366918704], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42022460500984915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20137010545444717, 0.20137010545444717, 0.2784952786445827], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.33943525], dtype=float32), 0.21111473]. 
=============================================
[2019-03-27 02:28:27,487] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[60.217377]
 [60.21557 ]
 [60.100464]
 [60.09424 ]
 [60.08035 ]], R is [[60.34178925]
 [60.46015549]
 [60.57752991]
 [60.6936264 ]
 [60.80843353]].
[2019-03-27 02:28:33,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0169266e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:28:33,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8323
[2019-03-27 02:28:33,536] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666667, 81.5, 1.0, 2.0, 0.7433220553183536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1038843.363983553, 1038843.363983553, 230470.2475516235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3557400.0000, 
sim time next is 3558000.0000, 
raw observation next is [26.33333333333334, 82.0, 1.0, 2.0, 0.6731278364712743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 940698.6977311402, 940698.6977311409, 215147.9812941711], 
processed observation next is [1.0, 0.17391304347826086, 0.44707740916271754, 0.82, 1.0, 1.0, 0.6061781162304509, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2613051938142056, 0.2613051938142058, 0.32111638999130016], 
reward next is 0.6789, 
noisyNet noise sample is [array([-0.01724156], dtype=float32), 0.9948133]. 
=============================================
[2019-03-27 02:28:33,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[51.67979 ]
 [52.295414]
 [52.155075]
 [52.070606]
 [52.093723]], R is [[52.12105942]
 [52.25586319]
 [52.41544342]
 [52.56570435]
 [52.71192169]].
[2019-03-27 02:28:38,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.4343885e-16 1.0000000e+00 2.9615174e-21 1.3042210e-15 1.0603519e-16], sum to 1.0000
[2019-03-27 02:28:38,150] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4393
[2019-03-27 02:28:38,162] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2269751.222859652 W.
[2019-03-27 02:28:38,167] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.9819594253814511, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.998800778691988, 6.9112, 168.9124358634007, 2269751.222859652, 2207604.407027595, 458075.3930252831], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3670800.0000, 
sim time next is 3671400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.8350392880762221, 1.0, 1.0, 0.8350392880762221, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2335430.139113098, 2335430.139113098, 437289.856572366], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.63, 1.0, 1.0, 0.8012521543087013, 1.0, 0.5, 0.8012521543087013, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6487305941980828, 0.6487305941980828, 0.6526714277199493], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9900177], dtype=float32), 0.6535317]. 
=============================================
[2019-03-27 02:28:40,902] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5509193e-07 9.9999809e-01 3.1103974e-11 5.5665004e-07 1.1594240e-06], sum to 1.0000
[2019-03-27 02:28:40,915] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2201
[2019-03-27 02:28:40,928] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2878377.915803615 W.
[2019-03-27 02:28:40,934] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.33333333333334, 64.66666666666667, 1.0, 2.0, 1.028948353774965, 1.0, 1.0, 1.028948353774965, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2878377.915803615, 2878377.915803615, 546736.0182713519], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3763200.0000, 
sim time next is 3763800.0000, 
raw observation next is [34.5, 63.5, 1.0, 2.0, 0.7885856033117291, 1.0, 2.0, 0.7148828411701271, 1.0, 1.0, 1.03, 7.0051047180883, 6.9112, 170.5573041426782, 2999863.296214161, 2932595.526847803, 550769.6429380333], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.635, 1.0, 1.0, 0.7452838594117218, 1.0, 1.0, 0.656485350807382, 1.0, 0.5, 1.0365853658536586, 0.009390471808830014, 0.0, 0.8375144448122397, 0.8332953600594891, 0.8146098685688342, 0.8220442431910945], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.12404899], dtype=float32), 0.7567907]. 
=============================================
[2019-03-27 02:28:46,307] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 02:28:46,308] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:28:46,311] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:28:46,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:28:46,313] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:28:46,314] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:28:46,313] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:28:46,315] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:28:46,315] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:28:46,315] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:28:46,317] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:28:46,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-27 02:28:46,371] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-27 02:28:46,390] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-27 02:28:46,391] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-27 02:28:46,391] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-27 02:28:50,571] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.07232798]
[2019-03-27 02:28:50,574] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.45, 52.5, 1.0, 2.0, 0.3197711849945583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505605.5864970844, 505605.5864970844, 167445.898558555]
[2019-03-27 02:28:50,575] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:28:50,578] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15908576982127354
[2019-03-27 02:28:54,378] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.07232798]
[2019-03-27 02:28:54,380] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.76666666666667, 70.33333333333334, 1.0, 2.0, 0.4857345677833766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689577.4547142242, 689577.4547142249, 182781.3678766381]
[2019-03-27 02:28:54,383] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:54,386] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5648002024968147
[2019-03-27 02:29:36,518] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.07232798]
[2019-03-27 02:29:36,519] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.05858590666667, 79.89523438666667, 1.0, 2.0, 0.4281060898710455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691458.0862263828, 691458.0862263828, 183320.8730514569]
[2019-03-27 02:29:36,521] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:29:36,525] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6131238082159862
[2019-03-27 02:29:42,059] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.07232798]
[2019-03-27 02:29:42,060] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.64944021, 63.40755401, 1.0, 2.0, 0.5670414486133774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792387.2656546247, 792387.2656546247, 194764.9956821476]
[2019-03-27 02:29:42,061] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:29:42,066] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19412938877417207
[2019-03-27 02:29:49,319] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.07232798]
[2019-03-27 02:29:49,320] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.17360943, 92.295443425, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 6.976075121449962, 6.9112, 172.86236624, 2956439.472797451, 2909338.71138087, 553802.0904808565]
[2019-03-27 02:29:49,322] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:29:49,325] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3345814e-27 1.0000000e+00 3.7566555e-36 2.9030564e-25 2.9096204e-32], sampled 0.09852599310314436
[2019-03-27 02:29:49,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2956439.472797451 W.
[2019-03-27 02:30:09,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06754642], dtype=float32), -0.07232798]
[2019-03-27 02:30:09,553] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.3, 74.66666666666667, 1.0, 2.0, 0.6508041996767234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909487.9728100742, 909487.9728100742, 210592.8598737288]
[2019-03-27 02:30:09,554] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:30:09,556] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.49185709769621466
[2019-03-27 02:30:40,943] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.5460 2842401159.2927 1142.0000
[2019-03-27 02:30:41,098] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7871.6798 3164226989.1583 1854.0000
[2019-03-27 02:30:41,251] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6324 3007605109.6715 1768.0000
[2019-03-27 02:30:41,544] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.6467 2927777213.9352 1347.0000
[2019-03-27 02:30:41,557] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8667.4765 2779248718.5361 921.0000
[2019-03-27 02:30:42,575] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1375000, evaluation results [1375000.0, 7871.6798498802, 3164226989.1582785, 1854.0, 8255.64672894756, 2927777213.9351707, 1347.0, 8667.476532843477, 2779248718.5360756, 921.0, 7998.6323795926155, 3007605109.671462, 1768.0, 8495.545984434264, 2842401159.292665, 1142.0]
[2019-03-27 02:30:45,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:30:45,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6718
[2019-03-27 02:30:45,662] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.9393009492233895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104286, 1312906.594729926, 1312906.594729926, 280980.5140446651], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3993600.0000, 
sim time next is 3994200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.9352387253610301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1307225.131319354, 1307225.131319353, 279818.2610296505], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.9219743679048555, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3631180920331539, 0.3631180920331536, 0.4176391955666426], 
reward next is 0.5824, 
noisyNet noise sample is [array([-0.60970783], dtype=float32), -0.062286768]. 
=============================================
[2019-03-27 02:30:48,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:30:48,604] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2483
[2019-03-27 02:30:48,609] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.66666666666667, 1.0, 2.0, 0.6045622374870508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844839.9029176588, 844839.9029176594, 201599.4660853402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5995248817101846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837797.7203946159, 837797.7203946159, 200657.9150921838], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5174998574821501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23272158899850443, 0.23272158899850443, 0.2994894255107221], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.49134672], dtype=float32), -2.1995375]. 
=============================================
[2019-03-27 02:31:03,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.24847113e-10 9.98100698e-01 8.29069073e-16 1.03279305e-08
 1.89936382e-03], sum to 1.0000
[2019-03-27 02:31:03,437] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6599
[2019-03-27 02:31:03,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2229569.87989588 W.
[2019-03-27 02:31:03,452] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.33333333333334, 71.0, 1.0, 2.0, 0.5314816018697981, 1.0, 2.0, 0.5314816018697981, 1.0, 1.0, 0.9230077787962748, 6.911199999999999, 6.9112, 170.5573041426782, 2229569.87989588, 2229569.879895881, 437601.6511110714], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4105200.0000, 
sim time next is 4105800.0000, 
raw observation next is [33.5, 71.0, 1.0, 2.0, 0.750048877806581, 1.0, 2.0, 0.750048877806581, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2097514.183551732, 2097514.183551732, 396307.5327979375], 
processed observation next is [1.0, 0.5217391304347826, 0.7867298578199052, 0.71, 1.0, 1.0, 0.6988540696464831, 1.0, 1.0, 0.6988540696464831, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.58264282876437, 0.58264282876437, 0.5915037802954292], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5802418], dtype=float32), 0.90159935]. 
=============================================
[2019-03-27 02:31:04,778] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:04,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4074
[2019-03-27 02:31:04,792] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.9182105831270352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1283409.746844955, 1283409.746844956, 274997.6915802917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4250400.0000, 
sim time next is 4251000.0000, 
raw observation next is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.9087680681200244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1270203.772790312, 1270203.772790313, 272362.4198184401], 
processed observation next is [1.0, 0.17391304347826086, 0.581358609794629, 0.7833333333333334, 1.0, 1.0, 0.890082009783162, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35283438133064227, 0.3528343813306425, 0.40651107435588074], 
reward next is 0.5935, 
noisyNet noise sample is [array([-1.812594], dtype=float32), 1.1864597]. 
=============================================
[2019-03-27 02:31:04,807] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[53.42798 ]
 [53.03618 ]
 [52.375443]
 [51.421913]
 [49.580284]], R is [[53.44783783]
 [53.50291443]
 [53.56041336]
 [53.61170578]
 [53.63357162]].
[2019-03-27 02:31:07,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:07,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4727
[2019-03-27 02:31:07,972] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6176909986112216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863194.0239259549, 863194.0239259555, 204088.3194560096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4411200.0000, 
sim time next is 4411800.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6172839849490032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862625.0105602542, 862625.0105602542, 204010.4118206888], 
processed observation next is [0.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5388963674084375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2396180584889595, 0.2396180584889595, 0.3044931519711773], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.87448096], dtype=float32), 0.4397164]. 
=============================================
[2019-03-27 02:31:13,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5614761e-07 9.9702924e-01 1.7790192e-10 2.5534595e-07 2.9704045e-03], sum to 1.0000
[2019-03-27 02:31:13,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6710
[2019-03-27 02:31:13,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2846415.919101759 W.
[2019-03-27 02:31:13,887] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.33333333333334, 56.00000000000001, 1.0, 2.0, 0.7155342403202548, 1.0, 2.0, 0.67835715967439, 1.0, 2.0, 1.03, 7.005098957048339, 6.9112, 170.5573041426782, 2846415.919101759, 2779152.27660267, 526283.1041757816], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4278000.0000, 
sim time next is 4278600.0000, 
raw observation next is [37.5, 55.5, 1.0, 2.0, 0.6624640488104987, 1.0, 2.0, 0.651822063919512, 1.0, 2.0, 1.03, 7.005094772667834, 6.9112, 170.5573041426782, 2734951.630137227, 2667690.985080102, 509737.4828934383], 
processed observation next is [1.0, 0.5217391304347826, 0.976303317535545, 0.555, 1.0, 1.0, 0.5933301792897574, 1.0, 1.0, 0.5805085107464, 1.0, 1.0, 1.0365853658536586, 0.00938947726678343, 0.0, 0.8375144448122397, 0.7597087861492298, 0.7410252736333617, 0.7608022132737885], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9980544], dtype=float32), 0.12348784]. 
=============================================
[2019-03-27 02:31:14,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:14,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3775
[2019-03-27 02:31:14,507] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.0, 50.0, 1.0, 2.0, 0.5702680169259311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796897.7837683539, 796897.7837683539, 195338.9931550254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.5713963860043857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798475.1697896112, 798475.169789612, 195539.38747306], 
processed observation next is [1.0, 0.8260869565217391, 0.9052132701421801, 0.5, 1.0, 1.0, 0.48361010361974177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.221798658274892, 0.2217986582748922, 0.2918498320493433], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.82971734], dtype=float32), 0.047731448]. 
=============================================
[2019-03-27 02:31:14,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[67.94456 ]
 [67.4761  ]
 [66.978806]
 [66.77086 ]
 [67.27655 ]], R is [[68.2664032 ]
 [68.29219055]
 [68.31784058]
 [68.34292603]
 [68.36734009]].
[2019-03-27 02:31:21,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:21,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2273
[2019-03-27 02:31:21,401] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5589505377687268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 781076.8191990427, 781076.8191990422, 193348.4007264182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4645200.0000, 
sim time next is 4645800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5595439798008855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781906.399618737, 781906.399618737, 193451.758029752], 
processed observation next is [1.0, 0.782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4693300961456452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21719622211631584, 0.21719622211631584, 0.2887339672085851], 
reward next is 0.7113, 
noisyNet noise sample is [array([1.2801126], dtype=float32), -0.9853464]. 
=============================================
[2019-03-27 02:31:25,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:25,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5231
[2019-03-27 02:31:25,365] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4724233350370068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664883.6020439009, 664883.6020439015, 180001.7606253187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4767004995338715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668384.50316243, 668384.50316243, 180320.1372688563], 
processed observation next is [1.0, 0.9565217391304348, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.3695186741371945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856623619895639, 0.1856623619895639, 0.26913453323709896], 
reward next is 0.7309, 
noisyNet noise sample is [array([-1.1109259], dtype=float32), 0.76590115]. 
=============================================
[2019-03-27 02:31:25,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.57165]
 [72.54706]
 [72.76355]
 [72.68988]
 [72.31837]], R is [[72.45531464]
 [72.46209717]
 [72.46926117]
 [72.47683716]
 [72.48485565]].
[2019-03-27 02:31:32,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:32,446] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5241
[2019-03-27 02:31:32,451] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6414176210425633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896364.8508104136, 896364.8508104136, 208705.0979931641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4773600.0000, 
sim time next is 4774200.0000, 
raw observation next is [27.16666666666666, 79.83333333333334, 1.0, 2.0, 0.5919076776101134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827149.0195213107, 827149.0195213107, 199241.0098672827], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.7983333333333335, 1.0, 1.0, 0.5083225031447149, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22976361653369742, 0.22976361653369742, 0.29737464159295923], 
reward next is 0.7026, 
noisyNet noise sample is [array([-0.2692889], dtype=float32), -0.2895679]. 
=============================================
[2019-03-27 02:31:32,719] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:32,730] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5735
[2019-03-27 02:31:32,733] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 71.33333333333334, 1.0, 2.0, 0.4976041737908742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695323.4254254154, 695323.4254254154, 183221.5086271375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [28.5, 72.0, 1.0, 2.0, 0.4954519769395319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692315.0894553369, 692315.0894553375, 182886.5316965814], 
processed observation next is [1.0, 0.8260869565217391, 0.5497630331753555, 0.72, 1.0, 1.0, 0.39211081558979743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19230974707092693, 0.19230974707092707, 0.27296497268146475], 
reward next is 0.7270, 
noisyNet noise sample is [array([0.68901086], dtype=float32), -0.2762011]. 
=============================================
[2019-03-27 02:31:33,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:33,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0188
[2019-03-27 02:31:33,215] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.83333333333334, 1.0, 2.0, 0.4955026115049634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692385.8662403717, 692385.8662403717, 182893.895497831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5181000.0000, 
sim time next is 5181600.0000, 
raw observation next is [27.0, 80.66666666666667, 1.0, 2.0, 0.4982420050701053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696214.9862049058, 696214.9862049058, 183320.6880899467], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.8066666666666668, 1.0, 1.0, 0.39547229526518707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19339305172358495, 0.19339305172358495, 0.2736129672984279], 
reward next is 0.7264, 
noisyNet noise sample is [array([1.0738093], dtype=float32), -0.18799095]. 
=============================================
[2019-03-27 02:31:33,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:31:33,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7199
[2019-03-27 02:31:33,637] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 91.5, 1.0, 2.0, 0.9952515552379048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1391162.717099196, 1391162.717099196, 297487.5177597047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4602600.0000, 
sim time next is 4603200.0000, 
raw observation next is [28.66666666666666, 90.66666666666666, 1.0, 2.0, 0.9771414880796578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1365832.178960124, 1365832.178960124, 292043.7306130881], 
processed observation next is [1.0, 0.2608695652173913, 0.5576619273301735, 0.9066666666666666, 1.0, 1.0, 0.9724596241923589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37939782748892337, 0.37939782748892337, 0.4358861650941614], 
reward next is 0.5641, 
noisyNet noise sample is [array([-1.280148], dtype=float32), -1.059507]. 
=============================================
[2019-03-27 02:31:38,758] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 02:31:38,759] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:31:38,760] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:31:38,760] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:31:38,761] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:31:38,762] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:31:38,762] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:31:38,763] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:31:38,764] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:31:38,766] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:31:38,762] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:31:38,794] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-27 02:31:38,816] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-27 02:31:38,817] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-27 02:31:38,817] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-27 02:31:38,861] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-27 02:32:03,855] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:32:03,855] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.2, 91.0, 1.0, 2.0, 0.8618038511760767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1204523.687623024, 1204523.687623024, 259643.2187026259]
[2019-03-27 02:32:03,856] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:32:03,859] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16406892686368102
[2019-03-27 02:32:12,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:32:12,011] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.4, 86.0, 1.0, 2.0, 0.8730304101673947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1220223.810903348, 1220223.810903349, 262624.1888260762]
[2019-03-27 02:32:12,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:32:12,015] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.536276363959499
[2019-03-27 02:32:26,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:32:26,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.775493335, 78.95161572, 1.0, 2.0, 0.4298265250873325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628024.7775392408, 628024.7775392408, 176837.2651504096]
[2019-03-27 02:32:26,853] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:32:26,858] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.25355604839303514
[2019-03-27 02:32:31,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:32:31,277] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.063748625, 65.41541169, 1.0, 2.0, 0.5887356774008332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822714.6573691195, 822714.6573691195, 198665.162384835]
[2019-03-27 02:32:31,277] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:32:31,282] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4060746014780229
[2019-03-27 02:32:52,033] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:32:52,034] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.601709565, 51.18808032666667, 1.0, 2.0, 0.4853387235838773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683536.0499454992, 683536.0499454992, 182019.3801297778]
[2019-03-27 02:32:52,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:32:52,038] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9331666943719659
[2019-03-27 02:33:12,793] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:33:12,794] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.98333333333333, 55.5, 1.0, 2.0, 0.5311670307068563, 1.0, 2.0, 0.5311670307068563, 1.0, 2.0, 0.9063322053340445, 6.9112, 6.9112, 170.5573041426782, 2228249.074783599, 2228249.074783599, 434220.0069908962]
[2019-03-27 02:33:12,795] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:33:12,798] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.04312737e-12 1.46608477e-06 1.05189806e-16 1.19375707e-11
 9.99998569e-01], sampled 0.828644860609673
[2019-03-27 02:33:31,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:33:31,001] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.01494229666667, 96.89239072666666, 1.0, 2.0, 0.5299561520046515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740545.9752184614, 740545.975218462, 188420.8437377231]
[2019-03-27 02:33:31,005] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:33:31,006] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7695268719099289
[2019-03-27 02:33:32,129] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05853008], dtype=float32), -0.057230894]
[2019-03-27 02:33:32,130] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.15, 62.5, 1.0, 2.0, 0.3520542051651956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560945.3608498283, 560945.3608498278, 171863.7888235725]
[2019-03-27 02:33:32,131] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:33:32,134] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6942128756390942
[2019-03-27 02:33:33,659] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7983.9203 3159982378.7300 1671.0000
[2019-03-27 02:33:34,014] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8048.2757 3005642422.2612 1648.0000
[2019-03-27 02:33:34,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8699.4617 2778301722.4529 838.0000
[2019-03-27 02:33:34,181] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8314.2505 2925466717.4478 1203.0000
[2019-03-27 02:33:34,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8523.9907 2840831425.2169 1086.0000
[2019-03-27 02:33:35,287] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1400000, evaluation results [1400000.0, 7983.920275218801, 3159982378.7299967, 1671.0, 8314.250506329332, 2925466717.4477663, 1203.0, 8699.461686489274, 2778301722.4529023, 838.0, 8048.2756918418345, 3005642422.2611966, 1648.0, 8523.990652902814, 2840831425.2168536, 1086.0]
[2019-03-27 02:33:40,307] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.867523e-16 2.411094e-16 5.265735e-22 6.125272e-16 1.000000e+00], sum to 1.0000
[2019-03-27 02:33:40,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4485
[2019-03-27 02:33:40,323] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.4999605209181049, 1.0, 2.0, 0.4999605209181049, 1.0, 2.0, 0.8651880327951583, 6.9112, 6.9112, 170.5573041426782, 2097211.595615883, 2097211.595615883, 414563.4976185235], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5107695046097293, 1.0, 2.0, 0.5107695046097293, 1.0, 2.0, 0.8845092399401413, 6.911200000000001, 6.9112, 170.5573041426782, 2142597.972967761, 2142597.972967761, 422206.3055212561], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.66, 1.0, 1.0, 0.41056566820449303, 1.0, 1.0, 0.41056566820449303, 1.0, 1.0, 0.8591576096830992, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5951661036021558, 0.5951661036021558, 0.6301586649570987], 
reward next is 0.3698, 
noisyNet noise sample is [array([1.5267249], dtype=float32), 0.99848485]. 
=============================================
[2019-03-27 02:33:40,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.98182 ]
 [57.561134]
 [57.15794 ]
 [56.569515]
 [56.354855]], R is [[58.17371368]
 [57.97322464]
 [57.77639008]
 [57.57327271]
 [57.37839508]].
[2019-03-27 02:33:42,416] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.9646121e-15 4.5963756e-16 3.8047239e-21 3.2982196e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 02:33:42,427] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8275
[2019-03-27 02:33:42,432] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5890408423784436, 1.0, 2.0, 0.5890408423784436, 1.0, 2.0, 1.022969144428082, 6.9112, 6.9112, 170.5573041426782, 2471270.138158389, 2471270.138158389, 482178.1756272899], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4809600.0000, 
sim time next is 4810200.0000, 
raw observation next is [31.83333333333334, 63.5, 1.0, 2.0, 0.5889613448319788, 1.0, 2.0, 0.5889613448319788, 1.0, 2.0, 1.022831083480113, 6.911199999999999, 6.9112, 170.5573041426782, 2470936.28327916, 2470936.283279161, 482113.402918908], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.635, 1.0, 1.0, 0.5047727046168419, 1.0, 1.0, 0.5047727046168419, 1.0, 1.0, 1.0278427847318452, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6863711897997666, 0.686371189799767, 0.7195722431625492], 
reward next is 0.2804, 
noisyNet noise sample is [array([1.1975604], dtype=float32), 0.6426039]. 
=============================================
[2019-03-27 02:33:42,762] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:33:42,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3569
[2019-03-27 02:33:42,777] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5299414630530386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740525.4421315795, 740525.4421315801, 188420.6451424913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5561092231212424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777104.9170712325, 777104.9170712325, 192852.4636751746], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4651918350858342, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21586247696423125, 0.21586247696423125, 0.28783949802264863], 
reward next is 0.7122, 
noisyNet noise sample is [array([1.2872357], dtype=float32), 2.0381866]. 
=============================================
[2019-03-27 02:33:42,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.021355]
 [80.89351 ]
 [80.91258 ]
 [80.90398 ]
 [80.904144]], R is [[80.62310791]
 [80.53565216]
 [80.4495163 ]
 [80.36460114]
 [80.28081512]].
[2019-03-27 02:33:47,717] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:33:47,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8611
[2019-03-27 02:33:47,730] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4896975195386722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684271.5635466398, 684271.5635466403, 181997.5674451162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4910400.0000, 
sim time next is 4911000.0000, 
raw observation next is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4895930152108962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684125.4889527609, 684125.4889527604, 181981.4572746074], 
processed observation next is [1.0, 0.8695652173913043, 0.5181674565560824, 0.7483333333333334, 1.0, 1.0, 0.38505182555529666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19003485804243359, 0.19003485804243342, 0.27161411533523494], 
reward next is 0.7284, 
noisyNet noise sample is [array([1.7398258], dtype=float32), 1.7072132]. 
=============================================
[2019-03-27 02:33:47,751] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.33153 ]
 [78.455475]
 [78.52923 ]
 [78.793724]
 [78.91932 ]], R is [[77.98360443]
 [77.93212891]
 [77.88075256]
 [77.82950592]
 [77.77838135]].
[2019-03-27 02:33:48,977] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:33:48,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0240
[2019-03-27 02:33:48,994] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.83333333333334, 1.0, 2.0, 0.4903505526481806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685184.3639620292, 685184.3639620292, 182097.7699068118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4914600.0000, 
sim time next is 4915200.0000, 
raw observation next is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.8066666666666668, 1.0, 1.0, 0.3924437600539641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1924170449651395, 0.1924170449651395, 0.27302871357898206], 
reward next is 0.7270, 
noisyNet noise sample is [array([1.0577312], dtype=float32), 0.6611254]. 
=============================================
[2019-03-27 02:33:49,829] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6917716e-31 1.0000000e+00 0.0000000e+00 9.9247006e-34 5.8801188e-28], sum to 1.0000
[2019-03-27 02:33:49,838] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0151
[2019-03-27 02:33:49,843] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7751479250943737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1083344.88008745, 1083344.880087451, 237899.5875674327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4933800.0000, 
sim time next is 4934400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7493853896952534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047321.46678686, 1047321.46678686, 231863.7486280597], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.6980546863798234, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2909226296630167, 0.2909226296630167, 0.34606529645979056], 
reward next is 0.6539, 
noisyNet noise sample is [array([-0.01638834], dtype=float32), -1.1577082]. 
=============================================
[2019-03-27 02:33:59,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:33:59,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3525
[2019-03-27 02:33:59,557] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5682425839325336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794066.3670615365, 794066.3670615365, 194979.8072214978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5140800.0000, 
sim time next is 5141400.0000, 
raw observation next is [32.0, 66.33333333333334, 1.0, 2.0, 0.5779105018786459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807581.5226610766, 807581.5226610772, 196702.8963192595], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6633333333333334, 1.0, 1.0, 0.4914584359983685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22432820073918794, 0.2243282007391881, 0.2935864124168052], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.72406423], dtype=float32), -0.074873745]. 
=============================================
[2019-03-27 02:34:07,010] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1665173e-33 1.0000000e+00 0.0000000e+00 4.1037151e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 02:34:07,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6923
[2019-03-27 02:34:07,024] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.98660263004508, 6.9112, 168.9124035686867, 1507284.606958002, 1453791.561157479, 311353.3411929866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5459400.0000, 
sim time next is 5460000.0000, 
raw observation next is [27.63333333333333, 92.0, 1.0, 2.0, 0.9298339555523581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128951144307, 1299666.018307709, 1299666.01830771, 278278.9615278119], 
processed observation next is [1.0, 0.17391304347826086, 0.5086887835703, 0.92, 1.0, 1.0, 0.9154625970510339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439643669834, 0.36101833841880804, 0.36101833841880837, 0.41534173362359983], 
reward next is 0.5847, 
noisyNet noise sample is [array([-1.0489799], dtype=float32), 0.23470192]. 
=============================================
[2019-03-27 02:34:07,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[39.213078]
 [40.386757]
 [41.15249 ]
 [41.687527]
 [41.77283 ]], R is [[40.29655838]
 [40.05187225]
 [40.19463348]
 [40.35578918]
 [40.5405426 ]].
[2019-03-27 02:34:14,741] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:34:14,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9262
[2019-03-27 02:34:14,755] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666666, 78.66666666666667, 1.0, 2.0, 0.6261678895408834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875044.9588910389, 875044.9588910383, 205722.8452377084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5344800.0000, 
sim time next is 5345400.0000, 
raw observation next is [31.08333333333334, 78.83333333333334, 1.0, 2.0, 0.6249132033491641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 873290.8635204057, 873290.8635204063, 205479.5455949387], 
processed observation next is [1.0, 0.8695652173913043, 0.6721958925750398, 0.7883333333333334, 1.0, 1.0, 0.5480881968062218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24258079542233493, 0.24258079542233507, 0.30668588894766974], 
reward next is 0.6933, 
noisyNet noise sample is [array([-0.82333666], dtype=float32), 2.2342935]. 
=============================================
[2019-03-27 02:34:16,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:34:16,142] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0843
[2019-03-27 02:34:16,149] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 90.0, 1.0, 2.0, 0.5414135592833833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756561.938858219, 756561.9388582197, 190338.7116191654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5610600.0000, 
sim time next is 5611200.0000, 
raw observation next is [26.76666666666667, 90.0, 1.0, 2.0, 0.5384643945517558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752439.3674601886, 752439.367460188, 189841.7069438901], 
processed observation next is [1.0, 0.9565217391304348, 0.46761453396524505, 0.9, 1.0, 1.0, 0.4439330054840431, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20901093540560794, 0.20901093540560778, 0.2833458312595375], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.47818667], dtype=float32), -0.24381286]. 
=============================================
[2019-03-27 02:34:25,510] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:34:25,518] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3978
[2019-03-27 02:34:25,522] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 90.0, 1.0, 2.0, 0.5495751823582499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767970.9725694183, 767970.9725694183, 191727.8888013666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5529600.0000, 
sim time next is 5530200.0000, 
raw observation next is [27.11666666666667, 90.33333333333333, 1.0, 2.0, 0.5486145661652951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766628.1325044517, 766628.1325044523, 191563.3728262976], 
processed observation next is [1.0, 0.0, 0.4842022116903636, 0.9033333333333333, 1.0, 1.0, 0.4561621279099941, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21295225902901435, 0.21295225902901452, 0.2859154818302949], 
reward next is 0.7141, 
noisyNet noise sample is [array([-1.7764747], dtype=float32), 0.09479352]. 
=============================================
[2019-03-27 02:34:31,465] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 02:34:31,467] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:34:31,468] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:34:31,469] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:34:31,469] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:34:31,470] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:34:31,470] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:34:31,472] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:34:31,473] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:34:31,475] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:34:31,476] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:34:31,501] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-27 02:34:31,521] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-27 02:34:31,542] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-27 02:34:31,560] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-27 02:34:31,575] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-27 02:35:11,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.01434605]
[2019-03-27 02:35:11,258] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 97.33333333333333, 1.0, 2.0, 0.4605251161129595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649945.3692947906, 649945.3692947912, 178475.3750857814]
[2019-03-27 02:35:11,259] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:35:11,261] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7506148798040854
[2019-03-27 02:35:21,031] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.01434605]
[2019-03-27 02:35:21,032] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.93848533333333, 81.97524065666667, 1.0, 2.0, 0.981853532180064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1372422.853542718, 1372422.853542718, 293450.6815370757]
[2019-03-27 02:35:21,034] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:35:21,037] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03070894034687155
[2019-03-27 02:35:22,049] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.01434605]
[2019-03-27 02:35:22,050] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.93848533333333, 81.97524065666667, 1.0, 2.0, 0.5905594546674279, 0.0, 2.0, 0.0, 1.0, 2.0, 1.025606471761289, 6.911200000000001, 6.9112, 168.9128584261062, 1651170.197629508, 1651170.197629508, 361687.2798011859]
[2019-03-27 02:35:22,051] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:35:22,054] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4805331e-30 1.0000000e+00 1.5939983e-34 1.3326526e-28 1.9847530e-28], sampled 0.9123971267266396
[2019-03-27 02:35:55,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.01434605]
[2019-03-27 02:35:55,917] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.97150522, 89.79750865, 1.0, 2.0, 0.5220448240503451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729487.1097198755, 729487.1097198761, 187120.8821880847]
[2019-03-27 02:35:55,918] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:35:55,921] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.053225700584705105
[2019-03-27 02:36:07,536] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.01434605]
[2019-03-27 02:36:07,536] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.59157812666667, 80.32374197, 1.0, 2.0, 0.5169484397819306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722363.1798172116, 722363.1798172122, 186293.7687927333]
[2019-03-27 02:36:07,537] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:36:07,540] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16908133449719054
[2019-03-27 02:36:18,431] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.01434605]
[2019-03-27 02:36:18,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.88333333333333, 72.5, 1.0, 2.0, 0.3903168009724985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585509.4695010895, 585509.4695010888, 173301.4172231799]
[2019-03-27 02:36:18,435] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:36:18,439] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42631906379896745
[2019-03-27 02:36:26,747] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8278.6664 2926610867.4082 1292.0000
[2019-03-27 02:36:26,948] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8679.2507 2778940754.2691 892.0000
[2019-03-27 02:36:26,993] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8015.5497 3007128026.1122 1738.0000
[2019-03-27 02:36:27,150] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7917.2451 3162467877.3959 1823.0000
[2019-03-27 02:36:27,281] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.8041 2842174692.7030 1157.0000
[2019-03-27 02:36:28,297] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1425000, evaluation results [1425000.0, 7917.245056737842, 3162467877.3959303, 1823.0, 8278.666438705452, 2926610867.4081674, 1292.0, 8679.250708952459, 2778940754.2691407, 892.0, 8015.549710086854, 3007128026.112227, 1738.0, 8499.804110171623, 2842174692.7029815, 1157.0]
[2019-03-27 02:36:29,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:36:29,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1091
[2019-03-27 02:36:29,400] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 90.66666666666667, 1.0, 2.0, 0.521970848881068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729383.7039258432, 729383.7039258427, 187110.2387879196], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6226800.0000, 
sim time next is 6227400.0000, 
raw observation next is [26.41666666666666, 90.83333333333334, 1.0, 2.0, 0.5222044214293757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729710.2020953537, 729710.202095353, 187148.3660790444], 
processed observation next is [0.0, 0.043478260869565216, 0.4510268562401261, 0.9083333333333334, 1.0, 1.0, 0.4243426764209346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2026972783598205, 0.2026972783598203, 0.2793259195209618], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.5531528], dtype=float32), 0.36173388]. 
=============================================
[2019-03-27 02:36:31,718] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:36:31,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4164
[2019-03-27 02:36:31,732] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.75, 65.5, 1.0, 2.0, 0.5292678855813084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739583.8760587026, 739583.8760587032, 188309.0389110315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6265800.0000, 
sim time next is 6266400.0000, 
raw observation next is [30.76666666666667, 65.0, 1.0, 2.0, 0.5271507721093261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736624.4561835225, 736624.4561835225, 187959.5725366013], 
processed observation next is [0.0, 0.5217391304347826, 0.6571879936808849, 0.65, 1.0, 1.0, 0.43030213507147724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20461790449542291, 0.20461790449542291, 0.28053667542776317], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.02722764], dtype=float32), -0.6324834]. 
=============================================
[2019-03-27 02:36:38,150] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.9281414e-33 1.0000000e+00 8.1300413e-37 2.2847307e-30 1.5429348e-28], sum to 1.0000
[2019-03-27 02:36:38,156] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6374
[2019-03-27 02:36:38,161] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 90.16666666666667, 1.0, 2.0, 0.6862671686628288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959069.2553212187, 959069.2553212187, 217909.5682786445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5897400.0000, 
sim time next is 5898000.0000, 
raw observation next is [27.2, 89.33333333333334, 1.0, 2.0, 0.7328978633174595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024267.817487945, 1024267.817487945, 228107.2523535618], 
processed observation next is [1.0, 0.2608695652173913, 0.4881516587677725, 0.8933333333333334, 1.0, 1.0, 0.6781901967680235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2845188381910958, 0.2845188381910958, 0.34045858560233105], 
reward next is 0.6595, 
noisyNet noise sample is [array([-1.3403224], dtype=float32), -1.8614676]. 
=============================================
[2019-03-27 02:36:38,175] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[55.727478]
 [55.83587 ]
 [55.897324]
 [56.18094 ]
 [56.492317]], R is [[55.01335144]
 [55.13798141]
 [55.26331711]
 [55.38130951]
 [55.50085831]].
[2019-03-27 02:36:44,686] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8999203e-29 1.0000000e+00 1.3587610e-35 8.4490161e-27 2.7822926e-19], sum to 1.0000
[2019-03-27 02:36:44,697] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2929
[2019-03-27 02:36:44,703] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 88.66666666666667, 1.0, 2.0, 0.5291646017967019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739439.5000042234, 739439.5000042241, 188291.7101906741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6481200.0000, 
sim time next is 6481800.0000, 
raw observation next is [26.8, 89.0, 1.0, 2.0, 0.529803634049017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740332.7766027597, 740332.7766027591, 188397.4127823111], 
processed observation next is [1.0, 0.0, 0.4691943127962086, 0.89, 1.0, 1.0, 0.43349835427592404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2056479935007666, 0.20564799350076643, 0.2811901683318076], 
reward next is 0.7188, 
noisyNet noise sample is [array([-0.16218258], dtype=float32), 0.105330385]. 
=============================================
[2019-03-27 02:36:48,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0633160e-25 1.0000000e+00 1.7624423e-29 2.7751665e-21 2.1463974e-10], sum to 1.0000
[2019-03-27 02:36:48,390] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2690
[2019-03-27 02:36:48,396] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 91.16666666666667, 1.0, 2.0, 0.5459869711336645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762955.0406100419, 762955.0406100419, 191114.8963855663], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5961000.0000, 
sim time next is 5961600.0000, 
raw observation next is [26.9, 91.0, 1.0, 2.0, 0.5434065583132508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759347.9166735118, 759347.9166735118, 190676.3978049479], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.91, 1.0, 1.0, 0.449887419654519, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21092997685375328, 0.21092997685375328, 0.2845916385148476], 
reward next is 0.7154, 
noisyNet noise sample is [array([0.52569395], dtype=float32), 0.4273549]. 
=============================================
[2019-03-27 02:36:54,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:36:54,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0602
[2019-03-27 02:36:54,701] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 74.0, 1.0, 2.0, 0.5419591470242147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 190431.8515311663], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6256800.0000, 
sim time next is 6257400.0000, 
raw observation next is [29.85, 73.16666666666667, 1.0, 2.0, 0.541994185220643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757373.5847509764, 757373.5847509764, 190437.789080957], 
processed observation next is [0.0, 0.43478260869565216, 0.613744075829384, 0.7316666666666667, 1.0, 1.0, 0.4481857653260759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21038155131971567, 0.21038155131971567, 0.2842355060909806], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.0315129], dtype=float32), -0.77709633]. 
=============================================
[2019-03-27 02:36:55,027] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9981864e-17 6.7095840e-09 9.1725774e-20 7.9855630e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 02:36:55,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8724
[2019-03-27 02:36:55,049] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 67.5, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.771778989449849, 6.911200000000001, 6.9112, 170.5573041426782, 1874117.748292412, 1874117.748292412, 379653.5834916316], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6085800.0000, 
sim time next is 6086400.0000, 
raw observation next is [30.73333333333333, 66.66666666666666, 1.0, 2.0, 0.4580597392549088, 1.0, 2.0, 0.4580597392549088, 1.0, 2.0, 0.7910710550853826, 6.9112, 6.9112, 170.5573041426782, 1921290.499283267, 1921290.499283267, 386631.4821238596], 
processed observation next is [1.0, 0.43478260869565216, 0.6556082148499209, 0.6666666666666665, 1.0, 1.0, 0.3470599268131431, 1.0, 1.0, 0.3470599268131431, 1.0, 1.0, 0.7452086037626616, 0.0, 0.0, 0.8375144448122397, 0.533691805356463, 0.533691805356463, 0.5770619136177009], 
reward next is 0.4229, 
noisyNet noise sample is [array([-0.6407618], dtype=float32), -0.5874164]. 
=============================================
[2019-03-27 02:37:01,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8408116e-28 1.0000000e+00 3.9359451e-31 1.1951081e-25 5.4274922e-18], sum to 1.0000
[2019-03-27 02:37:01,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1636
[2019-03-27 02:37:01,061] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 82.5, 1.0, 2.0, 0.7477932858261217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1045095.289048516, 1045095.289048517, 231498.6661047348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6419400.0000, 
sim time next is 6420000.0000, 
raw observation next is [27.63333333333333, 82.0, 1.0, 2.0, 0.7601599449829616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062387.236601977, 1062387.236601977, 234365.63250699], 
processed observation next is [1.0, 0.30434782608695654, 0.5086887835703, 0.82, 1.0, 1.0, 0.7110360782927247, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29510756572277136, 0.29510756572277136, 0.34979945150297015], 
reward next is 0.6502, 
noisyNet noise sample is [array([-0.726797], dtype=float32), 0.75326806]. 
=============================================
[2019-03-27 02:37:01,076] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[53.896523]
 [53.791214]
 [53.13816 ]
 [52.845497]
 [52.827427]], R is [[53.9847374 ]
 [54.09936905]
 [54.19512558]
 [54.2829361 ]
 [54.36890411]].
[2019-03-27 02:37:13,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:37:13,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6040
[2019-03-27 02:37:13,133] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 75.0, 1.0, 2.0, 0.5164639413585206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721685.9310168019, 721685.9310168013, 186215.6164827244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6372000.0000, 
sim time next is 6372600.0000, 
raw observation next is [28.43333333333333, 75.66666666666667, 1.0, 2.0, 0.5180286488138364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723873.1352450701, 723873.1352450701, 186468.7692289273], 
processed observation next is [0.0, 0.782608695652174, 0.546603475513428, 0.7566666666666667, 1.0, 1.0, 0.4193116250769113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20107587090140835, 0.20107587090140835, 0.2783115958640706], 
reward next is 0.7217, 
noisyNet noise sample is [array([-0.19380206], dtype=float32), 0.6694939]. 
=============================================
[2019-03-27 02:37:13,751] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:37:13,757] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8436
[2019-03-27 02:37:13,765] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 82.00000000000001, 1.0, 2.0, 0.5182384788019105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724166.4434743027, 724166.4434743021, 186502.6230447753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6387000.0000, 
sim time next is 6387600.0000, 
raw observation next is [27.33333333333334, 82.0, 1.0, 2.0, 0.5170421773306886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722494.2094832232, 722494.2094832226, 186308.9335406418], 
processed observation next is [0.0, 0.9565217391304348, 0.4944707740916275, 0.82, 1.0, 1.0, 0.4181231052176971, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.200692835967562, 0.20069283596756182, 0.27807303513528625], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.31329036], dtype=float32), -0.11504693]. 
=============================================
[2019-03-27 02:37:22,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:37:22,439] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4519
[2019-03-27 02:37:22,447] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 69.33333333333334, 1.0, 2.0, 0.7703379065677078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1201873.870929913, 1201873.870929913, 252678.3682327296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6771000.0000, 
sim time next is 6771600.0000, 
raw observation next is [25.2, 68.0, 1.0, 2.0, 0.7824940040863644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1220329.309675551, 1220329.309675551, 255869.7009395369], 
processed observation next is [1.0, 0.391304347826087, 0.3933649289099526, 0.68, 1.0, 1.0, 0.7379445832365836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3389803637987642, 0.3389803637987642, 0.38189507602915956], 
reward next is 0.6181, 
noisyNet noise sample is [array([0.3555938], dtype=float32), 0.21817057]. 
=============================================
[2019-03-27 02:37:24,636] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 02:37:24,638] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:37:24,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:37:24,641] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:37:24,641] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:37:24,642] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:37:24,642] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:37:24,643] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:37:24,644] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:37:24,644] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:37:24,649] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:37:24,675] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-27 02:37:24,696] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-27 02:37:24,716] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-27 02:37:24,733] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-27 02:37:24,753] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-27 02:37:28,089] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.014486646]
[2019-03-27 02:37:28,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.83333333333333, 72.0, 1.0, 2.0, 0.239868738614187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 397723.0561620012, 397723.0561620006, 159741.7392995639]
[2019-03-27 02:37:28,092] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:37:28,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2737971480819441
[2019-03-27 02:37:32,443] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.014486646]
[2019-03-27 02:37:32,445] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.96666666666667, 71.66666666666667, 1.0, 2.0, 0.5455306592616062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791128.7772348553, 791128.7772348553, 194688.512434128]
[2019-03-27 02:37:32,448] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:37:32,451] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9337023583953754
[2019-03-27 02:38:02,543] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.014486646]
[2019-03-27 02:38:02,544] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.72668171, 84.44772910333333, 1.0, 2.0, 0.5524017014762992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771922.1578633531, 771922.1578633526, 192212.4930197178]
[2019-03-27 02:38:02,545] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:38:02,546] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5819058675828073
[2019-03-27 02:38:25,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.014486646]
[2019-03-27 02:38:25,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.214579215, 72.92836447, 1.0, 2.0, 0.8018925179327173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1120742.78421752, 1120742.78421752, 244387.9765144238]
[2019-03-27 02:38:25,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:38:25,104] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37059403947427216
[2019-03-27 02:38:48,642] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.014486646]
[2019-03-27 02:38:48,643] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.91666666666666, 94.83333333333333, 1.0, 2.0, 0.817234511469615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104292, 1142196.62750742, 1142196.62750742, 248184.5119213736]
[2019-03-27 02:38:48,646] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:38:48,649] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.997204753456829
[2019-03-27 02:39:18,936] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:39:19,584] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:39:19,716] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:39:19,722] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:39:19,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:39:20,881] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1450000, evaluation results [1450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:39:20,926] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:20,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7039
[2019-03-27 02:39:20,933] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 55.5, 1.0, 2.0, 0.3219236388233854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507651.7098080955, 507651.709808095, 167574.4371042627], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6809400.0000, 
sim time next is 6810000.0000, 
raw observation next is [26.8, 56.0, 1.0, 2.0, 0.3208807839871262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506548.4462990309, 506548.4462990315, 167501.6409878361], 
processed observation next is [1.0, 0.8260869565217391, 0.4691943127962086, 0.56, 1.0, 1.0, 0.18178407709292316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1407079017497308, 0.14070790174973097, 0.25000244923557624], 
reward next is 0.7500, 
noisyNet noise sample is [array([-1.3966805], dtype=float32), -0.87060434]. 
=============================================
[2019-03-27 02:39:20,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.02645 ]
 [72.39785 ]
 [71.46223 ]
 [70.93241 ]
 [69.672615]], R is [[73.5333252 ]
 [73.54787445]
 [73.56225586]
 [73.5765152 ]
 [73.59057617]].
[2019-03-27 02:39:24,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1496492e-34 1.0000000e+00 3.3354536e-38 2.7008997e-35 4.4128274e-31], sum to 1.0000
[2019-03-27 02:39:24,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0203
[2019-03-27 02:39:24,168] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2149861.426173563 W.
[2019-03-27 02:39:24,175] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.5, 85.0, 1.0, 2.0, 0.5124992883556094, 1.0, 2.0, 0.5124992883556094, 1.0, 1.0, 0.8806934048621068, 6.9112, 6.9112, 170.5573041426782, 2149861.426173563, 2149861.426173563, 422154.7668975826], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6627600.0000, 
sim time next is 6628200.0000, 
raw observation next is [27.46666666666667, 85.00000000000001, 1.0, 2.0, 0.3592269866071954, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6139244991657418, 6.9112, 6.9112, 168.9129565104277, 1004072.089595698, 1004072.089595698, 241645.3444789068], 
processed observation next is [1.0, 0.7391304347826086, 0.500789889415482, 0.8500000000000001, 1.0, 1.0, 0.2279843212134884, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.529176218494807, 0.0, 0.0, 0.8294399451522865, 0.2789089137765828, 0.2789089137765828, 0.3606646932520997], 
reward next is 0.6393, 
noisyNet noise sample is [array([0.15152264], dtype=float32), 0.52040064]. 
=============================================
[2019-03-27 02:39:24,586] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:24,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2644
[2019-03-27 02:39:24,601] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 63.66666666666667, 1.0, 2.0, 0.3230403350281488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508310.362703042, 508310.362703042, 167600.9344674215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6816000.0000, 
sim time next is 6816600.0000, 
raw observation next is [25.45, 64.5, 1.0, 2.0, 0.3244562366740797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510224.5839530903, 510224.5839530903, 167740.2925411642], 
processed observation next is [1.0, 0.9130434782608695, 0.4052132701421801, 0.645, 1.0, 1.0, 0.18609185141455384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14172905109808065, 0.14172905109808065, 0.2503586455838272], 
reward next is 0.7496, 
noisyNet noise sample is [array([2.776947], dtype=float32), 0.23842612]. 
=============================================
[2019-03-27 02:39:31,369] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:31,380] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2252
[2019-03-27 02:39:31,386] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 78.66666666666667, 1.0, 2.0, 0.3776137079235777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570824.3978629658, 570824.3978629658, 172123.2898209765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6853200.0000, 
sim time next is 6853800.0000, 
raw observation next is [24.8, 78.33333333333333, 1.0, 2.0, 0.3801581559053236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573762.7834650266, 573762.7834650266, 172355.1561369583], 
processed observation next is [0.0, 0.30434782608695654, 0.3744075829383887, 0.7833333333333333, 1.0, 1.0, 0.2532025974762935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15937855096250741, 0.15937855096250741, 0.2572465016969527], 
reward next is 0.7428, 
noisyNet noise sample is [array([0.8739781], dtype=float32), -0.6671537]. 
=============================================
[2019-03-27 02:39:39,610] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:39,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8835
[2019-03-27 02:39:39,628] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 89.33333333333334, 1.0, 2.0, 0.4188181688535201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 615101.9956789002, 615101.9956789007, 175677.209279143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6925200.0000, 
sim time next is 6925800.0000, 
raw observation next is [23.96666666666667, 89.66666666666666, 1.0, 2.0, 0.4177167231459037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613994.5689308227, 613994.5689308232, 175586.1054938332], 
processed observation next is [0.0, 0.13043478260869565, 0.33491311216429714, 0.8966666666666666, 1.0, 1.0, 0.2984538833083177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17055404692522852, 0.1705540469252287, 0.2620688141699003], 
reward next is 0.7379, 
noisyNet noise sample is [array([0.28726956], dtype=float32), -1.2174213]. 
=============================================
[2019-03-27 02:39:44,176] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:44,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6455
[2019-03-27 02:39:44,192] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 92.5, 1.0, 2.0, 0.5898481219932419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830319.3067673407, 830319.30676734, 199647.9751432616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7093800.0000, 
sim time next is 7094400.0000, 
raw observation next is [24.6, 92.66666666666667, 1.0, 2.0, 0.569235914044757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802345.5808369551, 802345.5808369551, 196039.0806612568], 
processed observation next is [1.0, 0.08695652173913043, 0.36492890995260674, 0.9266666666666667, 1.0, 1.0, 0.48100712535512896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287377245470974, 0.22287377245470974, 0.2925956427779952], 
reward next is 0.7074, 
noisyNet noise sample is [array([-0.7094338], dtype=float32), 0.6463968]. 
=============================================
[2019-03-27 02:39:46,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:46,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2578
[2019-03-27 02:39:46,565] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 85.33333333333334, 1.0, 2.0, 0.3744422404162747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568530.9732790663, 568530.9732790668, 171997.7554990762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7237200.0000, 
sim time next is 7237800.0000, 
raw observation next is [23.5, 86.0, 1.0, 2.0, 0.3718227657788394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564717.0880237719, 564717.0880237719, 171670.4953149956], 
processed observation next is [1.0, 0.782608695652174, 0.31279620853080575, 0.86, 1.0, 1.0, 0.243159958769686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15686585778438109, 0.15686585778438109, 0.2562246198731278], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.22203793], dtype=float32), 1.9324576]. 
=============================================
[2019-03-27 02:39:53,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:53,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4271
[2019-03-27 02:39:53,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1695312.230634194 W.
[2019-03-27 02:39:53,596] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.9, 70.66666666666667, 1.0, 2.0, 0.6063347461047273, 0.0, 1.0, 0.0, 1.0, 1.0, 1.009803541761559, 6.911199999999999, 6.9112, 168.9129565102935, 1695312.230634194, 1695312.230634194, 361894.9595099255], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7119600.0000, 
sim time next is 7120200.0000, 
raw observation next is [28.0, 70.33333333333333, 1.0, 2.0, 0.407275883103746, 1.0, 1.0, 0.407275883103746, 1.0, 2.0, 0.6832351480007928, 6.9112, 6.9112, 170.5573041426782, 1708112.387080668, 1708112.387080668, 353413.7247219526], 
processed observation next is [1.0, 0.391304347826087, 0.5260663507109005, 0.7033333333333333, 1.0, 1.0, 0.2858745579563205, 1.0, 0.5, 0.2858745579563205, 1.0, 1.0, 0.6137014000009668, 0.0, 0.0, 0.8375144448122397, 0.47447566307796335, 0.47447566307796335, 0.5274831712267949], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3867537], dtype=float32), 0.55987203]. 
=============================================
[2019-03-27 02:39:54,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:39:54,737] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4335
[2019-03-27 02:39:54,745] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.96666666666667, 92.0, 1.0, 2.0, 0.6419716570852705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021055.069585225, 1021055.069585225, 223176.3890289467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7392000.0000, 
sim time next is 7392600.0000, 
raw observation next is [20.95, 92.0, 1.0, 2.0, 0.6429167467192056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1022892.708885162, 1022892.708885162, 223417.3396784005], 
processed observation next is [1.0, 0.5652173913043478, 0.19194312796208532, 0.92, 1.0, 1.0, 0.5697792129147056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2841368635792117, 0.2841368635792117, 0.33345871593791115], 
reward next is 0.6665, 
noisyNet noise sample is [array([-0.03388494], dtype=float32), -0.90867716]. 
=============================================
[2019-03-27 02:40:00,270] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:40:00,275] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5087
[2019-03-27 02:40:00,280] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 89.0, 1.0, 2.0, 0.417661102547572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624482.1407145852, 624482.1407145846, 176857.0954445016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7362000.0000, 
sim time next is 7362600.0000, 
raw observation next is [23.4, 89.16666666666667, 1.0, 2.0, 0.4810742533325779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721864.3952393344, 721864.3952393337, 186815.3841790281], 
processed observation next is [1.0, 0.21739130434782608, 0.30805687203791465, 0.8916666666666667, 1.0, 1.0, 0.37478825702720225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20051788756648178, 0.2005178875664816, 0.2788289316104897], 
reward next is 0.7212, 
noisyNet noise sample is [array([0.9398805], dtype=float32), -0.044262756]. 
=============================================
[2019-03-27 02:40:07,126] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:40:07,136] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9003
[2019-03-27 02:40:07,144] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 77.33333333333334, 1.0, 2.0, 0.3737902454831094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 567501.1864865342, 567501.1864865342, 171906.6403445806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7341600.0000, 
sim time next is 7342200.0000, 
raw observation next is [24.75, 77.0, 1.0, 2.0, 0.3726239147430377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566293.9078943753, 566293.9078943753, 171818.3161430039], 
processed observation next is [1.0, 1.0, 0.3720379146919432, 0.77, 1.0, 1.0, 0.24412519848558759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15730386330399312, 0.15730386330399312, 0.2564452479746327], 
reward next is 0.7436, 
noisyNet noise sample is [array([-0.99894947], dtype=float32), 0.83619386]. 
=============================================
[2019-03-27 02:40:07,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:40:07,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8669
[2019-03-27 02:40:07,606] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 95.0, 1.0, 2.0, 0.5300940610091275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763228.287883301, 763228.2878833016, 191303.9255806483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7612200.0000, 
sim time next is 7612800.0000, 
raw observation next is [23.76666666666667, 95.0, 1.0, 2.0, 0.5236451941342106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755012.724271016, 755012.724271016, 190340.6590097879], 
processed observation next is [1.0, 0.08695652173913043, 0.32543443917851517, 0.95, 1.0, 1.0, 0.4260785471496513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20972575674194888, 0.20972575674194888, 0.2840905358355043], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.5678009], dtype=float32), 0.3123799]. 
=============================================
[2019-03-27 02:40:10,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:40:10,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:40:10,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-27 02:40:16,831] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 02:40:16,833] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:40:16,834] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:40:16,834] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:40:16,835] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:40:16,836] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:40:16,838] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:40:16,837] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:40:16,840] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:40:16,838] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:40:16,844] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:40:16,874] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-27 02:40:16,875] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-27 02:40:16,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-27 02:40:16,935] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-27 02:40:16,955] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-27 02:40:20,830] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.04184035]
[2019-03-27 02:40:20,830] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.54262957166667, 91.83526154166667, 1.0, 2.0, 0.3273354254108292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512922.312840549, 512922.312840549, 167904.4031670879]
[2019-03-27 02:40:20,831] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:40:20,835] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18963307858161904
[2019-03-27 02:40:32,815] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.04184035]
[2019-03-27 02:40:32,817] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.86666666666667, 81.66666666666667, 1.0, 2.0, 0.2990871394026292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480023.8840984244, 480023.884098425, 165661.0253673312]
[2019-03-27 02:40:32,817] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:40:32,822] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5123567789605985
[2019-03-27 02:40:49,483] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.04184035]
[2019-03-27 02:40:49,484] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.43974788666667, 67.644910855, 1.0, 2.0, 0.5677312279013399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834010.3208023167, 834010.3208023167, 200013.9950922203]
[2019-03-27 02:40:49,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:40:49,490] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9718347697210559
[2019-03-27 02:41:25,583] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.04184035]
[2019-03-27 02:41:25,584] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.56666666666667, 67.66666666666667, 1.0, 2.0, 0.9029991274285604, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987962021444, 6.9112, 168.9123159595483, 2159234.461302946, 2091988.873704409, 434906.9704495575]
[2019-03-27 02:41:25,585] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:41:25,586] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23405830843109665
[2019-03-27 02:41:25,588] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2159234.461302946 W.
[2019-03-27 02:42:04,014] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.04139795], dtype=float32), -0.04184035]
[2019-03-27 02:42:04,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.06666666666667, 81.0, 1.0, 2.0, 0.6806032966481058, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00214970564372, 6.9112, 168.9123411462712, 1847986.289971439, 1783463.67360269, 380644.7691994638]
[2019-03-27 02:42:04,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:42:04,019] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4425703369675007
[2019-03-27 02:42:04,022] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1847986.289971439 W.
[2019-03-27 02:42:11,580] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:42:11,869] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:42:11,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:42:11,911] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:42:12,030] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:42:13,048] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1475000, evaluation results [1475000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:42:13,693] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:13,702] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7439
[2019-03-27 02:42:13,710] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 82.33333333333334, 1.0, 2.0, 0.4056674306009919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596655.12751652, 596655.12751652, 173970.7512253905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7500000.0000, 
sim time next is 7500600.0000, 
raw observation next is [24.85, 83.0, 1.0, 2.0, 0.4047198760001976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595707.356162778, 595707.356162778, 173897.1060322049], 
processed observation next is [0.0, 0.8260869565217391, 0.37677725118483424, 0.83, 1.0, 1.0, 0.2827950313255393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16547426560077166, 0.16547426560077166, 0.2595479194510521], 
reward next is 0.7405, 
noisyNet noise sample is [array([0.47288442], dtype=float32), -1.3018036]. 
=============================================
[2019-03-27 02:42:14,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:14,990] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9502
[2019-03-27 02:42:15,000] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 88.5, 1.0, 2.0, 0.5002615607298454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699037.9267299458, 699037.9267299464, 183636.8798914541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7677000.0000, 
sim time next is 7677600.0000, 
raw observation next is [25.96666666666667, 88.33333333333334, 1.0, 2.0, 0.4975514163924357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695249.6811494034, 695249.6811494041, 183213.1641964851], 
processed observation next is [1.0, 0.8695652173913043, 0.42969984202211703, 0.8833333333333334, 1.0, 1.0, 0.39464026071377795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19312491143038985, 0.19312491143039004, 0.27345248387535087], 
reward next is 0.7265, 
noisyNet noise sample is [array([-0.63302326], dtype=float32), -1.7944119]. 
=============================================
[2019-03-27 02:42:21,991] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.3806196e-26 1.0000000e+00 2.0798990e-29 4.8189600e-25 1.6453432e-20], sum to 1.0000
[2019-03-27 02:42:21,996] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6989
[2019-03-27 02:42:22,004] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2281947.413649644 W.
[2019-03-27 02:42:22,011] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 81.5, 1.0, 2.0, 0.8159338397775644, 1.0, 1.0, 0.8159338397775644, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2281947.413649644, 2281947.413649644, 427698.2949782705], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7720200.0000, 
sim time next is 7720800.0000, 
raw observation next is [28.2, 80.33333333333334, 1.0, 2.0, 0.7343336044448311, 1.0, 2.0, 0.7343336044448311, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2053524.246153267, 2053524.246153266, 389166.109801647], 
processed observation next is [1.0, 0.34782608695652173, 0.5355450236966824, 0.8033333333333335, 1.0, 1.0, 0.6799200053552181, 1.0, 1.0, 0.6799200053552181, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5704234017092409, 0.5704234017092406, 0.5808449400024582], 
reward next is 0.4192, 
noisyNet noise sample is [array([-2.2328591], dtype=float32), 0.76401556]. 
=============================================
[2019-03-27 02:42:25,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:25,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:25,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-27 02:42:26,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:26,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:26,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-27 02:42:26,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:26,253] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:26,300] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-27 02:42:27,693] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2703279e-34 1.0000000e+00 0.0000000e+00 1.2128721e-34 4.1082440e-35], sum to 1.0000
[2019-03-27 02:42:27,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2604
[2019-03-27 02:42:27,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2150732.963370247 W.
[2019-03-27 02:42:27,721] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.3, 71.0, 1.0, 2.0, 0.7690602648520954, 1.0, 2.0, 0.7690602648520954, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2150732.963370247, 2150732.963370247, 405089.8353983599], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7817400.0000, 
sim time next is 7818000.0000, 
raw observation next is [30.53333333333333, 70.66666666666667, 1.0, 2.0, 0.7720381379260326, 1.0, 2.0, 0.7720381379260326, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2159069.194946667, 2159069.194946667, 406489.1345252328], 
processed observation next is [1.0, 0.4782608695652174, 0.646129541864139, 0.7066666666666667, 1.0, 1.0, 0.7253471541277501, 1.0, 1.0, 0.7253471541277501, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5997414430407408, 0.5997414430407408, 0.6067002007839295], 
reward next is 0.3933, 
noisyNet noise sample is [array([1.3665946], dtype=float32), -2.389082]. 
=============================================
[2019-03-27 02:42:27,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[35.597572]
 [35.7023  ]
 [35.86604 ]
 [35.703835]
 [35.826256]], R is [[35.74354935]
 [35.78150558]
 [35.4236908 ]
 [35.06945419]
 [34.71876144]].
[2019-03-27 02:42:28,224] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:28,225] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:28,314] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-27 02:42:30,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:30,589] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1089
[2019-03-27 02:42:30,596] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.73333333333333, 85.33333333333334, 1.0, 2.0, 0.3612055407796835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 575655.3478477785, 575655.3478477779, 173093.5193641669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 22800.0000, 
sim time next is 23400.0000, 
raw observation next is [21.8, 85.0, 1.0, 2.0, 0.3692451520735859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588175.5020911928, 588175.5020911933, 174164.305900311], 
processed observation next is [1.0, 0.2608695652173913, 0.23222748815165886, 0.85, 1.0, 1.0, 0.2400544000886577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1633820839142202, 0.16338208391422038, 0.2599467252243448], 
reward next is 0.7401, 
noisyNet noise sample is [array([-0.5543122], dtype=float32), -0.70547926]. 
=============================================
[2019-03-27 02:42:31,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:31,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:31,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-27 02:42:32,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:32,524] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:32,608] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-27 02:42:34,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:34,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1115
[2019-03-27 02:42:34,020] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.71666666666667, 96.0, 1.0, 2.0, 0.824526832537876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231179.706479469, 1231179.706479469, 260869.5958023983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 139800.0000, 
sim time next is 140400.0000, 
raw observation next is [22.7, 96.0, 1.0, 2.0, 0.9013953810223885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1346780.443844496, 1346780.443844495, 282780.7736669516], 
processed observation next is [1.0, 0.6521739130434783, 0.27488151658767773, 0.96, 1.0, 1.0, 0.8811992542438415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37410567884569335, 0.37410567884569307, 0.4220608562193307], 
reward next is 0.5779, 
noisyNet noise sample is [array([-1.2115134], dtype=float32), -0.32012385]. 
=============================================
[2019-03-27 02:42:34,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:34,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:34,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-27 02:42:35,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:35,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:35,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-27 02:42:36,159] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:36,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3129
[2019-03-27 02:42:36,172] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.05, 91.0, 1.0, 2.0, 0.3044481571744767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484982.0902380803, 484982.0902380803, 165982.5573559715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 214200.0000, 
sim time next is 214800.0000, 
raw observation next is [21.13333333333333, 90.66666666666667, 1.0, 2.0, 0.3043012910266282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484301.3616666102, 484301.3616666108, 165926.8352959028], 
processed observation next is [0.0, 0.4782608695652174, 0.20063191153238533, 0.9066666666666667, 1.0, 1.0, 0.16180878436943152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13452815601850282, 0.13452815601850301, 0.2476519929789594], 
reward next is 0.7523, 
noisyNet noise sample is [array([0.08982906], dtype=float32), -1.8680339]. 
=============================================
[2019-03-27 02:42:36,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:36,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3116
[2019-03-27 02:42:36,740] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.3543341171355828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 547783.4350458741, 547783.4350458735, 170514.375543383], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 163200.0000, 
sim time next is 163800.0000, 
raw observation next is [21.5, 96.0, 1.0, 2.0, 0.3504340421043926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543278.860499487, 543278.8604994876, 170181.9920069493], 
processed observation next is [1.0, 0.9130434782608695, 0.21800947867298584, 0.96, 1.0, 1.0, 0.21739041217396698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15091079458319084, 0.150910794583191, 0.25400297314470044], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.09251632], dtype=float32), -0.61344546]. 
=============================================
[2019-03-27 02:42:37,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-27 02:42:37,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,084] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-27 02:42:37,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,406] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-27 02:42:37,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-27 02:42:37,620] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-27 02:42:37,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-27 02:42:37,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:42:37,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:42:37,922] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-27 02:42:38,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:38,425] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-27 02:42:38,428] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 90.66666666666667, 1.0, 2.0, 0.2773877877685151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 448052.7956005751, 448052.7956005757, 163469.7129210312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 282000.0000, 
sim time next is 282600.0000, 
raw observation next is [20.55, 90.0, 1.0, 2.0, 0.2790752241858487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 450205.4066099429, 450205.4066099423, 163613.9061713835], 
processed observation next is [0.0, 0.2608695652173913, 0.17298578199052142, 0.9, 1.0, 1.0, 0.13141593275403454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1250570573916508, 0.12505705739165066, 0.24419985995728882], 
reward next is 0.7558, 
noisyNet noise sample is [array([1.4889897], dtype=float32), -1.4168603]. 
=============================================
[2019-03-27 02:42:39,366] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:39,373] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1103
[2019-03-27 02:42:39,381] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 89.0, 1.0, 2.0, 0.2975081561231734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475638.0772194005, 475638.0772194005, 165336.4330564601], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 244800.0000, 
sim time next is 245400.0000, 
raw observation next is [21.06666666666667, 89.16666666666667, 1.0, 2.0, 0.2971860017110175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 475310.4594983634, 475310.4594983628, 165315.2776146498], 
processed observation next is [0.0, 0.8695652173913043, 0.19747235387045833, 0.8916666666666667, 1.0, 1.0, 0.1532361466397801, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13203068319398983, 0.13203068319398967, 0.24673922032037282], 
reward next is 0.7533, 
noisyNet noise sample is [array([-0.9745569], dtype=float32), 0.19639994]. 
=============================================
[2019-03-27 02:42:52,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:52,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5217
[2019-03-27 02:42:52,784] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.7, 91.0, 1.0, 2.0, 0.2919226770016959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 468235.5028795577, 468235.5028795577, 164831.5760485278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 252000.0000, 
sim time next is 252600.0000, 
raw observation next is [20.66666666666667, 91.00000000000001, 1.0, 2.0, 0.289635091541419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464814.8271679669, 464814.8271679669, 164596.5636014907], 
processed observation next is [0.0, 0.9565217391304348, 0.17851500789889443, 0.9100000000000001, 1.0, 1.0, 0.1441386645077337, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1291152297688797, 0.1291152297688797, 0.2456665128380458], 
reward next is 0.7543, 
noisyNet noise sample is [array([0.12709485], dtype=float32), -0.328817]. 
=============================================
[2019-03-27 02:42:58,925] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:42:58,934] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0842
[2019-03-27 02:42:58,941] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 86.0, 1.0, 2.0, 0.244680711747906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 402682.2676706034, 402682.2676706034, 160382.3105867617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.2444180700154164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 402250.722132477, 402250.7221324776, 160356.9301463709], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.08966032531977879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11173631170346583, 0.111736311703466, 0.23933870171100133], 
reward next is 0.7607, 
noisyNet noise sample is [array([-0.42871746], dtype=float32), 2.2227447]. 
=============================================
[2019-03-27 02:43:05,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:43:06,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1110
[2019-03-27 02:43:06,008] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.13333333333333, 69.5, 1.0, 2.0, 0.332286620568118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515907.662497577, 515907.6624975776, 168005.839660841], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1101000.0000, 
sim time next is 1101600.0000, 
raw observation next is [25.0, 70.0, 1.0, 2.0, 0.3344799801869318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519988.7483065563, 519988.7483065563, 168346.3379643153], 
processed observation next is [1.0, 0.782608695652174, 0.38388625592417064, 0.7, 1.0, 1.0, 0.1981686508276287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14444131897404341, 0.14444131897404341, 0.25126319099151534], 
reward next is 0.7487, 
noisyNet noise sample is [array([-0.08829787], dtype=float32), 1.040317]. 
=============================================
[2019-03-27 02:43:06,682] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 02:43:06,684] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:43:06,686] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:43:06,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:43:06,687] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:43:06,688] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:43:06,687] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:43:06,689] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:43:06,690] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:43:06,692] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:43:06,698] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:43:07,566] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-27 02:43:07,787] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-27 02:43:07,808] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-27 02:43:07,808] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-27 02:43:07,825] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-27 02:43:19,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:43:19,453] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.68333333333334, 50.0, 1.0, 2.0, 0.5838601693516999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 965929.806148052, 965929.8061480514, 212055.7982586904]
[2019-03-27 02:43:19,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:43:19,458] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8372069007406552
[2019-03-27 02:43:43,458] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:43:43,461] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 94.0, 1.0, 2.0, 0.3833823509428541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581615.8266646251, 581615.8266646257, 173139.9618283689]
[2019-03-27 02:43:43,461] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:43:43,464] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9034651191264121
[2019-03-27 02:43:52,425] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:43:52,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.5, 70.0, 1.0, 2.0, 0.9699010268589204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1355705.120090698, 1355705.120090698, 289890.4439423608]
[2019-03-27 02:43:52,428] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:43:52,431] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.79402836671234
[2019-03-27 02:44:02,054] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:44:02,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.83333333333334, 60.0, 1.0, 2.0, 1.013124031960694, 1.0, 2.0, 0.8271520554946095, 1.0, 1.0, 1.03, 7.005122434694049, 6.9112, 170.5573041426782, 3471633.522639806, 3404353.062148085, 638498.3036052954]
[2019-03-27 02:44:02,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:44:02,062] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.8745239e-38 1.0000000e+00 0.0000000e+00 1.1425079e-35 1.7944344e-23], sampled 0.3690461401522619
[2019-03-27 02:44:02,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3471633.522639806 W.
[2019-03-27 02:44:10,057] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:44:10,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.67861428000001, 57.17042808333333, 1.0, 2.0, 0.6237060117404956, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.969833601275023, 6.9112, 168.9125069451701, 1743922.208213366, 1702325.624166497, 369088.7179021353]
[2019-03-27 02:44:10,060] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:44:10,063] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.474892032211081
[2019-03-27 02:44:10,065] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1743922.208213366 W.
[2019-03-27 02:44:34,322] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:44:34,323] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.06126423333333, 89.85342331000001, 1.0, 2.0, 0.4223557409311268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 623348.9062970927, 623348.9062970933, 176548.8271979564]
[2019-03-27 02:44:34,325] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:44:34,327] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9500878584354404
[2019-03-27 02:44:37,905] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.025019182]
[2019-03-27 02:44:37,905] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.7, 76.0, 1.0, 2.0, 0.5694352563926871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795733.6406222389, 795733.6406222389, 195189.226147464]
[2019-03-27 02:44:37,909] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:44:37,912] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3541412702114126
[2019-03-27 02:45:02,080] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:45:02,245] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:45:02,468] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:45:02,536] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:45:02,552] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:45:03,571] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1500000, evaluation results [1500000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:45:05,715] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:05,723] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4875
[2019-03-27 02:45:05,728] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.45, 88.0, 1.0, 2.0, 0.2204669521490859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 366481.11293434, 366481.1129343406, 157820.9812425564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [18.4, 88.33333333333333, 1.0, 2.0, 0.220858691578882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 367166.2089805147, 367166.2089805154, 157849.1145801661], 
processed observation next is [1.0, 0.08695652173913043, 0.07109004739336493, 0.8833333333333333, 1.0, 1.0, 0.06127553202274938, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10199061360569854, 0.10199061360569874, 0.23559569340323297], 
reward next is 0.7644, 
noisyNet noise sample is [array([0.1382776], dtype=float32), -1.0644398]. 
=============================================
[2019-03-27 02:45:05,745] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.29411]
 [79.92608]
 [80.20257]
 [79.75921]
 [79.88154]], R is [[78.59220123]
 [78.57073212]
 [78.54858398]
 [78.52410126]
 [78.50293732]].
[2019-03-27 02:45:07,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:07,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6553
[2019-03-27 02:45:07,331] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 67.5, 1.0, 2.0, 0.4726403767214698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777850.2612768414, 777850.2612768414, 191015.0072088683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [22.4, 66.0, 1.0, 2.0, 0.4795799976515278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 789525.9277171761, 789525.9277171755, 192210.1840587171], 
processed observation next is [1.0, 0.43478260869565216, 0.2606635071090047, 0.66, 1.0, 1.0, 0.37298794897774434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2193127576992156, 0.2193127576992154, 0.2868808717294285], 
reward next is 0.7131, 
noisyNet noise sample is [array([0.0388536], dtype=float32), -0.5095232]. 
=============================================
[2019-03-27 02:45:07,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:07,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4973
[2019-03-27 02:45:07,998] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.18333333333333, 91.16666666666667, 1.0, 2.0, 0.245768296161783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410712.8942951671, 410712.8942951671, 159480.1815492728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 612600.0000, 
sim time next is 613200.0000, 
raw observation next is [17.16666666666667, 91.33333333333334, 1.0, 2.0, 0.2182630173528393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 364737.7274437897, 364737.7274437897, 156963.1787796891], 
processed observation next is [1.0, 0.08695652173913043, 0.012638230647709612, 0.9133333333333334, 1.0, 1.0, 0.05814821367811961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1013160354010527, 0.1013160354010527, 0.23427340116371506], 
reward next is 0.7657, 
noisyNet noise sample is [array([-0.6304368], dtype=float32), 0.6130458]. 
=============================================
[2019-03-27 02:45:26,801] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:26,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4465
[2019-03-27 02:45:26,819] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 91.0, 1.0, 2.0, 0.2880556518581609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463771.3538605892, 463771.3538605899, 164530.6134439055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1130400.0000, 
sim time next is 1131000.0000, 
raw observation next is [20.45, 91.33333333333334, 1.0, 2.0, 0.3483192195694967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560963.5639090809, 560963.5639090802, 171829.06402252], 
processed observation next is [1.0, 0.08695652173913043, 0.16824644549763035, 0.9133333333333334, 1.0, 1.0, 0.21484243321626106, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15582321219696693, 0.15582321219696674, 0.25646128958585074], 
reward next is 0.7435, 
noisyNet noise sample is [array([-1.2896411], dtype=float32), -0.44421542]. 
=============================================
[2019-03-27 02:45:26,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.99326 ]
 [78.9263  ]
 [79.15041 ]
 [79.361206]
 [79.618835]], R is [[79.485466  ]
 [79.44504547]
 [79.40512085]
 [79.36562347]
 [79.32635498]].
[2019-03-27 02:45:27,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:27,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0962
[2019-03-27 02:45:27,841] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 96.16666666666666, 1.0, 2.0, 0.3228718398974434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506304.7508742899, 506304.7508742893, 167405.9365321552], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1050600.0000, 
sim time next is 1051200.0000, 
raw observation next is [20.9, 96.0, 1.0, 2.0, 0.3177467728688037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500179.4133182035, 500179.4133182035, 166989.4617915878], 
processed observation next is [1.0, 0.17391304347826086, 0.1895734597156398, 0.96, 1.0, 1.0, 0.17800816008289602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13893872592172318, 0.13893872592172318, 0.24923800267401164], 
reward next is 0.7508, 
noisyNet noise sample is [array([-1.3060478], dtype=float32), 0.88710105]. 
=============================================
[2019-03-27 02:45:29,130] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:29,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8510
[2019-03-27 02:45:29,151] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 93.16666666666666, 1.0, 2.0, 0.3316361779264139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514014.6031773683, 514014.6031773683, 167830.4349445109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3315436675871201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513924.8772747177, 513924.8772747171, 167825.1107573162], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19463092480375918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14275691035408825, 0.14275691035408808, 0.25048523993629285], 
reward next is 0.7495, 
noisyNet noise sample is [array([-1.1512469], dtype=float32), -0.59515095]. 
=============================================
[2019-03-27 02:45:29,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:29,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5680
[2019-03-27 02:45:29,300] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 67.33333333333334, 1.0, 2.0, 0.3072081468250611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485678.3826556922, 485678.3826556922, 165965.2089310022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 913200.0000, 
sim time next is 913800.0000, 
raw observation next is [24.86666666666667, 66.66666666666666, 1.0, 2.0, 0.3079135301212406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486547.4680176543, 486547.4680176543, 166023.0520876434], 
processed observation next is [0.0, 0.5652173913043478, 0.3775671406003162, 0.6666666666666665, 1.0, 1.0, 0.1661608796641453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13515207444934843, 0.13515207444934843, 0.24779560013081103], 
reward next is 0.7522, 
noisyNet noise sample is [array([1.0807073], dtype=float32), -1.4638983]. 
=============================================
[2019-03-27 02:45:29,754] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:29,766] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2560
[2019-03-27 02:45:29,770] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.46666666666667, 68.66666666666667, 1.0, 2.0, 0.3056662332113999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483761.929607147, 483761.929607147, 165837.4167795269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 912000.0000, 
sim time next is 912600.0000, 
raw observation next is [24.6, 68.0, 1.0, 2.0, 0.3063932908364548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484645.9345619114, 484645.9345619107, 165895.769858236], 
processed observation next is [0.0, 0.5652173913043478, 0.36492890995260674, 0.68, 1.0, 1.0, 0.1643292660680178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13462387071164206, 0.13462387071164186, 0.24760562665408356], 
reward next is 0.7524, 
noisyNet noise sample is [array([-1.1996728], dtype=float32), 0.0065693515]. 
=============================================
[2019-03-27 02:45:52,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:45:52,727] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0921
[2019-03-27 02:45:52,730] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 57.0, 1.0, 2.0, 0.3597016180012442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549558.4744311062, 549558.4744311069, 170475.6977692278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1526400.0000, 
sim time next is 1527000.0000, 
raw observation next is [27.85, 57.33333333333333, 1.0, 2.0, 0.3572748588014932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 170285.8858871213], 
processed observation next is [0.0, 0.6956521739130435, 0.5189573459715641, 0.5733333333333333, 1.0, 1.0, 0.225632360001799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15191858677415285, 0.15191858677415285, 0.2541580386374945], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.0347044], dtype=float32), 1.529589]. 
=============================================
[2019-03-27 02:45:52,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.03504 ]
 [73.92283 ]
 [73.90572 ]
 [73.884544]
 [73.87533 ]], R is [[74.03619385]
 [74.04138947]
 [74.04625702]
 [74.05089569]
 [74.05545807]].
[2019-03-27 02:45:59,609] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 02:45:59,616] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:45:59,617] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:45:59,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:45:59,619] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:45:59,619] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:45:59,619] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:45:59,621] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:45:59,622] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:45:59,620] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:45:59,624] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:45:59,653] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-27 02:45:59,654] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-27 02:45:59,655] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-27 02:45:59,672] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-27 02:45:59,729] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-27 02:46:26,995] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:46:26,996] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.11347905, 93.15785697000001, 1.0, 2.0, 0.3703352949994411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571593.3020539898, 571593.3020539893, 172512.8078155461]
[2019-03-27 02:46:26,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:46:27,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8470174459911294
[2019-03-27 02:46:30,724] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:46:30,725] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.92956194, 73.95136636, 1.0, 2.0, 0.5736119552631084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801572.3972260237, 801572.3972260237, 195931.520852904]
[2019-03-27 02:46:30,725] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:46:30,727] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7932284094825691
[2019-03-27 02:46:31,821] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:46:31,822] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.04782680333334, 64.73577538333333, 1.0, 2.0, 0.5908499602227881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825670.359947965, 825670.359947965, 199054.598429736]
[2019-03-27 02:46:31,824] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:46:31,829] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5799950943423307
[2019-03-27 02:46:38,383] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:46:38,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.7, 90.0, 1.0, 2.0, 0.632583246504928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 968802.9882136934, 968802.9882136927, 217419.3970134303]
[2019-03-27 02:46:38,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:46:38,387] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7464810327206853
[2019-03-27 02:46:50,941] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:46:50,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.343852125, 76.442595205, 1.0, 2.0, 0.973661165581019, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564984927, 1435437.639609176, 1435437.639609176, 302239.0857551479]
[2019-03-27 02:46:50,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:46:50,947] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3989534843596183
[2019-03-27 02:46:55,825] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:46:55,825] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 77.0, 1.0, 2.0, 0.6090133766131698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851062.5981747742, 851062.5981747742, 202437.5336869241]
[2019-03-27 02:46:55,827] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:46:55,831] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5788460190094429
[2019-03-27 02:47:02,575] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:47:02,578] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.04185566833333, 79.09543877166666, 1.0, 2.0, 0.5259186639799549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734902.1501762158, 734902.1501762165, 187756.2502631731]
[2019-03-27 02:47:02,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:47:02,585] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.514910684897852
[2019-03-27 02:47:37,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0391542], dtype=float32), -0.021941518]
[2019-03-27 02:47:37,389] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.80078901, 86.66006702500002, 1.0, 2.0, 0.3572482058508221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552736.4815839655, 552736.4815839648, 170939.3402266213]
[2019-03-27 02:47:37,390] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:47:37,392] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8472080802126891
[2019-03-27 02:47:50,842] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:47:51,432] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.7251 3163979764.9030 1779.0000
[2019-03-27 02:47:51,436] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.5954 2927353094.4120 1339.0000
[2019-03-27 02:47:51,638] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:47:51,689] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.1658 2779182213.6838 934.0000
[2019-03-27 02:47:52,706] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1525000, evaluation results [1525000.0, 7884.725117256423, 3163979764.902981, 1779.0, 8254.595404889755, 2927353094.4119844, 1339.0, 8662.165764210004, 2779182213.6838145, 934.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:47:54,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:47:54,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8350
[2019-03-27 02:47:54,191] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4029470154690746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594825.9957638703, 594825.9957638709, 173869.0295096684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1425600.0000, 
sim time next is 1426200.0000, 
raw observation next is [24.33333333333333, 87.33333333333334, 1.0, 2.0, 0.4081313560275022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599762.2408565386, 599762.2408565386, 174243.1794307405], 
processed observation next is [0.0, 0.5217391304347826, 0.35229067930489716, 0.8733333333333334, 1.0, 1.0, 0.2869052482259063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16660062246014962, 0.16660062246014962, 0.260064446911553], 
reward next is 0.7399, 
noisyNet noise sample is [array([-0.797021], dtype=float32), 0.35526794]. 
=============================================
[2019-03-27 02:47:55,407] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:47:55,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3655
[2019-03-27 02:47:55,423] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1448400.0000, 
sim time next is 1449000.0000, 
raw observation next is [25.3, 80.0, 1.0, 2.0, 0.4123818573780608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606327.0677499198, 606327.0677499193, 174866.3882194718], 
processed observation next is [0.0, 0.782608695652174, 0.39810426540284366, 0.8, 1.0, 1.0, 0.2920263341904347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16842418548608884, 0.16842418548608867, 0.2609946092827937], 
reward next is 0.7390, 
noisyNet noise sample is [array([1.450743], dtype=float32), 0.0676377]. 
=============================================
[2019-03-27 02:47:55,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.81147]
 [70.81588]
 [70.81603]
 [70.69875]
 [70.66206]], R is [[70.84046173]
 [70.87083435]
 [70.90115356]
 [70.93084717]
 [70.95999146]].
[2019-03-27 02:47:56,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:47:56,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8031
[2019-03-27 02:47:56,426] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.3641120629178933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 556686.7973403676, 556686.797340367, 171091.2633168608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1465200.0000, 
sim time next is 1465800.0000, 
raw observation next is [21.96666666666667, 96.0, 1.0, 2.0, 0.3626962200673033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555066.3779374347, 555066.3779374347, 170969.5562830691], 
processed observation next is [0.0, 1.0, 0.24012638230647723, 0.96, 1.0, 1.0, 0.232164120563016, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15418510498262075, 0.15418510498262075, 0.25517844221353597], 
reward next is 0.7448, 
noisyNet noise sample is [array([-0.20276392], dtype=float32), -0.7052103]. 
=============================================
[2019-03-27 02:47:58,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:47:58,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4430
[2019-03-27 02:47:58,727] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 94.0, 1.0, 2.0, 0.4600946989538006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649294.4116061226, 649294.4116061226, 178406.6916801751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1740600.0000, 
sim time next is 1741200.0000, 
raw observation next is [24.36666666666667, 94.0, 1.0, 2.0, 0.4531129021271641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640392.1385679565, 640392.1385679565, 177514.3237377007], 
processed observation next is [1.0, 0.13043478260869565, 0.3538704581358612, 0.94, 1.0, 1.0, 0.3410998820809206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1778867051577657, 0.1778867051577657, 0.2649467518473145], 
reward next is 0.7351, 
noisyNet noise sample is [array([0.20575842], dtype=float32), -0.24571541]. 
=============================================
[2019-03-27 02:48:02,824] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:02,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3435
[2019-03-27 02:48:02,842] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 93.16666666666666, 1.0, 2.0, 0.523230978863991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 188570.2929640439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [24.5, 93.0, 1.0, 2.0, 0.4871256155342776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 182666.2265049748], 
processed observation next is [1.0, 0.2608695652173913, 0.3601895734597157, 0.93, 1.0, 1.0, 0.38207905486057536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1913762893031219, 0.19137628930312206, 0.2726361589626489], 
reward next is 0.7274, 
noisyNet noise sample is [array([-0.43093714], dtype=float32), 0.7131492]. 
=============================================
[2019-03-27 02:48:02,959] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:02,971] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4190
[2019-03-27 02:48:02,979] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 86.0, 1.0, 2.0, 0.3331711493497595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516632.8492831838, 516632.8492831832, 168043.0627979559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1580400.0000, 
sim time next is 1581000.0000, 
raw observation next is [22.9, 85.83333333333334, 1.0, 2.0, 0.3784540091892027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585719.1605597021, 585719.1605597021, 173782.2049204879], 
processed observation next is [1.0, 0.30434782608695654, 0.2843601895734597, 0.8583333333333334, 1.0, 1.0, 0.25114940866169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16269976682213946, 0.16269976682213946, 0.25937642525445953], 
reward next is 0.7406, 
noisyNet noise sample is [array([0.84153366], dtype=float32), -0.30020502]. 
=============================================
[2019-03-27 02:48:02,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[64.077385]
 [64.03059 ]
 [64.0037  ]
 [63.98921 ]
 [63.961956]], R is [[63.86445999]
 [63.9750061 ]
 [64.08463287]
 [64.19338989]
 [64.30109406]].
[2019-03-27 02:48:04,580] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:04,590] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4048
[2019-03-27 02:48:04,600] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1900932.335521999 W.
[2019-03-27 02:48:04,607] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.63333333333334, 76.0, 1.0, 2.0, 0.6798155996596267, 1.0, 2.0, 0.6798155996596267, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1900932.335521999, 1900932.335521999, 365599.491146503], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1700400.0000, 
sim time next is 1701000.0000, 
raw observation next is [28.7, 75.5, 1.0, 2.0, 0.7785508356605081, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.981620110637073, 6.9112, 168.9125374118722, 1985054.910014726, 1935096.583810088, 403482.2263045696], 
processed observation next is [1.0, 0.6956521739130435, 0.5592417061611374, 0.755, 1.0, 1.0, 0.7331937779042266, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007042011063707321, 0.0, 0.8294378871865148, 0.5514041416707572, 0.5375268288361356, 0.6022122780665218], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.22436404], dtype=float32), -1.3179575]. 
=============================================
[2019-03-27 02:48:04,639] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[53.681355]
 [53.059315]
 [53.8166  ]
 [54.67333 ]
 [54.07994 ]], R is [[52.81555939]
 [52.74173355]
 [52.63660812]
 [52.53466797]
 [52.0540123 ]].
[2019-03-27 02:48:04,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:04,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3359
[2019-03-27 02:48:04,718] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.78333333333333, 90.33333333333333, 1.0, 2.0, 0.4406602038399923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650784.5697517538, 650784.5697517538, 179247.920661659], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1842600.0000, 
sim time next is 1843200.0000, 
raw observation next is [23.9, 90.0, 1.0, 2.0, 0.4669593329886921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687786.5045120236, 687786.5045120242, 183018.8383844287], 
processed observation next is [1.0, 0.34782608695652173, 0.33175355450236965, 0.9, 1.0, 1.0, 0.3577823289020386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19105180680889547, 0.1910518068088956, 0.2731624453498936], 
reward next is 0.7268, 
noisyNet noise sample is [array([0.79164815], dtype=float32), 0.82797194]. 
=============================================
[2019-03-27 02:48:06,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:06,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5596
[2019-03-27 02:48:06,584] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.15, 96.5, 1.0, 2.0, 0.4165286868239017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611177.3138799083, 611177.3138799083, 175288.096109286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1632600.0000, 
sim time next is 1633200.0000, 
raw observation next is [23.13333333333333, 96.66666666666666, 1.0, 2.0, 0.4162418191892954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610666.0307506204, 610666.030750621, 175237.0200970593], 
processed observation next is [1.0, 0.9130434782608695, 0.29541864139020524, 0.9666666666666666, 1.0, 1.0, 0.2966768905895126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16962945298628346, 0.1696294529862836, 0.2615477911896408], 
reward next is 0.7385, 
noisyNet noise sample is [array([-0.35060978], dtype=float32), 1.1562114]. 
=============================================
[2019-03-27 02:48:07,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4046741e-38 1.0000000e+00 0.0000000e+00 9.2005636e-38 1.2974165e-32], sum to 1.0000
[2019-03-27 02:48:07,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8458
[2019-03-27 02:48:07,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2027790.328250264 W.
[2019-03-27 02:48:07,210] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.53333333333333, 66.66666666666666, 1.0, 2.0, 0.4834266267842534, 1.0, 1.0, 0.4834266267842534, 1.0, 2.0, 0.8319164324767331, 6.911199999999999, 6.9112, 170.5573041426782, 2027790.328250264, 2027790.328250265, 402542.9755575593], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2281200.0000, 
sim time next is 2281800.0000, 
raw observation next is [30.71666666666667, 65.83333333333334, 1.0, 2.0, 0.4838560456986646, 1.0, 2.0, 0.4838560456986646, 1.0, 2.0, 0.8338904009409872, 6.911200000000001, 6.9112, 170.5573041426782, 2029593.283090575, 2029593.283090574, 403044.5210545722], 
processed observation next is [1.0, 0.391304347826087, 0.6548183254344393, 0.6583333333333334, 1.0, 1.0, 0.3781398140947766, 1.0, 1.0, 0.3781398140947766, 1.0, 1.0, 0.797427318220716, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5637759119696042, 0.5637759119696039, 0.6015589866486152], 
reward next is 0.3984, 
noisyNet noise sample is [array([0.92710704], dtype=float32), 1.0115148]. 
=============================================
[2019-03-27 02:48:08,349] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0415219e-31 1.0000000e+00 5.8381455e-35 1.7055610e-29 2.7647015e-24], sum to 1.0000
[2019-03-27 02:48:08,360] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9186
[2019-03-27 02:48:08,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1757932.105714528 W.
[2019-03-27 02:48:08,371] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.53333333333333, 78.33333333333334, 1.0, 2.0, 0.419144973510058, 1.0, 2.0, 0.419144973510058, 1.0, 1.0, 0.7005748471914499, 6.911199999999999, 6.9112, 170.5573041426782, 1757932.105714528, 1757932.105714529, 359565.2543157793], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1941600.0000, 
sim time next is 1942200.0000, 
raw observation next is [26.6, 78.0, 1.0, 2.0, 0.4208838966794801, 1.0, 2.0, 0.4208838966794801, 1.0, 2.0, 0.7057747020206886, 6.9112, 6.9112, 170.5573041426782, 1765231.314754904, 1765231.314754904, 360905.349344604], 
processed observation next is [1.0, 0.4782608695652174, 0.4597156398104266, 0.78, 1.0, 1.0, 0.3022697550355182, 1.0, 1.0, 0.3022697550355182, 1.0, 1.0, 0.6411886610008398, 0.0, 0.0, 0.8375144448122397, 0.4903420318763622, 0.4903420318763622, 0.5386647005143343], 
reward next is 0.4613, 
noisyNet noise sample is [array([-0.9867769], dtype=float32), 0.40172106]. 
=============================================
[2019-03-27 02:48:09,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:09,274] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9343
[2019-03-27 02:48:09,280] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 94.33333333333333, 1.0, 2.0, 0.3691017803113059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569129.1276547808, 569129.1276547813, 172286.3898561079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1824000.0000, 
sim time next is 1824600.0000, 
raw observation next is [21.91666666666666, 94.66666666666667, 1.0, 2.0, 0.3638138941523408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560595.237147645, 560595.237147645, 171545.0667072892], 
processed observation next is [1.0, 0.08695652173913043, 0.23775671406003138, 0.9466666666666668, 1.0, 1.0, 0.2335107158461937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15572089920767915, 0.15572089920767915, 0.256037412995954], 
reward next is 0.7440, 
noisyNet noise sample is [array([-0.6558155], dtype=float32), 0.086757176]. 
=============================================
[2019-03-27 02:48:10,681] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:10,689] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4819
[2019-03-27 02:48:10,697] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1793760.516026865 W.
[2019-03-27 02:48:10,701] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333333, 74.5, 1.0, 2.0, 0.4276804220895681, 1.0, 1.0, 0.4276804220895681, 1.0, 2.0, 0.7323327453385963, 6.911199999999999, 6.9112, 170.5573041426782, 1793760.516026865, 1793760.516026865, 367156.6173055671], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1702200.0000, 
sim time next is 1702800.0000, 
raw observation next is [28.9, 74.0, 1.0, 2.0, 0.4546670935836096, 1.0, 2.0, 0.4546670935836096, 1.0, 2.0, 0.7793813505594623, 6.9112, 6.9112, 170.5573041426782, 1907047.686724413, 1907047.686724413, 383544.9949764168], 
processed observation next is [1.0, 0.7391304347826086, 0.5687203791469194, 0.74, 1.0, 1.0, 0.34297240190796346, 1.0, 1.0, 0.34297240190796346, 1.0, 1.0, 0.7309528665359296, 0.0, 0.0, 0.8375144448122397, 0.5297354685345591, 0.5297354685345591, 0.5724552163827117], 
reward next is 0.4275, 
noisyNet noise sample is [array([-0.08367662], dtype=float32), -0.27509233]. 
=============================================
[2019-03-27 02:48:15,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:15,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6822
[2019-03-27 02:48:15,077] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 84.5, 1.0, 2.0, 0.3927133440211353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618773.9956941775, 618773.9956941782, 176834.4018436307], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1789800.0000, 
sim time next is 1790400.0000, 
raw observation next is [22.26666666666667, 85.0, 1.0, 2.0, 0.3156928407504398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497527.7290635515, 497527.7290635522, 166804.4611558323], 
processed observation next is [1.0, 0.7391304347826086, 0.2543443917851502, 0.85, 1.0, 1.0, 0.17553354307281901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13820214696209765, 0.13820214696209784, 0.24896188232213778], 
reward next is 0.7510, 
noisyNet noise sample is [array([0.25890294], dtype=float32), 0.999083]. 
=============================================
[2019-03-27 02:48:15,875] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:15,887] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0464
[2019-03-27 02:48:15,892] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 81.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.914983226132993, 6.9112, 168.9125319223211, 1486490.029086788, 1483806.085005962, 316152.0432385501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1934400.0000, 
sim time next is 1935000.0000, 
raw observation next is [25.95, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.051251240683174, 6.9112, 168.9120686360958, 1583186.047197754, 1483829.399126598, 316145.1216261043], 
processed observation next is [1.0, 0.391304347826087, 0.42890995260663506, 0.81, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.014005124068317443, 0.0, 0.8294355852828462, 0.4397739019993761, 0.4121748330907216, 0.4718583904867228], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3187461], dtype=float32), 0.48759308]. 
=============================================
[2019-03-27 02:48:15,905] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[65.63945]
 [65.54608]
 [66.71831]
 [67.13419]
 [67.74912]], R is [[64.64131165]
 [64.50411224]
 [63.8976326 ]
 [63.81740189]
 [63.74449158]].
[2019-03-27 02:48:17,107] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:17,116] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6793
[2019-03-27 02:48:17,130] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1662520.221597081 W.
[2019-03-27 02:48:17,135] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.1, 76.0, 1.0, 2.0, 0.5946157406746014, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9904012991797915, 6.9112, 6.9112, 168.912956510431, 1662520.221597081, 1662520.221597081, 354684.7641599828], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [26.93333333333334, 76.66666666666667, 1.0, 2.0, 0.3953549634511281, 1.0, 1.0, 0.3953549634511281, 1.0, 2.0, 0.6630386034128865, 6.911199999999999, 6.9112, 170.5573041426782, 1658077.449864578, 1658077.449864579, 346984.0500070725], 
processed observation next is [1.0, 0.6086956521739131, 0.4755134281200636, 0.7666666666666667, 1.0, 1.0, 0.27151200415798565, 1.0, 0.5, 0.27151200415798565, 1.0, 1.0, 0.5890714675766907, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4605770694068272, 0.46057706940682747, 0.5178866418016007], 
reward next is 0.4821, 
noisyNet noise sample is [array([-0.2901331], dtype=float32), 0.24055214]. 
=============================================
[2019-03-27 02:48:23,105] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:23,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5346
[2019-03-27 02:48:23,126] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333334, 91.66666666666667, 1.0, 2.0, 0.5300725391990955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740708.6681487986, 740708.6681487993, 188441.9285015988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2154000.0000, 
sim time next is 2154600.0000, 
raw observation next is [26.35, 92.0, 1.0, 2.0, 0.5288973505835332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739065.9208751115, 739065.9208751115, 188247.5071025342], 
processed observation next is [0.0, 0.9565217391304348, 0.4478672985781992, 0.92, 1.0, 1.0, 0.4324064464861845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20529608913197542, 0.20529608913197542, 0.2809664285112451], 
reward next is 0.7190, 
noisyNet noise sample is [array([-1.1975356], dtype=float32), 0.83349466]. 
=============================================
[2019-03-27 02:48:25,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:25,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1471
[2019-03-27 02:48:25,985] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 94.33333333333333, 1.0, 2.0, 0.3987670624760263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595809.0149580113, 595809.0149580113, 174175.5988094222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1971600.0000, 
sim time next is 1972200.0000, 
raw observation next is [22.8, 94.66666666666667, 1.0, 2.0, 0.3977044069292393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595420.8676647046, 595420.8676647046, 174174.446602265], 
processed observation next is [1.0, 0.8260869565217391, 0.2796208530805688, 0.9466666666666668, 1.0, 1.0, 0.2743426589508907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16539468546241792, 0.16539468546241792, 0.2599618606003955], 
reward next is 0.7400, 
noisyNet noise sample is [array([-2.035038], dtype=float32), 0.17711551]. 
=============================================
[2019-03-27 02:48:27,543] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:27,553] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6445
[2019-03-27 02:48:27,558] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 74.0, 1.0, 2.0, 0.566709085033073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791922.645571247, 791922.645571247, 194708.4652772188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2131200.0000, 
sim time next is 2131800.0000, 
raw observation next is [30.55, 73.83333333333334, 1.0, 2.0, 0.5683691952604154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794243.3608116453, 794243.3608116459, 195001.7289771804], 
processed observation next is [0.0, 0.6956521739130435, 0.6469194312796209, 0.7383333333333334, 1.0, 1.0, 0.47996288585592217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22062315578101258, 0.22062315578101274, 0.2910473566823588], 
reward next is 0.7090, 
noisyNet noise sample is [array([-0.18197376], dtype=float32), 0.8198736]. 
=============================================
[2019-03-27 02:48:31,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:31,257] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0176
[2019-03-27 02:48:31,263] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 86.33333333333334, 1.0, 2.0, 0.4983523519080267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696369.2291284859, 696369.2291284852, 183338.3804753035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2049600.0000, 
sim time next is 2050200.0000, 
raw observation next is [26.3, 86.5, 1.0, 2.0, 0.4971257563367059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694654.6938360535, 694654.6938360529, 183146.9945599539], 
processed observation next is [0.0, 0.7391304347826086, 0.4454976303317536, 0.865, 1.0, 1.0, 0.3941274172731397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19295963717668152, 0.19295963717668135, 0.2733537232238118], 
reward next is 0.7266, 
noisyNet noise sample is [array([-0.9579648], dtype=float32), -0.034410708]. 
=============================================
[2019-03-27 02:48:34,609] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:34,618] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5131
[2019-03-27 02:48:34,624] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 85.0, 1.0, 2.0, 0.5183004901568588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724253.125288148, 724253.1252881474, 186513.1048810808], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2245200.0000, 
sim time next is 2245800.0000, 
raw observation next is [27.05, 85.0, 1.0, 2.0, 0.5177060535173704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723422.1995952782, 723422.1995952775, 186416.7020522391], 
processed observation next is [1.0, 1.0, 0.4810426540284361, 0.85, 1.0, 1.0, 0.41892295604502455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2009506109986884, 0.20095061099868822, 0.27823388366005836], 
reward next is 0.7218, 
noisyNet noise sample is [array([-0.0272171], dtype=float32), -0.4445322]. 
=============================================
[2019-03-27 02:48:43,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:43,827] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8273
[2019-03-27 02:48:43,834] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 80.66666666666667, 1.0, 2.0, 0.553764987147905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773827.8973694084, 773827.8973694084, 192448.6338663476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2328000.0000, 
sim time next is 2328600.0000, 
raw observation next is [28.68333333333333, 80.83333333333333, 1.0, 2.0, 0.5521788851898377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771610.6828985944, 771610.682898595, 192175.1877840316], 
processed observation next is [1.0, 0.9565217391304348, 0.5584518167456555, 0.8083333333333332, 1.0, 1.0, 0.4604564881805273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2143363008051651, 0.21433630080516528, 0.2868286384836292], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.4203049], dtype=float32), -1.0498635]. 
=============================================
[2019-03-27 02:48:45,044] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:48:45,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8792
[2019-03-27 02:48:45,060] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 80.0, 1.0, 2.0, 0.5719349268391016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799228.015407768, 799228.015407768, 195633.6308272183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2416200.0000, 
sim time next is 2416800.0000, 
raw observation next is [29.23333333333333, 80.0, 1.0, 2.0, 0.5704923211440605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797211.346178428, 797211.346178428, 195377.1786382939], 
processed observation next is [1.0, 1.0, 0.5845181674565559, 0.8, 1.0, 1.0, 0.48252086884826556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22144759616067444, 0.22144759616067444, 0.2916077293108864], 
reward next is 0.7084, 
noisyNet noise sample is [array([-0.84762573], dtype=float32), 0.54501474]. 
=============================================
[2019-03-27 02:48:48,892] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 02:48:48,894] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:48:48,895] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:48:48,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:48:48,898] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:48:48,899] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:48:48,897] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:48:48,900] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:48:48,901] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:48:48,902] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:48:48,903] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:48:48,923] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-27 02:48:48,923] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-27 02:48:48,962] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-27 02:48:48,979] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-27 02:48:48,981] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-27 02:49:21,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05208393], dtype=float32), -0.045998745]
[2019-03-27 02:49:21,019] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.15, 83.16666666666667, 1.0, 2.0, 0.4901939762201759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684965.5035809482, 684965.5035809482, 182072.9240984168]
[2019-03-27 02:49:21,020] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:49:21,026] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4086294377284889
[2019-03-27 02:49:42,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05208393], dtype=float32), -0.045998745]
[2019-03-27 02:49:42,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.01120307333333, 68.57499231, 1.0, 2.0, 0.4161167948894355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595151.1213940178, 595151.1213940178, 173283.9826462113]
[2019-03-27 02:49:42,955] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:49:42,956] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11496524498572469
[2019-03-27 02:49:43,031] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.05208393], dtype=float32), -0.045998745]
[2019-03-27 02:49:43,032] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.66666666666667, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.834316009260851, 6.9112, 168.9080855359514, 2109074.917352969, 1454203.53437525, 311351.4837704692]
[2019-03-27 02:49:43,033] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:49:43,036] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8632274392823046
[2019-03-27 02:49:43,037] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2109074.917352969 W.
[2019-03-27 02:50:17,349] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05208393], dtype=float32), -0.045998745]
[2019-03-27 02:50:17,350] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.51666666666667, 81.33333333333333, 1.0, 2.0, 0.5592374497398716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781477.8971350997, 781477.8971350997, 193396.9294118597]
[2019-03-27 02:50:17,352] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:50:17,357] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.565914619039517
[2019-03-27 02:50:29,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05208393], dtype=float32), -0.045998745]
[2019-03-27 02:50:29,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.7, 88.0, 1.0, 2.0, 0.5302902779771239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741013.0364068625, 741013.0364068625, 188477.3144240857]
[2019-03-27 02:50:29,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:50:29,980] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34599840500785994
[2019-03-27 02:50:43,891] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:50:44,162] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:50:44,220] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:50:44,247] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:50:44,279] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:50:45,294] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1550000, evaluation results [1550000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:50:51,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:50:51,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3641
[2019-03-27 02:50:51,354] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4298083115286231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 623859.9893660145, 623859.9893660152, 176318.8735887948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2712600.0000, 
sim time next is 2713200.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4305028345838982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624868.7322546768, 624868.7322546768, 176417.395474842], 
processed observation next is [0.0, 0.391304347826087, 0.28909952606635075, 1.0, 1.0, 1.0, 0.31385883684807014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1735746478485213, 0.1735746478485213, 0.2633095454848388], 
reward next is 0.7367, 
noisyNet noise sample is [array([-0.29028726], dtype=float32), 0.57751924]. 
=============================================
[2019-03-27 02:50:53,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5633296e-30 1.0000000e+00 1.0368908e-33 2.5579072e-31 3.3266340e-31], sum to 1.0000
[2019-03-27 02:50:53,414] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0044
[2019-03-27 02:50:53,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2189209.678568548 W.
[2019-03-27 02:50:53,427] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.81666666666666, 83.66666666666667, 1.0, 2.0, 0.9244138455651771, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989578037271666, 6.9112, 168.9124902747698, 2189209.678568548, 2133605.754843243, 441524.7123435329], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2477400.0000, 
sim time next is 2478000.0000, 
raw observation next is [27.93333333333333, 83.33333333333334, 1.0, 2.0, 0.7011917449900622, 1.0, 1.0, 0.7011917449900622, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1960760.013744464, 1960760.013744464, 374634.3288685944], 
processed observation next is [1.0, 0.6956521739130435, 0.522906793048973, 0.8333333333333335, 1.0, 1.0, 0.6399900542048942, 1.0, 0.5, 0.6399900542048942, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5446555593734622, 0.5446555593734622, 0.5591557147292454], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8268986], dtype=float32), 0.6429985]. 
=============================================
[2019-03-27 02:50:53,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[30.396608]
 [29.98555 ]
 [30.720367]
 [30.478437]
 [29.9369  ]], R is [[30.12529564]
 [29.82404327]
 [29.9270134 ]
 [30.11645126]
 [30.29224968]].
[2019-03-27 02:50:57,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:50:57,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1792
[2019-03-27 02:50:57,985] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3854612839127774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580578.2330739391, 580578.2330739397, 172927.0904247991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2727600.0000, 
sim time next is 2728200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3859103808522213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581254.2257732699, 581254.2257732705, 172987.745629178], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2601329889785799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16145950715924165, 0.1614595071592418, 0.2581906651181761], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.02159387], dtype=float32), 0.27284592]. 
=============================================
[2019-03-27 02:51:00,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:00,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2762
[2019-03-27 02:51:00,807] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3449059683428146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531314.878465243, 531314.8784652436, 169107.2311510369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2877600.0000, 
sim time next is 2878200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3452222761097398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531806.5372556637, 531806.537255663, 169147.2347606855], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2111111760358311, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14772403812657323, 0.14772403812657303, 0.2524585593443067], 
reward next is 0.7475, 
noisyNet noise sample is [array([1.1601263], dtype=float32), -0.6820067]. 
=============================================
[2019-03-27 02:51:05,012] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:05,020] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8594
[2019-03-27 02:51:05,027] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.395570217914681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590249.5026353665, 590249.5026353665, 173640.5113558316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2673000.0000, 
sim time next is 2673600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3959294546072412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590785.4356171512, 590785.4356171512, 173689.7185016228], 
processed observation next is [0.0, 0.9565217391304348, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27220416217739907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16410706544920867, 0.16410706544920867, 0.2592383858233176], 
reward next is 0.7408, 
noisyNet noise sample is [array([-1.2225587], dtype=float32), 0.33001104]. 
=============================================
[2019-03-27 02:51:14,330] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:14,344] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1252
[2019-03-27 02:51:14,347] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4137717687635458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609653.2496629966, 609653.2496629966, 175216.5964095425], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4094262076607182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603250.7644546658, 603250.7644546658, 174616.0758322382], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.2884653104346002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16756965679296273, 0.16756965679296273, 0.2606210087048331], 
reward next is 0.7394, 
noisyNet noise sample is [array([0.2829897], dtype=float32), 0.2554033]. 
=============================================
[2019-03-27 02:51:21,037] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:21,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7742
[2019-03-27 02:51:21,053] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3071500491668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485706.0995626082, 485706.0995626082, 165969.880051621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2959200.0000, 
sim time next is 2959800.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3258918714676126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514351.9279243697, 514351.9279243691, 168095.8772717536], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.95, 1.0, 1.0, 0.18782153188868989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14287553553454713, 0.142875535534547, 0.2508893690623188], 
reward next is 0.7491, 
noisyNet noise sample is [array([-1.6826597], dtype=float32), 0.3359489]. 
=============================================
[2019-03-27 02:51:22,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:22,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9464
[2019-03-27 02:51:22,573] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666667, 63.0, 1.0, 2.0, 0.5585817265559814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780561.2534818448, 780561.2534818455, 193283.7383347219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3244200.0000, 
sim time next is 3244800.0000, 
raw observation next is [32.33333333333334, 63.00000000000001, 1.0, 2.0, 0.5573041287044418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778775.2857123173, 778775.285712318, 193062.1271549221], 
processed observation next is [0.0, 0.5652173913043478, 0.7314375987361774, 0.6300000000000001, 1.0, 1.0, 0.4666314803667973, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21632646825342147, 0.21632646825342167, 0.288152428589436], 
reward next is 0.7118, 
noisyNet noise sample is [array([1.2611187], dtype=float32), 0.85706323]. 
=============================================
[2019-03-27 02:51:23,093] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:23,102] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8538
[2019-03-27 02:51:23,107] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.4725219784565339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660264.1175018304, 660264.1175018311, 179402.8188057252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.4734447424415968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661553.9149877997, 661553.9149877997, 179540.0543227098], 
processed observation next is [0.0, 0.30434782608695654, 0.4549763033175356, 0.815, 1.0, 1.0, 0.36559607523083953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1837649763854999, 0.1837649763854999, 0.2679702303324027], 
reward next is 0.7320, 
noisyNet noise sample is [array([0.24331473], dtype=float32), -1.0512452]. 
=============================================
[2019-03-27 02:51:30,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:30,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8550
[2019-03-27 02:51:30,423] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3835702333057812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577730.0896667422, 577730.0896667422, 172672.2091281781], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3102600.0000, 
sim time next is 3103200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3837926002778032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578065.3288533741, 578065.3288533748, 172702.1588169946], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2575814461178352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16057370245927058, 0.16057370245927077, 0.2577644161447681], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.7445983], dtype=float32), 0.93863857]. 
=============================================
[2019-03-27 02:51:36,476] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6435486e-36 1.0000000e+00 0.0000000e+00 1.4229143e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 02:51:36,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2421
[2019-03-27 02:51:36,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2288910.128865439 W.
[2019-03-27 02:51:36,495] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8184211526499868, 1.0, 2.0, 0.8184211526499868, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2288910.128865439, 2288910.128865439, 428936.2472805938], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3495600.0000, 
sim time next is 3496200.0000, 
raw observation next is [31.16666666666667, 66.16666666666667, 1.0, 2.0, 0.8015197557740118, 1.0, 2.0, 0.8015197557740118, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2241598.946245035, 2241598.946245034, 420610.7867813596], 
processed observation next is [1.0, 0.4782608695652174, 0.6761453396524489, 0.6616666666666667, 1.0, 1.0, 0.7608671756313395, 1.0, 1.0, 0.7608671756313395, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6226663739569542, 0.6226663739569539, 0.6277772937035218], 
reward next is 0.3722, 
noisyNet noise sample is [array([0.8072355], dtype=float32), 0.1821123]. 
=============================================
[2019-03-27 02:51:37,251] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:37,258] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4923
[2019-03-27 02:51:37,262] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4792880274897444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669721.429146401, 669721.4291464004, 180414.0429977915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3286800.0000, 
sim time next is 3287400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4782232211128876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668336.4610090401, 668336.4610090401, 180267.3459203605], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37135327844926214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856490169469556, 0.1856490169469556, 0.26905574017964257], 
reward next is 0.7309, 
noisyNet noise sample is [array([-1.1111045], dtype=float32), -2.1701255]. 
=============================================
[2019-03-27 02:51:39,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:51:39,706] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8550
[2019-03-27 02:51:39,712] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6046156823311563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844914.6186266005, 844914.6186266005, 201608.1970021895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262200.0000, 
sim time next is 3262800.0000, 
raw observation next is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
processed observation next is [0.0, 0.782608695652174, 0.6366508688783573, 0.7633333333333334, 1.0, 1.0, 0.5016342449849397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22760792859789694, 0.22760792859789694, 0.2958679742077879], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.86471117], dtype=float32), -0.5855396]. 
=============================================
[2019-03-27 02:51:40,951] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 02:51:40,953] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:51:40,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:51:40,954] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:51:40,955] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:51:40,957] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:51:40,958] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:51:40,956] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:51:40,959] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:51:40,960] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:51:40,960] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:51:40,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-27 02:51:41,004] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-27 02:51:41,024] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-27 02:51:41,024] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-27 02:51:41,040] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-27 02:51:54,515] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:51:54,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.0, 92.83333333333333, 1.0, 2.0, 0.352914206489034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 545763.0820235668, 545763.0820235674, 170351.4650705214]
[2019-03-27 02:51:54,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:51:54,523] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04891772506437342
[2019-03-27 02:51:56,701] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:51:56,704] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.54591046333334, 97.57582168666666, 1.0, 2.0, 0.3259108192203579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515289.4988480904, 515289.4988480904, 168184.0030824033]
[2019-03-27 02:51:56,705] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:51:56,707] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42125470271196996
[2019-03-27 02:52:03,126] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:03,188] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.23333333333333, 85.33333333333334, 1.0, 2.0, 0.2927013891584705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 471230.9782617509, 471230.9782617515, 165045.8844358966]
[2019-03-27 02:52:03,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:52:03,195] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2669277419846815
[2019-03-27 02:52:15,379] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:15,380] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.3, 77.0, 1.0, 2.0, 0.7166497822587589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001549.461716232, 1001549.461716232, 224478.8678559963]
[2019-03-27 02:52:15,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:52:15,386] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5586867201788275
[2019-03-27 02:52:32,129] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:32,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.87021468, 73.77199298, 1.0, 2.0, 0.4981560742766889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696094.8720689671, 696094.8720689671, 183306.7723308454]
[2019-03-27 02:52:32,132] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:52:32,133] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5278068072460002
[2019-03-27 02:52:44,849] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:44,852] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.93333333333334, 51.33333333333334, 1.0, 2.0, 0.6314806164873005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 882472.3698915576, 882472.3698915583, 206757.7328992305]
[2019-03-27 02:52:44,853] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:52:44,856] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.00650488738774746
[2019-03-27 02:52:48,567] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:48,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.99454261, 77.5522758, 1.0, 2.0, 0.5381782326510927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752039.3489930002, 752039.3489930009, 189794.5242445249]
[2019-03-27 02:52:48,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:52:48,570] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7318429351225945
[2019-03-27 02:52:53,108] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:53,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.15, 77.16666666666666, 1.0, 2.0, 0.5803811123349075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811035.3119517787, 811035.3119517787, 197148.0652333059]
[2019-03-27 02:52:53,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:52:53,113] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4417168659384305
[2019-03-27 02:52:54,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:54,531] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.5, 70.0, 1.0, 2.0, 0.5205520509824388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727400.4468327963, 727400.4468327957, 186878.614573153]
[2019-03-27 02:52:54,533] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:52:54,535] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06454893834439668
[2019-03-27 02:52:59,579] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:52:59,579] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.13333333333333, 78.66666666666667, 1.0, 2.0, 0.5634103417405368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787311.2536732071, 787311.2536732071, 194126.7606576658]
[2019-03-27 02:52:59,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:52:59,584] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.934834085843758
[2019-03-27 02:53:04,811] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:53:04,813] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.27457574166667, 63.513909545, 1.0, 2.0, 0.5741791669497317, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9971593290395767, 6.9112, 6.9112, 168.9125890070176, 1605337.222843072, 1605337.222843072, 351358.6600298684]
[2019-03-27 02:53:04,815] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:53:04,821] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.636702800717791
[2019-03-27 02:53:09,043] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:53:09,046] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.84271163, 83.60562997000001, 1.0, 2.0, 0.4928697612313913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695329.675714113, 695329.675714113, 183336.9719262998]
[2019-03-27 02:53:09,046] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:53:09,049] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.38888621331064255
[2019-03-27 02:53:13,152] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:53:13,154] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.17200962, 84.67553229, 1.0, 2.0, 0.5701462529423377, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9681104851746047, 6.9112, 6.9112, 168.9129565004731, 1594053.201062701, 1594053.201062701, 344127.2397286296]
[2019-03-27 02:53:13,157] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:53:13,160] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8636168405993067
[2019-03-27 02:53:18,034] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.07850361], dtype=float32), -0.044540305]
[2019-03-27 02:53:18,036] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.11666666666667, 61.66666666666667, 1.0, 2.0, 0.949030313743503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326514.296463948, 1326514.296463947, 283781.8801746072]
[2019-03-27 02:53:18,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:53:18,039] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6169931451550908
[2019-03-27 02:53:35,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.7195 2779188685.6423 933.0000
[2019-03-27 02:53:36,090] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.2332 3164021513.5955 1778.0000
[2019-03-27 02:53:36,247] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:53:36,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.2258 2927347157.7828 1339.0000
[2019-03-27 02:53:36,296] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:53:37,312] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1575000, evaluation results [1575000.0, 7885.233204732784, 3164021513.595523, 1778.0, 8256.225789972601, 2927347157.7828155, 1339.0, 8662.71948536019, 2779188685.642261, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:53:52,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:53:52,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0213
[2019-03-27 02:53:52,351] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 79.0, 1.0, 2.0, 0.5292705421878953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739587.5896174719, 739587.5896174725, 188308.8996478552], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3534600.0000, 
sim time next is 3535200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5231976593354678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731098.595647361, 731098.595647361, 187310.149921335], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42553934859694914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20308294323537807, 0.20308294323537807, 0.279567387942291], 
reward next is 0.7204, 
noisyNet noise sample is [array([0.3783791], dtype=float32), -0.82674783]. 
=============================================
[2019-03-27 02:53:58,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:53:58,621] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3425
[2019-03-27 02:53:58,627] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.6185177105756027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864349.7849800398, 864349.7849800398, 204236.5709040854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3646800.0000, 
sim time next is 3647400.0000, 
raw observation next is [26.16666666666667, 83.16666666666667, 1.0, 2.0, 0.6411440369276529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 895982.3626824658, 895982.3626824658, 208650.3477953359], 
processed observation next is [1.0, 0.21739130434782608, 0.4391785150078992, 0.8316666666666667, 1.0, 1.0, 0.567643417985124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24888398963401828, 0.24888398963401828, 0.31141842954527743], 
reward next is 0.6886, 
noisyNet noise sample is [array([-0.5198717], dtype=float32), -0.57025456]. 
=============================================
[2019-03-27 02:54:01,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:01,443] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5407
[2019-03-27 02:54:01,449] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 73.66666666666666, 1.0, 2.0, 0.5470029122215367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764375.2141804579, 764375.2141804579, 191289.7036300032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3693000.0000, 
sim time next is 3693600.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.5521228668724575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771532.3748875092, 771532.3748875092, 192166.4793977207], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.75, 1.0, 1.0, 0.46038899623187646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21431454857986365, 0.21431454857986365, 0.2868156408921204], 
reward next is 0.7132, 
noisyNet noise sample is [array([-1.8739706], dtype=float32), -2.1905978]. 
=============================================
[2019-03-27 02:54:11,451] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:11,461] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6621
[2019-03-27 02:54:11,466] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.929289861679488, 6.9112, 168.9126525651582, 1466597.264269796, 1453763.716882355, 311355.5335554228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3998400.0000, 
sim time next is 3999000.0000, 
raw observation next is [29.83333333333333, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.011999370684645, 6.9112, 168.9122115008688, 1525314.127889714, 1453803.90048471, 311355.9980509268], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.010079937068464506, 0.0, 0.8294362868142855, 0.4236983688582539, 0.4038344168013083, 0.46471044485212953], 
reward next is 0.0313, 
noisyNet noise sample is [array([0.06852219], dtype=float32), 0.061556794]. 
=============================================
[2019-03-27 02:54:11,486] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[47.917118]
 [49.138752]
 [49.7895  ]
 [49.55462 ]
 [50.48447 ]], R is [[46.41292191]
 [46.39363098]
 [46.48207092]
 [46.59064102]
 [46.68595886]].
[2019-03-27 02:54:12,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:12,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7085
[2019-03-27 02:54:12,795] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 69.0, 1.0, 2.0, 0.5548712087925728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775374.2891201531, 775374.2891201531, 192639.4390450597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3872400.0000, 
sim time next is 3873000.0000, 
raw observation next is [30.33333333333333, 69.5, 1.0, 2.0, 0.5482789331545527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766158.9533746095, 766158.95337461, 191505.6622400055], 
processed observation next is [0.0, 0.8260869565217391, 0.6366508688783569, 0.695, 1.0, 1.0, 0.45575775078861763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21282193149294706, 0.21282193149294723, 0.28582934662687387], 
reward next is 0.7142, 
noisyNet noise sample is [array([0.6225638], dtype=float32), -0.7321]. 
=============================================
[2019-03-27 02:54:12,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.90437]
 [75.77869]
 [75.67938]
 [75.66994]
 [75.74027]], R is [[75.9695816 ]
 [75.92236328]
 [75.87389374]
 [75.82427216]
 [75.77302551]].
[2019-03-27 02:54:14,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:14,656] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8457
[2019-03-27 02:54:14,660] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 90.66666666666667, 1.0, 2.0, 0.576724529448313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805923.5967779416, 805923.5967779416, 196489.2596508685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3903600.0000, 
sim time next is 3904200.0000, 
raw observation next is [27.5, 91.5, 1.0, 2.0, 0.5759854728022732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804890.4365129758, 804890.4365129764, 196356.6652942462], 
processed observation next is [0.0, 0.17391304347826086, 0.5023696682464456, 0.915, 1.0, 1.0, 0.48913912385816044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22358067680915997, 0.2235806768091601, 0.2930696496929048], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.23206238], dtype=float32), 1.9051954]. 
=============================================
[2019-03-27 02:54:17,307] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:17,314] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0464
[2019-03-27 02:54:17,327] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2378904.246218995 W.
[2019-03-27 02:54:17,330] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.045306080046971, 6.9112, 168.9119094393713, 2378904.246218995, 2283765.36721671, 475772.2332757305], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4035600.0000, 
sim time next is 4036200.0000, 
raw observation next is [30.0, 79.00000000000001, 1.0, 2.0, 0.3922472729747185, 1.0, 1.0, 0.3922472729747185, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1096408.981644039, 1096408.981644039, 270203.0016561481], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.7900000000000001, 1.0, 1.0, 0.26776779876472107, 1.0, 0.5, 0.26776779876472107, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3045580504566775, 0.3045580504566775, 0.4032880621733554], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.57662773], dtype=float32), 0.2930431]. 
=============================================
[2019-03-27 02:54:17,948] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:17,959] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0899
[2019-03-27 02:54:17,964] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.33333333333334, 67.33333333333333, 1.0, 2.0, 0.616656230138146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861747.397183462, 861747.397183462, 203891.3357227084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4218000.0000, 
sim time next is 4218600.0000, 
raw observation next is [33.16666666666666, 69.16666666666667, 1.0, 2.0, 0.6245771916449745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872821.1078463277, 872821.1078463277, 205415.340563011], 
processed observation next is [1.0, 0.8260869565217391, 0.7709320695102682, 0.6916666666666668, 1.0, 1.0, 0.5476833634276801, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24245030773509102, 0.24245030773509102, 0.3065900605418075], 
reward next is 0.6934, 
noisyNet noise sample is [array([0.9146855], dtype=float32), 0.16828586]. 
=============================================
[2019-03-27 02:54:19,806] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4297758e-31 1.0000000e+00 1.5577744e-34 3.5854653e-33 5.6618310e-34], sum to 1.0000
[2019-03-27 02:54:19,817] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5432
[2019-03-27 02:54:19,827] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1948957.175887842 W.
[2019-03-27 02:54:19,830] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.4646498277245378, 1.0, 1.0, 0.4646498277245378, 1.0, 1.0, 0.8069430887113999, 6.9112, 6.9112, 170.5573041426782, 1948957.175887842, 1948957.175887842, 391560.9254985899], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4005000.0000, 
sim time next is 4005600.0000, 
raw observation next is [27.66666666666667, 84.0, 1.0, 2.0, 0.7469230248111952, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989585224015492, 6.9112, 168.912435065408, 1940792.74696204, 1885183.742903302, 395762.1381395117], 
processed observation next is [1.0, 0.34782608695652173, 0.5102685624012641, 0.84, 1.0, 1.0, 0.6950879817002352, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00783852240154923, 0.0, 0.8294373846185212, 0.5391090963783445, 0.5236621508064727, 0.5906897584171816], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8762934], dtype=float32), 1.5242772]. 
=============================================
[2019-03-27 02:54:27,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:27,313] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4147
[2019-03-27 02:54:27,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3444361.335978139 W.
[2019-03-27 02:54:27,326] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.83333333333334, 64.5, 1.0, 2.0, 1.000146180134228, 1.0, 2.0, 0.8206631295813764, 1.0, 2.0, 1.03, 7.005121410346943, 6.9112, 170.5573041426782, 3444361.335978139, 3377081.609267917, 632916.6151904985], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [35.66666666666667, 65.0, 1.0, 2.0, 1.011571232598035, 1.0, 2.0, 0.8263756558132802, 1.0, 2.0, 1.03, 7.005122312128611, 6.9112, 170.5573041426782, 3468370.375410009, 3401090.002716896, 637828.8626005575], 
processed observation next is [1.0, 0.6521739130434783, 0.8894154818325437, 0.65, 1.0, 1.0, 1.013941244094018, 1.0, 1.0, 0.7908140431485303, 1.0, 1.0, 1.0365853658536586, 0.009392231212861101, 0.0, 0.8375144448122397, 0.9634362153916692, 0.9447472229769156, 0.9519833770157575], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.416119], dtype=float32), 1.0609701]. 
=============================================
[2019-03-27 02:54:27,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[29.610447]
 [29.401665]
 [28.855751]
 [29.455486]
 [29.793324]], R is [[29.32692528]
 [29.03365707]
 [28.74332047]
 [28.45588684]
 [28.17132759]].
[2019-03-27 02:54:33,597] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 02:54:33,599] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:54:33,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:54:33,602] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:54:33,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:54:33,604] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:54:33,604] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:54:33,606] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:54:33,606] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:54:33,608] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:54:33,609] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:54:33,633] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-27 02:54:33,653] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-27 02:54:33,682] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-27 02:54:33,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-27 02:54:33,719] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-27 02:54:39,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.01984455], dtype=float32), -0.050827127]
[2019-03-27 02:54:39,908] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.18333333333333, 88.16666666666667, 1.0, 2.0, 0.2635817827153241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429329.4209391063, 429329.4209391063, 162210.6649499334]
[2019-03-27 02:54:39,910] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:54:39,912] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8103309619174286
[2019-03-27 02:54:57,592] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.01984455], dtype=float32), -0.050827127]
[2019-03-27 02:54:57,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.13333333333333, 95.66666666666666, 1.0, 2.0, 0.4165940947227739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613991.6183269852, 613991.6183269846, 175631.8768296352]
[2019-03-27 02:54:57,595] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:54:57,596] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.018455381626874656
[2019-03-27 02:55:10,413] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.01984455], dtype=float32), -0.050827127]
[2019-03-27 02:55:10,413] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.83333333333334, 95.0, 1.0, 2.0, 0.4395226203979918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632163.5869788629, 632163.5869788629, 176980.9138791288]
[2019-03-27 02:55:10,414] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:55:10,416] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9210690659054301
[2019-03-27 02:55:33,344] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.01984455], dtype=float32), -0.050827127]
[2019-03-27 02:55:33,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.41765703333333, 73.29963779666667, 1.0, 2.0, 0.6552686548230439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 915729.6638409354, 915729.6638409354, 211494.0774250166]
[2019-03-27 02:55:33,347] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:55:33,351] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7650972263909596
[2019-03-27 02:55:35,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.01984455], dtype=float32), -0.050827127]
[2019-03-27 02:55:35,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.55, 43.0, 1.0, 2.0, 0.6840713500085315, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.991197666401471, 6.9112, 168.9119392273661, 1852839.214939154, 1796086.458883901, 381792.3915366593]
[2019-03-27 02:55:35,090] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:55:35,091] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.29627959055565356
[2019-03-27 02:55:35,093] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1852839.214939154 W.
[2019-03-27 02:55:35,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.01984455], dtype=float32), -0.050827127]
[2019-03-27 02:55:35,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.86485404666666, 58.70353918166666, 1.0, 2.0, 0.5227795409173229, 0.0, 2.0, 0.0, 1.0, 2.0, 0.907895176737358, 6.9112, 6.9112, 168.9129351886295, 1461530.990871338, 1461530.990871338, 320818.6826020606]
[2019-03-27 02:55:35,328] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:55:35,329] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4648755643442033
[2019-03-27 02:56:28,115] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:56:28,615] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:56:28,841] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:56:28,878] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:56:28,914] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:56:29,930] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1600000, evaluation results [1600000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:56:32,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6622852e-24 1.0000000e+00 3.2768814e-25 1.6902021e-24 3.8397095e-25], sum to 1.0000
[2019-03-27 02:56:32,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1548
[2019-03-27 02:56:32,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1836287.463147381 W.
[2019-03-27 02:56:32,418] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.4378113013467656, 1.0, 2.0, 0.4378113013467656, 1.0, 2.0, 0.7603334440295104, 6.9112, 6.9112, 170.5573041426782, 1836287.463147381, 1836287.463147381, 374808.269947818], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4260600.0000, 
sim time next is 4261200.0000, 
raw observation next is [30.66666666666666, 82.33333333333334, 1.0, 2.0, 0.4677553663889408, 1.0, 2.0, 0.4677553663889408, 1.0, 2.0, 0.8123363823541379, 6.911200000000001, 6.9112, 170.5573041426782, 1961995.174771455, 1961995.174771454, 393566.3285042875], 
processed observation next is [1.0, 0.30434782608695654, 0.6524486571879934, 0.8233333333333335, 1.0, 1.0, 0.3587414052878804, 1.0, 1.0, 0.3587414052878804, 1.0, 1.0, 0.7711419297001681, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5449986596587375, 0.5449986596587372, 0.5874124306034142], 
reward next is 0.4126, 
noisyNet noise sample is [array([1.9971052], dtype=float32), -1.0995106]. 
=============================================
[2019-03-27 02:56:34,130] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:34,139] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3476
[2019-03-27 02:56:34,144] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5814528363471861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812533.5317684455, 812533.5317684455, 197341.4723990394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4429800.0000, 
sim time next is 4430400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5808793035083024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811731.7595429552, 811731.7595429552, 197237.8598572956], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4950353054316896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22548104431748756, 0.22548104431748756, 0.29438486545865017], 
reward next is 0.7056, 
noisyNet noise sample is [array([-1.2972195], dtype=float32), 0.4348608]. 
=============================================
[2019-03-27 02:56:36,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0366625e-31 1.0000000e+00 3.9321594e-33 2.2637068e-33 4.5801166e-38], sum to 1.0000
[2019-03-27 02:56:36,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2498
[2019-03-27 02:56:36,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2789316.778846902 W.
[2019-03-27 02:56:36,155] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.688348884369929, 1.0, 2.0, 0.6647644816992272, 1.0, 1.0, 1.03, 7.00509681349434, 6.9112, 170.5573041426782, 2789316.778846902, 2722054.671862736, 517674.2147624143], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4381200.0000, 
sim time next is 4381800.0000, 
raw observation next is [32.83333333333334, 63.66666666666666, 1.0, 2.0, 0.4134051689071339, 1.0, 2.0, 0.4134051689071339, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1155581.370389024, 1155581.370389024, 275567.5085540642], 
processed observation next is [1.0, 0.7391304347826086, 0.7551342812006324, 0.6366666666666666, 1.0, 1.0, 0.2932592396471493, 1.0, 1.0, 0.2932592396471493, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3209948251080622, 0.3209948251080622, 0.411294788886663], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1010647], dtype=float32), 1.5495867]. 
=============================================
[2019-03-27 02:56:38,262] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2770739e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:56:38,270] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7554
[2019-03-27 02:56:38,275] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2569457.499888376 W.
[2019-03-27 02:56:38,280] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.9186304528292765, 1.0, 2.0, 0.9186304528292765, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2569457.499888376, 2569457.499888376, 481773.4104479793], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4366800.0000, 
sim time next is 4367400.0000, 
raw observation next is [36.16666666666667, 57.5, 1.0, 2.0, 0.9479109709443947, 1.0, 2.0, 0.9479109709443947, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2651443.544637057, 2651443.544637058, 498319.4577983265], 
processed observation next is [1.0, 0.5652173913043478, 0.9131121642969986, 0.575, 1.0, 1.0, 0.9372421336679454, 1.0, 1.0, 0.9372421336679454, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7365120957325159, 0.7365120957325161, 0.7437603847736216], 
reward next is 0.2562, 
noisyNet noise sample is [array([-0.30019146], dtype=float32), 0.6327107]. 
=============================================
[2019-03-27 02:56:45,700] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:45,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4492
[2019-03-27 02:56:45,715] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5181319455756227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724017.5273238159, 724017.5273238159, 186485.7547262563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4744800.0000, 
sim time next is 4745400.0000, 
raw observation next is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5171077194339162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722585.8265663391, 722585.8265663391, 186319.9044311771], 
processed observation next is [1.0, 0.9565217391304348, 0.5181674565560824, 0.7983333333333335, 1.0, 1.0, 0.4182020716071279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20071828515731643, 0.20071828515731643, 0.27808940959877176], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.7968241], dtype=float32), -0.51856005]. 
=============================================
[2019-03-27 02:56:48,778] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:48,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4651
[2019-03-27 02:56:48,788] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.708105031886024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 989602.2348464536, 989602.2348464536, 222601.007603375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4765800.0000, 
sim time next is 4766400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6332765027489061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884983.1045701196, 884983.1045701191, 207098.9061980101], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5581644611432603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24582864015836656, 0.24582864015836642, 0.30910284507165686], 
reward next is 0.6909, 
noisyNet noise sample is [array([-2.9338574], dtype=float32), -0.44512966]. 
=============================================
[2019-03-27 02:56:48,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:48,989] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3368
[2019-03-27 02:56:48,995] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5233320700164087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731286.4812083687, 731286.4812083694, 187332.1264624194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4565400.0000, 
sim time next is 4566000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5224396684751482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730039.0410538552, 730039.0410538552, 187186.3100326881], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4246261065965641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20278862251495977, 0.20278862251495977, 0.2793825522875942], 
reward next is 0.7206, 
noisyNet noise sample is [array([0.84619516], dtype=float32), 1.0322274]. 
=============================================
[2019-03-27 02:56:49,015] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.29947 ]
 [76.344765]
 [76.22814 ]
 [76.274155]
 [76.349236]], R is [[76.2047348 ]
 [76.16308594]
 [76.12162781]
 [76.07991028]
 [76.03804779]].
[2019-03-27 02:56:51,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:51,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0470
[2019-03-27 02:56:51,316] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5498011070409204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768286.7916807865, 768286.791680786, 191766.3772246501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4575600.0000, 
sim time next is 4576200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507766209849794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769650.4593724336, 769650.4593724343, 191933.6903364013], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45876701323491487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21379179427012043, 0.21379179427012063, 0.2864681945319423], 
reward next is 0.7135, 
noisyNet noise sample is [array([1.5805957], dtype=float32), 1.5153866]. 
=============================================
[2019-03-27 02:56:51,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:51,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3899
[2019-03-27 02:56:51,927] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7634146406370645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1066938.231714663, 1066938.231714663, 235125.1919017656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4848600.0000, 
sim time next is 4849200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.756665916836954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1057501.607943382, 1057501.607943382, 233548.500064555], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7068264058276553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29375044665093947, 0.29375044665093947, 0.3485798508426194], 
reward next is 0.6514, 
noisyNet noise sample is [array([-0.12191451], dtype=float32), 0.07956535]. 
=============================================
[2019-03-27 02:56:56,596] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:56:56,612] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8782
[2019-03-27 02:56:56,623] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4948129326373555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691421.8363920553, 691421.8363920553, 182787.0134091775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4752000.0000, 
sim time next is 4752600.0000, 
raw observation next is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4924095810815747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688062.4498286162, 688062.4498286168, 182415.0832402073], 
processed observation next is [1.0, 0.0, 0.5181674565560824, 0.7483333333333334, 1.0, 1.0, 0.3884452784115358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19112845828572672, 0.1911284582857269, 0.27226131826896616], 
reward next is 0.7277, 
noisyNet noise sample is [array([-1.395441], dtype=float32), 0.1695921]. 
=============================================
[2019-03-27 02:57:09,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:57:09,052] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4353
[2019-03-27 02:57:09,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1915029.192223277 W.
[2019-03-27 02:57:09,067] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7285129053245056, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990261915094681, 6.9112, 168.9124183239194, 1915029.192223277, 1858940.127264233, 391542.4397348982], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4882800.0000, 
sim time next is 4883400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4182760170518275, 1.0, 1.0, 0.4182760170518275, 1.0, 2.0, 0.7224918935668296, 6.911200000000001, 6.9112, 170.5573041426782, 1754284.640116423, 1754284.640116422, 362660.1646672614], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.29912773138774396, 1.0, 0.5, 0.29912773138774396, 1.0, 1.0, 0.6615754799595484, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4873012889212286, 0.4873012889212283, 0.5412838278615842], 
reward next is 0.4587, 
noisyNet noise sample is [array([-0.94085705], dtype=float32), -0.2694069]. 
=============================================
[2019-03-27 02:57:19,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:57:19,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9237
[2019-03-27 02:57:19,251] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 61.66666666666667, 1.0, 2.0, 0.5211783218355135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728275.8748730033, 728275.8748730033, 186981.1706634392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5067600.0000, 
sim time next is 5068200.0000, 
raw observation next is [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.5208317382904694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727791.405560802, 727791.4055608013, 186924.6560493465], 
processed observation next is [0.0, 0.6521739130434783, 0.6761453396524489, 0.6233333333333333, 1.0, 1.0, 0.42268884131381845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.202164279322445, 0.2021642793224448, 0.2789920239542485], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.04427309], dtype=float32), -0.052502166]. 
=============================================
[2019-03-27 02:57:22,273] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:57:22,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7615
[2019-03-27 02:57:22,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 81.5, 1.0, 2.0, 0.5017359629652348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701098.8550179816, 701098.8550179822, 183868.1858325907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5182200.0000, 
sim time next is 5182800.0000, 
raw observation next is [27.0, 82.33333333333334, 1.0, 2.0, 0.5056631574720717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706588.3305639104, 706588.3305639104, 184487.849993399], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.8233333333333335, 1.0, 1.0, 0.40441344273743574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1962745362677529, 0.1962745362677529, 0.27535499999014773], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.37164214], dtype=float32), 0.5084538]. 
=============================================
[2019-03-27 02:57:24,416] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6840672e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:57:24,427] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5537
[2019-03-27 02:57:24,435] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2023944.506686267 W.
[2019-03-27 02:57:24,441] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.7237659682625127, 1.0, 1.0, 0.7237659682625127, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2023944.506686267, 2023944.506686267, 384462.7587426238], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5217000.0000, 
sim time next is 5217600.0000, 
raw observation next is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.8385226635353412, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988876214905532, 6.9112, 168.9124943983552, 2068988.73937845, 2013882.709874963, 418213.18910473], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783573, 0.6866666666666668, 1.0, 1.0, 0.8054489922112544, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007767621490553189, 0.0, 0.8294376759704546, 0.5747190942717917, 0.5594118638541564, 0.6241987897085522], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.888367], dtype=float32), 0.6249273]. 
=============================================
[2019-03-27 02:57:26,202] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 02:57:26,203] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:57:26,204] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:57:26,204] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:57:26,206] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:57:26,206] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:57:26,207] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:57:26,208] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:57:26,207] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:57:26,210] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:57:26,213] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:57:26,245] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-27 02:57:26,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-27 02:57:26,287] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-27 02:57:26,313] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-27 02:57:26,330] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-27 02:57:35,482] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06965224], dtype=float32), -0.048358914]
[2019-03-27 02:57:35,482] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 70.0, 1.0, 2.0, 0.2508041021938299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412422.978333547, 412422.9783335476, 160985.8883752157]
[2019-03-27 02:57:35,483] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:57:35,486] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8430427964648557
[2019-03-27 02:57:55,002] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06965224], dtype=float32), -0.048358914]
[2019-03-27 02:57:55,003] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.89243539166667, 72.77483399333335, 1.0, 2.0, 0.6437652327706289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 899646.968138181, 899646.9681381816, 209173.8473044411]
[2019-03-27 02:57:55,005] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:57:55,007] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9081713325433478
[2019-03-27 02:58:34,369] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06965224], dtype=float32), -0.048358914]
[2019-03-27 02:58:34,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5061244382373603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707233.1157376021, 707233.1157376014, 184561.0802186313]
[2019-03-27 02:58:34,375] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:58:34,379] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9725266265708349
[2019-03-27 02:58:57,051] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06965224], dtype=float32), -0.048358914]
[2019-03-27 02:58:57,052] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.5, 69.0, 1.0, 2.0, 0.5103868625554859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713191.2149102808, 713191.2149102802, 185239.5793064028]
[2019-03-27 02:58:57,055] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:58:57,059] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03244627152435153
[2019-03-27 02:59:02,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06965224], dtype=float32), -0.048358914]
[2019-03-27 02:59:02,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.76134094166667, 92.73500234666666, 1.0, 2.0, 0.5193182374297095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725675.7718727257, 725675.7718727263, 186677.575990712]
[2019-03-27 02:59:02,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:59:02,512] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9777784230769557
[2019-03-27 02:59:21,175] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:59:21,434] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:59:21,488] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:59:21,523] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 02:59:21,600] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:59:22,620] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1625000, evaluation results [1625000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:59:36,876] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.259035e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:59:36,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9129
[2019-03-27 02:59:36,896] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3118181.164726953 W.
[2019-03-27 02:59:36,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.95, 62.0, 1.0, 2.0, 0.8449068090836059, 1.0, 2.0, 0.7430434440560656, 1.0, 1.0, 1.03, 7.00510916070604, 6.9112, 170.5573041426782, 3118181.164726953, 3050910.212932793, 571004.2074467819], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5416200.0000, 
sim time next is 5416800.0000, 
raw observation next is [32.26666666666667, 65.0, 1.0, 2.0, 0.8037656093194543, 1.0, 2.0, 0.7224728441739896, 1.0, 2.0, 1.03, 7.005105915404643, 6.9112, 170.5573041426782, 3031751.885733217, 2964483.258680459, 556105.5811660158], 
processed observation next is [1.0, 0.6956521739130435, 0.7282780410742499, 0.65, 1.0, 1.0, 0.7635730232764509, 1.0, 1.0, 0.6656299327397465, 1.0, 1.0, 1.0365853658536586, 0.00939059154046431, 0.0, 0.8375144448122397, 0.8421533015925603, 0.8234675718556831, 0.830008330098531], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37782714], dtype=float32), -0.8487944]. 
=============================================
[2019-03-27 02:59:42,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:59:42,264] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6060
[2019-03-27 02:59:42,268] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 89.5, 1.0, 2.0, 0.5121584139696577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715667.5335805215, 715667.5335805209, 185522.4835017365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5715000.0000, 
sim time next is 5715600.0000, 
raw observation next is [26.0, 89.66666666666667, 1.0, 2.0, 0.5126935680821485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716415.5864571687, 716415.5864571687, 185608.2069429588], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.8966666666666667, 1.0, 1.0, 0.41288381696644394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19900432957143574, 0.19900432957143574, 0.27702717454172954], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.00278256], dtype=float32), -0.6573416]. 
=============================================
[2019-03-27 02:59:43,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:59:43,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9617
[2019-03-27 02:59:43,689] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 79.0, 1.0, 2.0, 0.5815143960302137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812619.5893772576, 812619.589377257, 197352.7054057249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5516400.0000, 
sim time next is 5517000.0000, 
raw observation next is [29.7, 79.5, 1.0, 2.0, 0.5803214950781144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810951.969889811, 810951.969889811, 197137.1533172601], 
processed observation next is [1.0, 0.8695652173913043, 0.6066350710900474, 0.795, 1.0, 1.0, 0.49436324708206547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22526443608050306, 0.22526443608050306, 0.29423455718994046], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.62491316], dtype=float32), -2.6492019]. 
=============================================
[2019-03-27 02:59:43,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.45361 ]
 [64.36314 ]
 [64.260956]
 [64.267044]
 [64.30352 ]], R is [[64.80831909]
 [64.86568451]
 [64.9222641 ]
 [64.9790802 ]
 [65.03610992]].
[2019-03-27 02:59:46,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6448506e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:59:46,367] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3526
[2019-03-27 02:59:46,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2401603.572482007 W.
[2019-03-27 02:59:46,383] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.9, 58.66666666666667, 1.0, 2.0, 0.5724513748069603, 1.0, 2.0, 0.5724513748069603, 1.0, 1.0, 0.9941587254771758, 6.9112, 6.9112, 170.5573041426782, 2401603.572482007, 2401603.572482007, 468862.1636435257], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5571600.0000, 
sim time next is 5572200.0000, 
raw observation next is [33.0, 57.83333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.612652285069752, 6.9112, 168.9091018938267, 2786558.107356341, 2288935.070287823, 474771.9661956131], 
processed observation next is [1.0, 0.4782608695652174, 0.7630331753554502, 0.5783333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07014522850697516, 0.0, 0.8294210172201244, 0.7740439187100947, 0.6358152973021731, 0.7086148749188255], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.5222971], dtype=float32), 0.8171438]. 
=============================================
[2019-03-27 02:59:47,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:59:47,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5705
[2019-03-27 02:59:47,260] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.128619666676824, 6.9112, 168.9115556221111, 1608104.278596521, 1453860.563287383, 311349.8403572529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5801400.0000, 
sim time next is 5802000.0000, 
raw observation next is [26.3, 92.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.068328879655583, 6.9112, 168.9115731381314, 1565302.993129848, 1453831.27088801, 311349.7600506936], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.9233333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.01571288796555832, 0.0, 0.829433152161001, 0.4348063869805133, 0.4038420196911139, 0.4647011344040203], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.26987064], dtype=float32), -2.3002121]. 
=============================================
[2019-03-27 02:59:47,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[35.615852]
 [35.19111 ]
 [40.62165 ]
 [41.0467  ]
 [37.72776 ]], R is [[35.63954163]
 [35.2831459 ]
 [35.42248154]
 [35.06825638]
 [34.76449203]].
[2019-03-27 02:59:57,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:59:57,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9205
[2019-03-27 02:59:57,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2502233.600639197 W.
[2019-03-27 02:59:57,297] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.218989288157502, 6.9112, 168.9115823061546, 2502233.600639197, 2283879.062785008, 475425.948862568], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5913600.0000, 
sim time next is 5914200.0000, 
raw observation next is [31.93333333333334, 70.66666666666667, 1.0, 2.0, 0.8767591412423567, 1.0, 1.0, 0.8767591412423567, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2452226.324214462, 2452226.324214462, 458978.4589113237], 
processed observation next is [1.0, 0.43478260869565216, 0.7124802527646134, 0.7066666666666667, 1.0, 1.0, 0.8515170376413935, 1.0, 0.5, 0.8515170376413935, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6811739789484618, 0.6811739789484618, 0.6850424759870503], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8840311], dtype=float32), 2.3883994]. 
=============================================
[2019-03-27 02:59:59,392] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.2535493e-34 1.0000000e+00 3.5266404e-37 1.5568818e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 02:59:59,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5049
[2019-03-27 02:59:59,413] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2485002.08686876 W.
[2019-03-27 02:59:59,420] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.3, 72.0, 1.0, 2.0, 0.8884660147979015, 1.0, 2.0, 0.8884660147979015, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2485002.08686876, 2485002.08686876, 465242.8703689276], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5918400.0000, 
sim time next is 5919000.0000, 
raw observation next is [31.16666666666667, 72.16666666666667, 1.0, 2.0, 0.6114755080298321, 1.0, 2.0, 0.6114755080298321, 1.0, 1.0, 1.03, 6.947095677286045, 6.9112, 170.5573041426782, 2565489.473432641, 2539775.939649851, 492177.236185621], 
processed observation next is [1.0, 0.5217391304347826, 0.6761453396524489, 0.7216666666666667, 1.0, 1.0, 0.5318982024455808, 1.0, 1.0, 0.5318982024455808, 1.0, 0.5, 1.0365853658536586, 0.0035895677286045037, 0.0, 0.8375144448122397, 0.7126359648424003, 0.7054933165694031, 0.7345928898292851], 
reward next is 0.0859, 
noisyNet noise sample is [array([1.0245072], dtype=float32), 0.16935742]. 
=============================================
[2019-03-27 02:59:59,437] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[21.436428]
 [21.279467]
 [21.161144]
 [20.672827]
 [20.279148]], R is [[21.32419777]
 [21.41656303]
 [21.51542091]
 [21.47193718]
 [21.25721741]].
[2019-03-27 03:00:02,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2185564e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:00:02,244] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9054
[2019-03-27 03:00:02,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2424444.657631152 W.
[2019-03-27 03:00:02,261] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.1, 65.66666666666666, 1.0, 2.0, 0.5778905483525256, 1.0, 2.0, 0.5778905483525256, 1.0, 1.0, 1.003604771163644, 6.9112, 6.9112, 170.5573041426782, 2424444.657631152, 2424444.657631152, 473187.0152913361], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5830800.0000, 
sim time next is 5831400.0000, 
raw observation next is [32.15, 65.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.6102466624556, 6.9112, 168.90911665409, 2780051.736552814, 2284135.248680281, 474527.7531635691], 
processed observation next is [1.0, 0.4782608695652174, 0.7227488151658767, 0.6533333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.06990466624555998, 0.0, 0.8294210896997746, 0.7722365934868928, 0.6344820135223003, 0.7082503778560733], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.36202234], dtype=float32), -0.38979465]. 
=============================================
[2019-03-27 03:00:03,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:00:03,111] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4199
[2019-03-27 03:00:03,114] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 92.66666666666667, 1.0, 2.0, 0.6213281476404795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868278.8456684981, 868278.8456684987, 204779.9012124394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5980800.0000, 
sim time next is 5981400.0000, 
raw observation next is [26.35, 92.0, 1.0, 2.0, 0.6178315982302748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 863390.5851196287, 863390.585119628, 204108.6099141939], 
processed observation next is [1.0, 0.21739130434782608, 0.4478672985781992, 0.92, 1.0, 1.0, 0.5395561424461142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23983071808878575, 0.23983071808878556, 0.3046397162898416], 
reward next is 0.6954, 
noisyNet noise sample is [array([-1.247882], dtype=float32), -1.0905225]. 
=============================================
[2019-03-27 03:00:05,430] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:00:05,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0681
[2019-03-27 03:00:05,441] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 94.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.94598345494575, 6.9112, 168.9124660732522, 1478448.336140446, 1453771.827696021, 311349.5976433922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5883600.0000, 
sim time next is 5884200.0000, 
raw observation next is [25.95, 94.5, 1.0, 2.0, 0.9412184965235203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129281882811, 1315588.504604846, 1315588.504604847, 281526.7021241549], 
processed observation next is [1.0, 0.08695652173913043, 0.42890995260663506, 0.945, 1.0, 1.0, 0.9291789114741208, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294398060775806, 0.3654412512791239, 0.36544125127912414, 0.4201891076479924], 
reward next is 0.5798, 
noisyNet noise sample is [array([-1.6035955], dtype=float32), 0.023107871]. 
=============================================
[2019-03-27 03:00:08,123] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:00:08,134] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0019
[2019-03-27 03:00:08,140] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 66.33333333333333, 1.0, 2.0, 0.466026802769989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651185.5009498041, 651185.5009498034, 178445.7562445087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6543600.0000, 
sim time next is 6544200.0000, 
raw observation next is [29.4, 66.66666666666667, 1.0, 2.0, 0.4728487781191341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660720.9030123164, 660720.9030123164, 179452.3924174852], 
processed observation next is [1.0, 0.7391304347826086, 0.5924170616113744, 0.6666666666666667, 1.0, 1.0, 0.36487804592666767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1835335841700879, 0.1835335841700879, 0.2678393916678884], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.7243135], dtype=float32), 0.3648383]. 
=============================================
[2019-03-27 03:00:11,702] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:00:11,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7843
[2019-03-27 03:00:11,723] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333334, 89.0, 1.0, 2.0, 0.6905942257001743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965119.1339047726, 965119.1339047733, 218828.645237434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5984400.0000, 
sim time next is 5985000.0000, 
raw observation next is [27.3, 88.5, 1.0, 2.0, 0.6996532994182234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977785.1973600307, 977785.1973600313, 220771.1993436064], 
processed observation next is [1.0, 0.2608695652173913, 0.4928909952606636, 0.885, 1.0, 1.0, 0.6381365053231607, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2716069992666752, 0.27160699926667536, 0.3295092527516513], 
reward next is 0.6705, 
noisyNet noise sample is [array([-0.10123744], dtype=float32), 1.0484686]. 
=============================================
[2019-03-27 03:00:11,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[53.73995 ]
 [52.74405 ]
 [55.829994]
 [57.587875]
 [57.591118]], R is [[53.89007187]
 [54.02456284]
 [54.11433029]
 [54.24268723]
 [54.39477539]].
[2019-03-27 03:00:12,502] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1927208e-35 1.0000000e+00 6.2056856e-38 5.1690703e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 03:00:12,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6968
[2019-03-27 03:00:12,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2388249.329060765 W.
[2019-03-27 03:00:12,529] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.85, 78.33333333333334, 1.0, 2.0, 0.8539069013994455, 1.0, 2.0, 0.8539069013994455, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2388249.329060765, 2388249.329060765, 446971.7845269261], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5994600.0000, 
sim time next is 5995200.0000, 
raw observation next is [30.0, 77.66666666666667, 1.0, 2.0, 0.8703431406367416, 1.0, 2.0, 0.8703431406367416, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2434263.796243595, 2434263.796243594, 455573.1706994294], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.7766666666666667, 1.0, 1.0, 0.8437869164298091, 1.0, 1.0, 0.8437869164298091, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.676184387845443, 0.6761843878454428, 0.679959956267805], 
reward next is 0.3200, 
noisyNet noise sample is [array([0.4677206], dtype=float32), 0.14947483]. 
=============================================
[2019-03-27 03:00:13,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:00:13,982] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2998
[2019-03-27 03:00:13,988] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.88333333333333, 62.33333333333334, 1.0, 2.0, 0.5146297733259568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719122.070858105, 719122.0708581056, 185920.072765458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6270600.0000, 
sim time next is 6271200.0000, 
raw observation next is [30.9, 62.0, 1.0, 2.0, 0.5124857859022165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716125.1428498683, 716125.1428498676, 185575.5680452235], 
processed observation next is [0.0, 0.6086956521739131, 0.6635071090047393, 0.62, 1.0, 1.0, 0.41263347699062225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19892365079163007, 0.19892365079162988, 0.2769784597689903], 
reward next is 0.7230, 
noisyNet noise sample is [array([-1.3617512], dtype=float32), -0.282962]. 
=============================================
[2019-03-27 03:00:18,905] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 03:00:18,908] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:00:18,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:00:18,910] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:00:18,911] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:00:18,913] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:00:18,913] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:00:18,914] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:00:18,915] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:00:18,914] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:00:18,917] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:00:18,944] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-27 03:00:18,968] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-27 03:00:18,969] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-27 03:00:18,986] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-27 03:00:19,022] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-27 03:00:45,905] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.14759986], dtype=float32), -0.03114797]
[2019-03-27 03:00:45,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.2, 93.5, 1.0, 2.0, 0.3702351357364949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 567884.6126256549, 567884.6126256549, 172103.6699368212]
[2019-03-27 03:00:45,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:00:45,909] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14031532606013875
[2019-03-27 03:01:01,500] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.14759986], dtype=float32), -0.03114797]
[2019-03-27 03:01:01,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.95066712666667, 85.78826107, 1.0, 2.0, 0.4114492907724925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675706.3844689317, 675706.3844689311, 181172.2492319825]
[2019-03-27 03:01:01,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:01:01,508] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7970247277575336
[2019-03-27 03:01:04,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.14759986], dtype=float32), -0.03114797]
[2019-03-27 03:01:04,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.6, 53.0, 1.0, 2.0, 0.8671768654682391, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005985724196142, 6.9112, 168.9123159909066, 2109094.197252833, 2041850.197226081, 425113.827820532]
[2019-03-27 03:01:04,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:01:04,987] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6755875412268414
[2019-03-27 03:01:04,988] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2109094.197252833 W.
[2019-03-27 03:01:34,371] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.14759986], dtype=float32), -0.03114797]
[2019-03-27 03:01:34,373] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.08333333333333, 71.16666666666667, 1.0, 2.0, 0.7843005428758635, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005980547732049, 6.9112, 168.9123160111646, 1993101.652070459, 1925861.324383505, 404003.3972120745]
[2019-03-27 03:01:34,375] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:01:34,377] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5179256e-34 1.0000000e+00 3.8075387e-37 4.4041890e-36 0.0000000e+00], sampled 0.4984550657667951
[2019-03-27 03:01:34,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1993101.652070459 W.
[2019-03-27 03:01:54,648] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.14759986], dtype=float32), -0.03114797]
[2019-03-27 03:01:54,649] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.19270014666667, 72.44633459, 1.0, 2.0, 0.7782892297888468, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005980172412436, 6.9112, 168.9123158280877, 1984688.795375933, 1917448.7340255, 402558.790210577]
[2019-03-27 03:01:54,651] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:01:54,654] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2491258e-36 1.0000000e+00 0.0000000e+00 2.8511709e-37 0.0000000e+00], sampled 0.49673362608883775
[2019-03-27 03:01:54,656] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1984688.795375933 W.
[2019-03-27 03:01:57,442] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.14759986], dtype=float32), -0.03114797]
[2019-03-27 03:01:57,443] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.03333333333333, 52.0, 1.0, 2.0, 0.6419212548915012, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.989796312412987, 6.9112, 168.9121265563998, 1793859.189190845, 1738100.534060336, 373175.1478936316]
[2019-03-27 03:01:57,444] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:01:57,447] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9463051251300353
[2019-03-27 03:01:57,448] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1793859.189190845 W.
[2019-03-27 03:02:13,857] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:02:13,896] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:02:14,144] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:02:14,211] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:02:14,318] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:02:15,332] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1650000, evaluation results [1650000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:02:18,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:18,706] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6173
[2019-03-27 03:02:18,714] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 87.66666666666667, 1.0, 2.0, 0.8856103043301344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1237816.822793957, 1237816.822793957, 266011.4167593589], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6162000.0000, 
sim time next is 6162600.0000, 
raw observation next is [27.73333333333333, 87.33333333333333, 1.0, 2.0, 0.9435376608628683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1318832.130731554, 1318832.130731553, 282195.868357793], 
processed observation next is [1.0, 0.30434782608695654, 0.513428120063191, 0.8733333333333333, 1.0, 1.0, 0.9319730853769498, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3663422585365428, 0.3663422585365425, 0.4211878632205866], 
reward next is 0.5788, 
noisyNet noise sample is [array([-0.6050968], dtype=float32), 0.63258237]. 
=============================================
[2019-03-27 03:02:19,890] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:19,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9942
[2019-03-27 03:02:19,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2104405.090594392 W.
[2019-03-27 03:02:19,911] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.21666666666667, 77.5, 1.0, 2.0, 0.5016737168282107, 1.0, 2.0, 0.5016737168282107, 1.0, 2.0, 0.8712413400972541, 6.9112, 6.9112, 170.5573041426782, 2104405.090594392, 2104405.090594392, 416304.8288463641], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6173400.0000, 
sim time next is 6174000.0000, 
raw observation next is [29.3, 77.0, 1.0, 2.0, 0.4811087273233587, 1.0, 2.0, 0.4811087273233587, 1.0, 2.0, 0.8355267542732799, 6.9112, 6.9112, 170.5573041426782, 2018058.46639302, 2018058.46639302, 402327.5689435333], 
processed observation next is [1.0, 0.4782608695652174, 0.5876777251184835, 0.77, 1.0, 1.0, 0.37482979195585386, 1.0, 1.0, 0.37482979195585386, 1.0, 1.0, 0.7994228710649754, 0.0, 0.0, 0.8375144448122397, 0.5605717962202833, 0.5605717962202833, 0.6004889088709452], 
reward next is 0.3995, 
noisyNet noise sample is [array([-0.7229582], dtype=float32), 0.13634503]. 
=============================================
[2019-03-27 03:02:19,931] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[33.355526]
 [33.55211 ]
 [33.633827]
 [33.849205]
 [33.797363]], R is [[33.73313904]
 [33.77445984]
 [33.43671417]
 [33.10234833]
 [32.77132416]].
[2019-03-27 03:02:23,761] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:23,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8981
[2019-03-27 03:02:23,779] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 74.83333333333334, 1.0, 2.0, 0.5418757336280602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757208.0034848315, 757208.0034848315, 190417.7333256967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6256200.0000, 
sim time next is 6256800.0000, 
raw observation next is [29.7, 74.0, 1.0, 2.0, 0.5419591470242147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 190431.8515311663], 
processed observation next is [0.0, 0.43478260869565216, 0.6066350710900474, 0.74, 1.0, 1.0, 0.4481435506315839, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21036794597464986, 0.21036794597464986, 0.2842266440763676], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.5650671], dtype=float32), 0.56963843]. 
=============================================
[2019-03-27 03:02:24,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:24,872] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0546
[2019-03-27 03:02:24,877] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.7, 62.0, 1.0, 2.0, 0.5055397170101268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706415.7837007756, 706415.783700775, 184468.7126058145], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6274800.0000, 
sim time next is 6275400.0000, 
raw observation next is [30.68333333333333, 62.16666666666667, 1.0, 2.0, 0.5065657792210737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707850.029151847, 707850.0291518463, 184631.288402571], 
processed observation next is [0.0, 0.6521739130434783, 0.6532385466034754, 0.6216666666666667, 1.0, 1.0, 0.4055009388205707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19662500809773528, 0.1966250080977351, 0.2755690871680164], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.45867205], dtype=float32), 0.8787568]. 
=============================================
[2019-03-27 03:02:26,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:26,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0099
[2019-03-27 03:02:26,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2107539.614900127 W.
[2019-03-27 03:02:26,192] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.5024202285515516, 1.0, 1.0, 0.5024202285515516, 1.0, 2.0, 0.8596962642025487, 6.911199999999999, 6.9112, 170.5573041426782, 2107539.614900127, 2107539.614900128, 414473.9067511091], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6454200.0000, 
sim time next is 6454800.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.8647383706636886, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.980061746707744, 6.9112, 168.9125462567403, 2105681.129996495, 2056828.35549691, 425382.0643040329], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.67, 1.0, 1.0, 0.8370341815225164, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00688617467077437, 0.0, 0.8294379306188665, 0.5849114249990264, 0.5713412098602527, 0.6348986034388551], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.801833], dtype=float32), 1.5751181]. 
=============================================
[2019-03-27 03:02:31,767] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:31,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2729
[2019-03-27 03:02:31,787] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 80.5, 1.0, 2.0, 0.5284218663403171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738401.2619552452, 738401.2619552445, 188168.6547549789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6377400.0000, 
sim time next is 6378000.0000, 
raw observation next is [27.83333333333334, 81.0, 1.0, 2.0, 0.5292891957104711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739613.664593591, 739613.664593591, 188311.9368965194], 
processed observation next is [0.0, 0.8260869565217391, 0.5181674565560824, 0.81, 1.0, 1.0, 0.4328785490487603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2054482401648864, 0.2054482401648864, 0.28106259238286474], 
reward next is 0.7189, 
noisyNet noise sample is [array([1.1241115], dtype=float32), 2.3116376]. 
=============================================
[2019-03-27 03:02:31,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.46854 ]
 [68.563705]
 [68.688095]
 [68.83393 ]
 [68.81063 ]], R is [[68.40968323]
 [68.4447403 ]
 [68.47956848]
 [68.51386261]
 [68.54814148]].
[2019-03-27 03:02:42,047] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.568907e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:02:42,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6648
[2019-03-27 03:02:42,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2209714.648348828 W.
[2019-03-27 03:02:42,072] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.1, 55.0, 1.0, 2.0, 0.7901292356131753, 1.0, 1.0, 0.7901292356131753, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2209714.648348828, 2209714.648348827, 415085.1023748697], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6530400.0000, 
sim time next is 6531000.0000, 
raw observation next is [31.98333333333333, 55.5, 1.0, 2.0, 0.7969716620870961, 1.0, 2.0, 0.7969716620870961, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2228868.014959982, 2228868.014959982, 418390.0385200242], 
processed observation next is [1.0, 0.6086956521739131, 0.7148499210110584, 0.555, 1.0, 1.0, 0.7553875446832483, 1.0, 1.0, 0.7553875446832483, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6191300041555506, 0.6191300041555506, 0.6244627440597377], 
reward next is 0.3755, 
noisyNet noise sample is [array([0.3829799], dtype=float32), 0.13096611]. 
=============================================
[2019-03-27 03:02:42,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[33.969128]
 [34.27015 ]
 [33.55011 ]
 [33.31206 ]
 [32.084126]], R is [[34.29190063]
 [34.32945251]
 [34.0568428 ]
 [33.71627426]
 [33.37911224]].
[2019-03-27 03:02:50,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:50,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6254
[2019-03-27 03:02:50,865] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 87.33333333333334, 1.0, 2.0, 0.4244092032906673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621184.5767911512, 621184.5767911518, 176202.0838894907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6921600.0000, 
sim time next is 6922200.0000, 
raw observation next is [24.35, 87.66666666666667, 1.0, 2.0, 0.4255335402604308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622937.6104078983, 622937.6104078983, 176374.3481268774], 
processed observation next is [0.0, 0.08695652173913043, 0.35308056872037924, 0.8766666666666667, 1.0, 1.0, 0.307871735253531, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17303822511330508, 0.17303822511330508, 0.2632452957117573], 
reward next is 0.7368, 
noisyNet noise sample is [array([0.8958701], dtype=float32), -1.7075229]. 
=============================================
[2019-03-27 03:02:51,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:02:51,534] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3709
[2019-03-27 03:02:51,543] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 74.66666666666667, 1.0, 2.0, 0.3851772154447255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582164.1210056443, 582164.1210056443, 173128.0558478968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7334400.0000, 
sim time next is 7335000.0000, 
raw observation next is [25.25, 75.0, 1.0, 2.0, 0.3813718645247738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576381.1285167775, 576381.1285167775, 172610.949388337], 
processed observation next is [1.0, 0.9130434782608695, 0.39573459715639814, 0.75, 1.0, 1.0, 0.25466489701779976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1601058690324382, 0.1601058690324382, 0.2576282826691597], 
reward next is 0.7424, 
noisyNet noise sample is [array([1.3170668], dtype=float32), -1.164273]. 
=============================================
[2019-03-27 03:02:51,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.43775 ]
 [71.92986 ]
 [71.52042 ]
 [70.857544]
 [70.532104]], R is [[72.93170929]
 [72.94399261]
 [72.95700073]
 [72.96994781]
 [72.98195648]].
[2019-03-27 03:03:02,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:03:02,050] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6035
[2019-03-27 03:03:02,055] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.73333333333333, 35.83333333333334, 1.0, 2.0, 0.2642705186186445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429899.7820331121, 429899.7820331121, 162259.4320529815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6879000.0000, 
sim time next is 6879600.0000, 
raw observation next is [29.7, 37.0, 1.0, 2.0, 0.2719459181547744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 440749.8059422675, 440749.8059422681, 162980.1806510437], 
processed observation next is [0.0, 0.6521739130434783, 0.6066350710900474, 0.37, 1.0, 1.0, 0.12282640741539086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12243050165062987, 0.12243050165063003, 0.243254000971707], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.824801], dtype=float32), 0.5663689]. 
=============================================
[2019-03-27 03:03:04,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:03:04,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9189
[2019-03-27 03:03:04,732] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 81.0, 1.0, 2.0, 0.4173682019448138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613015.4431024941, 613015.4431024941, 175479.8766840029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6912000.0000, 
sim time next is 6912600.0000, 
raw observation next is [25.15, 81.5, 1.0, 2.0, 0.4182182325400535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613923.9468327624, 613923.9468327624, 175556.6733004142], 
processed observation next is [0.0, 0.0, 0.3909952606635071, 0.815, 1.0, 1.0, 0.2990581114940404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17053442967576735, 0.17053442967576735, 0.2620248855230063], 
reward next is 0.7380, 
noisyNet noise sample is [array([-3.0200078], dtype=float32), 0.3886161]. 
=============================================
[2019-03-27 03:03:09,214] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:03:09,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4698
[2019-03-27 03:03:09,225] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 88.33333333333334, 1.0, 2.0, 0.3261194198805615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512668.71813455, 512668.71813455, 167924.1603654595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7263600.0000, 
sim time next is 7264200.0000, 
raw observation next is [21.91666666666666, 88.16666666666667, 1.0, 2.0, 0.325042031924003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511445.2175434159, 511445.2175434153, 167840.525070873], 
processed observation next is [1.0, 0.043478260869565216, 0.23775671406003138, 0.8816666666666667, 1.0, 1.0, 0.18679762882409998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1420681159842822, 0.14206811598428204, 0.2505082463744373], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.51479477], dtype=float32), -0.27178603]. 
=============================================
[2019-03-27 03:03:11,577] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 03:03:11,580] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:03:11,582] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:03:11,583] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:03:11,584] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:03:11,583] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:03:11,585] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:03:11,588] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:03:11,586] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:03:11,588] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:03:11,589] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:03:11,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-27 03:03:11,636] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-27 03:03:11,657] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-27 03:03:11,675] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-27 03:03:11,676] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-27 03:03:17,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:03:17,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.63333333333333, 51.33333333333333, 1.0, 2.0, 0.2457332441916287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 405760.2747688879, 405760.2747688879, 160437.5137684526]
[2019-03-27 03:03:17,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:03:17,046] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5780672037938779
[2019-03-27 03:03:24,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:03:24,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 96.0, 1.0, 2.0, 0.4002298126347276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598518.648910109, 598518.648910109, 174437.8733917987]
[2019-03-27 03:03:24,694] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:03:24,696] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6205516574327926
[2019-03-27 03:04:24,893] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:04:24,894] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.16666666666666, 83.33333333333334, 1.0, 2.0, 0.5837752650104964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 815780.1851775057, 815780.1851775051, 197762.1974022397]
[2019-03-27 03:04:24,895] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:04:24,897] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.01807343700534214
[2019-03-27 03:04:34,437] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:04:34,438] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.35136149, 88.72698217, 1.0, 2.0, 0.3828918146817546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590014.6385164652, 590014.6385164658, 174111.9811320616]
[2019-03-27 03:04:34,439] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:04:34,441] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1493310956336389
[2019-03-27 03:04:48,167] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:04:48,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.2, 85.0, 1.0, 2.0, 0.5196470576909038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726135.4099857152, 726135.4099857152, 186731.8383390548]
[2019-03-27 03:04:48,171] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:04:48,175] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9498084240216872
[2019-03-27 03:04:55,632] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:04:55,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.71122966, 88.24806558666667, 1.0, 2.0, 0.3728367910458237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 574596.5263343882, 574596.5263343889, 172753.780276789]
[2019-03-27 03:04:55,635] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:04:55,638] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8873710085340053
[2019-03-27 03:04:57,381] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21243936], dtype=float32), -0.019094333]
[2019-03-27 03:04:57,383] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.73114392, 78.90630186, 1.0, 2.0, 0.4644003267299577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703233.2812927938, 703233.2812927938, 184871.7787779483]
[2019-03-27 03:04:57,384] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:04:57,387] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9740232229402279
[2019-03-27 03:05:06,702] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:05:06,987] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:05:07,154] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:05:07,209] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:05:07,238] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:05:08,255] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1675000, evaluation results [1675000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:05:08,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4155093e-34 1.0000000e+00 3.4961805e-37 4.9633206e-35 4.6791024e-37], sum to 1.0000
[2019-03-27 03:05:08,704] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0712
[2019-03-27 03:05:08,708] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 60.0, 1.0, 2.0, 0.3513775498302926, 1.0, 1.0, 0.3513775498302926, 1.0, 1.0, 0.6050837850679898, 6.911200000000001, 6.9112, 170.5573041426782, 1548465.228118524, 1548465.228118523, 333515.3811056748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7311600.0000, 
sim time next is 7312200.0000, 
raw observation next is [27.76666666666667, 60.33333333333333, 1.0, 2.0, 0.8125288194637312, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564583266, 1215471.077139757, 1215471.077139757, 257920.0518114436], 
processed observation next is [1.0, 0.6521739130434783, 0.515007898894155, 0.6033333333333333, 1.0, 1.0, 0.774131107787628, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399448964461, 0.33763085476104365, 0.33763085476104365, 0.3849553012111099], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09248992], dtype=float32), -0.21658012]. 
=============================================
[2019-03-27 03:05:09,834] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:09,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4270
[2019-03-27 03:05:09,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1898159.152539319 W.
[2019-03-27 03:05:09,857] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.6, 67.0, 1.0, 2.0, 0.4525498170674628, 1.0, 2.0, 0.4525498170674628, 1.0, 1.0, 0.7604616108675718, 6.911200000000001, 6.9112, 170.5573041426782, 1898159.152539319, 1898159.152539319, 379667.0547502236], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7124400.0000, 
sim time next is 7125000.0000, 
raw observation next is [28.48333333333333, 67.66666666666667, 1.0, 2.0, 0.5801843676852878, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9680169121785475, 6.9112, 6.9112, 168.912956510429, 1622139.883614711, 1622139.883614711, 346344.8748690925], 
processed observation next is [1.0, 0.4782608695652174, 0.5489731437598735, 0.6766666666666667, 1.0, 1.0, 0.4941980333557684, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.9609962343640821, 0.0, 0.0, 0.8294399451522929, 0.4505944121151975, 0.4505944121151975, 0.516932649058347], 
reward next is 0.4831, 
noisyNet noise sample is [array([0.35495174], dtype=float32), -0.6639842]. 
=============================================
[2019-03-27 03:05:09,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[36.555172]
 [36.769833]
 [37.035583]
 [37.06408 ]
 [37.08409 ]], R is [[37.25800705]
 [36.88542557]
 [36.51657104]
 [36.3521347 ]
 [35.98861313]].
[2019-03-27 03:05:19,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:19,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4088
[2019-03-27 03:05:19,798] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 91.0, 1.0, 2.0, 0.3532704015992723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544990.5873430512, 544990.5873430519, 170251.2897799421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7253400.0000, 
sim time next is 7254000.0000, 
raw observation next is [22.3, 91.0, 1.0, 2.0, 0.3529199585290324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544707.1351822099, 544707.1351822099, 170234.988984636], 
processed observation next is [1.0, 1.0, 0.25592417061611383, 0.91, 1.0, 1.0, 0.2203854922036535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15130753755061385, 0.15130753755061385, 0.254082073111397], 
reward next is 0.7459, 
noisyNet noise sample is [array([-0.91042864], dtype=float32), 1.0326748]. 
=============================================
[2019-03-27 03:05:19,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.70492 ]
 [83.65519 ]
 [83.58972 ]
 [83.54718 ]
 [83.520645]], R is [[83.79507446]
 [83.70301819]
 [83.61186981]
 [83.52161407]
 [83.43223572]].
[2019-03-27 03:05:22,455] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:22,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0845
[2019-03-27 03:05:22,472] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 74.5, 1.0, 2.0, 0.4143424388512938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636987.9260370054, 636987.9260370047, 178372.5192024837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7353000.0000, 
sim time next is 7353600.0000, 
raw observation next is [24.66666666666666, 75.33333333333333, 1.0, 2.0, 0.4143900331792998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636312.9102827344, 636312.9102827339, 178298.6179107785], 
processed observation next is [1.0, 0.08695652173913043, 0.36808846761453373, 0.7533333333333333, 1.0, 1.0, 0.2944458231075901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17675358618964845, 0.1767535861896483, 0.26611734016534105], 
reward next is 0.7339, 
noisyNet noise sample is [array([-0.35876238], dtype=float32), 0.29808053]. 
=============================================
[2019-03-27 03:05:26,448] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:26,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:26,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-27 03:05:30,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:30,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3592
[2019-03-27 03:05:30,306] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 84.0, 1.0, 2.0, 0.3879838743994324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579066.6838532434, 579066.6838532427, 172627.6638041803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7474800.0000, 
sim time next is 7475400.0000, 
raw observation next is [24.45, 83.5, 1.0, 2.0, 0.3895191598809441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580439.909540628, 580439.9095406285, 172722.3176108013], 
processed observation next is [0.0, 0.5217391304347826, 0.3578199052132702, 0.835, 1.0, 1.0, 0.26448091551920977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16123330820572998, 0.16123330820573015, 0.2577945038967184], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.0406682], dtype=float32), -0.60446715]. 
=============================================
[2019-03-27 03:05:43,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:43,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7774
[2019-03-27 03:05:43,213] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 91.66666666666667, 1.0, 2.0, 0.47725436230152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666878.844759051, 666878.8447590516, 180108.6580262962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7692600.0000, 
sim time next is 7693200.0000, 
raw observation next is [24.9, 92.0, 1.0, 2.0, 0.4764218507735118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665715.192146436, 665715.192146436, 179983.9668782419], 
processed observation next is [1.0, 0.043478260869565216, 0.3791469194312796, 0.92, 1.0, 1.0, 0.36918295273917084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18492088670734333, 0.18492088670734333, 0.26863278638543564], 
reward next is 0.7314, 
noisyNet noise sample is [array([1.351878], dtype=float32), -0.9457776]. 
=============================================
[2019-03-27 03:05:43,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3560470e-26 1.0000000e+00 2.0939422e-29 3.3170119e-26 1.8354803e-25], sum to 1.0000
[2019-03-27 03:05:43,518] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3277
[2019-03-27 03:05:43,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2027738.169441751 W.
[2019-03-27 03:05:43,531] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.73333333333333, 73.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.719739035374308, 6.9112, 168.9085130632426, 2027738.169441751, 1454147.839323367, 311346.739815235], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7638600.0000, 
sim time next is 7639200.0000, 
raw observation next is [28.0, 72.0, 1.0, 2.0, 0.4032956355950993, 1.0, 1.0, 0.4032956355950993, 1.0, 1.0, 0.6789855439229493, 6.9112, 6.9112, 170.5573041426782, 1691406.081112815, 1691406.081112815, 351621.950562336], 
processed observation next is [1.0, 0.43478260869565216, 0.5260663507109005, 0.72, 1.0, 1.0, 0.2810790790302401, 1.0, 0.5, 0.2810790790302401, 1.0, 0.5, 0.6085189560035966, 0.0, 0.0, 0.8375144448122397, 0.4698350225313375, 0.4698350225313375, 0.5248088814363224], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.92266136], dtype=float32), 0.34670666]. 
=============================================
[2019-03-27 03:05:44,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:44,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7882
[2019-03-27 03:05:44,972] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.41666666666666, 77.66666666666667, 1.0, 2.0, 0.518541432997288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724589.924356995, 724589.924356995, 186552.6544562156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7931400.0000, 
sim time next is 7932000.0000, 
raw observation next is [28.33333333333334, 78.33333333333334, 1.0, 2.0, 0.520807005037641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727756.832369921, 727756.832369921, 186920.7514987606], 
processed observation next is [1.0, 0.8260869565217391, 0.5418641390205374, 0.7833333333333334, 1.0, 1.0, 0.4226590422140253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20215467565831138, 0.20215467565831138, 0.2789861962668069], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.2777326], dtype=float32), 0.6101524]. 
=============================================
[2019-03-27 03:05:44,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[63.032074]
 [63.091583]
 [62.636818]
 [62.44619 ]
 [62.1709  ]], R is [[63.12974548]
 [63.22000885]
 [63.31007385]
 [63.40003204]
 [63.48988342]].
[2019-03-27 03:05:46,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:46,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:47,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-27 03:05:47,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:47,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:47,799] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-27 03:05:48,216] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:48,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:48,274] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-27 03:05:49,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:49,382] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:49,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-27 03:05:51,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:51,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5227
[2019-03-27 03:05:51,407] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 86.5, 1.0, 2.0, 0.591526285397432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 826615.8435479895, 826615.8435479889, 199171.7784399833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7799400.0000, 
sim time next is 7800000.0000, 
raw observation next is [26.66666666666666, 86.0, 1.0, 2.0, 0.6004791451257872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839131.7693257811, 839131.7693257811, 200828.6099888635], 
processed observation next is [1.0, 0.2608695652173913, 0.4628751974723536, 0.86, 1.0, 1.0, 0.5186495724407074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23309215814605033, 0.23309215814605033, 0.2997441940132291], 
reward next is 0.7003, 
noisyNet noise sample is [array([-0.7830332], dtype=float32), -0.2428483]. 
=============================================
[2019-03-27 03:05:51,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[58.718987]
 [58.36365 ]
 [57.937008]
 [58.72277 ]
 [59.577984]], R is [[58.81443405]
 [58.92901993]
 [59.04011917]
 [59.13415527]
 [59.23579788]].
[2019-03-27 03:05:51,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:51,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:51,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-27 03:05:53,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:53,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:53,172] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-27 03:05:56,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:56,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:56,654] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-27 03:05:56,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:56,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2227
[2019-03-27 03:05:56,934] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 67.5, 1.0, 2.0, 0.4726403767214698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777850.2612768414, 777850.2612768414, 191015.0072088683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [22.4, 66.0, 1.0, 2.0, 0.4795799976515278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 789525.9277171761, 789525.9277171755, 192210.1840587171], 
processed observation next is [1.0, 0.43478260869565216, 0.2606635071090047, 0.66, 1.0, 1.0, 0.37298794897774434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2193127576992156, 0.2193127576992154, 0.2868808717294285], 
reward next is 0.7131, 
noisyNet noise sample is [array([0.7329774], dtype=float32), -0.11473807]. 
=============================================
[2019-03-27 03:05:57,184] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:05:57,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7106
[2019-03-27 03:05:57,195] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.06666666666667, 70.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 10.90768320716488, 6.9112, 168.884218701786, 4290253.592598198, 1455493.227811432, 302919.6036515078], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7910400.0000, 
sim time next is 7911000.0000, 
raw observation next is [30.1, 70.5, 1.0, 2.0, 0.753545590720345, 1.0, 1.0, 0.6973628348744352, 1.0, 1.0, 1.03, 7.004431029070807, 6.9112, 170.5573041426782, 2926257.963555969, 2859472.785036759, 538801.7732718574], 
processed observation next is [1.0, 0.5652173913043478, 0.6255924170616115, 0.705, 1.0, 1.0, 0.7030669767715, 1.0, 0.5, 0.6353769094872713, 1.0, 0.5, 1.0365853658536586, 0.009323102907080738, 0.0, 0.8375144448122397, 0.8128494343211025, 0.7942979958435442, 0.804181751152026], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.75881875], dtype=float32), -0.21215996]. 
=============================================
[2019-03-27 03:05:57,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:57,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:57,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[37.549484]
 [42.29004 ]
 [41.874252]
 [41.263985]
 [40.28062 ]], R is [[39.50546646]
 [39.1104126 ]
 [39.22688293]
 [39.30892563]
 [38.91583633]].
[2019-03-27 03:05:57,266] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-27 03:05:58,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:58,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:58,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-27 03:05:59,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:59,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:59,162] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-27 03:05:59,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:59,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:59,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:05:59,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:05:59,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-27 03:06:00,002] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-27 03:06:00,236] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:06:00,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:00,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-27 03:06:00,332] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:06:00,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:00,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-27 03:06:00,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:06:00,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:00,852] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-27 03:06:00,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:06:00,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8730
[2019-03-27 03:06:00,990] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 85.33333333333334, 1.0, 2.0, 0.2900106673345639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463633.9896210026, 463633.989621002, 164497.3970264621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 286800.0000, 
sim time next is 287400.0000, 
raw observation next is [21.75, 84.66666666666666, 1.0, 2.0, 0.2924925719225698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466992.0263415549, 466992.0263415549, 164721.9358920194], 
processed observation next is [0.0, 0.30434782608695654, 0.2298578199052133, 0.8466666666666666, 1.0, 1.0, 0.14758141195490335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12972000731709857, 0.12972000731709857, 0.24585363565973042], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.30542782], dtype=float32), 0.48246017]. 
=============================================
[2019-03-27 03:06:01,460] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 03:06:01,462] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:06:01,463] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:01,466] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:06:01,466] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:01,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:06:01,467] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:06:01,467] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:06:01,468] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:01,470] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:01,471] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:01,481] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-27 03:06:01,496] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-27 03:06:01,520] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-27 03:06:01,533] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-27 03:06:01,536] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-27 03:06:17,385] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.20419069], dtype=float32), -0.011133529]
[2019-03-27 03:06:17,386] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.37555129166667, 94.64252674333333, 1.0, 2.0, 0.3655983145275999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 554435.1582531665, 554435.1582531658, 170758.5309956312]
[2019-03-27 03:06:17,387] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:06:17,389] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3438584850081571
[2019-03-27 03:06:31,234] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.20419069], dtype=float32), -0.011133529]
[2019-03-27 03:06:31,234] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.35, 79.5, 1.0, 2.0, 0.5019271728438487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701366.1296260753, 701366.1296260753, 183898.32651929]
[2019-03-27 03:06:31,235] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:06:31,237] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5658041251643798
[2019-03-27 03:06:56,947] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.20419069], dtype=float32), -0.011133529]
[2019-03-27 03:06:56,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.71634443666667, 63.58467403, 1.0, 2.0, 0.575991799873875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 804899.2814044409, 804899.2814044403, 196356.6122504244]
[2019-03-27 03:06:56,951] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:06:56,953] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26652151137853575
[2019-03-27 03:07:18,709] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.20419069], dtype=float32), -0.011133529]
[2019-03-27 03:07:18,710] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.48418623, 94.85093270333334, 1.0, 2.0, 0.8109735751851601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1133441.445284159, 1133441.445284159, 246625.8296554096]
[2019-03-27 03:07:18,711] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:07:18,716] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04880162325592585
[2019-03-27 03:07:55,164] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.20419069], dtype=float32), -0.011133529]
[2019-03-27 03:07:55,166] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 61.66666666666667, 1.0, 2.0, 0.4327378766354562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677948.7311713601, 677948.7311713601, 182382.1617618447]
[2019-03-27 03:07:55,169] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:07:55,173] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2884141614307182
[2019-03-27 03:07:56,429] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:07:56,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:07:56,587] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:07:56,616] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:07:56,654] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:07:57,672] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1700000, evaluation results [1700000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:07:59,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:07:59,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0921
[2019-03-27 03:07:59,795] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 83.33333333333334, 1.0, 2.0, 0.3641451087769509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558594.5174224947, 558594.5174224947, 171307.190417166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 72600.0000, 
sim time next is 73200.0000, 
raw observation next is [23.43333333333334, 83.66666666666667, 1.0, 2.0, 0.3627457052283415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 557055.4001379946, 557055.400137994, 171193.2174820227], 
processed observation next is [1.0, 0.8695652173913043, 0.30963665086887876, 0.8366666666666667, 1.0, 1.0, 0.23222374123896564, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15473761114944293, 0.15473761114944276, 0.25551226489854134], 
reward next is 0.7445, 
noisyNet noise sample is [array([0.9586233], dtype=float32), 0.6764562]. 
=============================================
[2019-03-27 03:08:00,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:00,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5445
[2019-03-27 03:08:00,827] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 80.0, 1.0, 2.0, 0.25827091080741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421985.7443882529, 421985.7443882535, 161711.2830845121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 374400.0000, 
sim time next is 375000.0000, 
raw observation next is [21.08333333333333, 79.66666666666667, 1.0, 2.0, 0.2615554802851839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427267.6970923222, 427267.6970923228, 162042.9994304557], 
processed observation next is [1.0, 0.34782608695652173, 0.1982622432859398, 0.7966666666666667, 1.0, 1.0, 0.1103078075725107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11868547141453395, 0.11868547141453412, 0.2418552230305309], 
reward next is 0.7581, 
noisyNet noise sample is [array([-0.20690471], dtype=float32), 0.6329263]. 
=============================================
[2019-03-27 03:08:00,841] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.69581 ]
 [73.433395]
 [73.368225]
 [73.27999 ]
 [73.131645]], R is [[73.72119904]
 [73.74262238]
 [73.76313782]
 [73.78340149]
 [73.8035965 ]].
[2019-03-27 03:08:07,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:07,261] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7081
[2019-03-27 03:08:07,267] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 82.0, 1.0, 2.0, 0.2625000250971872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 428190.0392884404, 428190.0392884411, 162121.5575474764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 372600.0000, 
sim time next is 373200.0000, 
raw observation next is [20.9, 81.33333333333333, 1.0, 2.0, 0.2632798901781312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429699.1839963064, 429699.1839963064, 162208.926143423], 
processed observation next is [1.0, 0.30434782608695654, 0.1895734597156398, 0.8133333333333332, 1.0, 1.0, 0.1123854098531701, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11936088444341844, 0.11936088444341844, 0.24210287484092988], 
reward next is 0.7579, 
noisyNet noise sample is [array([-0.08463439], dtype=float32), -0.27928355]. 
=============================================
[2019-03-27 03:08:09,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:09,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8548
[2019-03-27 03:08:09,892] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.05, 91.0, 1.0, 2.0, 0.3044481571744767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484982.0902380803, 484982.0902380803, 165982.5573559715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 214200.0000, 
sim time next is 214800.0000, 
raw observation next is [21.13333333333333, 90.66666666666667, 1.0, 2.0, 0.3043012910266282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484301.3616666102, 484301.3616666108, 165926.8352959028], 
processed observation next is [0.0, 0.4782608695652174, 0.20063191153238533, 0.9066666666666667, 1.0, 1.0, 0.16180878436943152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13452815601850282, 0.13452815601850301, 0.2476519929789594], 
reward next is 0.7523, 
noisyNet noise sample is [array([-0.33325008], dtype=float32), 1.4825583]. 
=============================================
[2019-03-27 03:08:18,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:18,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8919
[2019-03-27 03:08:18,822] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 97.0, 1.0, 2.0, 0.375666336588196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568340.9784093015, 568340.9784093022, 171919.3288672753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1037400.0000, 
sim time next is 1038000.0000, 
raw observation next is [22.26666666666667, 97.0, 1.0, 2.0, 0.3765383844681099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569045.7232164071, 569045.7232164064, 171962.2030524945], 
processed observation next is [1.0, 0.0, 0.2543443917851502, 0.97, 1.0, 1.0, 0.24884142707001192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15806825644900196, 0.15806825644900177, 0.25666000455596194], 
reward next is 0.7433, 
noisyNet noise sample is [array([0.5320618], dtype=float32), -1.3878617]. 
=============================================
[2019-03-27 03:08:18,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.97365]
 [68.20479]
 [71.9364 ]
 [71.91516]
 [71.87492]], R is [[66.92485046]
 [66.99900055]
 [67.07254791]
 [67.14552307]
 [67.21786499]].
[2019-03-27 03:08:21,232] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:21,244] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3608
[2019-03-27 03:08:21,249] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.86666666666666, 73.0, 1.0, 2.0, 0.5312156999476261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855929.6869163697, 855929.6869163697, 201017.8298843545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 395400.0000, 
sim time next is 396000.0000, 
raw observation next is [22.9, 73.0, 1.0, 2.0, 0.5363152684050118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863626.5943850549, 863626.5943850556, 201959.4301445944], 
processed observation next is [1.0, 0.6086956521739131, 0.2843601895734597, 0.73, 1.0, 1.0, 0.4413436968735081, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2398962762180708, 0.239896276218071, 0.3014319852904394], 
reward next is 0.6986, 
noisyNet noise sample is [array([-2.0882401], dtype=float32), -0.39434925]. 
=============================================
[2019-03-27 03:08:21,268] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[68.91909 ]
 [69.473175]
 [70.394394]
 [71.13489 ]
 [72.23116 ]], R is [[68.64381409]
 [68.65734863]
 [68.67420959]
 [68.70143127]
 [68.73498535]].
[2019-03-27 03:08:29,103] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:29,111] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6503
[2019-03-27 03:08:29,117] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 70.66666666666667, 1.0, 2.0, 0.3670622911218916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604694.740937864, 604694.740937864, 174821.7769256538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 636000.0000, 
sim time next is 636600.0000, 
raw observation next is [21.83333333333334, 69.83333333333333, 1.0, 2.0, 0.3649052994423705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 600844.6053875685, 600844.605387569, 174534.7587568623], 
processed observation next is [1.0, 0.34782608695652173, 0.23380726698262277, 0.6983333333333333, 1.0, 1.0, 0.23482566197875962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16690127927432458, 0.16690127927432474, 0.26049963993561537], 
reward next is 0.7395, 
noisyNet noise sample is [array([-0.35231137], dtype=float32), 1.0771605]. 
=============================================
[2019-03-27 03:08:32,321] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:32,329] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1059
[2019-03-27 03:08:32,332] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 64.0, 1.0, 2.0, 0.6167703106639623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1005722.353483141, 1005722.353483141, 219114.3159739852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 579600.0000, 
sim time next is 580200.0000, 
raw observation next is [23.36666666666667, 64.5, 1.0, 2.0, 0.3307820616167551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539556.480864077, 539556.4808640777, 169936.0921577853], 
processed observation next is [1.0, 0.7391304347826086, 0.30647709320695127, 0.645, 1.0, 1.0, 0.19371332724910253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1498768002400214, 0.1498768002400216, 0.25363595844445563], 
reward next is 0.7464, 
noisyNet noise sample is [array([-1.4070822], dtype=float32), 1.3605357]. 
=============================================
[2019-03-27 03:08:37,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:37,305] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1060
[2019-03-27 03:08:37,309] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.66666666666667, 93.0, 1.0, 2.0, 0.2334899960315701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388733.2498148087, 388733.2498148087, 158874.2273807432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 699600.0000, 
sim time next is 700200.0000, 
raw observation next is [17.65, 93.0, 1.0, 2.0, 0.2227443206335127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 370888.6993214728, 370888.6993214734, 157886.9921243822], 
processed observation next is [1.0, 0.08695652173913043, 0.035545023696682464, 0.93, 1.0, 1.0, 0.06354737425724422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10302463870040911, 0.10302463870040929, 0.23565222705131672], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.26258895], dtype=float32), 1.4441714]. 
=============================================
[2019-03-27 03:08:42,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:42,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1106
[2019-03-27 03:08:42,026] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 70.0, 1.0, 2.0, 0.3030077255568939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481149.0376832679, 481149.0376832685, 165681.0202890487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 835200.0000, 
sim time next is 835800.0000, 
raw observation next is [24.06666666666667, 70.5, 1.0, 2.0, 0.3044113315652733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482975.088490818, 482975.0884908187, 165805.0815643549], 
processed observation next is [0.0, 0.6956521739130435, 0.3396524486571882, 0.705, 1.0, 1.0, 0.16194136333165454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.134159746803005, 0.1341597468030052, 0.2474702709915745], 
reward next is 0.7525, 
noisyNet noise sample is [array([-2.0574682], dtype=float32), 1.2050565]. 
=============================================
[2019-03-27 03:08:42,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:42,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3200
[2019-03-27 03:08:42,980] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.83333333333333, 1.0, 2.0, 0.3670542747503737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569204.9785769202, 569204.9785769196, 172366.0823661349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 965400.0000, 
sim time next is 966000.0000, 
raw observation next is [21.9, 92.66666666666667, 1.0, 2.0, 0.3313461095703727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514104.4202601747, 514104.4202601747, 167854.2908459639], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.9266666666666667, 1.0, 1.0, 0.19439290309683457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1428067834056041, 0.1428067834056041, 0.2505287923074088], 
reward next is 0.7495, 
noisyNet noise sample is [array([1.4368088], dtype=float32), -0.7885913]. 
=============================================
[2019-03-27 03:08:42,993] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.49774 ]
 [75.98163 ]
 [75.866425]
 [76.023994]
 [76.19347 ]], R is [[75.50440216]
 [75.49209595]
 [75.48668671]
 [75.48132324]
 [75.4760437 ]].
[2019-03-27 03:08:52,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:08:52,504] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1781
[2019-03-27 03:08:52,508] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 89.5, 1.0, 2.0, 0.2928451233047513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 469547.5094624624, 469547.5094624631, 164921.6273135842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1125000.0000, 
sim time next is 1125600.0000, 
raw observation next is [20.86666666666667, 89.66666666666667, 1.0, 2.0, 0.292297394789205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468788.9812294227, 468788.9812294221, 164869.7039335522], 
processed observation next is [1.0, 0.0, 0.18799368088467638, 0.8966666666666667, 1.0, 1.0, 0.14734625878217472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13021916145261742, 0.13021916145261725, 0.24607418497545105], 
reward next is 0.7539, 
noisyNet noise sample is [array([-1.8278074], dtype=float32), -0.4416878]. 
=============================================
[2019-03-27 03:08:53,774] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 03:08:53,774] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:08:53,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:08:53,775] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:08:53,778] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:08:53,778] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:08:53,778] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:08:53,778] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:08:53,779] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:08:53,782] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:08:53,782] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:08:53,806] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-03-27 03:08:53,827] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-03-27 03:08:53,827] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-03-27 03:08:53,845] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-03-27 03:08:53,880] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-03-27 03:09:25,450] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.20413277], dtype=float32), 0.03478058]
[2019-03-27 03:09:25,451] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.36666666666667, 69.5, 1.0, 2.0, 0.5510609614399516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770047.9382764326, 770047.9382764326, 191977.8042408491]
[2019-03-27 03:09:25,452] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:09:25,454] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13096349712062072
[2019-03-27 03:09:49,619] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.20413277], dtype=float32), 0.03478058]
[2019-03-27 03:09:49,619] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.88237099000001, 55.86196542166667, 1.0, 2.0, 0.8675495103961564, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598574748337, 6.9112, 168.9123159743812, 2109615.774537668, 2042371.757996797, 425213.0070154527]
[2019-03-27 03:09:49,620] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:09:49,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.932110988272782
[2019-03-27 03:09:49,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2109615.774537668 W.
[2019-03-27 03:09:50,374] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.20413277], dtype=float32), 0.03478058]
[2019-03-27 03:09:50,376] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.05, 64.5, 1.0, 2.0, 0.8962597802007256, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002735698953186, 6.9112, 168.9123388084764, 2149801.245055448, 2084862.907277913, 433137.5860537725]
[2019-03-27 03:09:50,379] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:09:50,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8567154285494298
[2019-03-27 03:09:50,382] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2149801.245055448 W.
[2019-03-27 03:10:19,718] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.20413277], dtype=float32), 0.03478058]
[2019-03-27 03:10:19,720] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.13333333333334, 76.0, 1.0, 2.0, 0.5814682829228214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812555.1253709103, 812555.1253709103, 197343.8366341678]
[2019-03-27 03:10:19,721] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:10:19,723] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5633099011183966
[2019-03-27 03:10:22,241] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.20413277], dtype=float32), 0.03478058]
[2019-03-27 03:10:22,242] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.46666666666667, 83.33333333333334, 1.0, 2.0, 0.579938513718027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810416.5804964504, 810416.5804964511, 197066.4284221783]
[2019-03-27 03:10:22,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:10:22,246] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7958271374818294
[2019-03-27 03:10:44,638] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:10:44,682] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:10:44,732] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:10:44,734] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:10:44,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:10:45,821] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1725000, evaluation results [1725000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:10:47,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:10:47,929] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9899
[2019-03-27 03:10:47,934] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 57.83333333333334, 1.0, 2.0, 0.8785985113006702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1350195.598470133, 1350195.598470133, 280907.5849148553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1181400.0000, 
sim time next is 1182000.0000, 
raw observation next is [27.6, 57.66666666666667, 1.0, 2.0, 0.8817825880218564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1356244.2926832, 1356244.2926832, 281994.2797926581], 
processed observation next is [1.0, 0.6956521739130435, 0.5071090047393366, 0.5766666666666667, 1.0, 1.0, 0.8575693831588631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37673452574533334, 0.37673452574533334, 0.42088698476516134], 
reward next is 0.5791, 
noisyNet noise sample is [array([-0.22495376], dtype=float32), 0.086953804]. 
=============================================
[2019-03-27 03:10:47,950] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[62.892178]
 [62.40131 ]
 [62.321766]
 [62.100216]
 [61.782986]], R is [[63.02750015]
 [62.97795868]
 [62.90696335]
 [62.84299088]
 [62.77710724]].
[2019-03-27 03:10:52,503] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:10:52,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5252
[2019-03-27 03:10:52,520] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 72.33333333333334, 1.0, 2.0, 0.7266619368979589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1128648.394328016, 1128648.394328016, 240890.8065092744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1159800.0000, 
sim time next is 1160400.0000, 
raw observation next is [24.9, 71.66666666666667, 1.0, 2.0, 0.736174125031578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1140643.031615881, 1140643.031615881, 242980.9967971097], 
processed observation next is [1.0, 0.43478260869565216, 0.3791469194312796, 0.7166666666666667, 1.0, 1.0, 0.6821375000380457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31684528655996697, 0.31684528655996697, 0.3626582041747906], 
reward next is 0.6373, 
noisyNet noise sample is [array([1.734943], dtype=float32), -0.16144623]. 
=============================================
[2019-03-27 03:11:00,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:00,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7881
[2019-03-27 03:11:00,050] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 92.66666666666667, 1.0, 2.0, 0.354959699653924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551870.001663338, 551870.001663338, 170931.3663231286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1222800.0000, 
sim time next is 1223400.0000, 
raw observation next is [21.81666666666667, 92.83333333333333, 1.0, 2.0, 0.3491319941215574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542759.697251745, 542759.697251745, 170177.2541444965], 
processed observation next is [1.0, 0.13043478260869565, 0.2330173775671408, 0.9283333333333332, 1.0, 1.0, 0.21582167966452695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15076658256992917, 0.15076658256992917, 0.2539959017082037], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.64158255], dtype=float32), -0.31155923]. 
=============================================
[2019-03-27 03:11:03,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:03,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4414
[2019-03-27 03:11:03,442] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 95.5, 1.0, 2.0, 0.5617894000303773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797783.5587414213, 797783.5587414213, 195476.5331771226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1909800.0000, 
sim time next is 1910400.0000, 
raw observation next is [24.0, 95.33333333333333, 1.0, 2.0, 0.5244621872118427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747096.4184764257, 747096.4184764257, 189335.1929466597], 
processed observation next is [1.0, 0.08695652173913043, 0.3364928909952607, 0.9533333333333333, 1.0, 1.0, 0.4270628761588466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20752678291011825, 0.20752678291011825, 0.28258984021889505], 
reward next is 0.7174, 
noisyNet noise sample is [array([-0.5308641], dtype=float32), 0.29276597]. 
=============================================
[2019-03-27 03:11:07,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:07,332] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1773
[2019-03-27 03:11:07,339] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.8204654214065481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1218598.049620467, 1218598.049620467, 258919.2096522395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1332000.0000, 
sim time next is 1332600.0000, 
raw observation next is [22.98333333333333, 95.0, 1.0, 2.0, 0.8096201733490408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1202973.091506808, 1202973.091506808, 256087.5527103175], 
processed observation next is [1.0, 0.43478260869565216, 0.2883096366508688, 0.95, 1.0, 1.0, 0.7706267148783624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3341591920852245, 0.3341591920852245, 0.38222022792584703], 
reward next is 0.6178, 
noisyNet noise sample is [array([-1.485582], dtype=float32), -0.44569474]. 
=============================================
[2019-03-27 03:11:07,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:07,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5391
[2019-03-27 03:11:07,820] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 51.0, 1.0, 2.0, 0.3480171406727051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534257.5264887547, 534257.5264887554, 169288.3780549595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1512000.0000, 
sim time next is 1512600.0000, 
raw observation next is [29.16666666666667, 51.0, 1.0, 2.0, 0.3501266927854602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536672.4447150303, 536672.4447150296, 169459.796027057], 
processed observation next is [0.0, 0.5217391304347826, 0.581358609794629, 0.51, 1.0, 1.0, 0.2170201117897111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1490756790875084, 0.1490756790875082, 0.25292506869709996], 
reward next is 0.7471, 
noisyNet noise sample is [array([1.3727722], dtype=float32), 0.45675293]. 
=============================================
[2019-03-27 03:11:09,282] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:09,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8280
[2019-03-27 03:11:09,299] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.68333333333333, 85.0, 1.0, 2.0, 0.8268311027735775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253689.908224907, 1253689.908224907, 263925.0723206759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [23.7, 85.0, 1.0, 2.0, 0.8333633007763968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1262923.366575425, 1262923.366575424, 265653.1000151281], 
processed observation next is [1.0, 0.4782608695652174, 0.3222748815165877, 0.85, 1.0, 1.0, 0.7992328925016828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3508120462709514, 0.35081204627095114, 0.39649716420168374], 
reward next is 0.6035, 
noisyNet noise sample is [array([0.4950837], dtype=float32), -0.7633833]. 
=============================================
[2019-03-27 03:11:09,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:09,587] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9582
[2019-03-27 03:11:09,595] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 95.5, 1.0, 2.0, 0.4116340556942512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607754.4664720419, 607754.4664720419, 175073.8917299772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1621800.0000, 
sim time next is 1622400.0000, 
raw observation next is [23.1, 95.33333333333333, 1.0, 2.0, 0.4118243485850188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608498.4812234347, 608498.4812234347, 175156.9013538017], 
processed observation next is [1.0, 0.782608695652174, 0.2938388625592418, 0.9533333333333333, 1.0, 1.0, 0.29135463684942026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16902735589539852, 0.16902735589539852, 0.2614282109758234], 
reward next is 0.7386, 
noisyNet noise sample is [array([-0.43444473], dtype=float32), 0.57241786]. 
=============================================
[2019-03-27 03:11:13,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:13,896] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7312
[2019-03-27 03:11:13,902] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 93.0, 1.0, 2.0, 0.3300264527944478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 515928.093286766, 515928.0932867654, 168107.1509076998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1407600.0000, 
sim time next is 1408200.0000, 
raw observation next is [21.68333333333334, 92.5, 1.0, 2.0, 0.3308154636927673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516777.1966169412, 516777.1966169412, 168163.2932813919], 
processed observation next is [0.0, 0.30434782608695654, 0.22669826224328635, 0.925, 1.0, 1.0, 0.19375357071417748, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14354922128248365, 0.14354922128248365, 0.2509899899722267], 
reward next is 0.7490, 
noisyNet noise sample is [array([1.6383966], dtype=float32), 1.6495368]. 
=============================================
[2019-03-27 03:11:19,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:19,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9906
[2019-03-27 03:11:19,258] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 80.0, 1.0, 2.0, 0.3576428750531925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548685.4184535787, 548685.4184535787, 170471.7032823074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1539600.0000, 
sim time next is 1540200.0000, 
raw observation next is [23.8, 81.5, 1.0, 2.0, 0.3584798453612946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549739.9703249993, 549739.9703249993, 170553.3653356346], 
processed observation next is [0.0, 0.8260869565217391, 0.3270142180094788, 0.815, 1.0, 1.0, 0.22708415103770435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1527055473124998, 0.1527055473124998, 0.254557261694977], 
reward next is 0.7454, 
noisyNet noise sample is [array([-1.8739696], dtype=float32), 0.039611664]. 
=============================================
[2019-03-27 03:11:22,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:22,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0439
[2019-03-27 03:11:22,383] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.48333333333333, 95.66666666666666, 1.0, 2.0, 0.3424430376351973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532482.1838473212, 532482.1838473206, 169345.0301547416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [21.5, 96.0, 1.0, 2.0, 0.3442140665188314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534410.3794460463, 534410.3794460457, 169478.9182617124], 
processed observation next is [1.0, 1.0, 0.21800947867298584, 0.96, 1.0, 1.0, 0.20989646568533904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14844732762390175, 0.1484473276239016, 0.2529536093458394], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.3153655], dtype=float32), 1.4085414]. 
=============================================
[2019-03-27 03:11:24,712] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:24,720] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9871
[2019-03-27 03:11:24,725] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 90.0, 1.0, 2.0, 0.8642812292572144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238442.543499014, 1238442.543499014, 264706.07610416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1850400.0000, 
sim time next is 1851000.0000, 
raw observation next is [24.71666666666667, 89.66666666666667, 1.0, 2.0, 0.8951863576699861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1278924.57930032, 1278924.57930032, 272699.238558078], 
processed observation next is [1.0, 0.43478260869565216, 0.3704581358609796, 0.8966666666666667, 1.0, 1.0, 0.8737185032168507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35525682758342225, 0.35525682758342225, 0.40701378889265377], 
reward next is 0.5930, 
noisyNet noise sample is [array([0.3791793], dtype=float32), 0.25449023]. 
=============================================
[2019-03-27 03:11:24,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.17829 ]
 [69.76423 ]
 [69.737495]
 [69.742645]
 [69.94297 ]], R is [[69.96046448]
 [69.86578369]
 [69.7616272 ]
 [69.66197968]
 [69.56817627]].
[2019-03-27 03:11:38,015] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:38,022] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4502
[2019-03-27 03:11:38,025] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 95.83333333333333, 1.0, 2.0, 0.3448777964886703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535040.4741114986, 535040.474111498, 169519.1789990366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1811400.0000, 
sim time next is 1812000.0000, 
raw observation next is [21.56666666666667, 95.66666666666666, 1.0, 2.0, 0.3450588332541842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535111.3019836913, 535111.3019836913, 169519.2272163641], 
processed observation next is [1.0, 1.0, 0.22116903633491333, 0.9566666666666666, 1.0, 1.0, 0.21091425693275204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14864202832880313, 0.14864202832880313, 0.2530137719647225], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.8084747], dtype=float32), 1.0550896]. 
=============================================
[2019-03-27 03:11:38,046] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.98938 ]
 [73.997696]
 [73.87561 ]
 [73.854126]
 [73.82099 ]], R is [[74.01729584]
 [74.02410889]
 [74.03091431]
 [74.03784943]
 [74.04490662]].
[2019-03-27 03:11:40,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:40,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7377
[2019-03-27 03:11:40,602] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.18333333333334, 88.66666666666667, 1.0, 2.0, 0.5404045585068844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755151.4774460234, 755151.4774460227, 190168.8372535009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2148600.0000, 
sim time next is 2149200.0000, 
raw observation next is [27.1, 89.0, 1.0, 2.0, 0.5387283967521289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752808.4095776491, 752808.4095776498, 189886.5806052274], 
processed observation next is [0.0, 0.9130434782608695, 0.4834123222748816, 0.89, 1.0, 1.0, 0.44425108042425165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091134471049025, 0.2091134471049027, 0.28341280687347375], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.68637097], dtype=float32), 1.3477637]. 
=============================================
[2019-03-27 03:11:42,107] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 03:11:42,109] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:11:42,110] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:11:42,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:11:42,111] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:11:42,112] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:11:42,115] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:11:42,114] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:11:42,117] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:11:42,122] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:11:42,122] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:11:42,143] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-03-27 03:11:42,166] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-03-27 03:11:42,167] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-03-27 03:11:42,167] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-03-27 03:11:42,227] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-03-27 03:12:02,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.19237746], dtype=float32), 0.052059125]
[2019-03-27 03:12:02,552] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.83231723, 97.7750312, 1.0, 2.0, 0.2599353431853537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429156.4875946381, 429156.4875946381, 161853.9622819726]
[2019-03-27 03:12:02,553] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:12:02,556] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9886984322969335
[2019-03-27 03:12:13,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.19237746], dtype=float32), 0.052059125]
[2019-03-27 03:12:13,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.92766174666667, 94.5098285, 1.0, 2.0, 0.5012356950977797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700399.5771440862, 700399.5771440862, 183789.0582409855]
[2019-03-27 03:12:13,019] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:12:13,021] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42628528743651517
[2019-03-27 03:12:45,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.19237746], dtype=float32), 0.052059125]
[2019-03-27 03:12:45,624] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.22788702333334, 75.64885298, 1.0, 2.0, 0.5729468610911481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800642.6361676146, 800642.6361676146, 195814.4527071998]
[2019-03-27 03:12:45,625] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:12:45,629] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.046105159486108827
[2019-03-27 03:13:10,829] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.19237746], dtype=float32), 0.052059125]
[2019-03-27 03:13:10,831] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.50133082333333, 98.71309703166668, 1.0, 2.0, 0.6674252225036746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 932725.7735749533, 932725.7735749533, 213973.5641680525]
[2019-03-27 03:13:10,833] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:13:10,835] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8447812537393715
[2019-03-27 03:13:15,448] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.19237746], dtype=float32), 0.052059125]
[2019-03-27 03:13:15,449] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 68.0, 1.0, 2.0, 0.9903799114716331, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980281893369069, 6.9112, 168.9124894854889, 2281537.038898831, 2232528.101632, 461273.8331609231]
[2019-03-27 03:13:15,451] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:13:15,456] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3642368876016918
[2019-03-27 03:13:15,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2281537.038898831 W.
[2019-03-27 03:13:18,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.19237746], dtype=float32), 0.052059125]
[2019-03-27 03:13:18,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.16666666666666, 92.16666666666667, 1.0, 2.0, 0.5012260681235872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700386.1204977075, 700386.1204977075, 183787.3872879488]
[2019-03-27 03:13:18,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:13:18,129] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46548564390318403
[2019-03-27 03:13:37,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:13:37,282] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:13:37,453] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:13:37,484] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:13:37,492] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:13:38,510] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1750000, evaluation results [1750000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:13:52,414] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:13:52,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5618
[2019-03-27 03:13:52,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 73.5, 1.0, 2.0, 0.5695801170196924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795936.1459319879, 795936.1459319879, 195216.2238433336], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2133000.0000, 
sim time next is 2133600.0000, 
raw observation next is [30.7, 73.33333333333333, 1.0, 2.0, 0.570174314702858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796766.7943068421, 796766.7943068421, 195321.6356351433], 
processed observation next is [0.0, 0.6956521739130435, 0.6540284360189573, 0.7333333333333333, 1.0, 1.0, 0.4821377285576602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22132410952967838, 0.22132410952967838, 0.291524829306184], 
reward next is 0.7085, 
noisyNet noise sample is [array([-0.47899234], dtype=float32), -0.1342208]. 
=============================================
[2019-03-27 03:13:55,071] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:13:55,074] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2364
[2019-03-27 03:13:55,086] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 93.83333333333334, 1.0, 2.0, 0.5110489068622259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714116.6362485503, 714116.6362485496, 185345.2931599961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [25.6, 94.0, 1.0, 2.0, 0.5105656148420609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713441.0790447135, 713441.0790447135, 185268.0269526198], 
processed observation next is [1.0, 0.043478260869565216, 0.4123222748815167, 0.94, 1.0, 1.0, 0.410320017882001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19817807751242042, 0.19817807751242042, 0.27651944321286537], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.0137565], dtype=float32), -0.3021534]. 
=============================================
[2019-03-27 03:13:59,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:13:59,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3240
[2019-03-27 03:13:59,075] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.05, 75.33333333333333, 1.0, 2.0, 0.5553783215004101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776083.1847961639, 776083.1847961632, 192728.4417279382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2227800.0000, 
sim time next is 2228400.0000, 
raw observation next is [29.9, 76.0, 1.0, 2.0, 0.5557608008668448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776617.8551437154, 776617.8551437147, 192794.5471801717], 
processed observation next is [1.0, 0.8260869565217391, 0.6161137440758293, 0.76, 1.0, 1.0, 0.4647720492371623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21572718198436538, 0.2157271819843652, 0.2877530554927936], 
reward next is 0.7122, 
noisyNet noise sample is [array([-0.27095336], dtype=float32), 1.233279]. 
=============================================
[2019-03-27 03:13:59,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:13:59,751] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8945
[2019-03-27 03:13:59,757] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.4, 79.0, 1.0, 2.0, 0.5655846257790007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790350.7349213045, 790350.7349213045, 194509.6589926694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [29.3, 79.33333333333333, 1.0, 2.0, 0.5631109198253339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786892.6856301264, 786892.6856301264, 194074.9688327717], 
processed observation next is [1.0, 0.9130434782608695, 0.5876777251184835, 0.7933333333333333, 1.0, 1.0, 0.4736276142473902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.218581301563924, 0.218581301563924, 0.2896641325862264], 
reward next is 0.7103, 
noisyNet noise sample is [array([-0.9440773], dtype=float32), -0.94038796]. 
=============================================
[2019-03-27 03:14:02,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:02,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9331
[2019-03-27 03:14:02,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2367536.126454224 W.
[2019-03-27 03:14:02,132] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.81666666666667, 64.83333333333334, 1.0, 2.0, 0.564338670469114, 1.0, 2.0, 0.564338670469114, 1.0, 1.0, 0.9800696409546586, 6.911199999999999, 6.9112, 170.5573041426782, 2367536.126454224, 2367536.126454225, 462488.2420177903], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2296200.0000, 
sim time next is 2296800.0000, 
raw observation next is [31.8, 65.0, 1.0, 2.0, 0.5509376207780168, 1.0, 2.0, 0.5509376207780168, 1.0, 2.0, 0.9567964494360774, 6.911199999999999, 6.9112, 170.5573041426782, 2311263.517929797, 2311263.517929798, 452157.4736077331], 
processed observation next is [1.0, 0.6086956521739131, 0.7061611374407584, 0.65, 1.0, 1.0, 0.4589609888891768, 1.0, 1.0, 0.4589609888891768, 1.0, 1.0, 0.9473127432147286, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6420176438693881, 0.6420176438693883, 0.6748619009070643], 
reward next is 0.3251, 
noisyNet noise sample is [array([1.1810716], dtype=float32), 1.1147456]. 
=============================================
[2019-03-27 03:14:03,334] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:03,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4630
[2019-03-27 03:14:03,354] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2184794.999781028 W.
[2019-03-27 03:14:03,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.43333333333334, 74.0, 1.0, 2.0, 0.7812277774867377, 1.0, 2.0, 0.7812277774867377, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2184794.999781028, 2184794.999781028, 410833.1576055515], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2564400.0000, 
sim time next is 2565000.0000, 
raw observation next is [29.35, 74.5, 1.0, 2.0, 0.4992343079232169, 1.0, 2.0, 0.4992343079232169, 1.0, 1.0, 0.8626777959457189, 6.9112, 6.9112, 170.5573041426782, 2094162.332816554, 2094162.332816554, 413837.0235254061], 
processed observation next is [1.0, 0.6956521739130435, 0.590047393364929, 0.745, 1.0, 1.0, 0.3966678408713456, 1.0, 1.0, 0.3966678408713456, 1.0, 0.5, 0.832533897494779, 0.0, 0.0, 0.8375144448122397, 0.5817117591157095, 0.5817117591157095, 0.6176671992916508], 
reward next is 0.3823, 
noisyNet noise sample is [array([0.9813908], dtype=float32), 2.2358758]. 
=============================================
[2019-03-27 03:14:03,378] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[32.103397]
 [32.032215]
 [31.333342]
 [30.75737 ]
 [30.605442]], R is [[32.05298233]
 [32.11927032]
 [32.21094131]
 [32.22740936]
 [32.25257492]].
[2019-03-27 03:14:04,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:04,795] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3358
[2019-03-27 03:14:04,802] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4730146462875185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661537.8596975404, 661537.8596975404, 179551.5366612646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2620800.0000, 
sim time next is 2621400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4738631162323849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662292.3425520502, 662292.3425520497, 179621.9109307519], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36610014003901803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18397009515334728, 0.18397009515334714, 0.2680924043742566], 
reward next is 0.7319, 
noisyNet noise sample is [array([-0.28998157], dtype=float32), -0.51610404]. 
=============================================
[2019-03-27 03:14:06,231] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:06,241] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1946
[2019-03-27 03:14:06,246] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 95.5, 1.0, 2.0, 0.9791897872384039, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564263171, 1368697.103232593, 1368697.103232592, 292650.1002232545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2514600.0000, 
sim time next is 2515200.0000, 
raw observation next is [26.33333333333333, 95.66666666666667, 1.0, 2.0, 0.8756640798677482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.91295651041, 1223906.980113238, 1223906.980113237, 263331.8881184207], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271704, 0.9566666666666667, 1.0, 1.0, 0.8501976865876484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451521995, 0.3399741611425661, 0.3399741611425658, 0.39303266883346377], 
reward next is 0.6070, 
noisyNet noise sample is [array([0.65544754], dtype=float32), -0.53153044]. 
=============================================
[2019-03-27 03:14:11,268] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:11,276] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9317
[2019-03-27 03:14:11,280] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 92.83333333333333, 1.0, 2.0, 0.473072286715056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664660.3296078828, 664660.3296078828, 179952.5281528739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2596200.0000, 
sim time next is 2596800.0000, 
raw observation next is [24.56666666666667, 92.66666666666667, 1.0, 2.0, 0.4683071221736013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660557.0414491901, 660557.0414491895, 179576.8516253193], 
processed observation next is [0.0, 0.043478260869565216, 0.3633491311216432, 0.9266666666666667, 1.0, 1.0, 0.3594061712934956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18348806706921947, 0.1834880670692193, 0.2680251516795811], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.13171096], dtype=float32), 0.2434446]. 
=============================================
[2019-03-27 03:14:12,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:12,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9028
[2019-03-27 03:14:12,785] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3519854848168404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541574.4567812396, 541574.4567812396, 169927.0531026155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.349903153279406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538978.7727720743, 538978.772772075, 169731.5328070277], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.21675078708362172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971632577002064, 0.14971632577002084, 0.2533306459806383], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.16459274], dtype=float32), 0.076115295]. 
=============================================
[2019-03-27 03:14:17,190] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:17,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3136
[2019-03-27 03:14:17,206] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.3513653200284982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542214.1043638098, 542214.1043638104, 170026.38428374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3488649461903132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537651.3392306102, 537651.3392306102, 169630.7982640818], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21549993516905205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14934759423072505, 0.14934759423072505, 0.25318029591654], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.30506775], dtype=float32), -0.07965022]. 
=============================================
[2019-03-27 03:14:17,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:17,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0547
[2019-03-27 03:14:17,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1824661.38191659 W.
[2019-03-27 03:14:17,846] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.45, 73.5, 1.0, 2.0, 0.4350417461634966, 1.0, 1.0, 0.4350417461634966, 1.0, 2.0, 0.7497161116693014, 6.9112, 6.9112, 170.5573041426782, 1824661.38191659, 1824661.38191659, 372237.9759108739], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2550600.0000, 
sim time next is 2551200.0000, 
raw observation next is [29.56666666666667, 72.66666666666666, 1.0, 2.0, 0.6402387558715927, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.987182267770552, 6.9112, 168.9124928020681, 1790187.817534163, 1736283.529651075, 372865.5738025437], 
processed observation next is [1.0, 0.5217391304347826, 0.6003159557661929, 0.7266666666666666, 1.0, 1.0, 0.5665527179175815, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007598226777055217, 0.0, 0.8294376681319542, 0.4972743937594897, 0.4823009804586319, 0.5565157817948413], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7689296], dtype=float32), -2.1412992]. 
=============================================
[2019-03-27 03:14:18,577] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:18,587] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0307
[2019-03-27 03:14:18,594] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3841903961835303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578664.1240266134, 578664.1240266134, 172755.6668044197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2725200.0000, 
sim time next is 2725800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3840836943958654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578503.4916854348, 578503.4916854348, 172741.3072146917], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25793216192272944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069541435706522, 0.16069541435706522, 0.2578228465890921], 
reward next is 0.7422, 
noisyNet noise sample is [array([-1.0884484], dtype=float32), 0.12548473]. 
=============================================
[2019-03-27 03:14:29,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:29,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2586
[2019-03-27 03:14:29,904] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3473481168878181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535081.3018466077, 535081.3018466084, 169413.6413674864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.453426618058298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698485.0806789106, 698485.0806789106, 184441.2760997399], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.3414778530822867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19402363352191962, 0.19402363352191962, 0.2752854867160297], 
reward next is 0.7247, 
noisyNet noise sample is [array([-0.21588661], dtype=float32), -0.3054389]. 
=============================================
[2019-03-27 03:14:30,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:30,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9101
[2019-03-27 03:14:30,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2478186.049128398 W.
[2019-03-27 03:14:30,850] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 66.5, 1.0, 2.0, 0.8860314821848648, 1.0, 2.0, 0.8860314821848648, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2478186.049128398, 2478186.049128398, 463930.6830125432], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3429000.0000, 
sim time next is 3429600.0000, 
raw observation next is [31.66666666666666, 67.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.737021249037676, 6.9112, 168.9084199324309, 2870068.550439687, 2284218.268689049, 474214.6962290086], 
processed observation next is [1.0, 0.6956521739130435, 0.6998420221169034, 0.6766666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.08258212490376762, 0.0, 0.8294176684774678, 0.7972412640110241, 0.6345050746358469, 0.7077831287000128], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3250174], dtype=float32), -0.71633846]. 
=============================================
[2019-03-27 03:14:34,675] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 03:14:34,676] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:14:34,678] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:14:34,678] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:14:34,679] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:14:34,679] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:14:34,680] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:14:34,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:14:34,682] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:14:34,686] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:14:34,686] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:14:34,709] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-03-27 03:14:34,709] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-03-27 03:14:34,748] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-03-27 03:14:34,768] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-03-27 03:14:34,795] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-03-27 03:14:48,962] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17049313], dtype=float32), 0.062599465]
[2019-03-27 03:14:48,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.65, 81.50000000000001, 1.0, 2.0, 0.3257146511445683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513903.0956321385, 513903.0956321385, 168057.9809319357]
[2019-03-27 03:14:48,965] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:14:48,968] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5116127135019194
[2019-03-27 03:15:12,240] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17049313], dtype=float32), 0.062599465]
[2019-03-27 03:15:12,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.35, 94.0, 1.0, 2.0, 0.4699462124615699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665448.4150327033, 665448.415032704, 180151.6270086145]
[2019-03-27 03:15:12,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:15:12,247] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10724998550326592
[2019-03-27 03:15:26,516] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17049313], dtype=float32), 0.062599465]
[2019-03-27 03:15:26,518] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.13333333333334, 51.66666666666667, 1.0, 2.0, 0.8711542601299979, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005985972664734, 6.9112, 168.9123159692161, 2114661.222282593, 2047417.045992974, 426166.6889609165]
[2019-03-27 03:15:26,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:15:26,521] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3181461444583079
[2019-03-27 03:15:26,521] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2114661.222282593 W.
[2019-03-27 03:16:03,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17049313], dtype=float32), 0.062599465]
[2019-03-27 03:16:03,177] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.3, 68.0, 1.0, 2.0, 0.5664548299705926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791567.2156453243, 791567.2156453243, 194662.8791255205]
[2019-03-27 03:16:03,178] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:16:03,180] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.209017996396897
[2019-03-27 03:16:10,149] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17049313], dtype=float32), 0.062599465]
[2019-03-27 03:16:10,151] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.66666666666667, 68.66666666666667, 1.0, 2.0, 0.9143186731584073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1277966.636621513, 1277966.636621512, 273905.2748019209]
[2019-03-27 03:16:10,152] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:16:10,154] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9152822912196656
[2019-03-27 03:16:29,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:16:29,701] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:16:29,707] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:16:29,803] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:16:29,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:16:30,847] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1775000, evaluation results [1775000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:16:32,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0611758e-34 1.0000000e+00 2.2040452e-38 1.9313822e-33 4.3347121e-31], sum to 1.0000
[2019-03-27 03:16:32,901] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9116
[2019-03-27 03:16:32,909] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 84.0, 1.0, 2.0, 0.5416725455243023, 1.0, 1.0, 0.5416725455243023, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1514377.290227421, 1514377.290227422, 313699.8696432291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3166800.0000, 
sim time next is 3167400.0000, 
raw observation next is [26.83333333333333, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9745344429627, 6.9112, 168.9126697780123, 1498717.254087637, 1453785.696718759, 311348.2203927095], 
processed observation next is [1.0, 0.6521739130434783, 0.470774091627172, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.006333444296269963, 0.0, 0.8294385371648602, 0.41631034835767694, 0.40382936019965526, 0.4646988364070291], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2705899], dtype=float32), 0.16435681]. 
=============================================
[2019-03-27 03:16:40,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:16:40,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7399
[2019-03-27 03:16:40,867] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5604322365157103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 886033.3068963632, 886033.3068963637, 205436.9159682793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2997600.0000, 
sim time next is 2998200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5774007155431474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 912828.1563987812, 912828.1563987806, 208833.6884110711], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 1.0, 1.0, 0.4908442355941535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25356337677743923, 0.25356337677743906, 0.31169207225533], 
reward next is 0.6883, 
noisyNet noise sample is [array([0.78879255], dtype=float32), 0.36902544]. 
=============================================
[2019-03-27 03:16:46,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:16:46,138] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5424
[2019-03-27 03:16:46,144] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5180707140281979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723931.9355733166, 723931.9355733166, 186475.836349417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3274800.0000, 
sim time next is 3275400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5175730678558684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723236.3073694353, 723236.3073694347, 186395.2683746903], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41876273235646794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2008989742692876, 0.20089897426928743, 0.2782018930965527], 
reward next is 0.7218, 
noisyNet noise sample is [array([-0.13666666], dtype=float32), 1.5243838]. 
=============================================
[2019-03-27 03:16:53,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:16:53,798] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8312
[2019-03-27 03:16:53,804] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4557218816054515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645251.6411246454, 645251.6411246454, 178042.8757508932], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3215400.0000, 
sim time next is 3216000.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4564397554793406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646267.9345350269, 646267.9345350263, 178147.4588664022], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3451081391317357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17951887070417413, 0.17951887070417397, 0.26589172965134655], 
reward next is 0.7341, 
noisyNet noise sample is [array([2.3009186], dtype=float32), 0.5181336]. 
=============================================
[2019-03-27 03:16:53,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.619835]
 [78.60781 ]
 [78.43391 ]
 [78.40319 ]
 [78.50034 ]], R is [[78.59479523]
 [78.54310608]
 [78.4920578 ]
 [78.44145966]
 [78.39118958]].
[2019-03-27 03:16:56,382] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:16:56,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0653
[2019-03-27 03:16:56,400] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2361897.356860661 W.
[2019-03-27 03:16:56,408] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5629958519533483, 1.0, 1.0, 0.5629958519533483, 1.0, 2.0, 0.9777376092696427, 6.911199999999999, 6.9112, 170.5573041426782, 2361897.356860661, 2361897.356860662, 461442.5719549611], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3499200.0000, 
sim time next is 3499800.0000, 
raw observation next is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.5804798964894631, 1.0, 2.0, 0.5804798964894631, 1.0, 2.0, 1.008101612566991, 6.9112, 6.9112, 170.5573041426782, 2435318.427083667, 2435318.427083667, 475260.0419215487], 
processed observation next is [1.0, 0.5217391304347826, 0.7235387045813588, 0.6633333333333334, 1.0, 1.0, 0.49455409215597956, 1.0, 1.0, 0.49455409215597956, 1.0, 1.0, 1.009880015325599, 0.0, 0.0, 0.8375144448122397, 0.6764773408565742, 0.6764773408565742, 0.7093433461515652], 
reward next is 0.2907, 
noisyNet noise sample is [array([-1.0352833], dtype=float32), -1.215249]. 
=============================================
[2019-03-27 03:16:58,320] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:16:58,332] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7534
[2019-03-27 03:16:58,337] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 79.0, 1.0, 2.0, 0.5024311185079879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702070.5489921839, 702070.5489921839, 183977.3761344624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3278400.0000, 
sim time next is 3279000.0000, 
raw observation next is [27.16666666666666, 79.0, 1.0, 2.0, 0.4966002657223454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693920.1639008125, 693920.1639008125, 183064.6119109761], 
processed observation next is [0.0, 0.9565217391304348, 0.4865718799368086, 0.79, 1.0, 1.0, 0.3934942960510185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19275560108355902, 0.19275560108355902, 0.273230764046233], 
reward next is 0.7268, 
noisyNet noise sample is [array([0.43069965], dtype=float32), -0.1686201]. 
=============================================
[2019-03-27 03:16:58,350] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.72852 ]
 [82.56918 ]
 [82.418304]
 [82.29631 ]
 [82.19744 ]], R is [[82.77876282]
 [82.67638397]
 [82.57384491]
 [82.47135162]
 [82.36914062]].
[2019-03-27 03:17:02,879] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:02,890] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6315
[2019-03-27 03:17:02,898] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5512434416487011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770303.0270419847, 770303.0270419847, 192014.0610615548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3354000.0000, 
sim time next is 3354600.0000, 
raw observation next is [28.16666666666667, 83.16666666666666, 1.0, 2.0, 0.5502162279200217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768867.0875603695, 768867.0875603702, 191837.6460340769], 
processed observation next is [0.0, 0.8260869565217391, 0.5339652448657191, 0.8316666666666666, 1.0, 1.0, 0.458091840867496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21357419098899152, 0.21357419098899172, 0.28632484482698045], 
reward next is 0.7137, 
noisyNet noise sample is [array([0.126836], dtype=float32), -2.07802]. 
=============================================
[2019-03-27 03:17:12,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8447575e-32 1.0000000e+00 3.1371543e-34 1.5595376e-30 4.4700978e-29], sum to 1.0000
[2019-03-27 03:17:12,102] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6453
[2019-03-27 03:17:12,109] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1923309.877128724 W.
[2019-03-27 03:17:12,116] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.4585407525016811, 1.0, 1.0, 0.4585407525016811, 1.0, 1.0, 0.796333645351346, 6.9112, 6.9112, 170.5573041426782, 1923309.877128724, 1923309.877128724, 387660.8308080292], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4155600.0000, 
sim time next is 4156200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.355396677847009, 1.0, 2.0, 0.355396677847009, 1.0, 2.0, 0.6172064979429044, 6.9112, 6.9112, 170.5573041426782, 1490380.050495132, 1490380.050495132, 329572.6112795441], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.22336949138193857, 1.0, 1.0, 0.22336949138193857, 1.0, 1.0, 0.5331786560279321, 0.0, 0.0, 0.8375144448122397, 0.41399445847087, 0.41399445847087, 0.491899419820215], 
reward next is 0.5081, 
noisyNet noise sample is [array([-0.2837466], dtype=float32), 0.57947534]. 
=============================================
[2019-03-27 03:17:12,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:12,385] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3305
[2019-03-27 03:17:12,392] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5245500057643984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732988.9704993886, 732988.9704993893, 187531.6023650939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3703800.0000, 
sim time next is 3704400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5252597199990892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733981.0445219805, 733981.0445219798, 187647.9051374461], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42802375903504714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2038836234783279, 0.2038836234783277, 0.28007150020514343], 
reward next is 0.7199, 
noisyNet noise sample is [array([2.5794334], dtype=float32), -0.120050736]. 
=============================================
[2019-03-27 03:17:13,758] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:13,766] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9630
[2019-03-27 03:17:13,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2145642.203241511 W.
[2019-03-27 03:17:13,779] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.8932884295619833, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98744084124497, 6.9112, 168.9125034116202, 2145642.203241511, 2091554.471542549, 432883.3917579585], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3574800.0000, 
sim time next is 3575400.0000, 
raw observation next is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.5036056154268901, 1.0, 1.0, 0.5036056154268901, 1.0, 2.0, 0.8686823644922447, 6.911199999999999, 6.9112, 170.5573041426782, 2112516.94871128, 2112516.948711281, 416570.6124100229], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235385, 0.6933333333333335, 1.0, 1.0, 0.40193447641793983, 1.0, 0.5, 0.40193447641793983, 1.0, 1.0, 0.8398565420637129, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.586810263530911, 0.5868102635309115, 0.6217471827015267], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05994911], dtype=float32), 0.04017517]. 
=============================================
[2019-03-27 03:17:14,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:14,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6318
[2019-03-27 03:17:14,872] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 83.0, 1.0, 2.0, 0.6581222760653266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 919719.2901947432, 919719.2901947432, 212061.7720320993], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3559200.0000, 
sim time next is 3559800.0000, 
raw observation next is [26.08333333333334, 83.5, 1.0, 2.0, 0.6458875547944924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 902614.1242056534, 902614.1242056529, 209594.8043431386], 
processed observation next is [1.0, 0.17391304347826086, 0.43522906793049004, 0.835, 1.0, 1.0, 0.5733584997524005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25072614561268153, 0.25072614561268136, 0.31282806618378894], 
reward next is 0.6872, 
noisyNet noise sample is [array([-1.9850616], dtype=float32), -0.2184873]. 
=============================================
[2019-03-27 03:17:17,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:17,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3093
[2019-03-27 03:17:17,515] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4946573118244739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691204.310479273, 691204.3104792737, 182762.8828469088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3628800.0000, 
sim time next is 3629400.0000, 
raw observation next is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.4954583617351405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692324.0140958324, 692324.0140958324, 182887.4061905441], 
processed observation next is [1.0, 0.0, 0.5181674565560824, 0.7566666666666667, 1.0, 1.0, 0.3921185081146272, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19231222613773122, 0.19231222613773122, 0.2729662778963345], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.10213863], dtype=float32), 1.0758431]. 
=============================================
[2019-03-27 03:17:18,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:18,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3799
[2019-03-27 03:17:18,508] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6032727929619313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843037.2654826796, 843037.2654826796, 201357.582725948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3919800.0000, 
sim time next is 3920400.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.6059374622129535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846762.4640449545, 846762.4640449551, 201857.6914519609], 
processed observation next is [0.0, 0.391304347826087, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5252258580878958, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23521179556804292, 0.23521179556804309, 0.30128013649546403], 
reward next is 0.6987, 
noisyNet noise sample is [array([0.63526434], dtype=float32), -2.2692623]. 
=============================================
[2019-03-27 03:17:26,318] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:17:26,326] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4105
[2019-03-27 03:17:26,333] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.83333333333333, 63.66666666666666, 1.0, 2.0, 0.5849979929194846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817489.5091529789, 817489.5091529789, 197984.6422322174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3927000.0000, 
sim time next is 3927600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5854427739261425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 818111.295625029, 818111.2956250283, 198065.5999001538], 
processed observation next is [0.0, 0.4782608695652174, 0.7630331753554502, 0.63, 1.0, 1.0, 0.5005334625616175, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22725313767361918, 0.227253137673619, 0.2956202983584385], 
reward next is 0.7044, 
noisyNet noise sample is [array([-0.4472413], dtype=float32), 0.93375415]. 
=============================================
[2019-03-27 03:17:27,069] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 03:17:27,069] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:17:27,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:17:27,074] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:17:27,075] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:17:27,075] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:17:27,076] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:17:27,077] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:17:27,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:17:27,078] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:17:27,079] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:17:27,106] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-03-27 03:17:27,128] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-03-27 03:17:27,129] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-03-27 03:17:27,167] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-03-27 03:17:27,183] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-03-27 03:17:30,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:17:30,288] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.55, 50.5, 1.0, 2.0, 0.2097600835224009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 350883.0305808627, 350883.0305808627, 155050.81627285]
[2019-03-27 03:17:30,289] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:17:30,291] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7855226162751072
[2019-03-27 03:17:50,322] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:17:50,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.3, 82.0, 1.0, 2.0, 0.3740818607608128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563946.3405322988, 563946.3405322988, 171471.8240519863]
[2019-03-27 03:17:50,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:17:50,328] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7557189464649977
[2019-03-27 03:18:43,393] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:18:43,395] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.60000000000001, 61.66666666666667, 1.0, 2.0, 0.9198123013345693, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005989012414468, 6.9112, 168.9123159499865, 2182768.604814634, 2115522.272037767, 439606.7570699173]
[2019-03-27 03:18:43,395] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:18:43,399] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43306887626709556
[2019-03-27 03:18:43,400] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2182768.604814634 W.
[2019-03-27 03:18:54,467] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:18:54,468] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.58020939, 75.57018201833334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.028204897719648, 6.9112, 168.9123498545672, 1536818.776033374, 1453811.772716434, 311362.1456731877]
[2019-03-27 03:18:54,469] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:18:54,472] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9614834716694304
[2019-03-27 03:19:05,353] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:19:05,355] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.89130418166667, 54.58290045833333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.935328335100046, 6.9112, 168.9126579772939, 1496377.593186794, 1479260.152086669, 315430.1311996973]
[2019-03-27 03:19:05,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:19:05,359] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4570270434006535
[2019-03-27 03:19:12,170] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:19:12,172] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.9, 81.5, 1.0, 2.0, 0.829259752938468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1159012.714319361, 1159012.71431936, 251221.8583052636]
[2019-03-27 03:19:12,173] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:19:12,176] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04570654320387946
[2019-03-27 03:19:12,288] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.28939766], dtype=float32), 0.049792863]
[2019-03-27 03:19:12,292] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.78333333333333, 82.16666666666667, 1.0, 2.0, 0.741984179067227, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000303191805327, 6.9112, 168.9123539274369, 1933881.149325653, 1870668.503870453, 394232.8534298588]
[2019-03-27 03:19:12,293] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:19:12,297] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.676554924368389
[2019-03-27 03:19:12,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1933881.149325653 W.
[2019-03-27 03:19:21,274] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:19:22,014] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:19:22,028] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:19:22,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:19:22,152] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:19:23,170] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1800000, evaluation results [1800000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:19:35,504] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3450163e-20 1.0000000e+00 1.0831954e-20 4.1602578e-19 2.5393391e-15], sum to 1.0000
[2019-03-27 03:19:35,509] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6736
[2019-03-27 03:19:35,517] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2572331.419681767 W.
[2019-03-27 03:19:35,523] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 77.66666666666667, 1.0, 2.0, 0.6131045852094901, 1.0, 1.0, 0.6131045852094901, 1.0, 1.0, 1.03, 6.950276258993491, 6.9112, 170.5573041426782, 2572331.419681767, 2544339.505844141, 492779.105821643], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4242000.0000, 
sim time next is 4242600.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.4805928897440382, 1.0, 2.0, 0.4805928897440382, 1.0, 2.0, 0.8346309149880946, 6.9112, 6.9112, 170.5573041426782, 2015892.698519349, 2015892.698519349, 401985.2197529971], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.77, 1.0, 1.0, 0.3742083008964316, 1.0, 1.0, 0.3742083008964316, 1.0, 1.0, 0.7983303841318227, 0.0, 0.0, 0.8375144448122397, 0.5599701940331525, 0.5599701940331525, 0.5999779399298464], 
reward next is 0.4000, 
noisyNet noise sample is [array([1.7545396], dtype=float32), -0.75075746]. 
=============================================
[2019-03-27 03:19:41,085] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5275173e-25 1.0000000e+00 1.1775175e-26 3.5849722e-26 3.2617602e-21], sum to 1.0000
[2019-03-27 03:19:41,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-27 03:19:41,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2015892.698519349 W.
[2019-03-27 03:19:41,113] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 77.0, 1.0, 2.0, 0.4805928897440382, 1.0, 2.0, 0.4805928897440382, 1.0, 2.0, 0.8346309149880946, 6.9112, 6.9112, 170.5573041426782, 2015892.698519349, 2015892.698519349, 401985.2197529971], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4242600.0000, 
sim time next is 4243200.0000, 
raw observation next is [30.0, 76.33333333333334, 1.0, 2.0, 0.6147539037922838, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.953892443840362, 6.9112, 168.9126726331322, 1718871.265037627, 1688583.826918302, 367316.294322655], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.7633333333333334, 1.0, 1.0, 0.5358480768581733, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004269244384036241, 0.0, 0.8294385511848059, 0.4774642402882297, 0.4690510630328617, 0.5482332751084403], 
reward next is 0.2383, 
noisyNet noise sample is [array([-0.0896394], dtype=float32), 0.2835893]. 
=============================================
[2019-03-27 03:19:41,664] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:41,674] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0085
[2019-03-27 03:19:41,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2622534.031993973 W.
[2019-03-27 03:19:41,692] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 76.33333333333334, 1.0, 2.0, 0.6250576260757094, 1.0, 2.0, 0.6250576260757094, 1.0, 1.0, 1.03, 6.973613698509437, 6.9112, 170.5573041426782, 2622534.031993973, 2577824.56092473, 497244.1906196084], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4098000.0000, 
sim time next is 4098600.0000, 
raw observation next is [31.5, 75.0, 1.0, 2.0, 0.6247959994119759, 1.0, 2.0, 0.6247959994119759, 1.0, 2.0, 1.03, 6.973102881523799, 6.9112, 170.5573041426782, 2621435.181900489, 2577091.629816765, 497145.5380428238], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.75, 1.0, 1.0, 0.5479469872433445, 1.0, 1.0, 0.5479469872433445, 1.0, 1.0, 1.0365853658536586, 0.006190288152379875, 0.0, 0.8375144448122397, 0.7281764394168025, 0.7158587860602125, 0.7420082657355579], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7825012], dtype=float32), -2.6854188]. 
=============================================
[2019-03-27 03:19:42,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:42,837] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7360
[2019-03-27 03:19:42,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3431791.113143008 W.
[2019-03-27 03:19:42,848] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.5, 57.0, 1.0, 2.0, 0.9941643681778016, 1.0, 2.0, 0.8176722236031634, 1.0, 2.0, 1.03, 7.005120938215156, 6.9112, 170.5573041426782, 3431791.113143008, 3364511.72463999, 630363.0674787437], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [36.66666666666666, 56.0, 1.0, 2.0, 0.9801020835927159, 1.0, 2.0, 0.8106410813106207, 1.0, 2.0, 1.03, 7.005119828346142, 6.9112, 170.5573041426782, 3402241.025213868, 3334962.431755172, 624415.3336293228], 
processed observation next is [1.0, 0.4782608695652174, 0.9368088467614529, 0.56, 1.0, 1.0, 0.976026606738212, 1.0, 1.0, 0.7718567244706274, 1.0, 1.0, 1.0365853658536586, 0.009391982834614242, 0.0, 0.8375144448122397, 0.9450669514482967, 0.9263784532653255, 0.9319631845213774], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9400739], dtype=float32), 0.85301214]. 
=============================================
[2019-03-27 03:19:42,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[11.750218]
 [11.567273]
 [11.355102]
 [11.139253]
 [11.152467]], R is [[12.05004597]
 [11.9295454 ]
 [11.81025028]
 [11.69214821]
 [11.57522678]].
[2019-03-27 03:19:47,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:47,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2168
[2019-03-27 03:19:47,763] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.8512771823772495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1189802.557988619, 1189802.557988619, 256888.0568795293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4255200.0000, 
sim time next is 4255800.0000, 
raw observation next is [29.16666666666667, 79.00000000000001, 1.0, 2.0, 1.003181903310714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1402255.079634105, 1402255.079634106, 299898.6610264403], 
processed observation next is [1.0, 0.2608695652173913, 0.581358609794629, 0.7900000000000001, 1.0, 1.0, 1.0038336184466434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38951529989836253, 0.3895152998983628, 0.44760994183050795], 
reward next is 0.5524, 
noisyNet noise sample is [array([-0.92824006], dtype=float32), -2.011949]. 
=============================================
[2019-03-27 03:19:49,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:49,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5854
[2019-03-27 03:19:49,169] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 76.33333333333333, 1.0, 2.0, 0.6317607026905631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 882863.9433764647, 882863.9433764641, 206812.6283973069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4221600.0000, 
sim time next is 4222200.0000, 
raw observation next is [31.33333333333334, 77.66666666666667, 1.0, 2.0, 0.6299860219547007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880382.8599504848, 880382.8599504848, 206465.6320131386], 
processed observation next is [1.0, 0.8695652173913043, 0.6840442338072673, 0.7766666666666667, 1.0, 1.0, 0.5542000264514465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24455079443069022, 0.24455079443069022, 0.3081576597211024], 
reward next is 0.6918, 
noisyNet noise sample is [array([2.4807475], dtype=float32), 0.46753168]. 
=============================================
[2019-03-27 03:19:49,562] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:49,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7474
[2019-03-27 03:19:49,578] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 70.16666666666667, 1.0, 2.0, 0.5812358386290245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 812230.1789829684, 812230.1789829691, 197303.0774052033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4463400.0000, 
sim time next is 4464000.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.5955079195960712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832182.0733987939, 832182.0733987939, 199913.1216876702], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5126601440916521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23116168705522053, 0.23116168705522053, 0.2983777935636869], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.45471027], dtype=float32), 0.74823076]. 
=============================================
[2019-03-27 03:19:49,598] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.7947 ]
 [81.0414 ]
 [81.23194]
 [81.36773]
 [81.43418]], R is [[80.5687561 ]
 [80.46858978]
 [80.37316132]
 [80.28225708]
 [80.19548035]].
[2019-03-27 03:19:54,801] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:54,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2800
[2019-03-27 03:19:54,814] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6246589710612662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872935.4382042255, 872935.4382042255, 205430.1732259692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4318200.0000, 
sim time next is 4318800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.624745264154756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873056.0788470854, 873056.0788470854, 205446.8751922994], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5478858604274168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24251557745752372, 0.24251557745752372, 0.30663712715268565], 
reward next is 0.6934, 
noisyNet noise sample is [array([0.4058081], dtype=float32), 1.3497261]. 
=============================================
[2019-03-27 03:19:55,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:55,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4787
[2019-03-27 03:19:55,449] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.5780849467102173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807825.3874027558, 807825.3874027558, 196733.765324302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4583400.0000, 
sim time next is 4584000.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.5748120178281863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803250.0135053252, 803250.0135053252, 196147.0614414631], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4877253226845617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22312500375147923, 0.22312500375147923, 0.2927568081215867], 
reward next is 0.7072, 
noisyNet noise sample is [array([-0.49689204], dtype=float32), 0.41130063]. 
=============================================
[2019-03-27 03:19:55,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[60.435173]
 [60.819378]
 [60.980026]
 [61.20045 ]
 [61.59205 ]], R is [[60.38027954]
 [60.48284531]
 [60.58521652]
 [60.68771362]
 [60.79048157]].
[2019-03-27 03:19:56,275] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:56,282] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1737
[2019-03-27 03:19:56,287] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4483200.0000, 
sim time next is 4483800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5504258815900456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769160.1617394736, 769160.1617394736, 191873.5016085919], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4583444356506573, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2136556004831871, 0.2136556004831871, 0.2863783606098387], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.03536479], dtype=float32), 0.18540427]. 
=============================================
[2019-03-27 03:19:57,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:57,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9049
[2019-03-27 03:19:57,640] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3402241.022750569 W.
[2019-03-27 03:19:57,647] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.66666666666666, 56.0, 1.0, 2.0, 0.9801020824204684, 1.0, 2.0, 0.8106410807244967, 1.0, 2.0, 1.03, 7.00511982834605, 6.9112, 170.5573041426782, 3402241.022750569, 3334962.429291938, 624415.3331295729], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4362000.0000, 
sim time next is 4362600.0000, 
raw observation next is [36.83333333333334, 55.0, 1.0, 2.0, 0.9725898533267453, 1.0, 2.0, 0.8068849661776353, 1.0, 2.0, 1.03, 7.005119235463171, 6.9112, 170.5573041426782, 3386455.323144366, 3319177.154391859, 621268.2722146054], 
processed observation next is [1.0, 0.4782608695652174, 0.9447077409162722, 0.55, 1.0, 1.0, 0.9669757268996931, 1.0, 1.0, 0.7673312845513678, 1.0, 1.0, 1.0365853658536586, 0.009391923546317127, 0.0, 0.8375144448122397, 0.9406820342067683, 0.9219936539977387, 0.9272660779322468], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06355631], dtype=float32), -0.29859233]. 
=============================================
[2019-03-27 03:19:58,806] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:19:58,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6259
[2019-03-27 03:19:58,822] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5868582096367796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820090.0218771686, 820090.0218771686, 198323.3007647792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4435200.0000, 
sim time next is 4435800.0000, 
raw observation next is [30.16666666666666, 79.00000000000001, 1.0, 2.0, 0.5922119200917899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 827574.3424542204, 827574.3424542211, 199304.188262228], 
processed observation next is [0.0, 0.34782608695652173, 0.6287519747235385, 0.7900000000000001, 1.0, 1.0, 0.508689060351554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.229881761792839, 0.2298817617928392, 0.2974689377048179], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.4411981], dtype=float32), 1.4765818]. 
=============================================
[2019-03-27 03:20:04,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:20:04,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8099
[2019-03-27 03:20:04,292] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.606296123057291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 847263.8716847828, 847263.8716847834, 201925.1362408009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4467600.0000, 
sim time next is 4468200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6465546392236101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903546.7570592399, 903546.7570592399, 209737.4679518104], 
processed observation next is [0.0, 0.7391304347826086, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5741622159320604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2509852102942333, 0.2509852102942333, 0.3130409969430006], 
reward next is 0.6870, 
noisyNet noise sample is [array([-1.0825162], dtype=float32), -1.7256955]. 
=============================================
[2019-03-27 03:20:13,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:20:13,859] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8505
[2019-03-27 03:20:13,865] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2202702.448643328 W.
[2019-03-27 03:20:13,869] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666667, 68.33333333333334, 1.0, 2.0, 0.5250829692600358, 1.0, 2.0, 0.5250829692600358, 1.0, 1.0, 0.9118954700132572, 6.9112, 6.9112, 170.5573041426782, 2202702.448643328, 2202702.448643328, 432926.3291369941], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4623600.0000, 
sim time next is 4624200.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.7386634938548898, 1.0, 2.0, 0.7386634938548898, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2065644.221824787, 2065644.221824787, 391128.6413494204], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.6851367395842046, 1.0, 1.0, 0.6851367395842046, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5737900616179964, 0.5737900616179964, 0.5837740915662991], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1265686], dtype=float32), -1.2712699]. 
=============================================
[2019-03-27 03:20:19,612] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 03:20:19,614] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:20:19,615] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:20:19,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:20:19,617] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:20:19,617] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:20:19,616] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:20:19,618] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:20:19,619] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:20:19,622] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:20:19,620] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:20:19,654] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-03-27 03:20:19,655] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-03-27 03:20:19,690] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-03-27 03:20:19,708] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-03-27 03:20:19,708] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-03-27 03:20:24,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:20:24,389] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.35, 95.33333333333333, 1.0, 2.0, 0.2623846589048661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427461.9519288958, 427461.9519288958, 162091.0591451025]
[2019-03-27 03:20:24,391] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:20:24,392] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18887243353648508
[2019-03-27 03:20:29,296] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:20:29,298] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [14.807680185, 90.836825135, 1.0, 2.0, 0.1713296396145261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 286588.7178463676, 286588.717846367, 113110.4928603451]
[2019-03-27 03:20:29,300] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:20:29,303] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4812431650288739
[2019-03-27 03:20:34,569] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:20:34,570] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.66666666666667, 72.66666666666666, 1.0, 2.0, 0.3323018381283211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537473.9333906446, 537473.933390644, 169913.167709786]
[2019-03-27 03:20:34,571] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:20:34,573] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13815415279093035
[2019-03-27 03:21:00,720] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:21:00,721] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.9, 82.0, 1.0, 2.0, 0.5451618648324078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761801.6344172361, 761801.6344172361, 190973.4787150504]
[2019-03-27 03:21:00,722] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:21:00,724] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5889129837450491
[2019-03-27 03:21:26,472] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:21:26,473] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.5, 57.0, 1.0, 2.0, 0.8835246780031007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1234900.049258747, 1234900.049258747, 265452.7993782903]
[2019-03-27 03:21:26,475] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:21:26,478] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5360802254835665
[2019-03-27 03:21:29,788] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:21:29,790] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.58333333333334, 63.66666666666666, 1.0, 2.0, 0.5496088065401423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768017.9756581314, 768017.9756581307, 191733.5648645377]
[2019-03-27 03:21:29,791] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:21:29,793] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4398462485608752
[2019-03-27 03:21:47,688] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:21:47,689] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.8, 80.33333333333334, 1.0, 2.0, 0.5634359655044459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787347.0736665521, 787347.0736665528, 194131.092670125]
[2019-03-27 03:21:47,690] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:21:47,695] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5028182442738349
[2019-03-27 03:22:12,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:22:12,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.138950725, 83.72661435, 1.0, 2.0, 0.5049795264750383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705632.7416205003, 705632.7416205003, 184378.2427294215]
[2019-03-27 03:22:12,373] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:22:12,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13517310187539355
[2019-03-27 03:22:12,992] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.31838018], dtype=float32), 0.052733332]
[2019-03-27 03:22:12,994] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.11666666666667, 90.83333333333334, 1.0, 2.0, 0.8381606666026209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1171459.915470557, 1171459.915470558, 253491.8299410449]
[2019-03-27 03:22:12,996] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:22:12,998] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9619469125729337
[2019-03-27 03:22:14,085] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:22:14,107] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:22:14,389] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:22:14,390] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:22:14,429] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:22:15,446] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1825000, evaluation results [1825000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:22:16,566] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:16,577] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3598
[2019-03-27 03:22:16,585] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2397994.216180718 W.
[2019-03-27 03:22:16,589] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 65.0, 1.0, 2.0, 0.8573877983932411, 1.0, 2.0, 0.8573877983932411, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2397994.216180718, 2397994.216180718, 448775.1897496092], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4802400.0000, 
sim time next is 4803000.0000, 
raw observation next is [31.41666666666666, 65.16666666666667, 1.0, 2.0, 0.8111358106095169, 1.0, 2.0, 0.8111358106095169, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2268516.438202703, 2268516.438202703, 425328.1511293718], 
processed observation next is [1.0, 0.6086956521739131, 0.6879936808846759, 0.6516666666666667, 1.0, 1.0, 0.7724527838668878, 1.0, 1.0, 0.7724527838668878, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6301434550563063, 0.6301434550563063, 0.6348181360139877], 
reward next is 0.3652, 
noisyNet noise sample is [array([0.8662087], dtype=float32), -0.23719625]. 
=============================================
[2019-03-27 03:22:16,611] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[19.87201 ]
 [19.59254 ]
 [19.53212 ]
 [19.389236]
 [19.501822]], R is [[20.32243729]
 [20.44939995]
 [20.55796623]
 [20.65659714]
 [20.45003128]].
[2019-03-27 03:22:19,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:19,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7010
[2019-03-27 03:22:19,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.7540906326573604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053900.654719726, 1053900.654719726, 232953.0983669237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4953600.0000, 
sim time next is 4954200.0000, 
raw observation next is [29.16666666666667, 72.66666666666667, 1.0, 2.0, 1.018820496292723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1424129.483488284, 1424129.483488284, 304711.7909942135], 
processed observation next is [1.0, 0.34782608695652173, 0.581358609794629, 0.7266666666666667, 1.0, 1.0, 1.0226752967382204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.39559152319119, 0.39559152319119, 0.45479371790181117], 
reward next is 0.5452, 
noisyNet noise sample is [array([0.31704214], dtype=float32), 1.3981909]. 
=============================================
[2019-03-27 03:22:22,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:22,395] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3520
[2019-03-27 03:22:22,400] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.6177226191331104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863238.2300704888, 863238.2300704888, 204085.358952211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4850400.0000, 
sim time next is 4851000.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.6105353937534638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853190.387444691, 853190.387444691, 202717.0632754363], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.815, 1.0, 1.0, 0.5307655346427275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2369973298457475, 0.2369973298457475, 0.30256278100811385], 
reward next is 0.6974, 
noisyNet noise sample is [array([0.369212], dtype=float32), 1.0211047]. 
=============================================
[2019-03-27 03:22:22,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[64.215164]
 [63.474144]
 [61.475838]
 [61.558014]
 [61.298317]], R is [[64.61960602]
 [64.66880798]
 [64.70584106]
 [64.71020508]
 [64.71216583]].
[2019-03-27 03:22:24,991] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:25,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4552
[2019-03-27 03:22:25,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2306555.300407632 W.
[2019-03-27 03:22:25,015] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 1.008254174562725, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.993468469051654, 6.9112, 168.9124675812644, 2306555.300407632, 2248191.385938712, 466325.9934433121], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.7284930550645391, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990257795099996, 6.9112, 168.9124184823843, 1915001.413637844, 1858915.271482898, 391538.1133990792], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.6728831988729387, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007905779509999622, 0.0, 0.8294373031882831, 0.5319448371216233, 0.5163653531896939, 0.5843852438792226], 
reward next is 0.0203, 
noisyNet noise sample is [array([0.22081079], dtype=float32), -0.0664269]. 
=============================================
[2019-03-27 03:22:27,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:27,418] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5431
[2019-03-27 03:22:27,423] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4994365494971811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697884.722825183, 697884.722825183, 183507.6516266482], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.815, 1.0, 1.0, 0.39691150541829046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1938568674514397, 0.1938568674514397, 0.2738920173532063], 
reward next is 0.7261, 
noisyNet noise sample is [array([-1.9362507], dtype=float32), 0.69437546]. 
=============================================
[2019-03-27 03:22:28,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:28,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6840
[2019-03-27 03:22:28,374] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7242456710397478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1012170.107943948, 1012170.107943947, 226163.3074419636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4935600.0000, 
sim time next is 4936200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7784100719663944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1087906.384181408, 1087906.384181408, 238677.8849038491], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.7330241830920415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30219621782816886, 0.30219621782816886, 0.35623564911022254], 
reward next is 0.6438, 
noisyNet noise sample is [array([0.19332682], dtype=float32), 0.8343391]. 
=============================================
[2019-03-27 03:22:43,517] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:43,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4897
[2019-03-27 03:22:43,532] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.0, 1.0, 2.0, 0.5473436815239775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764851.5724996098, 764851.5724996098, 191346.2420496462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5263200.0000, 
sim time next is 5263800.0000, 
raw observation next is [28.5, 81.33333333333333, 1.0, 2.0, 0.5497328995329734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768191.4446982018, 768191.4446982018, 191754.9222320282], 
processed observation next is [1.0, 0.9565217391304348, 0.5497630331753555, 0.8133333333333332, 1.0, 1.0, 0.45750951750960656, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21338651241616718, 0.21338651241616718, 0.28620137646571375], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.7894119], dtype=float32), 0.03637659]. 
=============================================
[2019-03-27 03:22:49,875] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:49,886] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5647
[2019-03-27 03:22:49,893] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.1, 71.33333333333334, 1.0, 2.0, 0.5685703269645005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794524.5288780818, 794524.5288780824, 195037.5903009192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5512800.0000, 
sim time next is 5513400.0000, 
raw observation next is [30.85, 73.0, 1.0, 2.0, 0.5699094366103854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796396.5124781496, 796396.5124781496, 195274.9081794913], 
processed observation next is [1.0, 0.8260869565217391, 0.661137440758294, 0.73, 1.0, 1.0, 0.4818185983257655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22122125346615268, 0.22122125346615268, 0.2914550868350616], 
reward next is 0.7085, 
noisyNet noise sample is [array([1.2413015], dtype=float32), -0.5254929]. 
=============================================
[2019-03-27 03:22:50,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:22:50,155] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8494
[2019-03-27 03:22:50,161] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 95.0, 1.0, 2.0, 0.543770311091435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759856.4009861683, 759856.4009861677, 190737.9164334008], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5536800.0000, 
sim time next is 5537400.0000, 
raw observation next is [26.26666666666667, 95.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.693277045166149, 6.9112, 169.6884640040826, 2011510.098922495, 1454130.398308694, 311494.5481175783], 
processed observation next is [1.0, 0.08695652173913043, 0.44391785150079005, 0.95, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.07820770451661492, 0.0, 0.8332480419749988, 0.5587528052562486, 0.4039251106413039, 0.46491723599638557], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7334996], dtype=float32), -0.3600327]. 
=============================================
[2019-03-27 03:23:06,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:06,684] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6486
[2019-03-27 03:23:06,699] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2240611.122773903 W.
[2019-03-27 03:23:06,703] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.65, 62.66666666666666, 1.0, 2.0, 0.5341112399614565, 1.0, 2.0, 0.5341112399614565, 1.0, 1.0, 0.9275745905268798, 6.911200000000001, 6.9112, 170.5573041426782, 2240611.122773903, 2240611.122773903, 439536.507901318], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5836200.0000, 
sim time next is 5836800.0000, 
raw observation next is [32.7, 62.33333333333334, 1.0, 2.0, 0.9909175737276268, 1.0, 2.0, 0.9909175737276268, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2771872.660232372, 2771872.660232372, 523519.7196421903], 
processed observation next is [1.0, 0.5652173913043478, 0.7488151658767774, 0.6233333333333334, 1.0, 1.0, 0.9890573177441286, 1.0, 1.0, 0.9890573177441286, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7699646278423256, 0.7699646278423256, 0.7813727158838661], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07806739], dtype=float32), -0.79949]. 
=============================================
[2019-03-27 03:23:07,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:07,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8526
[2019-03-27 03:23:07,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2487887.406692731 W.
[2019-03-27 03:23:07,354] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.3, 63.0, 1.0, 2.0, 0.5929977206485129, 1.0, 2.0, 0.5929977206485129, 1.0, 1.0, 1.029840933423553, 6.9112, 6.9112, 170.5573041426782, 2487887.406692731, 2487887.406692731, 485410.4947354603], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5846400.0000, 
sim time next is 5847000.0000, 
raw observation next is [32.25, 63.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 9.445559661373075, 6.9112, 168.8982920340199, 4083274.831031333, 2285468.954566849, 468778.4303823747], 
processed observation next is [1.0, 0.6956521739130435, 0.7274881516587678, 0.6333333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.25343596613730746, 0.0, 0.8293679358596978, 1.1342430086198148, 0.6348524873796803, 0.6996692990781711], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.377976], dtype=float32), -0.36860794]. 
=============================================
[2019-03-27 03:23:07,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[17.598082]
 [17.699528]
 [17.763493]
 [18.143715]
 [17.894629]], R is [[16.61907768]
 [16.45288658]
 [16.28835869]
 [16.12547493]
 [15.96422005]].
[2019-03-27 03:23:08,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:08,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4149
[2019-03-27 03:23:08,876] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 95.16666666666667, 1.0, 2.0, 0.6821408317327252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953300.045834128, 953300.0458341275, 217036.8020659596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5893800.0000, 
sim time next is 5894400.0000, 
raw observation next is [26.0, 94.33333333333333, 1.0, 2.0, 0.6441872123015236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900236.924662269, 900236.924662269, 209258.855594898], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.9433333333333332, 1.0, 1.0, 0.5713098943391851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2500658124061858, 0.2500658124061858, 0.3123266501416388], 
reward next is 0.6877, 
noisyNet noise sample is [array([0.9850317], dtype=float32), 1.24542]. 
=============================================
[2019-03-27 03:23:11,723] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 03:23:11,726] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:23:11,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:23:11,732] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:23:11,736] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:23:11,736] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:23:11,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:23:11,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:23:11,740] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:23:11,738] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:23:11,742] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:23:11,775] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-03-27 03:23:11,796] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-03-27 03:23:11,819] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-03-27 03:23:11,820] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-03-27 03:23:11,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-03-27 03:23:21,561] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:23:21,563] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.14453262, 75.26257612, 1.0, 2.0, 0.5614207449341776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857035.8923787917, 857035.8923787911, 202593.8585744244]
[2019-03-27 03:23:21,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:23:21,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31781714531547256
[2019-03-27 03:23:25,094] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:23:25,097] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.99928879666667, 74.77080156166667, 1.0, 2.0, 0.2922796184641031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467769.6647339934, 467769.6647339934, 164790.0571325893]
[2019-03-27 03:23:25,098] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:23:25,100] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39563817518274724
[2019-03-27 03:23:30,243] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:23:30,243] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.32925133, 77.58645411, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.400489865745826, 6.9112, 168.9101205614257, 1801105.917529726, 1453992.674636759, 311352.049663619]
[2019-03-27 03:23:30,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:23:30,247] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6593278259955558
[2019-03-27 03:23:30,248] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1801105.917529726 W.
[2019-03-27 03:23:53,832] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:23:53,834] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.4120076, 99.23650842500001, 1.0, 2.0, 0.2648454878737259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433565.1041913931, 433565.1041913924, 162400.3925046647]
[2019-03-27 03:23:53,834] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:23:53,836] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7363717381223771
[2019-03-27 03:23:54,848] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:23:54,849] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.2, 56.0, 1.0, 2.0, 0.6294695605117367, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.982892950055492, 6.9112, 168.9119526346752, 1760050.830648236, 1709189.68667538, 370129.2093476196]
[2019-03-27 03:23:54,850] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:23:54,853] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33078997293081613
[2019-03-27 03:23:54,854] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1760050.830648236 W.
[2019-03-27 03:24:02,016] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:24:02,017] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.83333333333333, 80.66666666666667, 1.0, 2.0, 0.7387330742704793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1032426.831788324, 1032426.831788325, 229424.6209387329]
[2019-03-27 03:24:02,018] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:24:02,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.010901481658069367
[2019-03-27 03:24:02,033] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.28519467], dtype=float32), 0.024282923]
[2019-03-27 03:24:02,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.06666666666667, 62.16666666666666, 1.0, 2.0, 0.580023752129607, 0.0, 2.0, 0.0, 1.0, 1.0, 0.984140702698972, 6.911199999999999, 6.9112, 168.9124099266163, 1621690.478847758, 1621690.478847758, 349952.8885648031]
[2019-03-27 03:24:02,036] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:24:02,038] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32982802285867396
[2019-03-27 03:25:06,231] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:25:06,420] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:25:06,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:25:06,756] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:25:06,878] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:25:07,896] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1850000, evaluation results [1850000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:25:12,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:12,794] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9511
[2019-03-27 03:25:12,803] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 88.66666666666667, 1.0, 2.0, 0.5107669127942012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713722.4581090902, 713722.4581090902, 185299.9764501694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5712600.0000, 
sim time next is 5713200.0000, 
raw observation next is [26.2, 89.0, 1.0, 2.0, 0.5111576241807001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714268.6039890058, 714268.6039890052, 185362.4687883704], 
processed observation next is [0.0, 0.13043478260869565, 0.44075829383886256, 0.89, 1.0, 1.0, 0.4110332821454218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1984079455525016, 0.19840794555250144, 0.27666040117667223], 
reward next is 0.7233, 
noisyNet noise sample is [array([0.5450539], dtype=float32), -0.673945]. 
=============================================
[2019-03-27 03:25:27,788] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:27,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3795
[2019-03-27 03:25:27,808] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 86.0, 1.0, 2.0, 0.5396545159886338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754103.0096651644, 754103.0096651651, 190042.825805492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6247800.0000, 
sim time next is 6248400.0000, 
raw observation next is [27.76666666666667, 85.66666666666667, 1.0, 2.0, 0.5409758123877733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755950.0212551511, 755950.0212551504, 190265.5886259529], 
processed observation next is [0.0, 0.30434782608695654, 0.515007898894155, 0.8566666666666667, 1.0, 1.0, 0.4469588101057509, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20998611701531977, 0.20998611701531958, 0.28397849048649687], 
reward next is 0.7160, 
noisyNet noise sample is [array([1.7605596], dtype=float32), 0.36856434]. 
=============================================
[2019-03-27 03:25:32,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:32,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2198
[2019-03-27 03:25:32,582] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 86.0, 1.0, 2.0, 0.5333052148696199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745227.5040839148, 745227.5040839148, 188978.7343462476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6310800.0000, 
sim time next is 6311400.0000, 
raw observation next is [27.3, 86.0, 1.0, 2.0, 0.5333533618985864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745294.80715311, 745294.80715311, 188986.7504798246], 
processed observation next is [0.0, 0.043478260869565216, 0.4928909952606636, 0.86, 1.0, 1.0, 0.43777513481757396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20702633532030834, 0.20702633532030834, 0.2820697768355591], 
reward next is 0.7179, 
noisyNet noise sample is [array([0.3879913], dtype=float32), 1.676257]. 
=============================================
[2019-03-27 03:25:33,363] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:33,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4126
[2019-03-27 03:25:33,376] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 90.0, 1.0, 2.0, 0.5359879313480116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748977.5873112571, 748977.5873112571, 189426.8981917034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6138000.0000, 
sim time next is 6138600.0000, 
raw observation next is [26.86666666666667, 90.33333333333333, 1.0, 2.0, 0.5364100393162885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749567.6396923402, 749567.6396923402, 189497.5700859382], 
processed observation next is [1.0, 0.043478260869565216, 0.4723538704581361, 0.9033333333333333, 1.0, 1.0, 0.44145787869432346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20821323324787228, 0.20821323324787228, 0.2828321941581167], 
reward next is 0.7172, 
noisyNet noise sample is [array([-0.38384452], dtype=float32), 0.25486887]. 
=============================================
[2019-03-27 03:25:35,711] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:35,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8196
[2019-03-27 03:25:35,722] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 83.5, 1.0, 2.0, 0.5339543842505919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746134.9559035427, 746134.9559035427, 189086.947106703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6294600.0000, 
sim time next is 6295200.0000, 
raw observation next is [27.66666666666666, 83.66666666666667, 1.0, 2.0, 0.5337652825303202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 745870.616954214, 745870.6169542148, 189055.4122448209], 
processed observation next is [0.0, 0.8695652173913043, 0.5102685624012636, 0.8366666666666667, 1.0, 1.0, 0.4382714247353255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2071862824872817, 0.20718628248728188, 0.28217225708182225], 
reward next is 0.7178, 
noisyNet noise sample is [array([1.108801], dtype=float32), 0.8912079]. 
=============================================
[2019-03-27 03:25:36,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:36,148] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0009
[2019-03-27 03:25:36,154] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 77.0, 1.0, 2.0, 0.522073797086498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729527.6095259741, 729527.6095259741, 187126.5448271005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6373800.0000, 
sim time next is 6374400.0000, 
raw observation next is [28.23333333333333, 77.66666666666667, 1.0, 2.0, 0.5243435756053749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732700.4122814663, 732700.4122814657, 187497.7452966479], 
processed observation next is [0.0, 0.782608695652174, 0.537124802527646, 0.7766666666666667, 1.0, 1.0, 0.4269199706088854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2035278923004073, 0.20352789230040713, 0.27984738103977297], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.0254076], dtype=float32), -0.6251589]. 
=============================================
[2019-03-27 03:25:40,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:40,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8547
[2019-03-27 03:25:40,970] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 83.0, 1.0, 2.0, 0.5297764642891541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740294.797110556, 740294.7971105553, 188393.3284024545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6204000.0000, 
sim time next is 6204600.0000, 
raw observation next is [27.75, 83.5, 1.0, 2.0, 0.5296092229843834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740061.0173844877, 740061.0173844884, 188365.6451705839], 
processed observation next is [1.0, 0.8260869565217391, 0.514218009478673, 0.835, 1.0, 1.0, 0.4332641240775703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20557250482902437, 0.20557250482902456, 0.2811427539859461], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.7851056], dtype=float32), 0.3729152]. 
=============================================
[2019-03-27 03:25:42,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:42,799] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7126
[2019-03-27 03:25:42,802] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 78.66666666666666, 1.0, 2.0, 0.5071542185755535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708672.5595548726, 708672.5595548726, 184724.7360954249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6468000.0000, 
sim time next is 6468600.0000, 
raw observation next is [27.75, 79.33333333333334, 1.0, 2.0, 0.508538398243554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710607.3916309181, 710607.3916309188, 184944.8659699106], 
processed observation next is [1.0, 0.8695652173913043, 0.514218009478673, 0.7933333333333334, 1.0, 1.0, 0.4078775882452457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1973909421196995, 0.19739094211969968, 0.2760371133879263], 
reward next is 0.7240, 
noisyNet noise sample is [array([-1.312961], dtype=float32), 0.80228835]. 
=============================================
[2019-03-27 03:25:51,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:25:51,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1917
[2019-03-27 03:25:51,209] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 93.0, 1.0, 2.0, 0.4951438552691295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691884.3983671973, 691884.3983671967, 182838.6206398201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6658200.0000, 
sim time next is 6658800.0000, 
raw observation next is [25.23333333333333, 93.33333333333334, 1.0, 2.0, 0.4954802713285547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692354.6392340903, 692354.639234091, 182890.7910992971], 
processed observation next is [1.0, 0.043478260869565216, 0.39494470774091617, 0.9333333333333335, 1.0, 1.0, 0.3921449052151262, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19232073312058065, 0.19232073312058084, 0.27297132999895085], 
reward next is 0.7270, 
noisyNet noise sample is [array([0.9437717], dtype=float32), 0.215027]. 
=============================================
[2019-03-27 03:26:04,126] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 03:26:04,129] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:26:04,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:26:04,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:26:04,134] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:26:04,134] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:26:04,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:26:04,138] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:26:04,138] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:26:04,140] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:26:04,140] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:26:04,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-03-27 03:26:04,190] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-03-27 03:26:04,210] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-03-27 03:26:04,229] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-03-27 03:26:04,247] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-03-27 03:26:15,520] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:26:15,522] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.598529705, 47.302581695, 1.0, 2.0, 0.3343117661670576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550859.3674667679, 550859.3674667673, 170450.7553781955]
[2019-03-27 03:26:15,524] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:26:15,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7413728370642799
[2019-03-27 03:26:30,559] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:26:30,562] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 95.16666666666667, 1.0, 2.0, 0.3628187414379867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558332.0404360032, 558332.0404360032, 171333.4046020497]
[2019-03-27 03:26:30,563] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:26:30,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18955612446712367
[2019-03-27 03:26:31,774] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:26:31,775] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.05414786666666, 89.58020965666667, 1.0, 2.0, 0.5329461588054287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744725.5921450668, 744725.5921450668, 188919.695508392]
[2019-03-27 03:26:31,776] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:26:31,780] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8169039324522447
[2019-03-27 03:26:36,298] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:26:36,300] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.65, 81.5, 1.0, 2.0, 0.552308505228397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771791.8787847102, 771791.8787847102, 192197.6900240237]
[2019-03-27 03:26:36,301] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:26:36,303] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7376310092560173
[2019-03-27 03:26:44,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:26:44,599] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 65.0, 1.0, 2.0, 0.7214276821062627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008229.955772628, 1008229.955772628, 225537.08359621]
[2019-03-27 03:26:44,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:26:44,604] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4988641104006215
[2019-03-27 03:26:46,482] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:26:46,483] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.28333333333333, 69.5, 1.0, 2.0, 0.508111774384648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710011.0484929944, 710011.0484929938, 184876.7533099344]
[2019-03-27 03:26:46,483] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:26:46,487] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8996572023322202
[2019-03-27 03:27:05,819] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:27:05,820] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.7, 51.5, 1.0, 2.0, 0.5280695260473497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737908.7409215189, 737908.7409215183, 188110.9914148593]
[2019-03-27 03:27:05,821] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:27:05,824] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7911262365258693
[2019-03-27 03:27:07,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:27:07,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5807223512139673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811512.3476430596, 811512.3476430596, 197209.5223463447]
[2019-03-27 03:27:07,202] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:27:07,205] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9563388834477431
[2019-03-27 03:27:13,359] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:27:13,362] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.008639445, 79.03859555, 1.0, 2.0, 0.6024711742551417, 0.0, 2.0, 0.0, 1.0, 2.0, 1.006732079070283, 6.9112, 6.9112, 168.9129565104106, 1684501.108837922, 1684501.108837922, 360301.846216062]
[2019-03-27 03:27:13,365] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:27:13,367] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15522872687060685
[2019-03-27 03:27:13,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1684501.108837922 W.
[2019-03-27 03:27:25,314] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:27:25,316] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.63333333333333, 78.33333333333334, 1.0, 2.0, 0.6476863712999423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 905129.0032029548, 905129.0032029548, 209966.6924974145]
[2019-03-27 03:27:25,316] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:27:25,319] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21625329818077532
[2019-03-27 03:27:49,107] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.30772412], dtype=float32), 0.0007366026]
[2019-03-27 03:27:49,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.66666666666666, 77.16666666666667, 1.0, 2.0, 0.5720194025673438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799346.10711748, 799346.10711748, 195648.5161048248]
[2019-03-27 03:27:49,111] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:27:49,114] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44775443567279305
[2019-03-27 03:27:58,942] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:27:58,964] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:27:59,057] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:27:59,123] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:27:59,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:28:00,285] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1875000, evaluation results [1875000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:28:11,069] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:11,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4548
[2019-03-27 03:28:11,089] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.85, 59.5, 1.0, 2.0, 0.4562591219867195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646686.5410719265, 646686.5410719265, 178207.5732842541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6951000.0000, 
sim time next is 6951600.0000, 
raw observation next is [30.0, 59.0, 1.0, 2.0, 0.4585956939910301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648770.898782866, 648770.8987828653, 178392.1008403349], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.59, 1.0, 1.0, 0.3477056554108797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1802141385507961, 0.1802141385507959, 0.26625686692587297], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.7453622], dtype=float32), 0.6108452]. 
=============================================
[2019-03-27 03:28:11,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:11,850] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7054
[2019-03-27 03:28:11,856] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666667, 50.5, 1.0, 2.0, 0.9403810267632208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1463864.095074482, 1463864.095074482, 302138.8370573138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6783000.0000, 
sim time next is 6783600.0000, 
raw observation next is [28.63333333333334, 50.00000000000001, 1.0, 2.0, 0.8191776710525683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1276319.984959819, 1276319.984959818, 265824.3529516124], 
processed observation next is [1.0, 0.5217391304347826, 0.5560821484992104, 0.5000000000000001, 1.0, 1.0, 0.7821417723524919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35453332915550523, 0.354533329155505, 0.39675276559942146], 
reward next is 0.6032, 
noisyNet noise sample is [array([-1.0344657], dtype=float32), -0.7706853]. 
=============================================
[2019-03-27 03:28:18,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:18,432] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6911
[2019-03-27 03:28:18,436] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 88.66666666666667, 1.0, 2.0, 0.5825227407387057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814029.2099487165, 814029.2099487165, 197528.584119129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7181400.0000, 
sim time next is 7182000.0000, 
raw observation next is [25.8, 89.0, 1.0, 2.0, 0.5763088049091502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805342.4367698091, 805342.4367698084, 196409.3213774699], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.89, 1.0, 1.0, 0.4895286806134339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22370623243605808, 0.2237062324360579, 0.2931482408618954], 
reward next is 0.7069, 
noisyNet noise sample is [array([-1.0073857], dtype=float32), -0.29106423]. 
=============================================
[2019-03-27 03:28:18,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.975857]
 [62.405792]
 [61.12553 ]
 [60.227314]
 [60.414104]], R is [[63.26507187]
 [63.33760452]
 [63.40096283]
 [63.44098663]
 [63.45785522]].
[2019-03-27 03:28:27,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:27,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0483
[2019-03-27 03:28:27,830] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 81.0, 1.0, 2.0, 0.4831327692617746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675095.4923229417, 675095.4923229422, 180995.5421858316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7067400.0000, 
sim time next is 7068000.0000, 
raw observation next is [26.63333333333333, 82.0, 1.0, 2.0, 0.4842626667678272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676674.833879201, 676674.8338792004, 181167.0911568115], 
processed observation next is [1.0, 0.8260869565217391, 0.46129541864139006, 0.82, 1.0, 1.0, 0.3786297189973822, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18796523163311138, 0.1879652316331112, 0.2703986435176291], 
reward next is 0.7296, 
noisyNet noise sample is [array([0.2666857], dtype=float32), -0.9029635]. 
=============================================
[2019-03-27 03:28:27,848] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.882675]
 [69.9969  ]
 [69.850784]
 [69.64355 ]
 [68.7121  ]], R is [[69.79451752]
 [69.82643127]
 [69.85824585]
 [69.88983154]
 [69.92126465]].
[2019-03-27 03:28:31,112] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:31,120] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2645
[2019-03-27 03:28:31,128] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.23333333333333, 92.0, 1.0, 2.0, 0.4580419886240947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739880.830377802, 739880.830377802, 188125.9636183687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7374000.0000, 
sim time next is 7374600.0000, 
raw observation next is [20.3, 92.0, 1.0, 2.0, 0.5023951080979435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 810459.5593819983, 810459.5593819977, 195738.3423240198], 
processed observation next is [1.0, 0.34782608695652173, 0.16113744075829392, 0.92, 1.0, 1.0, 0.40047603385294395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2251276553838884, 0.22512765538388824, 0.2921467795880892], 
reward next is 0.7079, 
noisyNet noise sample is [array([0.42707804], dtype=float32), -0.36243254]. 
=============================================
[2019-03-27 03:28:40,053] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:40,060] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0065
[2019-03-27 03:28:40,066] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 86.66666666666666, 1.0, 2.0, 0.4034057216550294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595765.460110381, 595765.4601103804, 173964.466860212], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7505400.0000, 
sim time next is 7506000.0000, 
raw observation next is [24.2, 87.0, 1.0, 2.0, 0.4043482749404768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597252.8594319754, 597252.8594319754, 174104.4724803634], 
processed observation next is [0.0, 0.9130434782608695, 0.3459715639810427, 0.87, 1.0, 1.0, 0.28234731920539374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16590357206443762, 0.16590357206443762, 0.25985742161248265], 
reward next is 0.7401, 
noisyNet noise sample is [array([-0.4515008], dtype=float32), -0.5185843]. 
=============================================
[2019-03-27 03:28:40,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.45414 ]
 [82.47236 ]
 [82.47687 ]
 [82.505745]
 [82.54973 ]], R is [[82.49544525]
 [82.4108429 ]
 [82.32723999]
 [82.24445343]
 [82.16255951]].
[2019-03-27 03:28:41,637] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:41,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4158
[2019-03-27 03:28:41,652] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.36666666666667, 66.0, 1.0, 2.0, 0.4335926748821523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 620454.5489172856, 620454.5489172856, 175732.92935391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7578600.0000, 
sim time next is 7579200.0000, 
raw observation next is [28.23333333333333, 67.0, 1.0, 2.0, 0.4388319086265688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626804.18747406, 626804.18747406, 176327.478970654], 
processed observation next is [0.0, 0.7391304347826086, 0.537124802527646, 0.67, 1.0, 1.0, 0.3238938658151432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17411227429835, 0.17411227429835, 0.2631753417472448], 
reward next is 0.7368, 
noisyNet noise sample is [array([1.1387991], dtype=float32), 0.7201919]. 
=============================================
[2019-03-27 03:28:44,620] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:28:44,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:28:44,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-03-27 03:28:47,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:47,923] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7799
[2019-03-27 03:28:47,931] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 76.33333333333334, 1.0, 2.0, 0.4825350231453399, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674259.9791868431, 674259.9791868426, 180906.9658773352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7665600.0000, 
sim time next is 7666200.0000, 
raw observation next is [28.25, 77.0, 1.0, 2.0, 0.4816790765601698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673063.5616575942, 673063.5616575935, 180777.3391290419], 
processed observation next is [1.0, 0.7391304347826086, 0.537914691943128, 0.77, 1.0, 1.0, 0.375516959711048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18696210046044281, 0.18696210046044262, 0.2698169240731969], 
reward next is 0.7302, 
noisyNet noise sample is [array([-0.20169687], dtype=float32), 0.50397396]. 
=============================================
[2019-03-27 03:28:51,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:51,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4157
[2019-03-27 03:28:51,942] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 91.5, 1.0, 2.0, 0.3561163835505463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545035.5123831112, 545035.5123831106, 170126.5162257758], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7464600.0000, 
sim time next is 7465200.0000, 
raw observation next is [22.66666666666666, 91.0, 1.0, 2.0, 0.3575150991065352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 546297.0502730453, 546297.050273046, 170204.3564429357], 
processed observation next is [0.0, 0.391304347826087, 0.27330173775671385, 0.91, 1.0, 1.0, 0.225921806152452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15174918063140147, 0.15174918063140166, 0.25403635289990406], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.3510638], dtype=float32), 0.48236957]. 
=============================================
[2019-03-27 03:28:52,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:28:52,784] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7277
[2019-03-27 03:28:52,788] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 90.0, 1.0, 2.0, 0.3865041213515579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577846.516870782, 577846.516870782, 172549.472001667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7541400.0000, 
sim time next is 7542000.0000, 
raw observation next is [23.5, 90.0, 1.0, 2.0, 0.387280279622304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578346.6832125637, 578346.6832125637, 172573.3803197071], 
processed observation next is [0.0, 0.30434782608695654, 0.31279620853080575, 0.9, 1.0, 1.0, 0.2617834694244626, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16065185644793437, 0.16065185644793437, 0.25757220943239867], 
reward next is 0.7424, 
noisyNet noise sample is [array([1.0181398], dtype=float32), 0.46529242]. 
=============================================
[2019-03-27 03:28:52,800] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.388596]
 [75.377594]
 [75.37024 ]
 [75.38348 ]
 [75.42202 ]], R is [[75.52540588]
 [75.51261139]
 [75.50005341]
 [75.48783112]
 [75.47593689]].
[2019-03-27 03:28:56,198] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 03:28:56,202] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:28:56,204] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:28:56,205] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:28:56,205] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:28:56,206] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:28:56,207] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:28:56,208] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:28:56,208] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:28:56,208] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:28:56,211] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:28:56,243] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-03-27 03:28:56,244] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-03-27 03:28:56,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-03-27 03:28:56,304] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-03-27 03:28:56,322] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-03-27 03:29:17,874] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:29:17,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.78333333333333, 85.83333333333334, 1.0, 2.0, 0.3755588372526703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 566344.9099835204, 566344.9099835197, 171687.0290958794]
[2019-03-27 03:29:17,877] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:29:17,880] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2932051961525991
[2019-03-27 03:29:50,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:29:50,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333334, 66.83333333333334, 1.0, 2.0, 0.5622856942543256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785739.0867143802, 785739.0867143802, 193931.2111422344]
[2019-03-27 03:29:50,383] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:29:50,387] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9453744479686829
[2019-03-27 03:30:02,684] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:30:02,687] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 70.0, 1.0, 2.0, 0.5436324995674433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759663.7563861156, 759663.7563861156, 190716.6130714298]
[2019-03-27 03:30:02,689] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:30:02,693] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13698666180024877
[2019-03-27 03:30:05,506] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:30:05,507] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 80.66666666666667, 1.0, 2.0, 0.9875507753123299, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005897833198995, 6.9112, 168.9123932564848, 2277577.199145915, 2210395.521050163, 459484.872139245]
[2019-03-27 03:30:05,509] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:30:05,513] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23479734041967104
[2019-03-27 03:30:05,516] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2277577.199145915 W.
[2019-03-27 03:30:14,830] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:30:14,831] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.1, 49.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.38630818963777, 6.9112, 168.9102611344287, 2621041.338646539, 2283988.615455489, 475067.8357866183]
[2019-03-27 03:30:14,832] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:30:14,834] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9015357730349347
[2019-03-27 03:30:14,835] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2621041.338646539 W.
[2019-03-27 03:30:23,913] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:30:23,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.05, 65.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.365646186352042, 6.9112, 168.9103440840539, 1776370.520442694, 1453975.741284174, 311350.7129268255]
[2019-03-27 03:30:23,916] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:30:23,919] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3883033970378914
[2019-03-27 03:30:23,921] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1776370.520442694 W.
[2019-03-27 03:30:26,366] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.277345], dtype=float32), -0.002788284]
[2019-03-27 03:30:26,367] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.34563514666667, 80.27092805000001, 1.0, 2.0, 0.5654318222598297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790137.1271125901, 790137.1271125901, 194483.1817921043]
[2019-03-27 03:30:26,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:30:26,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9457094090444591
[2019-03-27 03:30:50,743] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:30:51,003] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:30:51,088] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:30:51,116] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:30:51,154] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:30:52,168] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1900000, evaluation results [1900000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:30:59,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:30:59,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8084
[2019-03-27 03:30:59,463] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 81.50000000000001, 1.0, 2.0, 0.868791640269004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1214295.946784884, 1214295.946784884, 261495.5614935686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7891800.0000, 
sim time next is 7892400.0000, 
raw observation next is [27.56666666666667, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.956398564560557, 6.9112, 168.9074413263596, 2195738.066200659, 1454262.442285448, 311349.1207600065], 
processed observation next is [1.0, 0.34782608695652173, 0.505529225908373, 0.81, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.10451985645605566, 0.0, 0.8294128630736498, 0.6099272406112942, 0.40396178952373557, 0.46470018023881565], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.10375325], dtype=float32), -0.972608]. 
=============================================
[2019-03-27 03:31:01,227] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:01,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6057
[2019-03-27 03:31:01,237] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 89.33333333333334, 1.0, 2.0, 0.4808545151576401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671911.0147086182, 671911.0147086189, 180650.2653881644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7688400.0000, 
sim time next is 7689000.0000, 
raw observation next is [25.25, 89.66666666666666, 1.0, 2.0, 0.4798613630518788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670522.8184421064, 670522.8184421064, 180500.4841322108], 
processed observation next is [1.0, 1.0, 0.39573459715639814, 0.8966666666666666, 1.0, 1.0, 0.3733269434359986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18625633845614065, 0.18625633845614065, 0.26940370766001615], 
reward next is 0.7306, 
noisyNet noise sample is [array([-0.02526894], dtype=float32), -0.32086772]. 
=============================================
[2019-03-27 03:31:01,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:01,251] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.57174 ]
 [71.497604]
 [71.44327 ]
 [71.39583 ]
 [71.35802 ]], R is [[71.66514587]
 [71.67886353]
 [71.69262695]
 [71.70644379]
 [71.72013092]].
[2019-03-27 03:31:01,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:01,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-03-27 03:31:02,357] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:02,357] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:02,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-03-27 03:31:02,511] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:02,518] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0661
[2019-03-27 03:31:02,522] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 91.66666666666667, 1.0, 2.0, 0.5106653634652096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713580.5100328617, 713580.5100328611, 185284.3236319518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7867200.0000, 
sim time next is 7867800.0000, 
raw observation next is [26.05, 92.0, 1.0, 2.0, 0.5125637422598751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716234.1123669059, 716234.1123669059, 185588.2525001215], 
processed observation next is [1.0, 0.043478260869565216, 0.43364928909952616, 0.92, 1.0, 1.0, 0.4127274003131025, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1989539201019183, 0.1989539201019183, 0.2769973917912261], 
reward next is 0.7230, 
noisyNet noise sample is [array([-0.35581937], dtype=float32), 2.05386]. 
=============================================
[2019-03-27 03:31:03,527] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:03,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:03,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-03-27 03:31:04,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:04,154] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:04,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-03-27 03:31:05,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:05,032] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8423
[2019-03-27 03:31:05,038] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.33333333333334, 1.0, 2.0, 0.4953321174589059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692147.5503904558, 692147.5503904558, 182867.9319834963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7782600.0000, 
sim time next is 7783200.0000, 
raw observation next is [26.4, 85.0, 1.0, 2.0, 0.4936117968912466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689742.8966346796, 689742.8966346796, 182601.2012829777], 
processed observation next is [1.0, 0.08695652173913043, 0.45023696682464454, 0.85, 1.0, 1.0, 0.38989373119427306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1915952490651888, 0.1915952490651888, 0.27253910639250406], 
reward next is 0.7275, 
noisyNet noise sample is [array([1.0217376], dtype=float32), 0.5072632]. 
=============================================
[2019-03-27 03:31:05,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:05,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:06,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-03-27 03:31:07,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:07,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:07,295] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-03-27 03:31:07,932] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:07,940] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3344
[2019-03-27 03:31:07,944] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2283034.659820668 W.
[2019-03-27 03:31:07,950] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8163222410937029, 1.0, 1.0, 0.8163222410937029, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2283034.659820668, 2283034.659820669, 427898.2390601223], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7819800.0000, 
sim time next is 7820400.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.4137238093806752, 1.0, 2.0, 0.4137238093806752, 1.0, 1.0, 0.7185014363397298, 6.911199999999999, 6.9112, 170.5573041426782, 1735176.841996234, 1735176.841996234, 360615.7135447729], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 0.2936431438321388, 1.0, 1.0, 0.2936431438321388, 1.0, 0.5, 0.6567090687069875, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4819935672211761, 0.4819935672211761, 0.5382324082757804], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.81824875], dtype=float32), 1.2703587]. 
=============================================
[2019-03-27 03:31:11,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:11,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:11,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-03-27 03:31:11,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:11,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:11,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-03-27 03:31:13,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:13,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:14,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-03-27 03:31:14,147] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:14,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:14,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-03-27 03:31:14,470] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:14,473] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:14,520] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-03-27 03:31:14,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:14,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:14,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-03-27 03:31:15,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:15,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:15,533] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-03-27 03:31:15,751] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:15,752] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:15,800] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-03-27 03:31:15,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:31:15,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:16,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-03-27 03:31:19,390] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:19,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4411
[2019-03-27 03:31:19,406] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 69.83333333333334, 1.0, 2.0, 0.249177029053214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 410153.8110339758, 410153.8110339764, 160818.9242645232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 760200.0000, 
sim time next is 760800.0000, 
raw observation next is [21.66666666666667, 71.66666666666667, 1.0, 2.0, 0.2502482864269294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 411660.5502184638, 411660.5502184638, 160928.9597454255], 
processed observation next is [1.0, 0.8260869565217391, 0.22590837282780438, 0.7166666666666667, 1.0, 1.0, 0.09668468244208363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11435015283846217, 0.11435015283846217, 0.24019247723197837], 
reward next is 0.7598, 
noisyNet noise sample is [array([0.13886647], dtype=float32), 0.4686888]. 
=============================================
[2019-03-27 03:31:25,671] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:25,683] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2894
[2019-03-27 03:31:25,688] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2869226459956143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462157.2970068382, 462157.2970068382, 164420.2911017553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 184200.0000, 
sim time next is 184800.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2866967877716635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461796.446713019, 461796.446713019, 164395.6499334722], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.14059853948393194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1282767907536164, 0.1282767907536164, 0.24536664169174954], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.3087644], dtype=float32), 0.9050667]. 
=============================================
[2019-03-27 03:31:27,022] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:27,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8782
[2019-03-27 03:31:27,041] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333333, 94.33333333333334, 1.0, 2.0, 0.2890022745360856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465186.9534925816, 465186.9534925816, 164627.7355783828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 196800.0000, 
sim time next is 197400.0000, 
raw observation next is [20.16666666666666, 94.16666666666667, 1.0, 2.0, 0.2890798779014609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 465186.8719295451, 465186.8719295457, 164627.5537880551], 
processed observation next is [0.0, 0.2608695652173913, 0.15481832543443896, 0.9416666666666668, 1.0, 1.0, 0.14346973241139865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12921857553598473, 0.12921857553598493, 0.24571276684784346], 
reward next is 0.7543, 
noisyNet noise sample is [array([0.1169027], dtype=float32), -1.4314538]. 
=============================================
[2019-03-27 03:31:27,674] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:27,683] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5499
[2019-03-27 03:31:27,687] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 81.5, 1.0, 2.0, 0.2323528972158815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383386.1076044721, 383386.1076044721, 159184.1578109189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 455400.0000, 
sim time next is 456000.0000, 
raw observation next is [20.1, 81.33333333333334, 1.0, 2.0, 0.2356152286388764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388645.8454703358, 388645.8454703358, 159492.8250227705], 
processed observation next is [1.0, 0.2608695652173913, 0.15165876777251197, 0.8133333333333335, 1.0, 1.0, 0.07905449233599567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1079571792973155, 0.1079571792973155, 0.23804899257129924], 
reward next is 0.7620, 
noisyNet noise sample is [array([-0.6722069], dtype=float32), 1.140456]. 
=============================================
[2019-03-27 03:31:27,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.16915 ]
 [79.18043 ]
 [79.22373 ]
 [79.244095]
 [79.10367 ]], R is [[79.11062622]
 [79.08193207]
 [79.05374908]
 [79.02591705]
 [78.99837494]].
[2019-03-27 03:31:30,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:30,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7610
[2019-03-27 03:31:30,203] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.9, 87.0, 1.0, 2.0, 0.2340918634516594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 388019.8986728564, 388019.898672857, 159212.6025994365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [18.88333333333333, 87.0, 1.0, 2.0, 0.2338091448563937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 387618.3220044767, 387618.3220044767, 159178.8085734092], 
processed observation next is [1.0, 1.0, 0.09399684044233794, 0.87, 1.0, 1.0, 0.07687848777878759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10767175611235463, 0.10767175611235463, 0.2375803113035958], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.71248776], dtype=float32), -0.74307275]. 
=============================================
[2019-03-27 03:31:33,731] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:33,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3318
[2019-03-27 03:31:33,747] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 90.66666666666667, 1.0, 2.0, 0.2773877877685151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 448052.7956005751, 448052.7956005757, 163469.7129210312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 282000.0000, 
sim time next is 282600.0000, 
raw observation next is [20.55, 90.0, 1.0, 2.0, 0.2790752241858487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 450205.4066099429, 450205.4066099423, 163613.9061713835], 
processed observation next is [0.0, 0.2608695652173913, 0.17298578199052142, 0.9, 1.0, 1.0, 0.13141593275403454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1250570573916508, 0.12505705739165066, 0.24419985995728882], 
reward next is 0.7558, 
noisyNet noise sample is [array([2.8270936], dtype=float32), -0.6667554]. 
=============================================
[2019-03-27 03:31:38,319] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:38,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9581
[2019-03-27 03:31:38,331] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.66666666666667, 1.0, 2.0, 0.2393789772093673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395749.9689925067, 395749.9689925067, 159798.8369622887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 682800.0000, 
sim time next is 683400.0000, 
raw observation next is [19.6, 83.33333333333333, 1.0, 2.0, 0.2397787823615639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396477.27191647, 396477.2719164694, 159832.2482227832], 
processed observation next is [1.0, 0.9130434782608695, 0.127962085308057, 0.8333333333333333, 1.0, 1.0, 0.08407082212236615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11013257553235278, 0.1101325755323526, 0.238555594362363], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.60825217], dtype=float32), -0.3345071]. 
=============================================
[2019-03-27 03:31:42,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:42,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0569
[2019-03-27 03:31:42,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.33333333333334, 1.0, 2.0, 0.2289946747030891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 378745.9979576253, 378745.9979576246, 158820.4414377433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 448800.0000, 
sim time next is 449400.0000, 
raw observation next is [19.7, 82.16666666666667, 1.0, 2.0, 0.2283710658019763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377793.6542760348, 377793.6542760348, 158757.3940653998], 
processed observation next is [1.0, 0.17391304347826086, 0.1327014218009479, 0.8216666666666668, 1.0, 1.0, 0.07032658530358589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.104942681743343, 0.104942681743343, 0.23695133442596986], 
reward next is 0.7630, 
noisyNet noise sample is [array([-0.58256835], dtype=float32), -1.1849252]. 
=============================================
[2019-03-27 03:31:45,237] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 03:31:45,240] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:31:45,241] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:31:45,242] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:45,242] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:45,243] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:31:45,244] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:31:45,244] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:45,245] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:45,248] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:31:45,253] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:31:45,269] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-03-27 03:31:45,290] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-03-27 03:31:45,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-03-27 03:31:45,307] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-03-27 03:31:45,325] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-03-27 03:32:07,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.3409348], dtype=float32), -0.015697967]
[2019-03-27 03:32:07,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.4, 78.66666666666667, 1.0, 2.0, 0.3347253848616578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524377.5141886877, 524377.514188687, 168796.0570575142]
[2019-03-27 03:32:07,082] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:32:07,083] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.144713961921895
[2019-03-27 03:32:26,240] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.3409348], dtype=float32), -0.015697967]
[2019-03-27 03:32:26,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.4, 59.0, 1.0, 2.0, 0.7250308427831075, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.988928880476979, 6.9112, 168.911805805556, 1910156.388456235, 1855013.22108487, 390806.5954861317]
[2019-03-27 03:32:26,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:32:26,246] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7666408513445411
[2019-03-27 03:32:26,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1910156.388456235 W.
[2019-03-27 03:32:41,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.3409348], dtype=float32), -0.015697967]
[2019-03-27 03:32:41,727] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.1, 55.0, 1.0, 2.0, 0.9581163581586679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1339222.394149594, 1339222.394149594, 286424.2727963197]
[2019-03-27 03:32:41,728] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:32:41,731] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8061980963897388
[2019-03-27 03:32:53,143] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.3409348], dtype=float32), -0.015697967]
[2019-03-27 03:32:53,144] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.97732462, 68.44529398333333, 1.0, 2.0, 0.6051338867805158, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.912199913244605, 6.9112, 168.9128206015965, 1691951.952036992, 1691242.579737679, 366349.7486523138]
[2019-03-27 03:32:53,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:32:53,148] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39946623961302963
[2019-03-27 03:32:53,149] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1691951.952036992 W.
[2019-03-27 03:32:53,444] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.3409348], dtype=float32), -0.015697967]
[2019-03-27 03:32:53,446] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.33333333333334, 82.5, 1.0, 2.0, 0.7697310751617751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1075770.468842834, 1075770.468842834, 236617.6538215202]
[2019-03-27 03:32:53,446] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:32:53,449] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9729818296636442
[2019-03-27 03:33:04,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.3409348], dtype=float32), -0.015697967]
[2019-03-27 03:33:04,709] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.420598645, 55.40138815, 1.0, 2.0, 0.888902475416824, 1.0, 1.0, 0.888902475416824, 0.0, 1.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 2486210.162262256, 2486210.162262256, 465684.1259866931]
[2019-03-27 03:33:04,711] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:33:04,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8440035202214967
[2019-03-27 03:33:04,715] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2486210.162262256 W.
[2019-03-27 03:33:39,599] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:33:39,976] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:33:40,084] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:33:40,174] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:33:40,240] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:33:41,258] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1925000, evaluation results [1925000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:33:45,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:33:45,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0213
[2019-03-27 03:33:45,575] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1907646.233870596 W.
[2019-03-27 03:33:45,584] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.5, 73.0, 1.0, 2.0, 0.6822145031191358, 1.0, 2.0, 0.6822145031191358, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1907646.233870596, 1907646.233870597, 366595.4418736065], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1260000.0000, 
sim time next is 1260600.0000, 
raw observation next is [28.46666666666667, 73.16666666666667, 1.0, 2.0, 0.6824341343163446, 1.0, 2.0, 0.6824341343163446, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1908260.925791431, 1908260.925791431, 366687.1549311689], 
processed observation next is [1.0, 0.6086956521739131, 0.5481832543443919, 0.7316666666666667, 1.0, 1.0, 0.6173905232727043, 1.0, 1.0, 0.6173905232727043, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5300724793865086, 0.5300724793865086, 0.5472942610912969], 
reward next is 0.4527, 
noisyNet noise sample is [array([1.1066854], dtype=float32), -0.13480052]. 
=============================================
[2019-03-27 03:33:48,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:33:48,083] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2801
[2019-03-27 03:33:48,089] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 87.66666666666667, 1.0, 2.0, 0.3384132657816531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524183.5858627975, 524183.585862798, 168622.8294463988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [22.55, 88.0, 1.0, 2.0, 0.3387289176552802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524766.1754814181, 524766.1754814174, 168672.0485317231], 
processed observation next is [0.0, 0.8695652173913043, 0.26777251184834133, 0.88, 1.0, 1.0, 0.2032878525967231, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14576838207817172, 0.14576838207817153, 0.2517493261667509], 
reward next is 0.7483, 
noisyNet noise sample is [array([-1.8318392], dtype=float32), 0.36746052]. 
=============================================
[2019-03-27 03:33:51,956] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:33:51,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6222
[2019-03-27 03:33:51,971] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 55.33333333333334, 1.0, 2.0, 0.3980005835323603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653537.3367999477, 653537.3367999477, 179179.4686179971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 651000.0000, 
sim time next is 651600.0000, 
raw observation next is [24.5, 55.0, 1.0, 2.0, 0.3502085986538186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 575125.4332449789, 575125.4332449782, 172544.3181289358], 
processed observation next is [1.0, 0.5652173913043478, 0.3601895734597157, 0.55, 1.0, 1.0, 0.21711879355881758, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1597570647902719, 0.1597570647902717, 0.25752883302826235], 
reward next is 0.7425, 
noisyNet noise sample is [array([2.1704924], dtype=float32), -0.3065718]. 
=============================================
[2019-03-27 03:34:02,862] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:02,876] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5149
[2019-03-27 03:34:02,882] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.85, 93.5, 1.0, 2.0, 0.3313024121703679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513390.6211512697, 513390.6211512697, 167778.2939011734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 963000.0000, 
sim time next is 963600.0000, 
raw observation next is [21.86666666666667, 93.33333333333334, 1.0, 2.0, 0.3315436398908417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513817.9847701254, 513817.9847701254, 167813.384624132], 
processed observation next is [1.0, 0.13043478260869565, 0.23538704581358633, 0.9333333333333335, 1.0, 1.0, 0.19463089143474901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14272721799170152, 0.14272721799170152, 0.25046773824497315], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.3113385], dtype=float32), -0.8046711]. 
=============================================
[2019-03-27 03:34:02,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:02,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0191
[2019-03-27 03:34:02,906] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 89.0, 1.0, 2.0, 0.3091584772471531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489622.0235564462, 489622.0235564456, 166271.5381219746], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 861600.0000, 
sim time next is 862200.0000, 
raw observation next is [21.5, 89.0, 1.0, 2.0, 0.3087097295269451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489279.8158706811, 489279.8158706804, 166253.6816097139], 
processed observation next is [0.0, 1.0, 0.21800947867298584, 0.89, 1.0, 1.0, 0.16712015605656033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13591105996407807, 0.13591105996407787, 0.24813982329808046], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.15145093], dtype=float32), -0.056523703]. 
=============================================
[2019-03-27 03:34:04,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:04,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1934
[2019-03-27 03:34:04,890] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 89.0, 1.0, 2.0, 0.3079426918569733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488430.3244859311, 488430.3244859317, 166198.4926851338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 862800.0000, 
sim time next is 863400.0000, 
raw observation next is [21.43333333333333, 89.0, 1.0, 2.0, 0.306553655944736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486589.5732191028, 486589.5732191022, 166071.0059825331], 
processed observation next is [0.0, 1.0, 0.21484992101105835, 0.89, 1.0, 1.0, 0.16452247704185058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13516377033863966, 0.1351637703386395, 0.24786717310825834], 
reward next is 0.7521, 
noisyNet noise sample is [array([-1.3755859], dtype=float32), -0.1789631]. 
=============================================
[2019-03-27 03:34:13,137] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:13,145] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4124
[2019-03-27 03:34:13,149] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 90.33333333333333, 1.0, 2.0, 0.4521071560793938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704014.5907130953, 704014.5907130947, 185005.6770500243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1217400.0000, 
sim time next is 1218000.0000, 
raw observation next is [22.03333333333333, 90.66666666666667, 1.0, 2.0, 0.3806363193510849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 592550.3243295372, 592550.3243295372, 174447.397134563], 
processed observation next is [1.0, 0.08695652173913043, 0.2432859399684044, 0.9066666666666667, 1.0, 1.0, 0.2537786980133553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16459731231376035, 0.16459731231376035, 0.2603692494545717], 
reward next is 0.7396, 
noisyNet noise sample is [array([0.16908057], dtype=float32), -0.16767469]. 
=============================================
[2019-03-27 03:34:13,168] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.64487]
 [72.64143]
 [72.29042]
 [72.35803]
 [72.41208]], R is [[73.23962402]
 [73.23109436]
 [73.2466507 ]
 [73.26190186]
 [73.27684784]].
[2019-03-27 03:34:17,085] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:17,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8038
[2019-03-27 03:34:17,097] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 79.0, 1.0, 2.0, 0.5602844662877203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892231.1014334069, 892231.1014334069, 205940.6190739584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1076400.0000, 
sim time next is 1077000.0000, 
raw observation next is [22.76666666666667, 78.50000000000001, 1.0, 2.0, 0.5762714572907568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916527.6782513242, 916527.6782513235, 209064.3108221342], 
processed observation next is [1.0, 0.4782608695652174, 0.2780410742496052, 0.7850000000000001, 1.0, 1.0, 0.4894836834828395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25459102173647896, 0.25459102173647874, 0.3120362848091555], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.43219393], dtype=float32), -1.2724817]. 
=============================================
[2019-03-27 03:34:17,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.018875]
 [73.84446 ]
 [73.874306]
 [73.99348 ]
 [74.068504]], R is [[73.82228851]
 [73.77669525]
 [73.73099518]
 [73.68714142]
 [73.64840698]].
[2019-03-27 03:34:20,021] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:20,028] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7820
[2019-03-27 03:34:20,034] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 92.0, 1.0, 2.0, 0.6149520775824487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 981640.356770511, 981640.356770511, 217479.7012252361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1357200.0000, 
sim time next is 1357800.0000, 
raw observation next is [20.81666666666667, 92.16666666666667, 1.0, 2.0, 0.3646134778447146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581773.2967046147, 581773.2967046147, 173611.6546320042], 
processed observation next is [1.0, 0.7391304347826086, 0.18562401263823086, 0.9216666666666667, 1.0, 1.0, 0.2344740696924272, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16160369352905962, 0.16160369352905962, 0.2591218725850809], 
reward next is 0.7409, 
noisyNet noise sample is [array([1.2144457], dtype=float32), -1.0656488]. 
=============================================
[2019-03-27 03:34:23,021] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:23,030] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5442
[2019-03-27 03:34:23,034] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 96.33333333333333, 1.0, 2.0, 0.3166039960387995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501664.4559588066, 501664.4559588066, 167168.4678762208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1380000.0000, 
sim time next is 1380600.0000, 
raw observation next is [20.6, 96.5, 1.0, 2.0, 0.3155863708731341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500212.599310751, 500212.5993107517, 167062.6315749347], 
processed observation next is [1.0, 1.0, 0.17535545023696694, 0.965, 1.0, 1.0, 0.17540526611220977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1389479442529864, 0.1389479442529866, 0.24934721130587267], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.7996749], dtype=float32), 0.5835351]. 
=============================================
[2019-03-27 03:34:24,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:24,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5021
[2019-03-27 03:34:24,594] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333334, 89.0, 1.0, 2.0, 0.3451865020357726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534531.4758566526, 534531.4758566519, 169450.6587296302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1549200.0000, 
sim time next is 1549800.0000, 
raw observation next is [22.35, 89.5, 1.0, 2.0, 0.3446836717122636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534035.2358530081, 534035.2358530075, 169418.3342282879], 
processed observation next is [0.0, 0.9565217391304348, 0.25829383886255936, 0.895, 1.0, 1.0, 0.2104622550750164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14834312107028003, 0.14834312107027986, 0.2528631854153551], 
reward next is 0.7471, 
noisyNet noise sample is [array([2.2075207], dtype=float32), -0.05479766]. 
=============================================
[2019-03-27 03:34:34,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:34,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4084
[2019-03-27 03:34:34,346] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.58333333333334, 98.0, 1.0, 2.0, 0.3146256245799867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496975.2427550306, 496975.2427550299, 166788.3153678298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1399800.0000, 
sim time next is 1400400.0000, 
raw observation next is [20.6, 98.0, 1.0, 2.0, 0.3148380840668547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497111.9423132369, 497111.9423132376, 166794.2627271379], 
processed observation next is [0.0, 0.21739130434782608, 0.17535545023696694, 0.98, 1.0, 1.0, 0.17450371574319842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1380866506425658, 0.138086650642566, 0.24894666078677297], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.12253013], dtype=float32), -0.89683217]. 
=============================================
[2019-03-27 03:34:37,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:37,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4802
[2019-03-27 03:34:37,214] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 74.83333333333334, 1.0, 2.0, 0.5460826286472423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763088.7591808831, 763088.7591808836, 191132.0399709794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [30.0, 74.0, 1.0, 2.0, 0.5477626437469091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765437.2362863581, 765437.2362863588, 191418.6536162527], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.74, 1.0, 1.0, 0.45513571535772174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21262145452398837, 0.21262145452398856, 0.2856994830093324], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.1030435], dtype=float32), -1.9034824]. 
=============================================
[2019-03-27 03:34:37,401] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 03:34:37,403] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:34:37,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:37,404] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:34:37,405] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:34:37,405] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:37,406] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:37,405] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:34:37,406] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:34:37,408] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:37,409] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:37,439] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-03-27 03:34:37,459] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-03-27 03:34:37,485] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-03-27 03:34:37,486] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-03-27 03:34:37,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-03-27 03:34:48,186] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.33640826], dtype=float32), -0.060344353]
[2019-03-27 03:34:48,187] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.26752785, 71.66368023333334, 1.0, 2.0, 0.2433516454688985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 402652.4675778062, 402652.4675778062, 160154.6975582953]
[2019-03-27 03:34:48,188] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:34:48,190] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14805029692942717
[2019-03-27 03:35:12,270] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.33640826], dtype=float32), -0.060344353]
[2019-03-27 03:35:12,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.491495855, 85.559088255, 1.0, 2.0, 0.5214659292557645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728677.9050016924, 728677.9050016917, 187026.2990374914]
[2019-03-27 03:35:12,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:35:12,274] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5815876351600412
[2019-03-27 03:35:30,945] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.33640826], dtype=float32), -0.060344353]
[2019-03-27 03:35:30,946] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 60.00000000000001, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.896190318055048, 6.9112, 168.9015517442391, 3693103.020948978, 2284977.652298467, 470787.2850159312]
[2019-03-27 03:35:30,948] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:35:30,953] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7336409843243138
[2019-03-27 03:35:30,954] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3693103.020948978 W.
[2019-03-27 03:35:46,979] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.33640826], dtype=float32), -0.060344353]
[2019-03-27 03:35:46,980] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 84.83333333333333, 1.0, 2.0, 0.4881101218819014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682052.7248942333, 682052.7248942339, 181753.4488174187]
[2019-03-27 03:35:46,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:35:46,985] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21125814237582363
[2019-03-27 03:35:51,967] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.33640826], dtype=float32), -0.060344353]
[2019-03-27 03:35:52,067] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.4884549, 92.05252404, 1.0, 2.0, 0.809557618891569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1131461.407133268, 1131461.407133267, 246275.6353848283]
[2019-03-27 03:35:52,068] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:35:52,071] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4216035871767623
[2019-03-27 03:36:28,536] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:36:28,673] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:36:28,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:36:28,732] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:36:28,940] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:36:29,960] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1950000, evaluation results [1950000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:36:36,408] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:36:36,418] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8578
[2019-03-27 03:36:36,422] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 58.33333333333333, 1.0, 2.0, 0.3491524393875517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537979.8451074867, 537979.845107486, 169654.2106731027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
processed observation next is [0.0, 0.6956521739130435, 0.490521327014218, 0.5866666666666667, 1.0, 1.0, 0.21240014993269515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1485342325336385, 0.1485342325336385, 0.2528694139334764], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.06573439], dtype=float32), 1.5565705]. 
=============================================
[2019-03-27 03:36:37,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:36:37,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6194
[2019-03-27 03:36:37,300] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 95.83333333333333, 1.0, 2.0, 0.3485798874432652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535157.5788026374, 535157.5788026374, 169363.1164635848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1828200.0000, 
sim time next is 1828800.0000, 
raw observation next is [21.9, 96.0, 1.0, 2.0, 0.3493305945499476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535988.9577849462, 535988.9577849468, 169420.9558366583], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.96, 1.0, 1.0, 0.21606095728909347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1488858216069295, 0.14888582160692965, 0.2528670982636691], 
reward next is 0.7471, 
noisyNet noise sample is [array([-1.0279652], dtype=float32), -0.50104]. 
=============================================
[2019-03-27 03:36:52,818] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:36:52,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0724
[2019-03-27 03:36:52,835] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.53333333333333, 77.83333333333334, 1.0, 2.0, 0.5632618569419116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787103.6837038591, 787103.6837038591, 194101.4101277471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2139000.0000, 
sim time next is 2139600.0000, 
raw observation next is [29.36666666666667, 78.66666666666667, 1.0, 2.0, 0.5605814238077436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783356.6577932658, 783356.6577932658, 193632.3713267458], 
processed observation next is [0.0, 0.782608695652174, 0.5908372827804109, 0.7866666666666667, 1.0, 1.0, 0.4705800286840284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2175990716092405, 0.2175990716092405, 0.28900353929365047], 
reward next is 0.7110, 
noisyNet noise sample is [array([-0.7415173], dtype=float32), 0.333033]. 
=============================================
[2019-03-27 03:36:54,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:36:54,382] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3060
[2019-03-27 03:36:54,388] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 94.0, 1.0, 2.0, 0.5063128050034075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707496.4177415373, 707496.4177415373, 184590.9725924404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [25.4, 94.0, 1.0, 2.0, 0.5054614807527384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706306.4239431076, 706306.4239431082, 184456.0371933277], 
processed observation next is [1.0, 0.043478260869565216, 0.4028436018957346, 0.94, 1.0, 1.0, 0.40417045873823904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19619622887308544, 0.1961962288730856, 0.27530751819899657], 
reward next is 0.7247, 
noisyNet noise sample is [array([2.0105076], dtype=float32), -1.6917115]. 
=============================================
[2019-03-27 03:36:54,408] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.3956 ]
 [66.45938]
 [66.40677]
 [66.4317 ]
 [66.38728]], R is [[66.26285553]
 [66.32471466]
 [66.38556671]
 [66.44547272]
 [66.50450134]].
[2019-03-27 03:36:59,307] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:36:59,317] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7629
[2019-03-27 03:36:59,323] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 95.0, 1.0, 2.0, 0.4409584748167353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634962.6307658377, 634962.6307658384, 177279.5159927135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1913400.0000, 
sim time next is 1914000.0000, 
raw observation next is [23.76666666666667, 95.0, 1.0, 2.0, 0.4317784329814955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622597.5390074137, 622597.5390074137, 176080.103448], 
processed observation next is [1.0, 0.13043478260869565, 0.32543443917851517, 0.95, 1.0, 1.0, 0.315395702387344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17294376083539267, 0.17294376083539267, 0.26280612454925373], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.2974361], dtype=float32), 0.9929241]. 
=============================================
[2019-03-27 03:36:59,339] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[63.533215]
 [63.671818]
 [63.87695 ]
 [63.47675 ]
 [63.42849 ]], R is [[63.51821899]
 [63.61844254]
 [63.7172699 ]
 [63.80974579]
 [63.89500046]].
[2019-03-27 03:37:01,644] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:01,653] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2784
[2019-03-27 03:37:01,663] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2267027.963569698 W.
[2019-03-27 03:37:01,668] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.08333333333334, 65.16666666666667, 1.0, 2.0, 0.5404027137173367, 1.0, 2.0, 0.5404027137173367, 1.0, 2.0, 0.9385007998186791, 6.9112, 6.9112, 170.5573041426782, 2267027.963569698, 2267027.963569698, 444210.0247529395], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2211000.0000, 
sim time next is 2211600.0000, 
raw observation next is [32.06666666666667, 65.33333333333334, 1.0, 2.0, 1.017814726263947, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005994271353083, 6.9112, 168.9123931200484, 2319937.22874855, 2252687.134389604, 468820.2790726706], 
processed observation next is [1.0, 0.6086956521739131, 0.7187993680884678, 0.6533333333333334, 1.0, 1.0, 1.0214635256192133, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479427135308338, 0.0, 0.8294371786476028, 0.6444270079857084, 0.6257464262193344, 0.6997317598099562], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5828339], dtype=float32), 0.12308405]. 
=============================================
[2019-03-27 03:37:03,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:03,079] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9882
[2019-03-27 03:37:03,088] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 86.0, 1.0, 2.0, 0.6677336840332496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933157.0374939744, 933157.0374939751, 214031.9107158564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2257200.0000, 
sim time next is 2257800.0000, 
raw observation next is [26.16666666666666, 86.16666666666667, 1.0, 2.0, 0.7564965720769556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1057264.817172797, 1057264.817172797, 233509.6306334129], 
processed observation next is [1.0, 0.13043478260869565, 0.4391785150078987, 0.8616666666666667, 1.0, 1.0, 0.706622375996332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29368467143688803, 0.29368467143688803, 0.3485218367662879], 
reward next is 0.6515, 
noisyNet noise sample is [array([-0.49482393], dtype=float32), -0.21488932]. 
=============================================
[2019-03-27 03:37:03,624] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:03,631] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0858
[2019-03-27 03:37:03,639] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333334, 85.33333333333334, 1.0, 2.0, 0.7321125345210029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1023169.74562647, 1023169.745626471, 227929.24923643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [27.65, 84.5, 1.0, 2.0, 0.7342333500924266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026135.140452633, 1026135.140452633, 228408.1018651827], 
processed observation next is [1.0, 0.30434782608695654, 0.509478672985782, 0.845, 1.0, 1.0, 0.6797992169788273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2850375390146203, 0.2850375390146203, 0.34090761472415326], 
reward next is 0.6591, 
noisyNet noise sample is [array([-0.32753417], dtype=float32), -1.2072008]. 
=============================================
[2019-03-27 03:37:03,656] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[49.106922]
 [48.79507 ]
 [50.367702]
 [50.987514]
 [50.689564]], R is [[49.27516556]
 [49.44221878]
 [49.59425735]
 [49.77016068]
 [49.96039581]].
[2019-03-27 03:37:04,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:04,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6606
[2019-03-27 03:37:04,687] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.66666666666666, 1.0, 2.0, 0.4728084141699411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661700.2802726309, 661700.2802726309, 179579.0945210082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [24.45, 94.83333333333333, 1.0, 2.0, 0.471409588423889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660603.3525919177, 660603.3525919183, 179482.4667023973], 
processed observation next is [0.0, 0.08695652173913043, 0.3578199052132702, 0.9483333333333333, 1.0, 1.0, 0.3631440824384205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1835009312755327, 0.18350093127553285, 0.26788427866029446], 
reward next is 0.7321, 
noisyNet noise sample is [array([0.29673862], dtype=float32), 0.44199106]. 
=============================================
[2019-03-27 03:37:11,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:11,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4247
[2019-03-27 03:37:11,083] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 84.5, 1.0, 2.0, 0.5519986809863414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771358.7754463197, 771358.7754463197, 192144.2854512214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2143800.0000, 
sim time next is 2144400.0000, 
raw observation next is [27.96666666666667, 85.33333333333334, 1.0, 2.0, 0.5507295009673426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769584.59041656, 769584.5904165606, 191926.0473097618], 
processed observation next is [0.0, 0.8260869565217391, 0.524486571879937, 0.8533333333333334, 1.0, 1.0, 0.4587102421293284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21377349733793333, 0.21377349733793352, 0.28645678702949523], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.49602047], dtype=float32), -0.38971874]. 
=============================================
[2019-03-27 03:37:16,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:16,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6041
[2019-03-27 03:37:16,430] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 96.33333333333333, 1.0, 2.0, 0.6876930569954549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 961062.8585341, 961062.8585341007, 218212.1327088874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2521200.0000, 
sim time next is 2521800.0000, 
raw observation next is [26.25, 96.5, 1.0, 2.0, 0.6952171063192489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 971582.6565672379, 971582.6565672379, 219817.1580888292], 
processed observation next is [1.0, 0.17391304347826086, 0.4431279620853081, 0.965, 1.0, 1.0, 0.6327916943605408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2698840712686772, 0.2698840712686772, 0.32808531058034207], 
reward next is 0.6719, 
noisyNet noise sample is [array([0.27036306], dtype=float32), -0.3939223]. 
=============================================
[2019-03-27 03:37:17,732] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:17,741] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4488
[2019-03-27 03:37:17,744] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.35, 74.0, 1.0, 2.0, 0.5545794842767985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774966.4860106757, 774966.4860106757, 192590.4977421933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2226600.0000, 
sim time next is 2227200.0000, 
raw observation next is [30.2, 74.66666666666667, 1.0, 2.0, 0.554832280312203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775319.8708003875, 775319.8708003875, 192634.0995559796], 
processed observation next is [1.0, 0.782608695652174, 0.6303317535545023, 0.7466666666666667, 1.0, 1.0, 0.4636533497737385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2153666307778854, 0.2153666307778854, 0.2875135814268352], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.32300612], dtype=float32), -1.5793427]. 
=============================================
[2019-03-27 03:37:18,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:18,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4600
[2019-03-27 03:37:18,342] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 94.66666666666666, 1.0, 2.0, 0.6889343645853419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962798.393559916, 962798.393559916, 218475.4372980976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2529600.0000, 
sim time next is 2530200.0000, 
raw observation next is [26.38333333333333, 94.33333333333334, 1.0, 2.0, 0.6745944939554251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 942749.2668443718, 942749.2668443718, 215457.5680809156], 
processed observation next is [1.0, 0.2608695652173913, 0.44944707740916257, 0.9433333333333335, 1.0, 1.0, 0.6079451734402712, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26187479634565886, 0.26187479634565886, 0.3215784598222621], 
reward next is 0.6784, 
noisyNet noise sample is [array([-1.0609071], dtype=float32), 2.0599375]. 
=============================================
[2019-03-27 03:37:26,265] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 03:37:26,266] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:37:26,267] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:37:26,268] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:37:26,268] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:37:26,269] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:37:26,270] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:37:26,270] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:37:26,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:37:26,272] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:37:26,272] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:37:26,302] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-03-27 03:37:26,324] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-03-27 03:37:26,344] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-03-27 03:37:26,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-03-27 03:37:26,360] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-03-27 03:37:29,924] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25028747], dtype=float32), -0.061896924]
[2019-03-27 03:37:29,925] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.35, 62.16666666666666, 1.0, 2.0, 0.2188988233804421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 365276.0088200931, 365276.0088200931, 157289.8469368227]
[2019-03-27 03:37:29,926] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:37:29,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17456691542409142
[2019-03-27 03:37:31,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.25028747], dtype=float32), -0.061896924]
[2019-03-27 03:37:31,021] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.4, 95.5, 1.0, 2.0, 0.2650526154486161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 431053.1406491161, 431053.1406491161, 162334.7419220064]
[2019-03-27 03:37:31,022] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:37:31,024] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9455928146271809
[2019-03-27 03:37:40,910] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.25028747], dtype=float32), -0.061896924]
[2019-03-27 03:37:40,911] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.59906307333333, 97.08610834333334, 1.0, 2.0, 0.4012173860440416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595688.0226150024, 595688.0226150018, 174053.2235757494]
[2019-03-27 03:37:40,913] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:37:40,915] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6296201679276242
[2019-03-27 03:38:15,034] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.25028747], dtype=float32), -0.061896924]
[2019-03-27 03:38:15,036] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.95, 89.33333333333334, 1.0, 2.0, 0.5373291143985411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750852.3889982343, 750852.3889982337, 189651.3903222882]
[2019-03-27 03:38:15,038] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:38:15,040] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8947858807722145
[2019-03-27 03:38:25,140] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.25028747], dtype=float32), -0.061896924]
[2019-03-27 03:38:25,142] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [38.43066316333334, 63.13991682333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.460075900113422, 6.9112, 171.5212843490159, 3305173.054903701, 2909768.677693861, 550897.6264931295]
[2019-03-27 03:38:25,143] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:38:25,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3860053457087165
[2019-03-27 03:38:25,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3305173.054903701 W.
[2019-03-27 03:39:21,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:39:21,412] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:39:21,449] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:39:21,548] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:39:21,618] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:39:22,637] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1975000, evaluation results [1975000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:39:29,717] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:29,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7095
[2019-03-27 03:39:29,739] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 97.0, 1.0, 2.0, 0.3664723963152283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557692.7297589987, 557692.7297589987, 171098.6210555367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2759400.0000, 
sim time next is 2760000.0000, 
raw observation next is [22.0, 96.0, 1.0, 2.0, 0.361777469942939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552609.0492098521, 552609.0492098528, 170729.5282432186], 
processed observation next is [0.0, 0.9565217391304348, 0.2417061611374408, 0.96, 1.0, 1.0, 0.23105719270233616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15350251366940337, 0.15350251366940357, 0.25482019140778894], 
reward next is 0.7452, 
noisyNet noise sample is [array([1.020777], dtype=float32), 0.48658562]. 
=============================================
[2019-03-27 03:39:29,754] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.60758 ]
 [81.561935]
 [81.52588 ]
 [81.527336]
 [81.39906 ]], R is [[81.58443451]
 [81.51322174]
 [81.44228363]
 [81.3717804 ]
 [81.30190277]].
[2019-03-27 03:39:34,190] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:34,200] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3765
[2019-03-27 03:39:34,211] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2305420.20115061 W.
[2019-03-27 03:39:34,215] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.15, 69.5, 1.0, 2.0, 0.8243190428254457, 1.0, 2.0, 0.8243190428254457, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2305420.20115061, 2305420.20115061, 431880.2473237219], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2554200.0000, 
sim time next is 2554800.0000, 
raw observation next is [30.26666666666667, 69.0, 1.0, 2.0, 0.5928626767571835, 1.0, 2.0, 0.5928626767571835, 1.0, 1.0, 1.025079225812117, 6.9112, 6.9112, 170.5573041426782, 2487320.274002412, 2487320.274002412, 484313.2682992652], 
processed observation next is [1.0, 0.5652173913043478, 0.6334913112164299, 0.69, 1.0, 1.0, 0.5094731045267271, 1.0, 1.0, 0.5094731045267271, 1.0, 0.5, 1.030584421722094, 0.0, 0.0, 0.8375144448122397, 0.6909222983340033, 0.6909222983340033, 0.7228556243272615], 
reward next is 0.2771, 
noisyNet noise sample is [array([0.27978608], dtype=float32), -0.18624057]. 
=============================================
[2019-03-27 03:39:34,765] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:34,774] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7625
[2019-03-27 03:39:34,781] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.349903153279406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538978.7727720743, 538978.772772075, 169731.5328070277], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2761800.0000, 
sim time next is 2762400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3488612530156051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537408.1714995031, 537408.1714995025, 169603.7349907542], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21549548556097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14928004763875086, 0.1492800476387507, 0.2531399029712749], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.8760726], dtype=float32), -0.3427423]. 
=============================================
[2019-03-27 03:39:40,754] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:40,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8664
[2019-03-27 03:39:40,773] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4759084983839051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664997.6485445339, 664997.6485445339, 179907.4664712944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2703000.0000, 
sim time next is 2703600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.475867988442044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664941.0253757458, 664941.0253757464, 179901.4110463232], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36851564872535425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18470584038215163, 0.1847058403821518, 0.2685095687258555], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.7325947], dtype=float32), -0.45321378]. 
=============================================
[2019-03-27 03:39:47,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:47,536] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1540
[2019-03-27 03:39:47,544] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.6007945280735193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 904782.5869874886, 904782.5869874893, 209003.5187868426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3057000.0000, 
sim time next is 3057600.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.5710010803508623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859872.9306723499, 859872.9306723499, 203122.4649665443], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.4831338317480268, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23885359185343052, 0.23885359185343052, 0.30316785815902136], 
reward next is 0.6968, 
noisyNet noise sample is [array([0.13645774], dtype=float32), 0.61811846]. 
=============================================
[2019-03-27 03:39:53,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:53,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2456
[2019-03-27 03:39:53,350] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3394237821965167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522871.2346575569, 522871.2346575569, 168427.9518024827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2863200.0000, 
sim time next is 2863800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3391349646522034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522432.1466490824, 522432.1466490824, 168393.0931199608], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20377706584602817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14512004073585624, 0.14512004073585624, 0.25133297480591166], 
reward next is 0.7487, 
noisyNet noise sample is [array([-1.0476059], dtype=float32), -0.89272547]. 
=============================================
[2019-03-27 03:39:57,802] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:57,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4599
[2019-03-27 03:39:57,819] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6046156823311563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844914.6186266005, 844914.6186266005, 201608.1970021895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262200.0000, 
sim time next is 3262800.0000, 
raw observation next is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
processed observation next is [0.0, 0.782608695652174, 0.6366508688783573, 0.7633333333333334, 1.0, 1.0, 0.5016342449849397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22760792859789694, 0.22760792859789694, 0.2958679742077879], 
reward next is 0.7041, 
noisyNet noise sample is [array([0.5578495], dtype=float32), 0.63479686]. 
=============================================
[2019-03-27 03:39:58,915] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:39:58,925] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1089
[2019-03-27 03:39:58,929] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 84.0, 1.0, 2.0, 0.367075604509191, 1.0, 1.0, 0.367075604509191, 1.0, 1.0, 0.618992821861477, 6.9112, 6.9112, 170.5573041426782, 1539391.592414735, 1539391.592414735, 333021.0021494601], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3165600.0000, 
sim time next is 3166200.0000, 
raw observation next is [26.5, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.244916923818596, 6.9112, 168.9110318731211, 1690664.646581668, 1453917.072767632, 311347.5029907055], 
processed observation next is [1.0, 0.6521739130434783, 0.4549763033175356, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.03337169238185957, 0.0, 0.8294304943020082, 0.46962906849490776, 0.40386585354656446, 0.4646977656577694], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.77638626], dtype=float32), 0.47460517]. 
=============================================
[2019-03-27 03:40:02,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:02,843] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0880
[2019-03-27 03:40:02,848] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.598822191377483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 836815.3687192408, 836815.3687192402, 200527.3486329291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3259200.0000, 
sim time next is 3259800.0000, 
raw observation next is [31.5, 73.0, 1.0, 2.0, 0.5976650794464909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835197.743714509, 835197.7437145095, 200312.3403662385], 
processed observation next is [0.0, 0.7391304347826086, 0.6919431279620853, 0.73, 1.0, 1.0, 0.5152591318632421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23199937325403025, 0.23199937325403042, 0.2989736423376694], 
reward next is 0.7010, 
noisyNet noise sample is [array([1.1881753], dtype=float32), 0.6404202]. 
=============================================
[2019-03-27 03:40:03,263] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:03,273] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6247
[2019-03-27 03:40:03,279] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 68.0, 1.0, 2.0, 0.5512106582084421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770257.1991235091, 770257.1991235097, 192009.8246502815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3238800.0000, 
sim time next is 3239400.0000, 
raw observation next is [31.66666666666667, 67.5, 1.0, 2.0, 0.558430229368891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780349.4737786412, 780349.4737786412, 193258.3575492141], 
processed observation next is [0.0, 0.4782608695652174, 0.6998420221169038, 0.675, 1.0, 1.0, 0.4679882281552903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21676374271628923, 0.21676374271628923, 0.2884453097749464], 
reward next is 0.7116, 
noisyNet noise sample is [array([0.94144654], dtype=float32), -0.7968546]. 
=============================================
[2019-03-27 03:40:03,926] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:03,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7105
[2019-03-27 03:40:03,939] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 99.0, 1.0, 2.0, 0.3242680589624113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505771.4355117322, 505771.4355117328, 167290.6144457537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3041400.0000, 
sim time next is 3042000.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3300879593457952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513241.08261196, 513241.0826119606, 167820.2310250227], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19287705945276531, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1425669673922111, 0.14256696739221128, 0.2504779567537652], 
reward next is 0.7495, 
noisyNet noise sample is [array([-1.9150766], dtype=float32), -1.2741157]. 
=============================================
[2019-03-27 03:40:03,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.94423]
 [70.91693]
 [71.77211]
 [72.05809]
 [72.0912 ]], R is [[71.14855957]
 [71.18738556]
 [71.22629547]
 [71.26535034]
 [71.30492401]].
[2019-03-27 03:40:04,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:04,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8470
[2019-03-27 03:40:04,715] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5557936098391063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776663.7190279795, 776663.719027979, 192800.3245058036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3322800.0000, 
sim time next is 3323400.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.5614823178100475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 784616.0323224632, 784616.0323224625, 193790.2628569866], 
processed observation next is [0.0, 0.4782608695652174, 0.6761453396524489, 0.695, 1.0, 1.0, 0.47166544314463554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2179488978673509, 0.2179488978673507, 0.28923919829400985], 
reward next is 0.7108, 
noisyNet noise sample is [array([-1.1479799], dtype=float32), -0.98844475]. 
=============================================
[2019-03-27 03:40:06,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:06,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9080
[2019-03-27 03:40:06,138] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3907757072072625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584246.9615002999, 584246.9615002999, 173127.884081503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3109800.0000, 
sim time next is 3110400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3896586440802436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581716.8524090784, 581716.8524090784, 172871.7893211565], 
processed observation next is [1.0, 0.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26464896877137783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16158801455807734, 0.16158801455807734, 0.25801759600172613], 
reward next is 0.7420, 
noisyNet noise sample is [array([-1.1981878], dtype=float32), 0.22909896]. 
=============================================
[2019-03-27 03:40:07,467] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:07,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8241
[2019-03-27 03:40:07,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 89.0, 1.0, 2.0, 0.7330855957331389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1094865.145498275, 1094865.145498275, 237409.9708477152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3145200.0000, 
sim time next is 3145800.0000, 
raw observation next is [23.83333333333333, 89.0, 1.0, 2.0, 0.7652901427489769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1136342.606158289, 1136342.60615829, 244539.8227294159], 
processed observation next is [1.0, 0.391304347826087, 0.32859399684044216, 0.89, 1.0, 1.0, 0.7172170394565986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31565072393285803, 0.31565072393285837, 0.3649848100439043], 
reward next is 0.6350, 
noisyNet noise sample is [array([2.387393], dtype=float32), 1.697945]. 
=============================================
[2019-03-27 03:40:07,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:07,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9320
[2019-03-27 03:40:07,681] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 85.0, 1.0, 2.0, 0.6745084869485554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 989521.3541769555, 989521.3541769562, 221548.375659918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3148800.0000, 
sim time next is 3149400.0000, 
raw observation next is [24.83333333333334, 84.0, 1.0, 2.0, 0.6496299206387757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 952143.2767183082, 952143.2767183089, 216070.7810334703], 
processed observation next is [1.0, 0.43478260869565216, 0.3759873617693526, 0.84, 1.0, 1.0, 0.5778673742635851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2644842435328634, 0.2644842435328636, 0.3224937030350303], 
reward next is 0.6775, 
noisyNet noise sample is [array([-1.011951], dtype=float32), -1.2415257]. 
=============================================
[2019-03-27 03:40:10,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:10,529] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8068
[2019-03-27 03:40:10,534] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.578124993567868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807881.3708210465, 807881.3708210465, 196741.2898296856], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.77, 1.0, 1.0, 0.4917168597203228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22441149189473514, 0.22441149189473514, 0.2936437161637098], 
reward next is 0.7064, 
noisyNet noise sample is [array([-1.8080581], dtype=float32), 0.7214638]. 
=============================================
[2019-03-27 03:40:13,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:13,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0919
[2019-03-27 03:40:13,200] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5776817430182279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807261.7298354707, 807261.7298354707, 196662.6073310398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3249000.0000, 
sim time next is 3249600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5786776747525947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808653.99059273, 808653.99059273, 196841.6482538433], 
processed observation next is [0.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.4923827406657767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22462610849798056, 0.22462610849798056, 0.29379350485648253], 
reward next is 0.7062, 
noisyNet noise sample is [array([0.5958595], dtype=float32), -0.06973985]. 
=============================================
[2019-03-27 03:40:16,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:16,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2070
[2019-03-27 03:40:16,386] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.5074617159312806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709102.3847715318, 709102.3847715311, 184773.0094002295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3277800.0000, 
sim time next is 3278400.0000, 
raw observation next is [27.33333333333334, 79.0, 1.0, 2.0, 0.5024311185079879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702070.5489921839, 702070.5489921839, 183977.3761344624], 
processed observation next is [0.0, 0.9565217391304348, 0.4944707740916275, 0.79, 1.0, 1.0, 0.4005194198891421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1950195969422733, 0.1950195969422733, 0.27459309870815285], 
reward next is 0.7254, 
noisyNet noise sample is [array([-0.08640141], dtype=float32), -0.6933052]. 
=============================================
[2019-03-27 03:40:18,860] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 03:40:18,862] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:40:18,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:40:18,863] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:40:18,864] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:40:18,868] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:40:18,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:40:18,869] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:40:18,871] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:40:18,871] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:40:18,873] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:40:19,623] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-03-27 03:40:19,655] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-03-27 03:40:19,712] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-03-27 03:40:19,753] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-03-27 03:40:19,790] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-03-27 03:40:27,350] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:40:27,353] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.9, 70.0, 1.0, 2.0, 0.2474979612916923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 406970.2575743961, 406970.2575743961, 160661.5392198199]
[2019-03-27 03:40:27,354] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:40:27,357] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12592941920309808
[2019-03-27 03:40:35,940] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:40:35,942] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.52122705666667, 92.83239759333334, 1.0, 2.0, 0.2301267291279724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381187.0785003603, 381187.0785003603, 158874.4115616032]
[2019-03-27 03:40:35,943] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:40:35,947] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.29169750283364027
[2019-03-27 03:41:09,225] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:41:09,226] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.97451720666667, 60.49968999333333, 1.0, 2.0, 0.5413657761167066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756495.1437110339, 756495.1437110332, 190330.8589577517]
[2019-03-27 03:41:09,227] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:41:09,234] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8011394961815939
[2019-03-27 03:41:21,524] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:41:21,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.38553542333334, 60.20063967833334, 1.0, 2.0, 0.8062784037344534, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981920364797, 6.9112, 168.9123159997328, 2023860.324012881, 1956619.022541256, 409402.01191599]
[2019-03-27 03:41:21,530] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:41:21,533] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6249230301530951
[2019-03-27 03:41:21,535] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2023860.324012881 W.
[2019-03-27 03:41:21,697] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:41:21,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.15488675333333, 71.15427396666666, 1.0, 2.0, 0.7171371030343024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1002230.835217787, 1002230.835217788, 224601.6155833141]
[2019-03-27 03:41:21,699] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:41:21,702] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2908382329791408
[2019-03-27 03:41:22,683] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:41:22,684] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.6, 43.0, 1.0, 2.0, 0.8413076617582338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1175860.766212154, 1175860.766212154, 254303.8837654542]
[2019-03-27 03:41:22,687] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:41:22,689] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5928005080338513
[2019-03-27 03:41:59,563] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2237281], dtype=float32), -0.030937456]
[2019-03-27 03:41:59,563] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 76.5, 1.0, 2.0, 0.6421103498592826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 897333.3307124211, 897333.3307124211, 208847.2483722198]
[2019-03-27 03:41:59,565] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:41:59,567] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8493515235466715
[2019-03-27 03:42:16,149] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:42:16,338] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:42:16,636] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:42:16,680] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:42:16,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:42:17,783] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2000000, evaluation results [2000000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:42:30,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:42:30,717] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2466
[2019-03-27 03:42:30,722] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.5000131816004845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698690.7412866844, 698690.7412866838, 183598.1716212607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3630600.0000, 
sim time next is 3631200.0000, 
raw observation next is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5019401521394582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701384.2721856466, 701384.2721856473, 183900.7382228879], 
processed observation next is [1.0, 0.0, 0.4944707740916275, 0.8066666666666668, 1.0, 1.0, 0.3999278941439255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19482896449601295, 0.19482896449601314, 0.27447871376550437], 
reward next is 0.7255, 
noisyNet noise sample is [array([1.4765513], dtype=float32), -0.8843559]. 
=============================================
[2019-03-27 03:42:32,745] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:42:32,753] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9530
[2019-03-27 03:42:32,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2275238.293480936 W.
[2019-03-27 03:42:32,764] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.9858797235353296, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005117771847504, 6.9112, 168.912398115495, 2275238.293480936, 2208610.013993548, 459009.777951619], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3664800.0000, 
sim time next is 3665400.0000, 
raw observation next is [31.16666666666667, 68.83333333333334, 1.0, 2.0, 0.873161888919469, 1.0, 1.0, 0.873161888919469, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2442155.260716264, 2442155.260716264, 457061.5882073686], 
processed observation next is [1.0, 0.43478260869565216, 0.6761453396524489, 0.6883333333333335, 1.0, 1.0, 0.8471829986981554, 1.0, 0.5, 0.8471829986981554, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6783764613100733, 0.6783764613100733, 0.682181474936371], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7645839], dtype=float32), -0.13966775]. 
=============================================
[2019-03-27 03:42:42,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:42:43,006] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4507
[2019-03-27 03:42:43,016] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3126528.840938243 W.
[2019-03-27 03:42:43,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 0.8488802355393983, 1.0, 2.0, 0.7450301572839617, 1.0, 1.0, 1.03, 7.00510947416176, 6.9112, 170.5573041426782, 3126528.840938243, 3059257.664603001, 572477.1235316852], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4027200.0000, 
sim time next is 4027800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.811816815534441, 1.0, 2.0, 0.7264984472814832, 1.0, 2.0, 1.03, 7.005106550464816, 6.9112, 170.5573041426782, 3048665.359015731, 2981396.277043538, 558972.1688962614], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.6, 1.0, 1.0, 0.7732732717282421, 1.0, 1.0, 0.6704800569656424, 1.0, 1.0, 1.0365853658536586, 0.009390655046481556, 0.0, 0.8375144448122397, 0.8468514886154809, 0.828165632512094, 0.8342868192481513], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11401593], dtype=float32), -1.2497433]. 
=============================================
[2019-03-27 03:42:43,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:42:43,930] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2817
[2019-03-27 03:42:43,940] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2639138.849905808 W.
[2019-03-27 03:42:43,946] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 64.0, 1.0, 2.0, 0.6290110609975991, 1.0, 2.0, 0.6290110609975991, 1.0, 1.0, 1.03, 6.981332697691689, 6.9112, 170.5573041426782, 2639138.849905808, 2588899.945733508, 498737.1189250873], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4016400.0000, 
sim time next is 4017000.0000, 
raw observation next is [32.66666666666666, 63.5, 1.0, 2.0, 0.9496921171292115, 1.0, 2.0, 0.9496921171292115, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2656430.963681183, 2656430.963681182, 499328.4787396114], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458132, 0.635, 1.0, 1.0, 0.9393880929267608, 1.0, 1.0, 0.9393880929267608, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7378974899114397, 0.7378974899114394, 0.7452663861785245], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1541406], dtype=float32), -0.082884565]. 
=============================================
[2019-03-27 03:42:43,963] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[20.186775]
 [20.36993 ]
 [20.325064]
 [20.243725]
 [19.85702 ]], R is [[19.89571381]
 [19.69675636]
 [19.78328896]
 [19.58545685]
 [19.38960266]].
[2019-03-27 03:43:02,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:02,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5950
[2019-03-27 03:43:02,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3208194.515736371 W.
[2019-03-27 03:43:02,911] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.8877510363164354, 1.0, 2.0, 0.7644655576724801, 1.0, 2.0, 1.03, 7.005112540822907, 6.9112, 170.5573041426782, 3208194.515736371, 3140921.142627005, 587192.8143721684], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4363800.0000, 
sim time next is 4364400.0000, 
raw observation next is [37.0, 54.00000000000001, 1.0, 2.0, 0.6404492781971619, 1.0, 2.0, 0.6404492781971619, 1.0, 2.0, 1.03, 7.003666155306047, 6.9112, 170.5573041426782, 2687181.73704914, 2620944.468720702, 503112.6866906253], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.566806359273689, 1.0, 1.0, 0.566806359273689, 1.0, 1.0, 1.0365853658536586, 0.009246615530604707, 0.0, 0.8375144448122397, 0.7464393714025389, 0.728040130200195, 0.750914457747202], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.196478], dtype=float32), -1.2475002]. 
=============================================
[2019-03-27 03:43:03,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:03,310] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8870
[2019-03-27 03:43:03,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2040815.741134773 W.
[2019-03-27 03:43:03,323] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.16666666666667, 77.66666666666667, 1.0, 2.0, 0.7297934089475581, 1.0, 1.0, 0.7297934089475581, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2040815.741134773, 2040815.741134773, 387161.0067630804], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4176600.0000, 
sim time next is 4177200.0000, 
raw observation next is [32.33333333333334, 76.33333333333334, 1.0, 2.0, 0.6459170521994602, 1.0, 2.0, 0.6435485656139928, 1.0, 1.0, 1.03, 7.005093468153397, 6.9112, 170.5573041426782, 2700199.705821628, 2632939.995241258, 504791.5690601734], 
processed observation next is [1.0, 0.34782608695652173, 0.7314375987361774, 0.7633333333333334, 1.0, 1.0, 0.5733940387945303, 1.0, 1.0, 0.5705404404987865, 1.0, 0.5, 1.0365853658536586, 0.009389346815339738, 0.0, 0.8375144448122397, 0.7500554738393411, 0.7313722209003494, 0.753420252328617], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1145052], dtype=float32), 0.12326818]. 
=============================================
[2019-03-27 03:43:13,941] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 03:43:13,945] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:43:13,946] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:43:13,947] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:43:13,948] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:43:13,949] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:43:13,950] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:43:13,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:43:13,950] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:43:13,952] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:43:13,953] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:43:13,983] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-03-27 03:43:13,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-03-27 03:43:14,025] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-03-27 03:43:14,026] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-03-27 03:43:14,068] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-03-27 03:43:26,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:26,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.13333333333333, 82.0, 1.0, 2.0, 0.2974824761847182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474551.9814762044, 474551.9814762044, 165246.6110955798]
[2019-03-27 03:43:26,515] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:43:26,519] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6647034614933758
[2019-03-27 03:43:28,731] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:28,732] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.5, 76.0, 1.0, 2.0, 0.379302958454551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610255.2996098516, 610255.2996098516, 175996.4569478661]
[2019-03-27 03:43:28,735] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:43:28,738] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3481654426566214
[2019-03-27 03:43:29,546] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:29,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.52224601833334, 98.35973275333333, 1.0, 2.0, 0.3260901501386336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514090.7364392683, 514090.736439269, 168064.4377569428]
[2019-03-27 03:43:29,548] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:43:29,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7327713319233389
[2019-03-27 03:43:33,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:33,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 91.0, 1.0, 2.0, 0.7091606161431733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1130087.578516714, 1130087.578516714, 239288.0164668727]
[2019-03-27 03:43:33,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:43:33,530] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04244329970408689
[2019-03-27 03:43:37,425] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:37,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.6, 84.66666666666667, 1.0, 2.0, 0.3620404534379976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551517.0714856548, 551517.0714856548, 170590.6122349397]
[2019-03-27 03:43:37,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:43:37,430] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2167523178685551
[2019-03-27 03:43:37,451] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:37,452] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.8, 84.0, 1.0, 2.0, 0.3657361060723174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555317.5316026412, 555317.5316026412, 170855.9052261979]
[2019-03-27 03:43:37,455] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:43:37,458] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19187398640361453
[2019-03-27 03:43:57,213] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:43:57,214] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.15, 62.5, 1.0, 2.0, 0.6881224405477329, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.997376108168464, 6.9112, 168.911355136875, 1858508.026188202, 1797372.308701342, 382426.0451033798]
[2019-03-27 03:43:57,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:43:57,221] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.28746923765640764
[2019-03-27 03:43:57,223] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1858508.026188202 W.
[2019-03-27 03:44:10,019] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:44:10,019] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.51666666666667, 72.0, 1.0, 2.0, 0.5702809900745125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 796915.9193665325, 796915.9193665319, 195339.5082018498]
[2019-03-27 03:44:10,020] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:44:10,023] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7310664983624114
[2019-03-27 03:44:29,961] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:44:29,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.13333333333333, 74.66666666666667, 1.0, 2.0, 0.579816643840743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810246.2123668478, 810246.2123668478, 197045.1144625258]
[2019-03-27 03:44:29,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:44:29,969] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8888393346380775
[2019-03-27 03:44:59,609] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17867313], dtype=float32), -0.036299914]
[2019-03-27 03:44:59,611] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.78333333333333, 82.66666666666667, 1.0, 2.0, 0.7662928381511277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1070962.791029041, 1070962.791029041, 235807.5338189714]
[2019-03-27 03:44:59,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:44:59,615] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.003635537734833161
[2019-03-27 03:45:08,418] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:45:08,499] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:45:08,977] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:45:09,008] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:45:09,040] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:45:10,057] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2025000, evaluation results [2025000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:45:17,809] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:17,822] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5967
[2019-03-27 03:45:17,825] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 84.0, 1.0, 2.0, 0.513577110352523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717650.6268661107, 717650.6268661113, 185750.8214494177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5037000.0000, 
sim time next is 5037600.0000, 
raw observation next is [27.33333333333334, 84.0, 1.0, 2.0, 0.5187711790941363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724911.0722911513, 724911.0722911519, 186589.6351240006], 
processed observation next is [0.0, 0.30434782608695654, 0.4944707740916275, 0.84, 1.0, 1.0, 0.4202062398724533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20136418674754203, 0.20136418674754217, 0.27849199272238895], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.48400432], dtype=float32), -0.24375731]. 
=============================================
[2019-03-27 03:45:20,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:20,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2761
[2019-03-27 03:45:20,714] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 93.16666666666667, 1.0, 2.0, 0.8433098547727365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104289, 1178660.701405936, 1178660.701405936, 254821.0991854982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4675800.0000, 
sim time next is 4676400.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.8323788100031557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1163374.444399417, 1163374.444399417, 252015.7520095047], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.94, 1.0, 1.0, 0.7980467590399466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3231595678887269, 0.3231595678887269, 0.37614291344702194], 
reward next is 0.6239, 
noisyNet noise sample is [array([0.74937826], dtype=float32), -0.85666454]. 
=============================================
[2019-03-27 03:45:21,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:21,701] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2929
[2019-03-27 03:45:21,706] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5817448407078488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812941.7405118359, 812941.7405118353, 197394.2623503959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5820091056976335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813311.1710184453, 813311.1710184453, 197442.0592235731], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49639651288871506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259197697273459, 0.2259197697273459, 0.29468964063219866], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.40001655], dtype=float32), -0.61416465]. 
=============================================
[2019-03-27 03:45:21,724] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.822174]
 [73.87002 ]
 [73.909065]
 [73.92955 ]
 [73.93018 ]], R is [[73.9173584 ]
 [73.88356781]
 [73.85023499]
 [73.81742859]
 [73.78511047]].
[2019-03-27 03:45:30,399] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:30,411] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3003
[2019-03-27 03:45:30,418] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 89.83333333333333, 1.0, 2.0, 0.9787719358586844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1368112.660358884, 1368112.660358883, 292529.9889960755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4603800.0000, 
sim time next is 4604400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.001329001628105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399663.372522734, 1399663.372522734, 299336.5388062986], 
processed observation next is [1.0, 0.30434782608695654, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0016012067808493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.388795381256315, 0.388795381256315, 0.44677095344223666], 
reward next is 0.5532, 
noisyNet noise sample is [array([1.2448652], dtype=float32), 0.3922456]. 
=============================================
[2019-03-27 03:45:31,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:31,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8981
[2019-03-27 03:45:31,335] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 69.33333333333333, 1.0, 2.0, 0.477786027618984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667621.9867698707, 667621.9867698707, 180190.1178127909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4902600.0000, 
sim time next is 4903200.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.4822587121336495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673873.7594510508, 673873.7594510501, 180864.0671064137], 
processed observation next is [1.0, 0.782608695652174, 0.5734597156398105, 0.7, 1.0, 1.0, 0.3762153158236741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18718715540306965, 0.18718715540306946, 0.26994636881554285], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.5253017], dtype=float32), 0.9590609]. 
=============================================
[2019-03-27 03:45:42,672] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:42,680] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3929
[2019-03-27 03:45:42,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1937229.027086124 W.
[2019-03-27 03:45:42,696] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.56666666666667, 64.16666666666667, 1.0, 2.0, 0.6927843776127355, 1.0, 2.0, 0.6927843776127355, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1937229.027086124, 1937229.027086124, 371045.6594922715], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4974600.0000, 
sim time next is 4975200.0000, 
raw observation next is [30.6, 64.0, 1.0, 2.0, 0.5019865378792988, 1.0, 2.0, 0.5019865378792988, 1.0, 1.0, 0.8594338916266968, 6.911199999999999, 6.9112, 170.5573041426782, 2105718.592209806, 2105718.592209807, 414265.72356235], 
processed observation next is [1.0, 0.6086956521739131, 0.6492890995260664, 0.64, 1.0, 1.0, 0.3999837805774684, 1.0, 1.0, 0.3999837805774684, 1.0, 0.5, 0.8285779166179229, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5849218311693906, 0.5849218311693909, 0.6183070500930596], 
reward next is 0.3817, 
noisyNet noise sample is [array([-0.18904915], dtype=float32), -1.0838255]. 
=============================================
[2019-03-27 03:45:51,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:45:51,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8559
[2019-03-27 03:45:51,759] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4941889482615396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 690549.6347464543, 690549.6347464537, 182690.9289405063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5029200.0000, 
sim time next is 5029800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4964281780333137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 693679.6201163708, 693679.6201163714, 183038.5277597571], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.39328696148592013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19268878336565856, 0.19268878336565873, 0.2731918324772494], 
reward next is 0.7268, 
noisyNet noise sample is [array([-1.42253], dtype=float32), -0.06132248]. 
=============================================
[2019-03-27 03:46:00,468] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:46:00,477] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5670
[2019-03-27 03:46:00,481] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.0, 1.0, 2.0, 0.5027379709191163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702499.4700279715, 702499.4700279721, 184026.252663648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5122200.0000, 
sim time next is 5122800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5081391713278461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710049.3444636571, 710049.3444636571, 184881.266525944], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4073965919612604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19723592901768253, 0.19723592901768253, 0.2759421888446925], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.13687763], dtype=float32), -0.27478528]. 
=============================================
[2019-03-27 03:46:02,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:46:02,909] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4135
[2019-03-27 03:46:02,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3132549.490251297 W.
[2019-03-27 03:46:02,923] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.28333333333333, 63.33333333333334, 1.0, 2.0, 0.8517459995009042, 1.0, 2.0, 0.7464630392647146, 1.0, 1.0, 1.03, 7.005109700238785, 6.9112, 170.5573041426782, 3132549.490251297, 3065278.151967885, 573543.6663938725], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5393400.0000, 
sim time next is 5394000.0000, 
raw observation next is [34.46666666666667, 62.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.962943290247956, 6.9112, 170.5573041426782, 3663613.926309956, 2910207.42532362, 547545.221190937], 
processed observation next is [1.0, 0.43478260869565216, 0.8325434439178516, 0.6266666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.10517432902479565, 0.0, 0.8375144448122397, 1.0176705350860988, 0.8083909514787834, 0.8172316734193089], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7665176], dtype=float32), 0.9712066]. 
=============================================
[2019-03-27 03:46:02,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[12.250032]
 [12.383623]
 [12.385218]
 [12.160603]
 [12.411922]], R is [[11.97139072]
 [11.85167694]
 [11.95870972]
 [11.83912277]
 [11.72073174]].
[2019-03-27 03:46:06,210] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 03:46:06,213] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:46:06,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:06,214] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:46:06,215] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:46:06,215] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:06,216] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:06,216] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:46:06,217] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:46:06,218] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:06,218] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:06,230] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-03-27 03:46:06,252] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-03-27 03:46:06,274] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-03-27 03:46:06,301] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-03-27 03:46:06,302] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-03-27 03:46:11,704] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.16751668], dtype=float32), -0.03167137]
[2019-03-27 03:46:11,706] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.61666666666667, 50.66666666666667, 1.0, 2.0, 0.2428006691772058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401452.0672106271, 401452.0672106271, 160122.4089453641]
[2019-03-27 03:46:11,708] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:46:11,711] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7038186252792666
[2019-03-27 03:46:30,192] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.16751668], dtype=float32), -0.03167137]
[2019-03-27 03:46:30,194] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.18333333333333, 95.16666666666667, 1.0, 2.0, 0.4220320413904774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622330.594883517, 622330.5948835163, 176436.4455362703]
[2019-03-27 03:46:30,196] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:46:30,198] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09645135507422975
[2019-03-27 03:47:07,447] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.16751668], dtype=float32), -0.03167137]
[2019-03-27 03:47:07,448] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 79.0, 1.0, 2.0, 0.5600890667431703, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9726894846481219, 6.9112, 6.9112, 168.9124739698369, 1565913.905150956, 1565913.905150956, 342670.3842678004]
[2019-03-27 03:47:07,450] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:47:07,452] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2712067525938121
[2019-03-27 03:47:14,056] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.16751668], dtype=float32), -0.03167137]
[2019-03-27 03:47:14,059] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.13946188833333, 74.70455881000001, 1.0, 2.0, 0.7623102595207866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1065393.98896129, 1065393.98896129, 234874.7571963266]
[2019-03-27 03:47:14,061] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:47:14,064] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14031654824587736
[2019-03-27 03:47:32,015] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.16751668], dtype=float32), -0.03167137]
[2019-03-27 03:47:32,016] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.43333333333333, 82.66666666666667, 1.0, 2.0, 0.6067210474593665, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.938062582076148, 6.9112, 168.9123949752855, 1696393.197151924, 1677336.020242375, 365817.9955790116]
[2019-03-27 03:47:32,019] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:47:32,022] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9100245770577091
[2019-03-27 03:47:32,024] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1696393.197151924 W.
[2019-03-27 03:47:51,740] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.16751668], dtype=float32), -0.03167137]
[2019-03-27 03:47:51,741] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.3, 87.33333333333334, 1.0, 2.0, 0.3723478901464754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565879.6792143573, 565879.6792143566, 171782.4665505339]
[2019-03-27 03:47:51,744] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:47:51,748] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8761558328461404
[2019-03-27 03:48:01,835] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:48:01,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:48:01,938] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:48:02,013] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:48:02,030] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:48:03,047] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2050000, evaluation results [2050000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:48:08,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:08,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1371
[2019-03-27 03:48:08,940] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 82.66666666666667, 1.0, 2.0, 0.5529708411243391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772717.758790971, 772717.758790971, 192311.8374464314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5266200.0000, 
sim time next is 5266800.0000, 
raw observation next is [28.5, 83.0, 1.0, 2.0, 0.5541845953824116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774414.4691675132, 774414.4691675126, 192521.3302356667], 
processed observation next is [1.0, 1.0, 0.5497630331753555, 0.83, 1.0, 1.0, 0.4628730064848332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2151151303243092, 0.21511513032430907, 0.2873452690084578], 
reward next is 0.7127, 
noisyNet noise sample is [array([0.47178966], dtype=float32), 1.131829]. 
=============================================
[2019-03-27 03:48:14,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:14,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2619
[2019-03-27 03:48:14,967] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 88.5, 1.0, 2.0, 0.5009055488701574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699938.0967232737, 699938.0967232744, 183738.0839278941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5635800.0000, 
sim time next is 5636400.0000, 
raw observation next is [26.26666666666667, 87.66666666666666, 1.0, 2.0, 0.5021355239562239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701657.3644563508, 701657.3644563508, 183931.39000548], 
processed observation next is [0.0, 0.21739130434782608, 0.44391785150079005, 0.8766666666666666, 1.0, 1.0, 0.40016328187496847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19490482346009744, 0.19490482346009744, 0.2745244626947463], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.7923398], dtype=float32), 0.42714986]. 
=============================================
[2019-03-27 03:48:19,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:19,105] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4699
[2019-03-27 03:48:19,118] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 87.66666666666667, 1.0, 2.0, 0.5097269183795204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712268.7299873319, 712268.7299873313, 185133.8424928221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5710800.0000, 
sim time next is 5711400.0000, 
raw observation next is [26.3, 88.0, 1.0, 2.0, 0.5099865169177973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712631.6026108376, 712631.6026108376, 185175.2902713618], 
processed observation next is [0.0, 0.08695652173913043, 0.4454976303317536, 0.88, 1.0, 1.0, 0.4096223095395148, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19795322294745488, 0.19795322294745488, 0.2763810302557639], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.5089056], dtype=float32), 0.33591935]. 
=============================================
[2019-03-27 03:48:19,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:19,319] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6055
[2019-03-27 03:48:19,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3252449.561106212 W.
[2019-03-27 03:48:19,334] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 65.0, 1.0, 2.0, 0.9088142594067199, 1.0, 2.0, 0.7749971692176225, 1.0, 1.0, 1.03, 7.005114202745889, 6.9112, 170.5573041426782, 3252449.561106212, 3185174.997493811, 595402.3236673953], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5482800.0000, 
sim time next is 5483400.0000, 
raw observation next is [35.11666666666667, 63.83333333333334, 1.0, 2.0, 0.8519459200426767, 1.0, 2.0, 0.7465629995356009, 1.0, 2.0, 1.03, 7.005109716010382, 6.9112, 170.5573041426782, 3132969.502015124, 3065698.152433876, 573619.0600531973], 
processed observation next is [1.0, 0.4782608695652174, 0.863349131121643, 0.6383333333333334, 1.0, 1.0, 0.8216215904128634, 1.0, 1.0, 0.6946542163079529, 1.0, 1.0, 1.0365853658536586, 0.009390971601038167, 0.0, 0.8375144448122397, 0.8702693061153123, 0.8515828201205211, 0.8561478508256677], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8955394], dtype=float32), 0.8270424]. 
=============================================
[2019-03-27 03:48:20,445] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:20,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9953
[2019-03-27 03:48:20,458] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 69.0, 1.0, 2.0, 0.519301015276218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725651.6980632185, 725651.6980632185, 186675.3680298512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5734800.0000, 
sim time next is 5735400.0000, 
raw observation next is [29.86666666666667, 68.16666666666667, 1.0, 2.0, 0.5174264080171369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723031.3009158104, 723031.3009158111, 186371.6084840485], 
processed observation next is [0.0, 0.391304347826087, 0.6145339652448659, 0.6816666666666668, 1.0, 1.0, 0.4185860337555866, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20084202803216955, 0.20084202803216974, 0.2781665798269381], 
reward next is 0.7218, 
noisyNet noise sample is [array([1.379828], dtype=float32), -1.1758497]. 
=============================================
[2019-03-27 03:48:33,201] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:33,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0334
[2019-03-27 03:48:33,215] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 64.5, 1.0, 2.0, 0.5448891107539391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761420.3550409013, 761420.3550409013, 190928.5325565219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6348600.0000, 
sim time next is 6349200.0000, 
raw observation next is [31.66666666666666, 64.0, 1.0, 2.0, 0.5462490005464246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763321.3286423986, 763321.3286423986, 191160.036054712], 
processed observation next is [0.0, 0.4782608695652174, 0.6998420221169034, 0.64, 1.0, 1.0, 0.4533120488511139, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21203370240066627, 0.21203370240066627, 0.2853134866488239], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.1775511], dtype=float32), 1.0220952]. 
=============================================
[2019-03-27 03:48:37,711] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:37,721] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9611
[2019-03-27 03:48:37,726] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.6993744570760846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977395.3280066288, 977395.3280066282, 220709.5056729922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6065400.0000, 
sim time next is 6066000.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.6735150494075706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 941240.0690682312, 941240.0690682305, 215231.6120407878], 
processed observation next is [1.0, 0.21739130434782608, 0.4360189573459717, 0.93, 1.0, 1.0, 0.6066446378404464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2614555747411753, 0.26145557474117515, 0.3212412120011758], 
reward next is 0.6788, 
noisyNet noise sample is [array([-0.1352428], dtype=float32), -1.4231035]. 
=============================================
[2019-03-27 03:48:37,756] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[40.034622]
 [39.868988]
 [40.486523]
 [41.978653]
 [41.64098 ]], R is [[40.7636528 ]
 [41.02659988]
 [41.28238678]
 [41.53011322]
 [41.79031372]].
[2019-03-27 03:48:47,743] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:47,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3830
[2019-03-27 03:48:47,756] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666666, 78.0, 1.0, 2.0, 0.5257320437657211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734641.2827991112, 734641.2827991118, 187726.511673101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6115200.0000, 
sim time next is 6115800.0000, 
raw observation next is [28.38333333333333, 79.0, 1.0, 2.0, 0.5254278565227833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734216.0742925534, 734216.074292554, 187676.5075501182], 
processed observation next is [1.0, 0.782608695652174, 0.5442338072669825, 0.79, 1.0, 1.0, 0.4282263331599799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20394890952570927, 0.20394890952570946, 0.28011419037331076], 
reward next is 0.7199, 
noisyNet noise sample is [array([-1.5957259], dtype=float32), -2.6670518]. 
=============================================
[2019-03-27 03:48:51,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:51,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3713
[2019-03-27 03:48:51,333] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 77.33333333333334, 1.0, 2.0, 0.541993001003538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757371.9293559034, 757371.9293559034, 190437.4756021247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6254400.0000, 
sim time next is 6255000.0000, 
raw observation next is [29.25, 76.5, 1.0, 2.0, 0.5417929320526041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757092.2567262761, 757092.2567262755, 190403.6895709978], 
processed observation next is [0.0, 0.391304347826087, 0.5853080568720379, 0.765, 1.0, 1.0, 0.4479432916296435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21030340464618782, 0.21030340464618766, 0.2841846112999967], 
reward next is 0.7158, 
noisyNet noise sample is [array([1.9971606], dtype=float32), 1.2986377]. 
=============================================
[2019-03-27 03:48:51,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.82818 ]
 [68.833565]
 [68.84497 ]
 [68.68775 ]
 [68.71832 ]], R is [[68.8506546 ]
 [68.87791443]
 [68.9048233 ]
 [68.93144989]
 [68.95783234]].
[2019-03-27 03:48:57,380] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:57,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4396
[2019-03-27 03:48:57,394] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 82.0, 1.0, 2.0, 0.5238429018108497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732000.5461796583, 732000.546179659, 187415.5353258191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6384000.0000, 
sim time next is 6384600.0000, 
raw observation next is [27.45, 82.0, 1.0, 2.0, 0.5227449343679943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730465.7556748218, 730465.7556748218, 187235.9598224992], 
processed observation next is [0.0, 0.9130434782608695, 0.5, 0.82, 1.0, 1.0, 0.42499389682890876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20290715435411716, 0.20290715435411716, 0.2794566564514913], 
reward next is 0.7205, 
noisyNet noise sample is [array([0.21900713], dtype=float32), -1.4252031]. 
=============================================
[2019-03-27 03:48:57,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:57,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4029
[2019-03-27 03:48:57,627] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 85.0, 1.0, 2.0, 0.9583710299077124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104176, 1339578.590226802, 1339578.590226803, 286496.9089171372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6404400.0000, 
sim time next is 6405000.0000, 
raw observation next is [26.86666666666667, 85.16666666666667, 1.0, 2.0, 0.9363101816634012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1308723.676941654, 1308723.676941655, 280119.2126122009], 
processed observation next is [1.0, 0.13043478260869565, 0.4723538704581361, 0.8516666666666667, 1.0, 1.0, 0.9232652791125315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.363534354706015, 0.3635343547060153, 0.4180883770331356], 
reward next is 0.5819, 
noisyNet noise sample is [array([-0.62538224], dtype=float32), 0.7597266]. 
=============================================
[2019-03-27 03:48:57,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[33.403313]
 [33.243572]
 [33.003845]
 [34.511703]
 [35.15704 ]], R is [[33.737854  ]
 [33.97286606]
 [33.63313675]
 [33.29680634]
 [32.96384048]].
[2019-03-27 03:48:58,804] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:48:58,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6935
[2019-03-27 03:48:58,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2250169.141356412 W.
[2019-03-27 03:48:58,826] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.1, 75.5, 1.0, 2.0, 0.8045814106710664, 1.0, 1.0, 0.8045814106710664, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2250169.141356412, 2250169.141356412, 422104.4003282908], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6195000.0000, 
sim time next is 6195600.0000, 
raw observation next is [29.0, 76.0, 1.0, 2.0, 0.9617874381728555, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98874037757809, 6.9112, 168.9124950481685, 2241517.757035579, 2186508.094732071, 452407.9383697992], 
processed observation next is [1.0, 0.7391304347826086, 0.5734597156398105, 0.76, 1.0, 1.0, 0.9539607688829584, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007754037757808962, 0.0, 0.8294376791613355, 0.622643821398772, 0.6073633596477974, 0.675235728910148], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1702756], dtype=float32), -1.5378975]. 
=============================================
[2019-03-27 03:48:59,361] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 03:48:59,364] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:48:59,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:48:59,365] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:48:59,366] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:48:59,367] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:48:59,368] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:48:59,369] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:48:59,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:48:59,371] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:48:59,373] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:48:59,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-03-27 03:48:59,422] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-03-27 03:48:59,444] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-03-27 03:48:59,464] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-03-27 03:48:59,484] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-03-27 03:49:08,626] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1871021], dtype=float32), -0.007829512]
[2019-03-27 03:49:08,628] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.76666666666667, 57.33333333333333, 1.0, 2.0, 0.2982273284774095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 478053.0056813839, 478053.0056813846, 165518.2239170283]
[2019-03-27 03:49:08,629] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:49:08,632] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4552965656402349
[2019-03-27 03:49:10,496] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1871021], dtype=float32), -0.007829512]
[2019-03-27 03:49:10,497] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.16666666666667, 69.66666666666667, 1.0, 2.0, 0.4431802604336015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669163.7191125653, 669163.7191125659, 181347.1879936122]
[2019-03-27 03:49:10,498] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:49:10,500] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5892252304994475
[2019-03-27 03:49:17,350] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1871021], dtype=float32), -0.007829512]
[2019-03-27 03:49:17,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.68597876333333, 76.51848122166668, 1.0, 2.0, 0.2700570151827015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 441726.186570967, 441726.1865709677, 162938.3262078434]
[2019-03-27 03:49:17,353] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:49:17,356] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4416184853515417
[2019-03-27 03:49:47,446] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1871021], dtype=float32), -0.007829512]
[2019-03-27 03:49:47,447] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.5369806270146702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750365.2480657027, 750365.2480657027, 189592.9827728655]
[2019-03-27 03:49:47,448] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:49:47,449] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03059795157563161
[2019-03-27 03:50:02,642] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1871021], dtype=float32), -0.007829512]
[2019-03-27 03:50:02,643] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333333, 75.66666666666666, 1.0, 2.0, 0.6294337730479428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 879610.7923422247, 879610.7923422254, 206358.3176680486]
[2019-03-27 03:50:02,646] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:50:02,649] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8210432667142169
[2019-03-27 03:50:54,576] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:50:54,648] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:50:54,664] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:50:54,684] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:50:54,878] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:50:55,896] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2075000, evaluation results [2075000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:50:56,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:50:56,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5515
[2019-03-27 03:50:56,347] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.98333333333333, 89.83333333333333, 1.0, 2.0, 0.6892076481477727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963180.4855339851, 963180.4855339851, 218533.4147144271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6156600.0000, 
sim time next is 6157200.0000, 
raw observation next is [27.06666666666667, 89.66666666666667, 1.0, 2.0, 0.6813980085189678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 952261.4760156533, 952261.4760156539, 216881.8717542713], 
processed observation next is [1.0, 0.2608695652173913, 0.48183254344391807, 0.8966666666666667, 1.0, 1.0, 0.6161421789385154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2645170766710148, 0.264517076671015, 0.3237042862004049], 
reward next is 0.6763, 
noisyNet noise sample is [array([2.9920485], dtype=float32), 0.3297132]. 
=============================================
[2019-03-27 03:50:58,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:50:58,132] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5783
[2019-03-27 03:50:58,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2118125.508219883 W.
[2019-03-27 03:50:58,146] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.7574119897785846, 1.0, 2.0, 0.7574119897785846, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2118125.508219883, 2118125.508219884, 399664.0213716471], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6452400.0000, 
sim time next is 6453000.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.9051525459888402, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.97874070137656, 6.9112, 168.9125544094342, 2162248.664287879, 2114333.080224177, 436446.2629946704], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.885725959022699, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006754070137655966, 0.0, 0.8294379706523257, 0.6006246289688553, 0.5873147445067158, 0.6514123328278663], 
reward next is 0.0109, 
noisyNet noise sample is [array([2.2752252], dtype=float32), -1.268927]. 
=============================================
[2019-03-27 03:50:58,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[40.513012]
 [40.121586]
 [39.64661 ]
 [39.105583]
 [38.994892]], R is [[40.41872406]
 [40.41802597]
 [40.42804718]
 [40.42446518]
 [40.43099976]].
[2019-03-27 03:51:00,349] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:00,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4200
[2019-03-27 03:51:00,361] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 88.66666666666667, 1.0, 2.0, 0.6540641099500806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914045.6032298539, 914045.6032298539, 211242.1877923063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6505800.0000, 
sim time next is 6506400.0000, 
raw observation next is [26.93333333333334, 88.33333333333334, 1.0, 2.0, 0.6913109286306033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 966121.196061123, 966121.196061123, 218980.6237337792], 
processed observation next is [1.0, 0.30434782608695654, 0.4755134281200636, 0.8833333333333334, 1.0, 1.0, 0.6280854561814497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2683669989058675, 0.2683669989058675, 0.3268367518414615], 
reward next is 0.6732, 
noisyNet noise sample is [array([1.7252102], dtype=float32), -0.07078562]. 
=============================================
[2019-03-27 03:51:00,815] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:00,825] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5405
[2019-03-27 03:51:00,830] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.81666666666666, 45.16666666666667, 1.0, 2.0, 0.3151149670062715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499939.8225847081, 499939.8225847087, 167050.0548113685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6868200.0000, 
sim time next is 6868800.0000, 
raw observation next is [28.9, 44.0, 1.0, 2.0, 0.3082228783828225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490716.1005350184, 490716.1005350178, 166395.1013074031], 
processed observation next is [0.0, 0.5217391304347826, 0.5687203791469194, 0.44, 1.0, 1.0, 0.1665335884130391, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13631002792639402, 0.13631002792639382, 0.24835089747373595], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.4503592], dtype=float32), -0.91398525]. 
=============================================
[2019-03-27 03:51:00,914] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:00,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9833
[2019-03-27 03:51:00,933] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 87.33333333333334, 1.0, 2.0, 0.5239686055046059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732176.260853341, 732176.2608533415, 187436.7961721911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6214800.0000, 
sim time next is 6215400.0000, 
raw observation next is [26.9, 87.5, 1.0, 2.0, 0.5230926273526413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730951.7770331706, 730951.7770331706, 187293.4710517119], 
processed observation next is [1.0, 0.9565217391304348, 0.4739336492890995, 0.875, 1.0, 1.0, 0.4254128040393268, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20304216028699182, 0.20304216028699182, 0.2795424941070327], 
reward next is 0.7205, 
noisyNet noise sample is [array([1.1675667], dtype=float32), 1.0189594]. 
=============================================
[2019-03-27 03:51:03,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:03,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6136
[2019-03-27 03:51:03,458] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 73.0, 1.0, 2.0, 0.5260816389337305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735129.9650390347, 735129.9650390347, 187783.6821021277], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6339600.0000, 
sim time next is 6340200.0000, 
raw observation next is [29.43333333333333, 72.5, 1.0, 2.0, 0.528160415660438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738035.7915012733, 738035.7915012733, 188126.0752981704], 
processed observation next is [0.0, 0.391304347826087, 0.5939968404423379, 0.725, 1.0, 1.0, 0.4315185730848651, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20500994208368703, 0.20500994208368703, 0.2807851870121946], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.06578416], dtype=float32), -0.8796544]. 
=============================================
[2019-03-27 03:51:12,349] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:12,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1259
[2019-03-27 03:51:12,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 90.0, 1.0, 2.0, 0.4974812046635599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695151.539223384, 695151.539223384, 183202.4392411917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6654000.0000, 
sim time next is 6654600.0000, 
raw observation next is [25.75, 90.5, 1.0, 2.0, 0.4968094246478205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694212.5258494577, 694212.5258494583, 183097.758938083], 
processed observation next is [1.0, 0.0, 0.41943127962085314, 0.905, 1.0, 1.0, 0.3937462947564102, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19283681273596048, 0.19283681273596065, 0.2732802372210194], 
reward next is 0.7267, 
noisyNet noise sample is [array([0.1501751], dtype=float32), -0.47989094]. 
=============================================
[2019-03-27 03:51:15,760] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:15,768] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1185
[2019-03-27 03:51:15,775] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.88333333333333, 67.5, 1.0, 2.0, 0.3403410717829706, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5776431767265277, 6.9112, 6.9112, 168.912956510431, 951260.598543492, 951260.598543492, 234200.7242605214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6455400.0000, 
sim time next is 6456000.0000, 
raw observation next is [29.76666666666667, 68.0, 1.0, 2.0, 0.4760806269071955, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665238.2429108326, 665238.2429108326, 179935.4362984128], 
processed observation next is [1.0, 0.7391304347826086, 0.6097946287519749, 0.68, 1.0, 1.0, 0.36877183964722354, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18478840080856462, 0.18478840080856462, 0.2685603526841982], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.5751149], dtype=float32), -0.23739302]. 
=============================================
[2019-03-27 03:51:15,794] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[49.33237 ]
 [37.993294]
 [38.271557]
 [38.113247]
 [37.746914]], R is [[62.64881134]
 [62.67277145]
 [62.43647003]
 [62.2212677 ]
 [62.00579453]].
[2019-03-27 03:51:17,758] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:17,768] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8213
[2019-03-27 03:51:17,772] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 84.66666666666667, 1.0, 2.0, 0.3306257402689252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525818.6063668791, 525818.6063668791, 169039.1116392021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6752400.0000, 
sim time next is 6753000.0000, 
raw observation next is [21.85, 84.83333333333334, 1.0, 2.0, 0.3229107677361266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513866.1637048055, 513866.1637048061, 168120.7060968961], 
processed observation next is [1.0, 0.13043478260869565, 0.23459715639810438, 0.8483333333333334, 1.0, 1.0, 0.18422984064593564, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14274060102911262, 0.14274060102911282, 0.25092642701029266], 
reward next is 0.7491, 
noisyNet noise sample is [array([0.4853325], dtype=float32), -0.76812875]. 
=============================================
[2019-03-27 03:51:17,785] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[68.38408 ]
 [68.12544 ]
 [68.143684]
 [68.14066 ]
 [68.72296 ]], R is [[68.50912476]
 [68.5717392 ]
 [68.6306839 ]
 [68.68940735]
 [68.74299622]].
[2019-03-27 03:51:18,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:18,389] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7697
[2019-03-27 03:51:18,394] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 67.0, 1.0, 2.0, 0.4556756665210065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636717.3849302514, 636717.384930252, 176944.1996499515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6715800.0000, 
sim time next is 6716400.0000, 
raw observation next is [29.06666666666667, 67.0, 1.0, 2.0, 0.4625701751973517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646354.0395152944, 646354.0395152944, 177940.2774385937], 
processed observation next is [1.0, 0.7391304347826086, 0.5766192733017379, 0.67, 1.0, 1.0, 0.35249418698476115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17954278875424842, 0.17954278875424842, 0.2655825036396921], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.21551493], dtype=float32), -1.2924001]. 
=============================================
[2019-03-27 03:51:22,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:22,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9588
[2019-03-27 03:51:22,055] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 39.33333333333333, 1.0, 2.0, 0.2861820342438035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461700.470572328, 461700.4705723287, 164388.0575221332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6871200.0000, 
sim time next is 6871800.0000, 
raw observation next is [29.31666666666667, 38.16666666666667, 1.0, 2.0, 0.2806612041765062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454205.2490424214, 454205.2490424214, 163873.6512266393], 
processed observation next is [0.0, 0.5217391304347826, 0.5884676145339655, 0.3816666666666667, 1.0, 1.0, 0.13332675201988697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12616812473400593, 0.12616812473400593, 0.24458753914423778], 
reward next is 0.7554, 
noisyNet noise sample is [array([1.0613941], dtype=float32), -0.9392606]. 
=============================================
[2019-03-27 03:51:25,325] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:25,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9671
[2019-03-27 03:51:25,342] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 86.5, 1.0, 2.0, 0.663437006082933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 927149.8141302812, 927149.8141302812, 213149.8031518069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6593400.0000, 
sim time next is 6594000.0000, 
raw observation next is [26.86666666666667, 86.0, 1.0, 2.0, 0.6336157989283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 885457.457484027, 885457.4574840264, 207167.517779636], 
processed observation next is [1.0, 0.30434782608695654, 0.4723538704581361, 0.86, 1.0, 1.0, 0.5585732517209296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24596040485667417, 0.245960404856674, 0.30920525041736713], 
reward next is 0.6908, 
noisyNet noise sample is [array([1.6461687], dtype=float32), -1.2801934]. 
=============================================
[2019-03-27 03:51:25,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[61.39564 ]
 [61.19021 ]
 [60.988052]
 [60.022648]
 [59.687275]], R is [[62.25007248]
 [62.3094368 ]
 [62.37049866]
 [62.43017578]
 [62.48090744]].
[2019-03-27 03:51:50,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:51:50,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5945
[2019-03-27 03:51:50,682] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666666, 73.66666666666667, 1.0, 2.0, 0.3593461384240796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551516.5078251158, 551516.5078251165, 170715.8312288684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7348200.0000, 
sim time next is 7348800.0000, 
raw observation next is [24.93333333333333, 73.33333333333334, 1.0, 2.0, 0.3574353477227357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549117.5728930659, 549117.5728930659, 170529.9301087906], 
processed observation next is [1.0, 0.043478260869565216, 0.38072669826224315, 0.7333333333333334, 1.0, 1.0, 0.22582572014787436, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15253265913696273, 0.15253265913696273, 0.2545222837444636], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.8106402], dtype=float32), -1.4171973]. 
=============================================
[2019-03-27 03:51:51,805] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 03:51:51,809] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:51:51,810] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:51:51,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:51:51,811] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:51:51,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:51:51,812] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:51:51,815] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:51:51,813] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:51:51,817] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:51:51,818] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:51:51,843] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-03-27 03:51:51,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-03-27 03:51:51,890] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-03-27 03:51:51,911] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-03-27 03:51:51,933] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-03-27 03:52:11,144] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1678833], dtype=float32), 0.00756737]
[2019-03-27 03:52:11,147] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.66666666666667, 51.66666666666667, 1.0, 2.0, 0.2050125055102064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342656.6753029868, 342656.6753029868, 155796.3560960849]
[2019-03-27 03:52:11,149] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:52:11,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5737701395020296
[2019-03-27 03:52:18,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1678833], dtype=float32), 0.00756737]
[2019-03-27 03:52:18,279] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.84926787, 95.46658033333334, 1.0, 2.0, 0.4723848558497918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728116.4753571985, 728116.4753571978, 187543.1338472034]
[2019-03-27 03:52:18,279] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:52:18,281] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7923682192574033
[2019-03-27 03:52:36,112] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1678833], dtype=float32), 0.00756737]
[2019-03-27 03:52:36,113] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.77154287, 100.0, 1.0, 2.0, 0.2333663852794311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385971.9661447619, 385971.9661447619, 159223.1765087801]
[2019-03-27 03:52:36,114] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:52:36,118] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3029420100763853
[2019-03-27 03:52:42,807] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1678833], dtype=float32), 0.00756737]
[2019-03-27 03:52:42,810] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.36666666666667, 75.66666666666667, 1.0, 2.0, 0.5522907055173588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771766.9965621214, 771766.9965621207, 192194.0387918495]
[2019-03-27 03:52:42,812] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:52:42,815] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4467221e-36 1.0000000e+00 0.0000000e+00 7.8004795e-35 8.9956624e-31], sampled 0.13129746645792806
[2019-03-27 03:53:03,504] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1678833], dtype=float32), 0.00756737]
[2019-03-27 03:53:03,504] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 60.33333333333333, 1.0, 2.0, 0.5731522666331652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800929.7804915749, 800929.7804915749, 195849.4661102887]
[2019-03-27 03:53:03,505] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:53:03,508] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1649778675653777
[2019-03-27 03:53:47,375] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:53:47,422] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:53:47,441] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:53:47,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:53:47,631] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:53:48,646] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2100000, evaluation results [2100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:53:49,011] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:53:49,024] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9873
[2019-03-27 03:53:49,033] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1978300.635782381 W.
[2019-03-27 03:53:49,036] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 55.0, 1.0, 2.0, 0.4716391331088731, 1.0, 1.0, 0.4716391331088731, 1.0, 2.0, 0.7883117271230153, 6.911199999999999, 6.9112, 170.5573041426782, 1978300.635782381, 1978300.635782381, 390717.3685207037], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7053600.0000, 
sim time next is 7054200.0000, 
raw observation next is [30.3, 56.5, 1.0, 2.0, 0.7252527601567546, 1.0, 2.0, 0.7252527601567546, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2028106.118079553, 2028106.118079553, 385106.3942138229], 
processed observation next is [1.0, 0.6521739130434783, 0.6350710900473934, 0.565, 1.0, 1.0, 0.6689792291045236, 1.0, 1.0, 0.6689792291045236, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5633628105776536, 0.5633628105776536, 0.5747856630057058], 
reward next is 0.4252, 
noisyNet noise sample is [array([1.3594582], dtype=float32), -0.3892458]. 
=============================================
[2019-03-27 03:53:57,025] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:53:57,035] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0237
[2019-03-27 03:53:57,039] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 66.33333333333334, 1.0, 2.0, 0.8321700845592294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1263723.673949615, 1263723.673949614, 265646.1136713862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7297800.0000, 
sim time next is 7298400.0000, 
raw observation next is [26.56666666666667, 65.66666666666667, 1.0, 2.0, 0.7915253984119822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1203102.992149177, 1203102.992149176, 254728.7923880796], 
processed observation next is [1.0, 0.4782608695652174, 0.45813586097946307, 0.6566666666666667, 1.0, 1.0, 0.7488257812192557, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33419527559699364, 0.33419527559699336, 0.38019222744489495], 
reward next is 0.6198, 
noisyNet noise sample is [array([1.3681753], dtype=float32), 0.65734166]. 
=============================================
[2019-03-27 03:54:03,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:03,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:03,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-03-27 03:54:03,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:03,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6638
[2019-03-27 03:54:03,996] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 95.0, 1.0, 2.0, 0.4480936154768843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660530.4493628057, 660530.4493628057, 180205.2251658387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7621200.0000, 
sim time next is 7621800.0000, 
raw observation next is [23.28333333333333, 94.83333333333334, 1.0, 2.0, 0.4266384581965208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628160.9977933567, 628160.9977933567, 176974.5281979145], 
processed observation next is [1.0, 0.21739130434782608, 0.3025276461295418, 0.9483333333333335, 1.0, 1.0, 0.3092029616825551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17448916605371018, 0.17448916605371018, 0.264141086862559], 
reward next is 0.7359, 
noisyNet noise sample is [array([0.8005103], dtype=float32), -0.41644615]. 
=============================================
[2019-03-27 03:54:04,785] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2107274: loss 0.4268
[2019-03-27 03:54:04,786] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2107274: learning rate 0.0005
[2019-03-27 03:54:05,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:05,745] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0652
[2019-03-27 03:54:05,750] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 92.66666666666667, 1.0, 2.0, 0.3161169590869229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498397.8951746832, 498397.8951746839, 166874.1749414471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7442400.0000, 
sim time next is 7443000.0000, 
raw observation next is [21.3, 93.0, 1.0, 2.0, 0.3175647437520359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500191.5668919739, 500191.5668919745, 166997.5080082818], 
processed observation next is [0.0, 0.13043478260869565, 0.2085308056872039, 0.93, 1.0, 1.0, 0.17778884789401916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1389421019144372, 0.13894210191443734, 0.2492500119526594], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.50413567], dtype=float32), 0.4853544]. 
=============================================
[2019-03-27 03:54:05,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.53906 ]
 [73.99424 ]
 [74.43571 ]
 [74.692505]
 [75.18455 ]], R is [[73.17485809]
 [73.19403839]
 [73.21308899]
 [73.23202515]
 [73.25086212]].
[2019-03-27 03:54:07,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:07,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0926
[2019-03-27 03:54:07,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 73.66666666666667, 1.0, 2.0, 0.4606569003450017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709042.0398696957, 709042.0398696951, 185532.1371341849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7352400.0000, 
sim time next is 7353000.0000, 
raw observation next is [24.75, 74.5, 1.0, 2.0, 0.4143424388512938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636987.9260370054, 636987.9260370047, 178372.5192024837], 
processed observation next is [1.0, 0.08695652173913043, 0.3720379146919432, 0.745, 1.0, 1.0, 0.29438848054372746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17694109056583482, 0.17694109056583462, 0.26622764060072196], 
reward next is 0.7338, 
noisyNet noise sample is [array([-1.3337224], dtype=float32), -0.33869186]. 
=============================================
[2019-03-27 03:54:07,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.25839 ]
 [76.27277 ]
 [77.02159 ]
 [76.597755]
 [76.69638 ]], R is [[77.20977783]
 [77.1607666 ]
 [77.08335114]
 [77.0585022 ]
 [77.0338974 ]].
[2019-03-27 03:54:09,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:09,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9626
[2019-03-27 03:54:09,770] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 86.0, 1.0, 2.0, 0.6720123608028685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 939139.1270139074, 939139.1270139068, 214919.337831165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7717200.0000, 
sim time next is 7717800.0000, 
raw observation next is [27.23333333333333, 85.5, 1.0, 2.0, 0.6991511749132241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977083.1414031907, 977083.1414031901, 220661.5659767481], 
processed observation next is [1.0, 0.30434782608695654, 0.4897314375987361, 0.855, 1.0, 1.0, 0.637531536040029, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27141198372310854, 0.2714119837231084, 0.32934562086081803], 
reward next is 0.6707, 
noisyNet noise sample is [array([-0.73528475], dtype=float32), -0.5874422]. 
=============================================
[2019-03-27 03:54:10,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:10,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6213
[2019-03-27 03:54:10,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1980576.234669797 W.
[2019-03-27 03:54:10,368] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333333, 58.66666666666667, 1.0, 2.0, 0.4721811489758346, 1.0, 2.0, 0.4721811489758346, 1.0, 1.0, 0.7977032812726199, 6.9112, 6.9112, 170.5573041426782, 1980576.234669797, 1980576.234669797, 392586.1680825304], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7652400.0000, 
sim time next is 7653000.0000, 
raw observation next is [30.91666666666667, 58.33333333333334, 1.0, 2.0, 0.4736254652927116, 1.0, 2.0, 0.4736254652927116, 1.0, 2.0, 0.8028646092129674, 6.9112, 6.9112, 170.5573041426782, 1986640.074487375, 1986640.074487375, 393989.0597228205], 
processed observation next is [1.0, 0.5652173913043478, 0.6642969984202214, 0.5833333333333335, 1.0, 1.0, 0.36581381360567666, 1.0, 1.0, 0.36581381360567666, 1.0, 1.0, 0.7595909868450822, 0.0, 0.0, 0.8375144448122397, 0.551844465135382, 0.551844465135382, 0.5880433727206276], 
reward next is 0.4120, 
noisyNet noise sample is [array([0.9842542], dtype=float32), 3.2283912]. 
=============================================
[2019-03-27 03:54:10,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[58.276817]
 [59.163612]
 [58.97212 ]
 [69.098755]
 [69.49499 ]], R is [[57.66404343]
 [57.5014534 ]
 [57.32026291]
 [56.74706268]
 [56.73973846]].
[2019-03-27 03:54:15,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:15,585] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7587
[2019-03-27 03:54:15,591] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 81.0, 1.0, 2.0, 0.4081175830053905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599235.8159278613, 599235.815927862, 174178.4707554535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7498800.0000, 
sim time next is 7499400.0000, 
raw observation next is [25.08333333333333, 81.66666666666667, 1.0, 2.0, 0.4067048293573075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 597741.7182179198, 597741.7182179204, 174057.7359285391], 
processed observation next is [0.0, 0.8260869565217391, 0.3878357030015796, 0.8166666666666668, 1.0, 1.0, 0.2851865413943464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1660393661716444, 0.16603936617164455, 0.2597876655649837], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.17972335], dtype=float32), 0.6086588]. 
=============================================
[2019-03-27 03:54:22,344] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2115082: loss 0.0226
[2019-03-27 03:54:22,348] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2115083: learning rate 0.0005
[2019-03-27 03:54:24,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:24,554] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:24,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-03-27 03:54:25,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:25,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:25,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-03-27 03:54:26,102] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2116919: loss 0.5231
[2019-03-27 03:54:26,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2116921: learning rate 0.0005
[2019-03-27 03:54:26,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:26,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:26,406] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-03-27 03:54:26,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:26,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:26,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-03-27 03:54:26,888] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2117348: loss 0.7716
[2019-03-27 03:54:26,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2117349: learning rate 0.0005
[2019-03-27 03:54:26,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:26,925] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9545
[2019-03-27 03:54:26,930] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 95.0, 1.0, 2.0, 0.5593320930472424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781610.2002654336, 781610.2002654336, 193408.544843594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7700400.0000, 
sim time next is 7701000.0000, 
raw observation next is [24.58333333333334, 94.83333333333334, 1.0, 2.0, 0.5757440303894298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804552.9136077813, 804552.9136077813, 196307.1476578459], 
processed observation next is [1.0, 0.13043478260869565, 0.3641390205371251, 0.9483333333333335, 1.0, 1.0, 0.4888482293848551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22348692044660592, 0.22348692044660592, 0.2929957427729043], 
reward next is 0.7070, 
noisyNet noise sample is [array([0.96636313], dtype=float32), 1.1972564]. 
=============================================
[2019-03-27 03:54:26,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[51.590984]
 [51.48196 ]
 [51.61244 ]
 [51.528217]
 [50.144283]], R is [[51.81572723]
 [52.00889969]
 [52.19777679]
 [52.38082504]
 [52.55911255]].
[2019-03-27 03:54:27,554] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:27,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4496
[2019-03-27 03:54:27,566] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666667, 85.66666666666667, 1.0, 2.0, 0.2425464766337609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399465.4648517597, 399465.4648517604, 160169.9418480094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 433200.0000, 
sim time next is 433800.0000, 
raw observation next is [19.65, 85.5, 1.0, 2.0, 0.2416396750564468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398145.4098398074, 398145.4098398067, 160078.15637442], 
processed observation next is [1.0, 0.0, 0.13033175355450236, 0.855, 1.0, 1.0, 0.0863128615137913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11059594717772428, 0.11059594717772409, 0.23892262145435822], 
reward next is 0.7611, 
noisyNet noise sample is [array([-0.48527324], dtype=float32), 0.21130295]. 
=============================================
[2019-03-27 03:54:27,882] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2117938: loss 0.1334
[2019-03-27 03:54:27,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2117938: learning rate 0.0005
[2019-03-27 03:54:28,351] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:28,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:28,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-03-27 03:54:28,488] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2118196: loss 0.0334
[2019-03-27 03:54:28,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2118196: learning rate 0.0005
[2019-03-27 03:54:28,663] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:28,670] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2150
[2019-03-27 03:54:28,676] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 92.33333333333334, 1.0, 2.0, 0.5314335195758074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742611.1291705581, 742611.1291705575, 188667.7318336927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7947600.0000, 
sim time next is 7948200.0000, 
raw observation next is [26.41666666666666, 92.66666666666666, 1.0, 2.0, 0.5322698190556402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743780.161061396, 743780.1610613954, 188806.6799080083], 
processed observation next is [1.0, 1.0, 0.4510268562401261, 0.9266666666666665, 1.0, 1.0, 0.43646966151281946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20660560029483221, 0.20660560029483208, 0.28180101478807207], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.37461993], dtype=float32), -0.11843079]. 
=============================================
[2019-03-27 03:54:29,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:29,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:29,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-03-27 03:54:30,016] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2119019: loss 0.2779
[2019-03-27 03:54:30,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2119021: learning rate 0.0005
[2019-03-27 03:54:31,454] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2119778: loss 0.0140
[2019-03-27 03:54:31,464] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2119778: learning rate 0.0005
[2019-03-27 03:54:33,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:33,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8799
[2019-03-27 03:54:33,039] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 84.33333333333334, 1.0, 2.0, 0.5362200177786424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749302.0141029314, 749302.0141029321, 189466.2023328312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7845600.0000, 
sim time next is 7846200.0000, 
raw observation next is [27.75, 85.5, 1.0, 2.0, 0.5388911077864368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753035.8593542437, 753035.8593542431, 189914.3213791161], 
processed observation next is [1.0, 0.8260869565217391, 0.514218009478673, 0.855, 1.0, 1.0, 0.444447117814984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20917662759840103, 0.2091766275984009, 0.2834542110136061], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.1593257], dtype=float32), 2.1703205]. 
=============================================
[2019-03-27 03:54:34,142] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:34,148] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2707
[2019-03-27 03:54:34,153] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.81666666666667, 94.83333333333333, 1.0, 2.0, 0.6812387293344793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1019081.761564686, 1019081.761564686, 225470.4249203383], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 125400.0000, 
sim time next is 126000.0000, 
raw observation next is [22.8, 95.0, 1.0, 2.0, 0.7321840579104695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095165.905318714, 1095165.905318714, 237395.6173608549], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.95, 1.0, 1.0, 0.6773301902535777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30421275147742055, 0.30421275147742055, 0.35432181695649984], 
reward next is 0.6457, 
noisyNet noise sample is [array([1.4142509], dtype=float32), -0.16145009]. 
=============================================
[2019-03-27 03:54:34,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[47.273895]
 [46.742508]
 [45.647835]
 [45.904587]
 [46.010468]], R is [[46.92042923]
 [47.11470413]
 [47.30153275]
 [47.46342087]
 [47.62557983]].
[2019-03-27 03:54:34,529] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:34,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:34,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:34,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:34,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-03-27 03:54:34,669] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-03-27 03:54:36,037] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2121940: loss 0.3106
[2019-03-27 03:54:36,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2121940: learning rate 0.0005
[2019-03-27 03:54:36,476] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2122098: loss 5.9802
[2019-03-27 03:54:36,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2122098: learning rate 0.0005
[2019-03-27 03:54:36,488] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:54:36,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0477
[2019-03-27 03:54:36,504] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2122105: loss 8.7007
[2019-03-27 03:54:36,505] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 90.0, 1.0, 2.0, 0.528340222514659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738287.1356138209, 738287.1356138209, 188155.5502397603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7944000.0000, 
sim time next is 7944600.0000, 
raw observation next is [26.56666666666667, 90.5, 1.0, 2.0, 0.527304807204318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736839.7747948361, 736839.7747948361, 187984.8601255989], 
processed observation next is [1.0, 0.9565217391304348, 0.45813586097946307, 0.905, 1.0, 1.0, 0.43048771952327475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2046777152207878, 0.2046777152207878, 0.28057441809790884], 
reward next is 0.7194, 
noisyNet noise sample is [array([2.4215603], dtype=float32), 0.11294046]. 
=============================================
[2019-03-27 03:54:36,510] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2122106: learning rate 0.0005
[2019-03-27 03:54:37,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:37,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:37,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-03-27 03:54:38,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:38,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:38,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-03-27 03:54:38,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:38,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:38,852] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-03-27 03:54:39,300] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2123542: loss 1.4373
[2019-03-27 03:54:39,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2123542: learning rate 0.0005
[2019-03-27 03:54:39,471] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:39,472] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:39,515] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-03-27 03:54:39,592] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2123705: loss 0.6548
[2019-03-27 03:54:39,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2123705: learning rate 0.0005
[2019-03-27 03:54:39,610] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2123715: loss 0.0098
[2019-03-27 03:54:39,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2123715: learning rate 0.0005
[2019-03-27 03:54:39,802] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:39,803] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:39,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:39,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:39,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-03-27 03:54:39,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-03-27 03:54:40,265] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2124096: loss 0.5870
[2019-03-27 03:54:40,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2124096: learning rate 0.0005
[2019-03-27 03:54:40,289] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:54:40,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:40,334] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2124140: loss 0.0044
[2019-03-27 03:54:40,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2124140: learning rate 0.0005
[2019-03-27 03:54:40,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-03-27 03:54:41,005] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2124570: loss 1.4552
[2019-03-27 03:54:41,008] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2124570: learning rate 0.0005
[2019-03-27 03:54:41,390] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2124810: loss 0.0048
[2019-03-27 03:54:41,391] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2124811: learning rate 0.0005
[2019-03-27 03:54:41,438] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2124837: loss 0.7435
[2019-03-27 03:54:41,441] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2124838: learning rate 0.0005
[2019-03-27 03:54:41,491] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2124866: loss 0.1307
[2019-03-27 03:54:41,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2124867: learning rate 0.0005
[2019-03-27 03:54:41,766] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 03:54:41,769] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:54:41,769] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:41,770] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:54:41,771] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:54:41,772] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:41,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:54:41,774] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:41,775] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:41,772] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:54:41,777] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:54:41,805] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-03-27 03:54:41,806] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-03-27 03:54:41,857] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-03-27 03:54:41,878] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-03-27 03:54:41,880] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-03-27 03:54:45,720] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.17167929], dtype=float32), 0.035290785]
[2019-03-27 03:54:45,722] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.352457985, 93.186129175, 1.0, 2.0, 0.2864309759442163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462206.1652765374, 462206.1652765374, 164422.6530266158]
[2019-03-27 03:54:45,724] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:54:45,727] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20873868904591797
[2019-03-27 03:55:20,679] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17167929], dtype=float32), 0.035290785]
[2019-03-27 03:55:20,680] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.8, 84.5, 1.0, 2.0, 0.4823663341627776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678506.5377015005, 678506.5377014998, 181456.3830112746]
[2019-03-27 03:55:20,682] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:55:20,686] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3680437658144152
[2019-03-27 03:55:28,906] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17167929], dtype=float32), 0.035290785]
[2019-03-27 03:55:28,908] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.05, 61.5, 1.0, 2.0, 0.5212724121392446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728407.3983645316, 728407.3983645316, 186995.7469254201]
[2019-03-27 03:55:28,911] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:55:28,913] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4593049032447799
[2019-03-27 03:55:34,094] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.17167929], dtype=float32), 0.035290785]
[2019-03-27 03:55:34,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.58053087, 80.33386850666668, 1.0, 2.0, 0.5259485136358983, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8811826301391705, 6.9112, 6.9112, 168.9129519670701, 1470396.601031727, 1470396.601031727, 316201.8402358486]
[2019-03-27 03:55:34,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:55:34,101] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07601931887856872
[2019-03-27 03:55:40,332] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17167929], dtype=float32), 0.035290785]
[2019-03-27 03:55:40,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.58333333333334, 74.0, 1.0, 2.0, 0.5851707308986549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817730.9900566618, 817730.9900566618, 198015.3454595612]
[2019-03-27 03:55:40,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:55:40,336] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.49460897507356516
[2019-03-27 03:56:05,335] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17167929], dtype=float32), 0.035290785]
[2019-03-27 03:56:05,336] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.96666666666667, 55.0, 1.0, 2.0, 0.5927589534029973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 828339.0809660284, 828339.0809660291, 199404.9370868744]
[2019-03-27 03:56:05,337] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:56:05,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22097357291329867
[2019-03-27 03:56:37,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:56:37,411] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:56:37,484] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:56:37,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:56:37,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:56:38,526] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2125000, evaluation results [2125000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:56:38,664] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2125068: loss 0.0060
[2019-03-27 03:56:38,669] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2125068: learning rate 0.0005
[2019-03-27 03:56:38,851] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2125150: loss 1.9982
[2019-03-27 03:56:38,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2125151: learning rate 0.0005
[2019-03-27 03:56:39,927] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:56:39,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6980
[2019-03-27 03:56:39,942] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.68333333333333, 86.0, 1.0, 2.0, 0.2684175566462462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 435523.5626446234, 435523.5626446228, 162636.5003922702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 342600.0000, 
sim time next is 343200.0000, 
raw observation next is [20.66666666666667, 86.0, 1.0, 2.0, 0.2677749890158124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434603.4281754196, 434603.4281754189, 162575.8184142708], 
processed observation next is [0.0, 1.0, 0.17851500789889443, 0.86, 1.0, 1.0, 0.11780119158531617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12072317449317212, 0.12072317449317192, 0.2426504752451803], 
reward next is 0.7573, 
noisyNet noise sample is [array([-1.2626822], dtype=float32), 0.5617394]. 
=============================================
[2019-03-27 03:56:40,640] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2125943: loss 0.0824
[2019-03-27 03:56:40,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2125943: learning rate 0.0005
[2019-03-27 03:56:42,522] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2126784: loss 0.0315
[2019-03-27 03:56:42,525] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2126785: learning rate 0.0005
[2019-03-27 03:56:43,852] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4887362e-35 1.0000000e+00 0.0000000e+00 3.9914620e-33 7.3054381e-26], sum to 1.0000
[2019-03-27 03:56:43,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0323
[2019-03-27 03:56:43,863] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 75.0, 1.0, 2.0, 0.4802929978796964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772164.0973032619, 772164.0973032626, 191668.8623320326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 398400.0000, 
sim time next is 399000.0000, 
raw observation next is [22.65, 75.5, 1.0, 2.0, 0.500781793906928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804724.0981767258, 804724.0981767265, 195251.5084414976], 
processed observation next is [1.0, 0.6086956521739131, 0.2725118483412322, 0.755, 1.0, 1.0, 0.3985322818155759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22353447171575716, 0.22353447171575735, 0.2914201618529815], 
reward next is 0.7086, 
noisyNet noise sample is [array([0.26603702], dtype=float32), 1.0918322]. 
=============================================
[2019-03-27 03:56:43,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.89214 ]
 [73.91952 ]
 [73.28529 ]
 [71.7833  ]
 [71.471504]], R is [[72.50327301]
 [72.49217224]
 [72.50164795]
 [72.50621796]
 [72.48612976]].
[2019-03-27 03:56:47,825] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.22201015e-29 1.00000000e+00 1.65672475e-34 2.69082073e-28
 1.19243983e-17], sum to 1.0000
[2019-03-27 03:56:47,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6675
[2019-03-27 03:56:47,841] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.65, 85.5, 1.0, 2.0, 0.2416396750564468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398145.4098398074, 398145.4098398067, 160078.15637442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 433800.0000, 
sim time next is 434400.0000, 
raw observation next is [19.63333333333333, 85.33333333333334, 1.0, 2.0, 0.2408108809142203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 396950.562634402, 396950.5626344027, 159993.4607243839], 
processed observation next is [1.0, 0.0, 0.1295418641390204, 0.8533333333333334, 1.0, 1.0, 0.08531431435448227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11026404517622278, 0.11026404517622297, 0.2387962100363939], 
reward next is 0.7612, 
noisyNet noise sample is [array([-1.77553], dtype=float32), -1.1965617]. 
=============================================
[2019-03-27 03:56:47,924] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2129193: loss 0.0028
[2019-03-27 03:56:47,926] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2129194: learning rate 0.0005
[2019-03-27 03:56:48,186] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2129312: loss 0.0212
[2019-03-27 03:56:48,188] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2129312: learning rate 0.0005
[2019-03-27 03:56:48,221] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2129325: loss 0.0273
[2019-03-27 03:56:48,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2129325: learning rate 0.0005
[2019-03-27 03:56:52,323] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2131158: loss 0.0087
[2019-03-27 03:56:52,327] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2131158: learning rate 0.0005
[2019-03-27 03:56:52,755] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2131333: loss 0.0059
[2019-03-27 03:56:52,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2131337: learning rate 0.0005
[2019-03-27 03:56:52,862] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2131382: loss 0.2084
[2019-03-27 03:56:52,864] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2131382: learning rate 0.0005
[2019-03-27 03:56:54,107] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2131936: loss 0.1035
[2019-03-27 03:56:54,111] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2131937: learning rate 0.0005
[2019-03-27 03:56:54,258] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2132002: loss 0.0336
[2019-03-27 03:56:54,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2132004: learning rate 0.0005
[2019-03-27 03:56:55,427] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2132522: loss 0.0089
[2019-03-27 03:56:55,429] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2132523: learning rate 0.0005
[2019-03-27 03:56:55,949] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2132761: loss 0.1388
[2019-03-27 03:56:55,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2132763: learning rate 0.0005
[2019-03-27 03:56:56,208] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2132876: loss 0.0024
[2019-03-27 03:56:56,215] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2132876: learning rate 0.0005
[2019-03-27 03:56:56,392] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2132958: loss 0.0025
[2019-03-27 03:56:56,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2132958: learning rate 0.0005
[2019-03-27 03:56:56,791] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2133134: loss 0.4318
[2019-03-27 03:56:56,793] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2133135: learning rate 0.0005
[2019-03-27 03:56:56,994] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2133225: loss 0.0147
[2019-03-27 03:56:56,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2133225: learning rate 0.0005
[2019-03-27 03:56:58,562] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2133924: loss 0.0084
[2019-03-27 03:56:58,568] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2133924: learning rate 0.0005
[2019-03-27 03:57:00,478] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2134779: loss 0.0196
[2019-03-27 03:57:00,481] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2134779: learning rate 0.0005
[2019-03-27 03:57:01,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1677472e-28 1.0000000e+00 3.6260850e-32 1.4608673e-26 3.4771846e-16], sum to 1.0000
[2019-03-27 03:57:01,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2707
[2019-03-27 03:57:01,403] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 53.0, 1.0, 2.0, 0.2303694607800544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 381146.2829916509, 381146.2829916502, 158936.2768521967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 753600.0000, 
sim time next is 754200.0000, 
raw observation next is [23.95, 54.5, 1.0, 2.0, 0.2284614883460498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377813.0948803165, 377813.0948803165, 158775.7680412834], 
processed observation next is [1.0, 0.7391304347826086, 0.3341232227488152, 0.545, 1.0, 1.0, 0.07043552812777082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10494808191119903, 0.10494808191119903, 0.23697875827057222], 
reward next is 0.7630, 
noisyNet noise sample is [array([0.61964226], dtype=float32), 0.6273925]. 
=============================================
[2019-03-27 03:57:05,883] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2137177: loss 7.9765
[2019-03-27 03:57:05,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2137178: learning rate 0.0005
[2019-03-27 03:57:06,140] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2137293: loss 0.0476
[2019-03-27 03:57:06,142] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2137293: learning rate 0.0005
[2019-03-27 03:57:06,409] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2137408: loss 0.0185
[2019-03-27 03:57:06,414] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2137410: learning rate 0.0005
[2019-03-27 03:57:07,211] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1804642e-32 1.0000000e+00 1.1726742e-37 4.5193018e-31 4.6788728e-21], sum to 1.0000
[2019-03-27 03:57:07,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6002
[2019-03-27 03:57:07,225] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.61666666666667, 64.5, 1.0, 2.0, 0.4529028689701042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745795.1515899012, 745795.1515899012, 187695.6014266481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 555000.0000, 
sim time next is 555600.0000, 
raw observation next is [22.83333333333333, 63.00000000000001, 1.0, 2.0, 0.4305719741467672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709213.5452492372, 709213.5452492366, 184102.8378267748], 
processed observation next is [1.0, 0.43478260869565216, 0.2812006319115322, 0.6300000000000001, 1.0, 1.0, 0.31394213752622546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19700376256923255, 0.1970037625692324, 0.27478035496533554], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.83360714], dtype=float32), 1.374106]. 
=============================================
[2019-03-27 03:57:10,203] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2139094: loss 0.0347
[2019-03-27 03:57:10,206] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2139094: learning rate 0.0005
[2019-03-27 03:57:10,657] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2139292: loss 0.0286
[2019-03-27 03:57:10,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2139293: learning rate 0.0005
[2019-03-27 03:57:10,889] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2139400: loss 0.0140
[2019-03-27 03:57:10,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2139400: learning rate 0.0005
[2019-03-27 03:57:12,240] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2140000: loss 0.0057
[2019-03-27 03:57:12,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2140000: learning rate 0.0005
[2019-03-27 03:57:12,259] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2140005: loss 0.1971
[2019-03-27 03:57:12,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2140005: learning rate 0.0005
[2019-03-27 03:57:13,253] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2140453: loss 0.0709
[2019-03-27 03:57:13,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2140453: learning rate 0.0005
[2019-03-27 03:57:13,795] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2140692: loss 0.0310
[2019-03-27 03:57:13,797] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2140692: learning rate 0.0005
[2019-03-27 03:57:14,233] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2140888: loss 0.1183
[2019-03-27 03:57:14,234] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2140888: learning rate 0.0005
[2019-03-27 03:57:14,327] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2140926: loss 0.0562
[2019-03-27 03:57:14,329] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2140927: learning rate 0.0005
[2019-03-27 03:57:14,905] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2141182: loss 0.0244
[2019-03-27 03:57:14,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2141182: learning rate 0.0005
[2019-03-27 03:57:14,977] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2141217: loss 0.1036
[2019-03-27 03:57:14,980] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2141217: learning rate 0.0005
[2019-03-27 03:57:16,010] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5016146e-28 1.0000000e+00 1.6167849e-35 1.8191176e-28 2.2145203e-16], sum to 1.0000
[2019-03-27 03:57:16,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0953
[2019-03-27 03:57:16,024] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333334, 89.0, 1.0, 2.0, 0.2947343000337526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470860.63752339, 470860.63752339, 164996.0497487575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 870600.0000, 
sim time next is 871200.0000, 
raw observation next is [21.1, 89.0, 1.0, 2.0, 0.2935083253260662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469225.7463031584, 469225.7463031584, 164885.6876700621], 
processed observation next is [0.0, 0.08695652173913043, 0.1990521327014219, 0.89, 1.0, 1.0, 0.1488052112362243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13034048508421067, 0.13034048508421067, 0.24609804129860013], 
reward next is 0.7539, 
noisyNet noise sample is [array([-2.6702647], dtype=float32), -2.3471656]. 
=============================================
[2019-03-27 03:57:16,606] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2141945: loss 0.0180
[2019-03-27 03:57:16,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2141947: learning rate 0.0005
[2019-03-27 03:57:18,463] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2142772: loss 0.0109
[2019-03-27 03:57:18,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2142772: learning rate 0.0005
[2019-03-27 03:57:18,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4422198e-26 1.0000000e+00 3.7360998e-31 5.8629736e-24 3.2246107e-12], sum to 1.0000
[2019-03-27 03:57:18,845] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0681
[2019-03-27 03:57:18,850] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 49.66666666666666, 1.0, 2.0, 0.6109809502398384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1006422.373508957, 1006422.373508958, 217980.0518838072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 747600.0000, 
sim time next is 748200.0000, 
raw observation next is [25.16666666666667, 49.83333333333334, 1.0, 2.0, 0.6262301618591973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031855.432564975, 1031855.432564975, 221349.0490915797], 
processed observation next is [1.0, 0.6521739130434783, 0.39178515007898923, 0.4983333333333334, 1.0, 1.0, 0.5496748938062618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28662650904582637, 0.28662650904582637, 0.33037171506205926], 
reward next is 0.6696, 
noisyNet noise sample is [array([0.17277116], dtype=float32), -1.0905818]. 
=============================================
[2019-03-27 03:57:19,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4387216e-30 1.0000000e+00 1.4071647e-35 2.0217469e-27 1.1442075e-15], sum to 1.0000
[2019-03-27 03:57:19,318] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6223
[2019-03-27 03:57:19,324] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.1, 91.0, 1.0, 2.0, 0.2244193785912199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 373045.611749599, 373045.6117495984, 158173.3704268954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 694800.0000, 
sim time next is 695400.0000, 
raw observation next is [18.03333333333333, 91.33333333333334, 1.0, 2.0, 0.22362272208739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 371807.4158450303, 371807.4158450303, 158085.7699185944], 
processed observation next is [1.0, 0.043478260869565216, 0.05371248025276459, 0.9133333333333334, 1.0, 1.0, 0.06460568926191566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10327983773473064, 0.10327983773473064, 0.23594891032626028], 
reward next is 0.7641, 
noisyNet noise sample is [array([0.99336416], dtype=float32), 1.2526246]. 
=============================================
[2019-03-27 03:57:19,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6022013e-32 1.0000000e+00 4.9631424e-38 1.4753033e-28 2.2262491e-18], sum to 1.0000
[2019-03-27 03:57:19,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7429
[2019-03-27 03:57:19,808] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 95.0, 1.0, 2.0, 0.5948771918637855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 915602.8568341546, 915602.856834154, 210007.0210694036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 993600.0000, 
sim time next is 994200.0000, 
raw observation next is [21.86666666666667, 95.0, 1.0, 2.0, 0.6881659048390057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1059848.747751766, 1059848.747751765, 230567.1253872909], 
processed observation next is [1.0, 0.5217391304347826, 0.23538704581358633, 0.95, 1.0, 1.0, 0.6242962708903683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2944024299310461, 0.2944024299310458, 0.34413003789147895], 
reward next is 0.6559, 
noisyNet noise sample is [array([0.66973484], dtype=float32), 0.34595832]. 
=============================================
[2019-03-27 03:57:20,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9037526e-26 1.0000000e+00 3.2268437e-31 1.8644792e-23 6.1726228e-11], sum to 1.0000
[2019-03-27 03:57:20,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9929
[2019-03-27 03:57:20,394] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 96.0, 1.0, 2.0, 0.3567285750800349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548838.7047987876, 548838.7047987876, 170529.861653184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1020000.0000, 
sim time next is 1020600.0000, 
raw observation next is [21.8, 96.0, 1.0, 2.0, 0.356832011537996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548996.6058785615, 548996.6058785615, 170543.0319410249], 
processed observation next is [1.0, 0.8260869565217391, 0.23222748815165886, 0.96, 1.0, 1.0, 0.22509880908192287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1524990571884893, 0.1524990571884893, 0.2545418387179476], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.4500964], dtype=float32), 0.17021552]. 
=============================================
[2019-03-27 03:57:20,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1524826e-36 1.0000000e+00 0.0000000e+00 8.2455244e-34 4.2856563e-28], sum to 1.0000
[2019-03-27 03:57:20,750] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7772
[2019-03-27 03:57:20,755] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 84.33333333333333, 1.0, 2.0, 0.3033964635681946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482440.7962521837, 482440.7962521837, 165785.8971832283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 853800.0000, 
sim time next is 854400.0000, 
raw observation next is [21.93333333333333, 84.66666666666667, 1.0, 2.0, 0.3028843034339982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481511.6152346244, 481511.6152346244, 165717.0841528537], 
processed observation next is [0.0, 0.9130434782608695, 0.23854660347551332, 0.8466666666666667, 1.0, 1.0, 0.16010157040240747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13375322645406235, 0.13375322645406235, 0.24733893157142342], 
reward next is 0.7527, 
noisyNet noise sample is [array([0.82353896], dtype=float32), 1.1372776]. 
=============================================
[2019-03-27 03:57:24,028] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2145225: loss 0.0883
[2019-03-27 03:57:24,031] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2145226: learning rate 0.0005
[2019-03-27 03:57:24,336] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2145361: loss 0.0028
[2019-03-27 03:57:24,338] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2145363: learning rate 0.0005
[2019-03-27 03:57:24,342] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2145365: loss 0.0022
[2019-03-27 03:57:24,348] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2145366: learning rate 0.0005
[2019-03-27 03:57:28,434] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2147133: loss 0.0032
[2019-03-27 03:57:28,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2147133: learning rate 0.0005
[2019-03-27 03:57:28,759] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2147280: loss 0.0280
[2019-03-27 03:57:28,764] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2147283: learning rate 0.0005
[2019-03-27 03:57:29,069] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2147420: loss 6.3457
[2019-03-27 03:57:29,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2147421: learning rate 0.0005
[2019-03-27 03:57:29,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0510191e-36 1.0000000e+00 0.0000000e+00 1.9438886e-33 4.5481261e-26], sum to 1.0000
[2019-03-27 03:57:29,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7231
[2019-03-27 03:57:29,571] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 74.0, 1.0, 2.0, 0.3174580353314876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498329.8766802831, 498329.8766802831, 166815.7604252182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 925200.0000, 
sim time next is 925800.0000, 
raw observation next is [23.93333333333333, 74.66666666666667, 1.0, 2.0, 0.3173503564230457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497794.5797429986, 497794.5797429986, 166765.8123437208], 
processed observation next is [0.0, 0.7391304347826086, 0.3333333333333332, 0.7466666666666667, 1.0, 1.0, 0.17753054990728398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13827627215083294, 0.13827627215083294, 0.24890419752794152], 
reward next is 0.7511, 
noisyNet noise sample is [array([-1.0804724], dtype=float32), 0.54556483]. 
=============================================
[2019-03-27 03:57:29,745] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9796497e-25 1.0000000e+00 1.3905356e-30 9.2950049e-22 1.9779289e-12], sum to 1.0000
[2019-03-27 03:57:29,757] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4348
[2019-03-27 03:57:29,763] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 90.83333333333334, 1.0, 2.0, 0.2876488060305038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462853.398360409, 462853.398360409, 164467.2992103646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1129800.0000, 
sim time next is 1130400.0000, 
raw observation next is [20.5, 91.0, 1.0, 2.0, 0.2880556518581609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463771.3538605892, 463771.3538605899, 164530.6134439055], 
processed observation next is [1.0, 0.08695652173913043, 0.1706161137440759, 0.91, 1.0, 1.0, 0.14223572513031432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1288253760723859, 0.1288253760723861, 0.2455680797670231], 
reward next is 0.7544, 
noisyNet noise sample is [array([-0.83817], dtype=float32), 1.9247366]. 
=============================================
[2019-03-27 03:57:30,289] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2147982: loss 0.0038
[2019-03-27 03:57:30,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2147982: learning rate 0.0005
[2019-03-27 03:57:30,420] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2148046: loss 5.2279
[2019-03-27 03:57:30,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2148046: learning rate 0.0005
[2019-03-27 03:57:31,247] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2148430: loss 0.0021
[2019-03-27 03:57:31,252] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2148432: learning rate 0.0005
[2019-03-27 03:57:31,929] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2148738: loss 6.0918
[2019-03-27 03:57:31,931] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2148739: learning rate 0.0005
[2019-03-27 03:57:32,229] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2148878: loss 0.0217
[2019-03-27 03:57:32,231] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2148878: learning rate 0.0005
[2019-03-27 03:57:32,318] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2148918: loss 0.0099
[2019-03-27 03:57:32,322] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2148918: learning rate 0.0005
[2019-03-27 03:57:32,769] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2149121: loss 4.8794
[2019-03-27 03:57:32,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2149121: learning rate 0.0005
[2019-03-27 03:57:32,920] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2149194: loss 0.0024
[2019-03-27 03:57:32,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2149194: learning rate 0.0005
[2019-03-27 03:57:33,999] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8753105e-27 1.0000000e+00 1.1347911e-32 1.0771372e-22 2.2934828e-13], sum to 1.0000
[2019-03-27 03:57:34,011] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0026
[2019-03-27 03:57:34,018] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 62.0, 1.0, 2.0, 0.8520371013654272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1293168.079774459, 1293168.07977446, 271151.0565299764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1171800.0000, 
sim time next is 1172400.0000, 
raw observation next is [27.4, 61.66666666666667, 1.0, 2.0, 0.8490237964735008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1287123.343506175, 1287123.343506175, 270109.6205624607], 
processed observation next is [1.0, 0.5652173913043478, 0.4976303317535545, 0.6166666666666667, 1.0, 1.0, 0.8181009596066274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3575342620850486, 0.3575342620850486, 0.4031486874066577], 
reward next is 0.5969, 
noisyNet noise sample is [array([-0.7393094], dtype=float32), -1.0186667]. 
=============================================
[2019-03-27 03:57:34,411] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2149882: loss 4.4121
[2019-03-27 03:57:34,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2149885: learning rate 0.0005
[2019-03-27 03:57:34,675] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 03:57:34,679] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:57:34,680] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:57:34,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:34,680] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:57:34,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:57:34,681] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:34,683] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:57:34,686] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:34,684] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:34,688] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:34,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-03-27 03:57:34,734] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-03-27 03:57:34,758] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-03-27 03:57:34,776] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-03-27 03:57:34,777] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-03-27 03:57:51,520] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.061562866]
[2019-03-27 03:57:51,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.2, 91.0, 1.0, 2.0, 0.2766008333362995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447880.1518896346, 447880.1518896346, 163451.2025194356]
[2019-03-27 03:57:51,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:57:51,528] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6870282e-26 1.0000000e+00 3.3431212e-31 1.8254123e-24 1.1027688e-15], sampled 0.061949671679968454
[2019-03-27 03:58:12,345] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.061562866]
[2019-03-27 03:58:12,346] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3393744085700938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522788.8774418494, 522788.8774418494, 168421.1684589302]
[2019-03-27 03:58:12,347] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:58:12,349] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.161320e-27 1.000000e+00 9.889035e-32 3.846101e-25 6.851215e-17], sampled 0.6044999693617897
[2019-03-27 03:58:25,788] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.061562866]
[2019-03-27 03:58:25,789] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 83.16666666666666, 1.0, 2.0, 0.5731412756036788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800914.415705872, 800914.415705872, 195849.1126084233]
[2019-03-27 03:58:25,789] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:58:25,792] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3392090e-24 1.0000000e+00 1.4607146e-29 1.5930475e-22 1.8452262e-12], sampled 0.6886016606216336
[2019-03-27 03:58:34,793] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.061562866]
[2019-03-27 03:58:34,796] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5083310982753803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710317.6236566935, 710317.6236566928, 184911.1557499463]
[2019-03-27 03:58:34,797] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:58:34,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1130703e-23 1.0000000e+00 4.1738773e-28 1.9536314e-21 6.4466540e-12], sampled 0.3609969954635093
[2019-03-27 03:59:03,872] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.061562866]
[2019-03-27 03:59:03,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.81666666666667, 69.5, 1.0, 2.0, 0.5345913394113851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747025.3346332994, 747025.3346332994, 189192.6567046977]
[2019-03-27 03:59:03,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:59:03,880] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7353741e-24 1.0000000e+00 2.0014253e-29 2.0160067e-22 2.0753163e-12], sampled 0.579438624156338
[2019-03-27 03:59:11,201] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.061562866]
[2019-03-27 03:59:11,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.76117738, 61.54025163, 1.0, 2.0, 0.4741266996170305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663481.4451345849, 663481.4451345849, 179767.2848389963]
[2019-03-27 03:59:11,205] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:59:11,209] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.7170177e-26 1.0000000e+00 5.7762194e-31 2.1231205e-24 5.2701522e-16], sampled 0.7326717439340186
[2019-03-27 03:59:25,548] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:59:25,714] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:59:25,715] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:59:25,752] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:59:25,773] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:59:26,788] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2150000, evaluation results [2150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:59:28,378] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2150717: loss 5.4713
[2019-03-27 03:59:28,381] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2150717: learning rate 0.0005
[2019-03-27 03:59:28,710] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1137212e-25 1.0000000e+00 1.8900663e-30 3.3706408e-23 8.3207631e-14], sum to 1.0000
[2019-03-27 03:59:28,720] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5652
[2019-03-27 03:59:28,726] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 90.33333333333333, 1.0, 2.0, 0.4521071560793938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704014.5907130953, 704014.5907130947, 185005.6770500243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1217400.0000, 
sim time next is 1218000.0000, 
raw observation next is [22.03333333333333, 90.66666666666667, 1.0, 2.0, 0.3806363193510849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 592550.3243295372, 592550.3243295372, 174447.397134563], 
processed observation next is [1.0, 0.08695652173913043, 0.2432859399684044, 0.9066666666666667, 1.0, 1.0, 0.2537786980133553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16459731231376035, 0.16459731231376035, 0.2603692494545717], 
reward next is 0.7396, 
noisyNet noise sample is [array([-1.0541648], dtype=float32), 0.077701]. 
=============================================
[2019-03-27 03:59:28,748] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.83024 ]
 [73.31653 ]
 [73.19095 ]
 [73.335175]
 [73.4244  ]], R is [[73.95523071]
 [73.93954468]
 [73.94801331]
 [73.95625305]
 [73.96425629]].
[2019-03-27 03:59:32,484] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1216680e-27 1.0000000e+00 2.1738177e-31 9.6532860e-25 9.6868244e-16], sum to 1.0000
[2019-03-27 03:59:32,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0087
[2019-03-27 03:59:32,499] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 57.16666666666666, 1.0, 2.0, 0.8749218469490505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1349022.648056325, 1349022.648056325, 280355.8618688554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1183800.0000, 
sim time next is 1184400.0000, 
raw observation next is [27.6, 57.0, 1.0, 2.0, 0.8641109400539346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333435.593337787, 1333435.593337787, 277297.3716745664], 
processed observation next is [1.0, 0.7391304347826086, 0.5071090047393366, 0.57, 1.0, 1.0, 0.8362782410288369, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3703987759271631, 0.3703987759271631, 0.4138766741411439], 
reward next is 0.5861, 
noisyNet noise sample is [array([-0.75870687], dtype=float32), 0.7177169]. 
=============================================
[2019-03-27 03:59:34,043] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2153247: loss 0.0196
[2019-03-27 03:59:34,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2153247: learning rate 0.0005
[2019-03-27 03:59:34,093] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2153269: loss 5.3446
[2019-03-27 03:59:34,095] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2153269: learning rate 0.0005
[2019-03-27 03:59:34,231] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2153330: loss 5.5177
[2019-03-27 03:59:34,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2153330: learning rate 0.0005
[2019-03-27 03:59:38,231] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2155058: loss 5.0816
[2019-03-27 03:59:38,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2155059: learning rate 0.0005
[2019-03-27 03:59:38,738] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2155286: loss 5.9806
[2019-03-27 03:59:38,740] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2155287: learning rate 0.0005
[2019-03-27 03:59:39,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7247071e-28 1.0000000e+00 2.4423237e-34 5.4633209e-27 1.8425128e-14], sum to 1.0000
[2019-03-27 03:59:39,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4276
[2019-03-27 03:59:39,122] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 91.66666666666667, 1.0, 2.0, 0.4715397438129229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662967.2775267791, 662967.2775267785, 179783.0596463284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1286400.0000, 
sim time next is 1287000.0000, 
raw observation next is [24.75, 92.0, 1.0, 2.0, 0.4708594676202532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662180.2061036015, 662180.2061036015, 179703.4527588577], 
processed observation next is [1.0, 0.9130434782608695, 0.3720379146919432, 0.92, 1.0, 1.0, 0.3624812862894617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18393894613988931, 0.18393894613988931, 0.26821410859531], 
reward next is 0.7318, 
noisyNet noise sample is [array([0.45947483], dtype=float32), -0.964471]. 
=============================================
[2019-03-27 03:59:39,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.3034  ]
 [81.33184 ]
 [81.105286]
 [81.15933 ]
 [80.91082 ]], R is [[80.97867584]
 [80.90055847]
 [80.82334137]
 [80.74710083]
 [80.67184448]].
[2019-03-27 03:59:39,217] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2155496: loss 0.0431
[2019-03-27 03:59:39,222] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2155497: learning rate 0.0005
[2019-03-27 03:59:40,108] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2155899: loss 5.0903
[2019-03-27 03:59:40,110] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2155900: learning rate 0.0005
[2019-03-27 03:59:40,282] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.802765e-35], sum to 1.0000
[2019-03-27 03:59:40,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1068
[2019-03-27 03:59:40,297] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [27.1, 59.0, 1.0, 2.0, 0.3437258197622903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531896.1454583206, 531896.14545832, 169226.5476227], 
processed observation next is [0.0, 0.7391304347826086, 0.4834123222748816, 0.59, 1.0, 1.0, 0.20930821658107263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14774892929397795, 0.1477489292939778, 0.25257693675029846], 
reward next is 0.7474, 
noisyNet noise sample is [array([-0.56188333], dtype=float32), 1.2619675]. 
=============================================
[2019-03-27 03:59:40,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.167534]
 [78.132576]
 [78.08041 ]
 [78.03538 ]
 [77.98782 ]], R is [[78.33883667]
 [78.30258179]
 [78.26634216]
 [78.23007202]
 [78.19384766]].
[2019-03-27 03:59:40,499] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2156073: loss 0.1069
[2019-03-27 03:59:40,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2156074: learning rate 0.0005
[2019-03-27 03:59:41,161] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2156369: loss 5.6049
[2019-03-27 03:59:41,166] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2156369: learning rate 0.0005
[2019-03-27 03:59:41,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6741985e-34], sum to 1.0000
[2019-03-27 03:59:41,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6151
[2019-03-27 03:59:41,574] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [27.1, 59.0, 1.0, 2.0, 0.3437258197622903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531896.1454583206, 531896.14545832, 169226.5476227], 
processed observation next is [0.0, 0.7391304347826086, 0.4834123222748816, 0.59, 1.0, 1.0, 0.20930821658107263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14774892929397795, 0.1477489292939778, 0.25257693675029846], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.44783056], dtype=float32), 0.17877218]. 
=============================================
[2019-03-27 03:59:41,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.35768 ]
 [78.31857 ]
 [78.262146]
 [78.21185 ]
 [78.15907 ]], R is [[78.52263641]
 [78.48454285]
 [78.44648743]
 [78.40841675]
 [78.37041473]].
[2019-03-27 03:59:42,058] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2156761: loss 4.3615
[2019-03-27 03:59:42,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2156761: learning rate 0.0005
[2019-03-27 03:59:42,136] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2156798: loss 0.2666
[2019-03-27 03:59:42,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2156799: learning rate 0.0005
[2019-03-27 03:59:42,180] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2156816: loss 4.5415
[2019-03-27 03:59:42,184] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2156816: learning rate 0.0005
[2019-03-27 03:59:42,801] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2157090: loss 0.0438
[2019-03-27 03:59:42,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2157092: learning rate 0.0005
[2019-03-27 03:59:42,845] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2157111: loss 5.4497
[2019-03-27 03:59:42,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2157111: learning rate 0.0005
[2019-03-27 03:59:44,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:59:44,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7234
[2019-03-27 03:59:44,074] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 63.66666666666667, 1.0, 2.0, 0.345033003962754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 533590.4046474983, 533590.4046474977, 169354.2843945845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1532400.0000, 
sim time next is 1533000.0000, 
raw observation next is [26.1, 64.83333333333333, 1.0, 2.0, 0.3440100814731696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531925.144956696, 531925.1449566953, 169216.9894561035], 
processed observation next is [0.0, 0.7391304347826086, 0.4360189573459717, 0.6483333333333333, 1.0, 1.0, 0.20965070057008384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14775698471019333, 0.14775698471019313, 0.2525626708300052], 
reward next is 0.7474, 
noisyNet noise sample is [array([-0.0773688], dtype=float32), -0.2391929]. 
=============================================
[2019-03-27 03:59:44,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.90822 ]
 [78.881645]
 [78.851326]
 [78.81025 ]
 [78.77936 ]], R is [[78.88363647]
 [78.84203339]
 [78.80088043]
 [78.76019287]
 [78.72000122]].
[2019-03-27 03:59:44,648] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2157908: loss 0.1402
[2019-03-27 03:59:44,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2157908: learning rate 0.0005
[2019-03-27 03:59:46,585] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2158768: loss 0.0969
[2019-03-27 03:59:46,590] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2158770: learning rate 0.0005
[2019-03-27 03:59:48,192] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.765509e-35], sum to 1.0000
[2019-03-27 03:59:48,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5833
[2019-03-27 03:59:48,207] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 69.83333333333334, 1.0, 2.0, 0.4519402142382168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639298.9911641382, 639298.9911641382, 177417.5316918393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1440600.0000, 
sim time next is 1441200.0000, 
raw observation next is [27.86666666666667, 69.66666666666667, 1.0, 2.0, 0.4480711582908161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636194.6363583707, 636194.6363583714, 177165.4249158481], 
processed observation next is [0.0, 0.6956521739130435, 0.519747235387046, 0.6966666666666668, 1.0, 1.0, 0.3350254919166459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17672073232176963, 0.17672073232176982, 0.2644260073370867], 
reward next is 0.7356, 
noisyNet noise sample is [array([-1.0611913], dtype=float32), -0.40804726]. 
=============================================
[2019-03-27 03:59:52,323] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2161316: loss 0.0404
[2019-03-27 03:59:52,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2161318: learning rate 0.0005
[2019-03-27 03:59:52,501] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2161393: loss 0.0275
[2019-03-27 03:59:52,505] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2161393: learning rate 0.0005
[2019-03-27 03:59:52,576] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2161423: loss 0.0349
[2019-03-27 03:59:52,578] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2161423: learning rate 0.0005
[2019-03-27 03:59:56,321] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2163092: loss 0.0765
[2019-03-27 03:59:56,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2163092: learning rate 0.0005
[2019-03-27 03:59:56,777] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2163297: loss 0.0525
[2019-03-27 03:59:56,780] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2163299: learning rate 0.0005
[2019-03-27 03:59:57,142] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2163458: loss 0.0582
[2019-03-27 03:59:57,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2163459: learning rate 0.0005
[2019-03-27 03:59:58,130] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2163907: loss 0.0936
[2019-03-27 03:59:58,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2163908: learning rate 0.0005
[2019-03-27 03:59:58,434] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2164039: loss 0.0541
[2019-03-27 03:59:58,436] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2164039: learning rate 0.0005
[2019-03-27 03:59:59,412] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2164479: loss 0.1006
[2019-03-27 03:59:59,417] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2164479: learning rate 0.0005
[2019-03-27 04:00:00,022] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2164749: loss 0.0485
[2019-03-27 04:00:00,025] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2164751: learning rate 0.0005
[2019-03-27 04:00:00,236] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2164846: loss 0.0565
[2019-03-27 04:00:00,239] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2164847: learning rate 0.0005
[2019-03-27 04:00:00,257] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2164854: loss 0.0512
[2019-03-27 04:00:00,258] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2164854: learning rate 0.0005
[2019-03-27 04:00:00,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1987648e-26 1.0000000e+00 1.2913303e-29 3.9314552e-25 5.5890688e-17], sum to 1.0000
[2019-03-27 04:00:00,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6071
[2019-03-27 04:00:00,575] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 88.0, 1.0, 2.0, 0.3212464011813297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502674.9943207598, 502674.9943207592, 167100.9012685133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [22.3, 87.66666666666667, 1.0, 2.0, 0.3527663989668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551096.573161713, 551096.573161713, 170923.8904187363], 
processed observation next is [1.0, 0.2608695652173913, 0.25592417061611383, 0.8766666666666667, 1.0, 1.0, 0.22020048068292628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15308238143380914, 0.15308238143380914, 0.2551102842070691], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.04353629], dtype=float32), 0.6310905]. 
=============================================
[2019-03-27 04:00:00,787] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2165088: loss 0.0664
[2019-03-27 04:00:00,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2165088: learning rate 0.0005
[2019-03-27 04:00:01,026] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2165194: loss 0.1315
[2019-03-27 04:00:01,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2165195: learning rate 0.0005
[2019-03-27 04:00:02,629] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2165908: loss 0.0775
[2019-03-27 04:00:02,631] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2165908: learning rate 0.0005
[2019-03-27 04:00:04,594] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2166784: loss 0.0272
[2019-03-27 04:00:04,597] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2166784: learning rate 0.0005
[2019-03-27 04:00:05,602] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7284269e-25 1.0000000e+00 5.6116562e-30 1.0777654e-23 3.1884243e-15], sum to 1.0000
[2019-03-27 04:00:05,612] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9015
[2019-03-27 04:00:05,621] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 94.00000000000001, 1.0, 2.0, 0.496998911905481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694477.3908476771, 694477.3908476764, 183127.1937534992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1728600.0000, 
sim time next is 1729200.0000, 
raw observation next is [25.2, 94.0, 1.0, 2.0, 0.4949720499850276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691644.2497879133, 691644.2497879126, 182812.0390953481], 
processed observation next is [1.0, 0.0, 0.3933649289099526, 0.94, 1.0, 1.0, 0.39153259034340676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1921234027188648, 0.1921234027188646, 0.2728537896945494], 
reward next is 0.7271, 
noisyNet noise sample is [array([1.013671], dtype=float32), 0.9657291]. 
=============================================
[2019-03-27 04:00:10,332] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2169343: loss 0.1700
[2019-03-27 04:00:10,332] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2169343: learning rate 0.0005
[2019-03-27 04:00:10,393] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2169370: loss 0.1471
[2019-03-27 04:00:10,396] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2169370: learning rate 0.0005
[2019-03-27 04:00:11,261] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2169756: loss 0.4247
[2019-03-27 04:00:11,264] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2169756: learning rate 0.0005
[2019-03-27 04:00:11,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.40796683e-23 1.00000000e+00 1.11616176e-26 1.82539503e-20
 4.48071451e-15], sum to 1.0000
[2019-03-27 04:00:11,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9384
[2019-03-27 04:00:11,358] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333334, 84.33333333333333, 1.0, 2.0, 0.9243969494924512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1292061.873386663, 1292061.873386664, 276735.2945806855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [27.2, 84.0, 1.0, 2.0, 0.9031370624274396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1262328.521679329, 1262328.52167933, 270800.7116514287], 
processed observation next is [1.0, 0.5652173913043478, 0.4881516587677725, 0.84, 1.0, 1.0, 0.8832976655752285, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35064681157759137, 0.35064681157759164, 0.4041801666439235], 
reward next is 0.5958, 
noisyNet noise sample is [array([-0.36162862], dtype=float32), 0.1336674]. 
=============================================
[2019-03-27 04:00:14,019] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2170989: loss 0.0037
[2019-03-27 04:00:14,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2170989: learning rate 0.0005
[2019-03-27 04:00:14,451] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2171182: loss 0.0559
[2019-03-27 04:00:14,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2171182: learning rate 0.0005
[2019-03-27 04:00:14,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2368395e-25 1.0000000e+00 6.5180052e-31 9.0982660e-23 2.4710803e-15], sum to 1.0000
[2019-03-27 04:00:14,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8117
[2019-03-27 04:00:14,782] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.78333333333333, 94.16666666666667, 1.0, 2.0, 0.3452024229164352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534722.3565779442, 534722.356577945, 169470.8104871143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3456426327659531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535458.5648562041, 535458.5648562048, 169532.0914627482], 
processed observation next is [1.0, 0.043478260869565216, 0.23222748815165886, 0.94, 1.0, 1.0, 0.2116176298384977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14873849023783448, 0.14873849023783467, 0.25303297233246], 
reward next is 0.7470, 
noisyNet noise sample is [array([-2.0346372], dtype=float32), -1.3399503]. 
=============================================
[2019-03-27 04:00:14,808] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.83213]
 [68.96361]
 [69.00345]
 [69.10326]
 [69.38854]], R is [[68.89879608]
 [68.9568634 ]
 [69.01439667]
 [69.07132721]
 [69.1276474 ]].
[2019-03-27 04:00:14,919] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.9205091e-27 1.0000000e+00 1.5179052e-31 5.9316092e-24 7.7339442e-17], sum to 1.0000
[2019-03-27 04:00:14,938] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8977
[2019-03-27 04:00:14,943] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 88.66666666666667, 1.0, 2.0, 0.7909689597154299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1140693.659455749, 1140693.659455748, 246594.0888840664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1761600.0000, 
sim time next is 1762200.0000, 
raw observation next is [24.5, 88.5, 1.0, 2.0, 0.7681212971222121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1111738.9730765, 1111738.973076499, 241469.6755546128], 
processed observation next is [1.0, 0.391304347826087, 0.3601895734597157, 0.885, 1.0, 1.0, 0.7206280688219423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3088163814101389, 0.3088163814101386, 0.3604025008277803], 
reward next is 0.6396, 
noisyNet noise sample is [array([0.3947273], dtype=float32), -2.1148307]. 
=============================================
[2019-03-27 04:00:15,139] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2171469: loss 0.0099
[2019-03-27 04:00:15,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2171469: learning rate 0.0005
[2019-03-27 04:00:16,090] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2171842: loss 0.0264
[2019-03-27 04:00:16,102] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2171842: learning rate 0.0005
[2019-03-27 04:00:16,499] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2172030: loss 0.0038
[2019-03-27 04:00:16,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2172031: learning rate 0.0005
[2019-03-27 04:00:17,293] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2172398: loss 0.0285
[2019-03-27 04:00:17,296] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2172399: learning rate 0.0005
[2019-03-27 04:00:17,973] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2172713: loss 0.1067
[2019-03-27 04:00:17,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2172713: learning rate 0.0005
[2019-03-27 04:00:18,093] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2172767: loss 0.0707
[2019-03-27 04:00:18,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2172769: learning rate 0.0005
[2019-03-27 04:00:18,202] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2172824: loss 0.0036
[2019-03-27 04:00:18,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2172826: learning rate 0.0005
[2019-03-27 04:00:18,851] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2173122: loss 0.0048
[2019-03-27 04:00:18,856] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2173122: learning rate 0.0005
[2019-03-27 04:00:18,888] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2173138: loss 0.0502
[2019-03-27 04:00:18,891] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2173138: learning rate 0.0005
[2019-03-27 04:00:20,394] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2173838: loss 0.0249
[2019-03-27 04:00:20,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2173838: learning rate 0.0005
[2019-03-27 04:00:21,276] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4560850e-20 1.0000000e+00 5.4280347e-24 2.6410247e-18 3.8101496e-13], sum to 1.0000
[2019-03-27 04:00:21,281] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1931
[2019-03-27 04:00:21,291] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2003494.130511868 W.
[2019-03-27 04:00:21,298] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.48333333333333, 76.5, 1.0, 2.0, 0.7164597111598873, 1.0, 2.0, 0.7164597111598873, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2003494.130511868, 2003494.130511868, 381252.3589353504], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2195400.0000, 
sim time next is 2196000.0000, 
raw observation next is [29.6, 76.0, 1.0, 2.0, 0.7145404218203892, 1.0, 2.0, 0.7145404218203892, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1998122.061695378, 1998122.061695378, 380413.5208403018], 
processed observation next is [1.0, 0.43478260869565216, 0.6018957345971565, 0.76, 1.0, 1.0, 0.656072797373963, 1.0, 1.0, 0.656072797373963, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5550339060264939, 0.5550339060264939, 0.5677813743885102], 
reward next is 0.4322, 
noisyNet noise sample is [array([0.49744222], dtype=float32), 1.2499342]. 
=============================================
[2019-03-27 04:00:21,323] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[54.93525 ]
 [54.993225]
 [55.737957]
 [56.663296]
 [57.728172]], R is [[55.00658798]
 [54.88748932]
 [54.33861542]
 [53.79523087]
 [53.25727844]].
[2019-03-27 04:00:22,254] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2174708: loss 0.0073
[2019-03-27 04:00:22,256] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2174708: learning rate 0.0005
[2019-03-27 04:00:22,890] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 04:00:22,892] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:00:22,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:00:22,894] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:00:22,894] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:00:22,895] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:00:22,896] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:00:22,898] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:00:22,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:00:22,899] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:00:22,900] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:00:23,075] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-03-27 04:00:23,099] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-03-27 04:00:23,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-03-27 04:00:23,117] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-03-27 04:00:23,120] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-03-27 04:00:40,034] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:00:40,035] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.93084724666667, 94.66576080000002, 1.0, 2.0, 0.3025333307066825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486857.6586817062, 486857.6586817057, 166149.6691544108]
[2019-03-27 04:00:40,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:00:40,040] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7290301843920267
[2019-03-27 04:00:42,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:00:42,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.33333333333334, 53.5, 1.0, 2.0, 0.2032750961930581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 339766.9681190295, 339766.9681190295, 155642.9452991217]
[2019-03-27 04:00:42,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:00:42,433] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1237387328745927
[2019-03-27 04:01:11,365] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:01:11,367] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.03333333333333, 57.00000000000001, 1.0, 2.0, 0.5224059709805937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729991.9371659124, 729991.9371659124, 187180.6816134113]
[2019-03-27 04:01:11,368] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:01:11,370] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17497682145039428
[2019-03-27 04:01:24,878] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:01:24,879] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.1, 50.33333333333334, 1.0, 2.0, 0.5871877786947494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 820550.7476387648, 820550.7476387654, 198383.8901255574]
[2019-03-27 04:01:24,881] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:01:24,884] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3533384976407419
[2019-03-27 04:01:34,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:01:34,720] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.45, 65.83333333333334, 1.0, 2.0, 0.6900812018925658, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564911561, 964401.8474184087, 964401.8474184081, 218723.0710807033]
[2019-03-27 04:01:34,722] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:01:34,725] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20687642797576866
[2019-03-27 04:01:47,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:01:47,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.96666666666667, 88.66666666666666, 1.0, 2.0, 0.5385480237658156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752556.2706470994, 752556.2706471, 189855.7897335218]
[2019-03-27 04:01:47,014] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:01:47,016] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2831197e-37], sampled 0.6049632471082105
[2019-03-27 04:02:12,425] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:02:12,427] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.08867696, 84.97550562666667, 1.0, 2.0, 0.3580677748027102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 551861.6440941344, 551861.6440941337, 170809.7250711511]
[2019-03-27 04:02:12,428] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:02:12,432] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8129697063246278
[2019-03-27 04:02:16,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.060804266]
[2019-03-27 04:02:16,964] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.43345015833334, 92.311421855, 1.0, 2.0, 0.5102826091003307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713045.4870150879, 713045.4870150879, 185221.9245220947]
[2019-03-27 04:02:16,967] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:02:16,971] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6989171247827294
[2019-03-27 04:02:18,443] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:02:18,683] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:02:18,695] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:02:18,828] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:02:18,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:02:19,959] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2175000, evaluation results [2175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:02:24,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:02:24,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8998
[2019-03-27 04:02:24,092] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 88.66666666666667, 1.0, 2.0, 0.5116572900808024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714967.0495474691, 714967.0495474685, 185442.8376573193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2031600.0000, 
sim time next is 2032200.0000, 
raw observation next is [26.55, 88.0, 1.0, 2.0, 0.5122492992232585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715794.5754063062, 715794.5754063068, 185537.7050539116], 
processed observation next is [0.0, 0.5217391304347826, 0.4573459715639811, 0.88, 1.0, 1.0, 0.41234855328103426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1988318265017517, 0.19883182650175188, 0.27692194784165913], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.6192293], dtype=float32), -0.68916273]. 
=============================================
[2019-03-27 04:02:24,752] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5828266e-35 1.0000000e+00 0.0000000e+00 2.7689635e-33 2.0497545e-30], sum to 1.0000
[2019-03-27 04:02:24,763] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4097
[2019-03-27 04:02:24,769] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 96.16666666666666, 1.0, 2.0, 0.4104242768634576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606185.1451200444, 606185.1451200444, 174933.1700387081], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1977000.0000, 
sim time next is 1977600.0000, 
raw observation next is [23.06666666666667, 96.33333333333333, 1.0, 2.0, 0.412213088772087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607604.927948057, 607604.927948057, 175031.0478428343], 
processed observation next is [1.0, 0.9130434782608695, 0.29225908372827825, 0.9633333333333333, 1.0, 1.0, 0.29182299852058674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16877914665223803, 0.16877914665223803, 0.26124036991467803], 
reward next is 0.7388, 
noisyNet noise sample is [array([-1.6615657], dtype=float32), 0.4922593]. 
=============================================
[2019-03-27 04:02:25,139] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2177309: loss 0.0072
[2019-03-27 04:02:25,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2177311: learning rate 0.0005
[2019-03-27 04:02:25,167] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2177322: loss 0.0083
[2019-03-27 04:02:25,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2177323: learning rate 0.0005
[2019-03-27 04:02:26,533] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2177910: loss 0.3085
[2019-03-27 04:02:26,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2177912: learning rate 0.0005
[2019-03-27 04:02:28,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8341669e-30 1.0000000e+00 1.5241751e-34 7.3042323e-28 1.3014964e-24], sum to 1.0000
[2019-03-27 04:02:28,538] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6621
[2019-03-27 04:02:28,543] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 94.33333333333334, 1.0, 2.0, 1.001253187031836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399557.328591492, 1399557.328591492, 299305.3428096813], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2168400.0000, 
sim time next is 2169000.0000, 
raw observation next is [25.15, 94.5, 1.0, 2.0, 0.8989701569902202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1256500.92769338, 1256500.92769338, 269651.8516837089], 
processed observation next is [1.0, 0.08695652173913043, 0.3909952606635071, 0.945, 1.0, 1.0, 0.8782772975785785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34902803547038336, 0.34902803547038336, 0.4024654502741924], 
reward next is 0.5975, 
noisyNet noise sample is [array([-0.68217236], dtype=float32), 1.6490207]. 
=============================================
[2019-03-27 04:02:28,554] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.180527]
 [63.741756]
 [66.54791 ]
 [65.845184]
 [65.933136]], R is [[62.96938705]
 [62.89297104]
 [62.8038826 ]
 [62.90026474]
 [62.99575043]].
[2019-03-27 04:02:28,778] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2178904: loss 0.0226
[2019-03-27 04:02:28,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2178904: learning rate 0.0005
[2019-03-27 04:02:29,108] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2179049: loss 0.0586
[2019-03-27 04:02:29,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2179050: learning rate 0.0005
[2019-03-27 04:02:30,485] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2179666: loss 1.7036
[2019-03-27 04:02:30,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2179666: learning rate 0.0005
[2019-03-27 04:02:30,606] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2179719: loss 0.0088
[2019-03-27 04:02:30,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2179719: learning rate 0.0005
[2019-03-27 04:02:31,908] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2180300: loss 0.0054
[2019-03-27 04:02:31,910] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2180300: loss 1.9603
[2019-03-27 04:02:31,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2180300: learning rate 0.0005
[2019-03-27 04:02:31,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2180300: learning rate 0.0005
[2019-03-27 04:02:32,599] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2180605: loss 0.0438
[2019-03-27 04:02:32,602] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2180606: learning rate 0.0005
[2019-03-27 04:02:32,872] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2180725: loss 0.0061
[2019-03-27 04:02:32,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2180727: learning rate 0.0005
[2019-03-27 04:02:33,323] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2180933: loss 0.0133
[2019-03-27 04:02:33,325] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2180933: learning rate 0.0005
[2019-03-27 04:02:33,557] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2181034: loss 1.2881
[2019-03-27 04:02:33,559] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2181034: learning rate 0.0005
[2019-03-27 04:02:33,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.816592e-37 0.000000e+00], sum to 1.0000
[2019-03-27 04:02:33,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8932
[2019-03-27 04:02:33,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1828528.892752555 W.
[2019-03-27 04:02:33,838] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.56666666666666, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.439117523475206, 6.9112, 168.9105158702278, 1828528.892752555, 1454011.443598608, 311353.6122401083], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2379000.0000, 
sim time next is 2379600.0000, 
raw observation next is [32.6, 63.0, 1.0, 2.0, 0.6885538652715957, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005974766833833, 6.9112, 168.9119633842306, 1859111.726935185, 1791875.640765406, 382175.7570850166], 
processed observation next is [1.0, 0.5652173913043478, 0.7440758293838864, 0.63, 1.0, 1.0, 0.624763693098308, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009477476683383302, 0.0, 0.8294350684480036, 0.5164199241486626, 0.4977432335459461, 0.5704115777388307], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05881441], dtype=float32), -0.9456274]. 
=============================================
[2019-03-27 04:02:34,386] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2181407: loss 1.9170
[2019-03-27 04:02:34,392] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2181407: learning rate 0.0005
[2019-03-27 04:02:35,819] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2182041: loss 0.8864
[2019-03-27 04:02:35,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2182041: learning rate 0.0005
[2019-03-27 04:02:37,761] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2182904: loss 0.4748
[2019-03-27 04:02:37,766] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2182904: learning rate 0.0005
[2019-03-27 04:02:41,175] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.6773905e-18 1.0000000e+00 6.1851069e-21 9.5309299e-17 1.2648584e-12], sum to 1.0000
[2019-03-27 04:02:41,180] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0626
[2019-03-27 04:02:41,187] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2180667.894729891 W.
[2019-03-27 04:02:41,192] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.3, 89.0, 1.0, 2.0, 0.7797535291312769, 1.0, 1.0, 0.7797535291312769, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2180667.894729891, 2180667.894729891, 410126.9229349095], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2467800.0000, 
sim time next is 2468400.0000, 
raw observation next is [26.33333333333333, 89.0, 1.0, 2.0, 0.4761119294335013, 1.0, 2.0, 0.4761119294335013, 1.0, 1.0, 0.8111017685465027, 6.911199999999999, 6.9112, 170.5573041426782, 1997079.364147188, 1997079.364147189, 396296.7921648706], 
processed observation next is [1.0, 0.5652173913043478, 0.44707740916271704, 0.89, 1.0, 1.0, 0.3688095535343389, 1.0, 1.0, 0.3688095535343389, 1.0, 0.5, 0.7696363031054911, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5547442678186633, 0.5547442678186636, 0.5914877494998069], 
reward next is 0.4085, 
noisyNet noise sample is [array([0.12262975], dtype=float32), 0.567759]. 
=============================================
[2019-03-27 04:02:43,322] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2185359: loss 0.2803
[2019-03-27 04:02:43,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2185360: learning rate 0.0005
[2019-03-27 04:02:43,476] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2185426: loss 0.1146
[2019-03-27 04:02:43,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2185428: learning rate 0.0005
[2019-03-27 04:02:43,513] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2185444: loss 0.0195
[2019-03-27 04:02:43,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2185444: learning rate 0.0005
[2019-03-27 04:02:46,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:02:46,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4256
[2019-03-27 04:02:46,765] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.442320514599603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635328.231742431, 635328.231742431, 177274.088662323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2686800.0000, 
sim time next is 2687400.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4420703068049817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634967.3739069798, 634967.3739069805, 177237.9213649489], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.32779555036744784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17637982608527217, 0.17637982608527236, 0.26453421099246105], 
reward next is 0.7355, 
noisyNet noise sample is [array([-2.4445431], dtype=float32), -0.6762208]. 
=============================================
[2019-03-27 04:02:46,984] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2186991: loss 0.3812
[2019-03-27 04:02:46,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2186991: learning rate 0.0005
[2019-03-27 04:02:47,154] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2187070: loss 0.1545
[2019-03-27 04:02:47,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2187072: learning rate 0.0005
[2019-03-27 04:02:48,219] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2187536: loss 0.2926
[2019-03-27 04:02:48,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2187536: learning rate 0.0005
[2019-03-27 04:02:48,805] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2187801: loss 0.0931
[2019-03-27 04:02:48,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2187801: learning rate 0.0005
[2019-03-27 04:02:49,513] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:02:49,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4081
[2019-03-27 04:02:49,530] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 92.0, 1.0, 2.0, 0.4379643431404646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 632869.7156598038, 632869.7156598032, 177129.8350404189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2602800.0000, 
sim time next is 2603400.0000, 
raw observation next is [24.08333333333334, 92.0, 1.0, 2.0, 0.4366324199952233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 631501.1169488276, 631501.1169488283, 177008.9383551589], 
processed observation next is [0.0, 0.13043478260869565, 0.34044233807267016, 0.92, 1.0, 1.0, 0.3212438795123172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1754169769302299, 0.1754169769302301, 0.26419244530620734], 
reward next is 0.7358, 
noisyNet noise sample is [array([0.7993675], dtype=float32), 0.9389766]. 
=============================================
[2019-03-27 04:02:49,708] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2188194: loss 0.3423
[2019-03-27 04:02:49,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2188194: learning rate 0.0005
[2019-03-27 04:02:50,039] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2188343: loss 0.2923
[2019-03-27 04:02:50,042] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2188343: learning rate 0.0005
[2019-03-27 04:02:50,670] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2188630: loss 0.0067
[2019-03-27 04:02:50,672] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2188631: learning rate 0.0005
[2019-03-27 04:02:51,095] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2188815: loss 0.0170
[2019-03-27 04:02:51,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2188815: learning rate 0.0005
[2019-03-27 04:02:51,445] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2188973: loss 0.4354
[2019-03-27 04:02:51,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2188973: learning rate 0.0005
[2019-03-27 04:02:51,580] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2189035: loss 0.0126
[2019-03-27 04:02:51,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2189035: learning rate 0.0005
[2019-03-27 04:02:52,227] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2189321: loss 0.2296
[2019-03-27 04:02:52,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2189322: learning rate 0.0005
[2019-03-27 04:02:53,580] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2189923: loss 0.1533
[2019-03-27 04:02:53,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2189924: learning rate 0.0005
[2019-03-27 04:02:55,733] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2190888: loss 0.2001
[2019-03-27 04:02:55,735] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2190888: learning rate 0.0005
[2019-03-27 04:02:56,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:02:56,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9209
[2019-03-27 04:02:56,704] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 93.0, 1.0, 2.0, 0.5563324514544964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777416.9696505311, 777416.9696505311, 192892.673262696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2492400.0000, 
sim time next is 2493000.0000, 
raw observation next is [26.95, 93.0, 1.0, 2.0, 0.5557684444988097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776628.5402283682, 776628.5402283682, 192794.9312219789], 
processed observation next is [1.0, 0.8695652173913043, 0.476303317535545, 0.93, 1.0, 1.0, 0.4647812584323008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21573015006343563, 0.21573015006343563, 0.28775362868952076], 
reward next is 0.7122, 
noisyNet noise sample is [array([-0.657528], dtype=float32), -0.021622846]. 
=============================================
[2019-03-27 04:02:56,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.60873]
 [78.78413]
 [78.5327 ]
 [78.31794]
 [78.17311]], R is [[78.56819916]
 [78.49461365]
 [78.42204285]
 [78.35044861]
 [78.27970886]].
[2019-03-27 04:03:00,856] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2193155: loss 0.0446
[2019-03-27 04:03:00,858] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2193155: learning rate 0.0005
[2019-03-27 04:03:01,350] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2193372: loss 0.1332
[2019-03-27 04:03:01,354] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2193372: learning rate 0.0005
[2019-03-27 04:03:01,594] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2193480: loss 0.0966
[2019-03-27 04:03:01,597] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2193481: learning rate 0.0005
[2019-03-27 04:03:04,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:04,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3837
[2019-03-27 04:03:04,841] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4749165977153533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663739.5949625361, 663739.5949625368, 179775.6461363039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2631600.0000, 
sim time next is 2632200.0000, 
raw observation next is [26.16666666666667, 83.16666666666667, 1.0, 2.0, 0.4756558446764272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664644.4993947353, 664644.4993947353, 179869.5014180462], 
processed observation next is [0.0, 0.4782608695652174, 0.4391785150078992, 0.8316666666666667, 1.0, 1.0, 0.36826005382702065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18462347205409316, 0.18462347205409316, 0.2684619424149943], 
reward next is 0.7315, 
noisyNet noise sample is [array([2.0325263], dtype=float32), 0.10846837]. 
=============================================
[2019-03-27 04:03:05,202] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2195094: loss 0.3116
[2019-03-27 04:03:05,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2195094: learning rate 0.0005
[2019-03-27 04:03:05,297] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2195137: loss 0.2262
[2019-03-27 04:03:05,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2195137: learning rate 0.0005
[2019-03-27 04:03:05,768] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2195346: loss 0.0406
[2019-03-27 04:03:05,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2195346: learning rate 0.0005
[2019-03-27 04:03:06,973] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2195880: loss 0.1832
[2019-03-27 04:03:06,976] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2195880: learning rate 0.0005
[2019-03-27 04:03:07,228] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2195997: loss 0.0353
[2019-03-27 04:03:07,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2195998: learning rate 0.0005
[2019-03-27 04:03:08,635] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2196551: loss 0.3458
[2019-03-27 04:03:08,637] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2196551: learning rate 0.0005
[2019-03-27 04:03:09,059] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2196745: loss 0.0065
[2019-03-27 04:03:09,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2196745: learning rate 0.0005
[2019-03-27 04:03:09,192] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2196804: loss 0.2684
[2019-03-27 04:03:09,198] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2196804: learning rate 0.0005
[2019-03-27 04:03:09,737] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2197056: loss 0.2222
[2019-03-27 04:03:09,741] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2197056: learning rate 0.0005
[2019-03-27 04:03:10,018] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2197188: loss 0.0349
[2019-03-27 04:03:10,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2197190: learning rate 0.0005
[2019-03-27 04:03:10,264] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2197301: loss 0.3042
[2019-03-27 04:03:10,268] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2197303: learning rate 0.0005
[2019-03-27 04:03:11,349] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2197808: loss 0.0068
[2019-03-27 04:03:11,353] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2197809: learning rate 0.0005
[2019-03-27 04:03:12,506] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:12,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1747
[2019-03-27 04:03:12,519] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4140642209017884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610085.4948599548, 610085.4948599555, 175257.3911826799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2832000.0000, 
sim time next is 2832600.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4144579502658128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610664.6346321506, 610664.6346321513, 175312.0125101796], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.294527650922666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1696290651755974, 0.1696290651755976, 0.26165972016444716], 
reward next is 0.7383, 
noisyNet noise sample is [array([0.32256117], dtype=float32), -0.5577112]. 
=============================================
[2019-03-27 04:03:13,562] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2198841: loss 0.0257
[2019-03-27 04:03:13,564] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2198841: learning rate 0.0005
[2019-03-27 04:03:14,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:14,366] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9553
[2019-03-27 04:03:14,373] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3665892650802927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564709.399389607, 564709.399389607, 171892.6060205585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2791200.0000, 
sim time next is 2791800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3647429561324327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 561865.4513084091, 561865.4513084085, 171649.2313229368], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.23463006762943697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15607373647455808, 0.1560737364745579, 0.2561928825715475], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.04140419], dtype=float32), -0.21533237]. 
=============================================
[2019-03-27 04:03:16,063] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 04:03:16,065] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:03:16,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:03:16,067] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:03:16,069] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:03:16,069] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:03:16,069] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:03:16,071] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:03:16,072] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:03:16,073] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:03:16,074] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:03:16,093] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-03-27 04:03:16,114] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-03-27 04:03:16,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-03-27 04:03:16,166] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-03-27 04:03:16,185] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-03-27 04:04:05,085] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.047524396]
[2019-03-27 04:04:05,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.53333333333333, 62.33333333333333, 1.0, 2.0, 0.6187080328673876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104292, 864615.859803293, 864615.8598032923, 204277.2356630138]
[2019-03-27 04:04:05,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:04:05,096] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31880279367799635
[2019-03-27 04:04:45,435] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.047524396]
[2019-03-27 04:04:45,436] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.96666666666667, 61.0, 1.0, 2.0, 0.5505368927625606, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9559316324242632, 6.9112, 6.9112, 168.9129419958881, 1539188.282129384, 1539188.282129384, 336880.4517838178]
[2019-03-27 04:04:45,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:04:45,441] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7250465546487884
[2019-03-27 04:05:04,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.047524396]
[2019-03-27 04:05:04,395] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.98039134166667, 93.84962847833333, 1.0, 2.0, 0.2469472750338841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 409194.1701211748, 409194.1701211748, 160450.8401192673]
[2019-03-27 04:05:04,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:05:04,401] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05225442029896865
[2019-03-27 04:05:11,763] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:05:11,807] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:05:11,875] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:05:12,204] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:05:12,225] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:05:13,241] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2200000, evaluation results [2200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:05:15,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:15,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2550
[2019-03-27 04:05:15,988] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 91.5, 1.0, 2.0, 0.6531035362529316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1000958.272467623, 1000958.272467623, 221994.5514946665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3141000.0000, 
sim time next is 3141600.0000, 
raw observation next is [22.66666666666666, 90.66666666666666, 1.0, 2.0, 0.669519341242912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024165.140682468, 1024165.140682468, 225473.9364249042], 
processed observation next is [1.0, 0.34782608695652173, 0.27330173775671385, 0.9066666666666666, 1.0, 1.0, 0.6018305316179663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2844903168562411, 0.2844903168562411, 0.33652826332075253], 
reward next is 0.6635, 
noisyNet noise sample is [array([-1.1793034], dtype=float32), -0.6290274]. 
=============================================
[2019-03-27 04:05:16,163] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2201306: loss 0.0062
[2019-03-27 04:05:16,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2201307: learning rate 0.0005
[2019-03-27 04:05:16,363] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2201392: loss 0.0063
[2019-03-27 04:05:16,365] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2201392: learning rate 0.0005
[2019-03-27 04:05:16,511] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:16,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9129
[2019-03-27 04:05:16,525] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.7332911583064474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073819.927410601, 1073819.927410601, 234747.6309682543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3150000.0000, 
sim time next is 3150600.0000, 
raw observation next is [25.16666666666667, 82.33333333333334, 1.0, 2.0, 0.7645639183483103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117259.567119488, 1117259.567119488, 242017.6803069109], 
processed observation next is [1.0, 0.4782608695652174, 0.39178515007898923, 0.8233333333333335, 1.0, 1.0, 0.716342070299169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31034987975541334, 0.31034987975541334, 0.3612204183685237], 
reward next is 0.6388, 
noisyNet noise sample is [array([1.0254588], dtype=float32), -0.7347156]. 
=============================================
[2019-03-27 04:05:17,063] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2201705: loss 6.2258
[2019-03-27 04:05:17,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2201706: learning rate 0.0005
[2019-03-27 04:05:20,070] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2203040: loss 0.0095
[2019-03-27 04:05:20,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2203040: learning rate 0.0005
[2019-03-27 04:05:20,255] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2203120: loss 0.0253
[2019-03-27 04:05:20,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2203122: learning rate 0.0005
[2019-03-27 04:05:20,728] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2203334: loss 0.0608
[2019-03-27 04:05:20,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2203334: learning rate 0.0005
[2019-03-27 04:05:21,872] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2203845: loss 0.0261
[2019-03-27 04:05:21,875] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2203847: learning rate 0.0005
[2019-03-27 04:05:22,169] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2203978: loss 0.0083
[2019-03-27 04:05:22,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2203978: learning rate 0.0005
[2019-03-27 04:05:23,423] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2204537: loss 0.0232
[2019-03-27 04:05:23,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2204537: learning rate 0.0005
[2019-03-27 04:05:23,722] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2204670: loss 0.0213
[2019-03-27 04:05:23,726] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2204671: learning rate 0.0005
[2019-03-27 04:05:23,835] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2204718: loss 0.0085
[2019-03-27 04:05:23,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2204718: learning rate 0.0005
[2019-03-27 04:05:24,611] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2205066: loss 0.0157
[2019-03-27 04:05:24,614] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2205067: learning rate 0.0005
[2019-03-27 04:05:24,758] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2205128: loss 0.0130
[2019-03-27 04:05:24,761] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2205128: learning rate 0.0005
[2019-03-27 04:05:24,979] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2205225: loss 0.0140
[2019-03-27 04:05:24,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2205227: learning rate 0.0005
[2019-03-27 04:05:26,253] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2205779: loss 0.0143
[2019-03-27 04:05:26,254] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2205779: learning rate 0.0005
[2019-03-27 04:05:28,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.235187e-38 0.000000e+00], sum to 1.0000
[2019-03-27 04:05:28,255] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0354
[2019-03-27 04:05:28,264] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.7583974758994021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123210.192958733, 1123210.192958733, 242446.031017916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3072000.0000, 
sim time next is 3072600.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.8339151105066627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229316.07873723, 1229316.078737229, 261326.0559229381], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.97, 1.0, 1.0, 0.7998977235020032, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34147668853811947, 0.3414766885381192, 0.39003888943722104], 
reward next is 0.6100, 
noisyNet noise sample is [array([0.71751183], dtype=float32), -0.23116097]. 
=============================================
[2019-03-27 04:05:28,603] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2206821: loss 0.0096
[2019-03-27 04:05:28,606] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2206821: learning rate 0.0005
[2019-03-27 04:05:29,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0354579e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1727405e-38], sum to 1.0000
[2019-03-27 04:05:29,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9852
[2019-03-27 04:05:29,662] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 89.0, 1.0, 2.0, 0.8138384672348753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137447.650540901, 1137447.650540901, 247337.8339284521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3393600.0000, 
sim time next is 3394200.0000, 
raw observation next is [27.83333333333334, 89.0, 1.0, 2.0, 0.8402982449068611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174449.165960978, 1174449.165960978, 254045.2440379111], 
processed observation next is [1.0, 0.2608695652173913, 0.5181674565560824, 0.89, 1.0, 1.0, 0.8075882468757363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.326235879433605, 0.326235879433605, 0.37917200602673296], 
reward next is 0.6208, 
noisyNet noise sample is [array([-0.27152678], dtype=float32), 0.5788449]. 
=============================================
[2019-03-27 04:05:30,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:30,425] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2011
[2019-03-27 04:05:30,438] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7239280371469882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1011725.986990375, 1011725.986990374, 226091.4214379836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3466800.0000, 
sim time next is 3467400.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.7126459838344051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995951.3542207689, 995951.3542207689, 223595.1156155694], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.6537903419691627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27665315395021356, 0.27665315395021356, 0.33372405315756626], 
reward next is 0.6663, 
noisyNet noise sample is [array([0.9561602], dtype=float32), -0.4316504]. 
=============================================
[2019-03-27 04:05:31,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:31,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1407
[2019-03-27 04:05:31,800] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5022602342015936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701831.6854603944, 701831.685460395, 183951.3551340682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3175800.0000, 
sim time next is 3176400.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.504063230437664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704351.9315865864, 704351.9315865864, 184235.3274688491], 
processed observation next is [1.0, 0.782608695652174, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.4024858198044144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19565331432960734, 0.19565331432960734, 0.2749781006997748], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.5595007], dtype=float32), 0.3569588]. 
=============================================
[2019-03-27 04:05:33,899] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2209175: loss 0.0201
[2019-03-27 04:05:33,902] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2209178: learning rate 0.0005
[2019-03-27 04:05:34,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:34,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8883
[2019-03-27 04:05:34,299] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163185270364803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721482.6658881874, 721482.665888188, 186192.4860737829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5163119925786053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 186191.4310685439], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41724336455253647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2004093143888005, 0.2004093143888005, 0.27789765831125957], 
reward next is 0.7221, 
noisyNet noise sample is [array([-1.1654989], dtype=float32), 0.8476873]. 
=============================================
[2019-03-27 04:05:34,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.289635]
 [73.95056 ]
 [74.15783 ]
 [74.82636 ]
 [74.80233 ]], R is [[72.63047791]
 [72.62627411]
 [72.62204742]
 [72.61779022]
 [72.6136322 ]].
[2019-03-27 04:05:34,327] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2209365: loss 0.0218
[2019-03-27 04:05:34,329] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2209365: learning rate 0.0005
[2019-03-27 04:05:35,559] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2209919: loss 0.0226
[2019-03-27 04:05:35,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2209919: learning rate 0.0005
[2019-03-27 04:05:37,678] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2210854: loss 0.0287
[2019-03-27 04:05:37,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2210855: learning rate 0.0005
[2019-03-27 04:05:37,985] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2210992: loss 0.0230
[2019-03-27 04:05:37,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2210992: learning rate 0.0005
[2019-03-27 04:05:39,555] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2211689: loss 2.3762
[2019-03-27 04:05:39,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2211690: learning rate 0.0005
[2019-03-27 04:05:39,693] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2211752: loss 0.0163
[2019-03-27 04:05:39,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2211752: learning rate 0.0005
[2019-03-27 04:05:40,970] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2212302: loss 0.0146
[2019-03-27 04:05:40,973] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2212302: learning rate 0.0005
[2019-03-27 04:05:41,001] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2212315: loss 5.2692
[2019-03-27 04:05:41,004] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2212315: learning rate 0.0005
[2019-03-27 04:05:41,377] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2212480: loss 0.0203
[2019-03-27 04:05:41,378] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2212481: learning rate 0.0005
[2019-03-27 04:05:42,210] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2212854: loss 0.0156
[2019-03-27 04:05:42,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2212854: learning rate 0.0005
[2019-03-27 04:05:42,475] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2212971: loss 0.0138
[2019-03-27 04:05:42,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2212971: learning rate 0.0005
[2019-03-27 04:05:42,632] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2213041: loss 10.8835
[2019-03-27 04:05:42,636] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2213042: learning rate 0.0005
[2019-03-27 04:05:43,558] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2213453: loss 8.3662
[2019-03-27 04:05:43,561] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2213454: learning rate 0.0005
[2019-03-27 04:05:44,834] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2214015: loss 3.0870
[2019-03-27 04:05:44,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2214015: learning rate 0.0005
[2019-03-27 04:05:46,054] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:46,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7603
[2019-03-27 04:05:46,075] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5113292983090572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714508.5743503075, 714508.574350308, 185390.1024351827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3451200.0000, 
sim time next is 3451800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5110902771966108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714174.4647164326, 714174.4647164332, 185351.8733841149], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4109521412007359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1983817957545646, 0.19838179575456477, 0.27664458714047], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.4418084], dtype=float32), 1.0963886]. 
=============================================
[2019-03-27 04:05:47,111] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2215032: loss 6.4886
[2019-03-27 04:05:47,114] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2215033: learning rate 0.0005
[2019-03-27 04:05:49,262] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0962975e-38 1.0000000e+00 0.0000000e+00 2.0215273e-38 1.8275262e-36], sum to 1.0000
[2019-03-27 04:05:49,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7285
[2019-03-27 04:05:49,278] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.0, 1.0, 2.0, 0.7631696739617206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1066595.697582479, 1066595.697582479, 235069.5729072058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3567000.0000, 
sim time next is 3567600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.7939722997936991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1109667.527557437, 1109667.527557437, 242435.3599111704], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.79, 1.0, 1.0, 0.7517738551731314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30824097987706583, 0.30824097987706583, 0.3618438207629409], 
reward next is 0.6382, 
noisyNet noise sample is [array([-2.0079482], dtype=float32), -1.5305852]. 
=============================================
[2019-03-27 04:05:50,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:50,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3341
[2019-03-27 04:05:50,724] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 76.5, 1.0, 2.0, 0.6064170940601058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869819.2734582076, 869819.2734582076, 204832.4250598399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3731400.0000, 
sim time next is 3732000.0000, 
raw observation next is [26.33333333333334, 77.33333333333333, 1.0, 2.0, 0.6237971950812394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 896020.8228380596, 896020.822838059, 208406.2818289112], 
processed observation next is [1.0, 0.17391304347826086, 0.44707740916271754, 0.7733333333333333, 1.0, 1.0, 0.5467436085316137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2488946730105721, 0.24889467301057194, 0.31105415198344954], 
reward next is 0.6889, 
noisyNet noise sample is [array([0.30865508], dtype=float32), 0.5444441]. 
=============================================
[2019-03-27 04:05:50,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[56.148613]
 [55.944218]
 [55.66282 ]
 [54.79888 ]
 [54.851715]], R is [[56.09476089]
 [56.22809219]
 [56.35877228]
 [56.47954941]
 [56.58361053]].
[2019-03-27 04:05:52,075] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2217231: loss 0.4610
[2019-03-27 04:05:52,078] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2217232: learning rate 0.0005
[2019-03-27 04:05:52,356] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2217364: loss 0.0204
[2019-03-27 04:05:52,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2217364: learning rate 0.0005
[2019-03-27 04:05:53,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:05:53,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7113
[2019-03-27 04:05:53,767] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5199642814043567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726578.8381689326, 726578.8381689332, 186783.0717770881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5197819404245921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726323.954523323, 726323.9545233224, 186753.4403347651], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4214240246079423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20175665403425638, 0.20175665403425622, 0.2787364781115897], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.8166801], dtype=float32), 0.5874566]. 
=============================================
[2019-03-27 04:05:54,007] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2218090: loss 55.0794
[2019-03-27 04:05:54,009] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2218090: learning rate 0.0005
[2019-03-27 04:05:54,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7785978e-29 1.0000000e+00 4.3423871e-33 5.2856606e-29 2.3188009e-28], sum to 1.0000
[2019-03-27 04:05:54,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4145
[2019-03-27 04:05:54,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3627670.888276043 W.
[2019-03-27 04:05:54,699] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 9.973916485824319, 6.9112, 168.8951409120766, 3627670.888276043, 1455103.571998481, 306390.2503823172], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3588000.0000, 
sim time next is 3588600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6148981326139551, 1.0, 1.0, 0.6148981326139551, 1.0, 1.0, 1.03, 6.953777969605904, 6.9112, 170.5573041426782, 2579864.16624543, 2549363.834655652, 493443.8250936509], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5360218465228375, 1.0, 0.5, 0.5360218465228375, 1.0, 0.5, 1.0365853658536586, 0.004257796960590365, 0.0, 0.8375144448122397, 0.7166289350681749, 0.708156620737681, 0.7364833210352999], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5722431], dtype=float32), 1.5030639]. 
=============================================
[2019-03-27 04:05:55,696] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2218848: loss 1.6843
[2019-03-27 04:05:55,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2218848: learning rate 0.0005
[2019-03-27 04:05:56,306] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2219064: loss 5.4615
[2019-03-27 04:05:56,308] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2219064: learning rate 0.0005
[2019-03-27 04:05:57,309] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2219506: loss 0.0165
[2019-03-27 04:05:57,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2219508: learning rate 0.0005
[2019-03-27 04:05:58,005] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2219817: loss 0.8806
[2019-03-27 04:05:58,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2219818: learning rate 0.0005
[2019-03-27 04:05:58,648] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2220101: loss 0.0455
[2019-03-27 04:05:58,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2220101: learning rate 0.0005
[2019-03-27 04:05:59,141] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2220323: loss 5.0160
[2019-03-27 04:05:59,144] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2220324: learning rate 0.0005
[2019-03-27 04:05:59,430] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2220444: loss 1.9295
[2019-03-27 04:05:59,436] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2220446: learning rate 0.0005
[2019-03-27 04:06:00,398] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2220828: loss 1.2731
[2019-03-27 04:06:00,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2220828: learning rate 0.0005
[2019-03-27 04:06:00,453] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:00,464] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3714
[2019-03-27 04:06:00,469] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 76.5, 1.0, 2.0, 0.5079923929074889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709844.1747993702, 709844.1747993702, 184857.5613631158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3627000.0000, 
sim time next is 3627600.0000, 
raw observation next is [28.0, 75.66666666666666, 1.0, 2.0, 0.5036069638713233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703714.157063147, 703714.1570631478, 184162.8827910981], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7566666666666666, 1.0, 1.0, 0.4019361010497871, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19547615473976307, 0.19547615473976326, 0.27486997431507176], 
reward next is 0.7251, 
noisyNet noise sample is [array([-1.6519666], dtype=float32), -0.8048152]. 
=============================================
[2019-03-27 04:06:00,640] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2220915: loss 0.0231
[2019-03-27 04:06:00,645] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2220916: learning rate 0.0005
[2019-03-27 04:06:00,888] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2221031: loss 1.8773
[2019-03-27 04:06:00,889] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2221031: learning rate 0.0005
[2019-03-27 04:06:01,620] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2221369: loss 0.0285
[2019-03-27 04:06:01,623] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2221370: learning rate 0.0005
[2019-03-27 04:06:02,637] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2221840: loss 0.0479
[2019-03-27 04:06:02,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2221840: learning rate 0.0005
[2019-03-27 04:06:02,925] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:02,932] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8146
[2019-03-27 04:06:02,935] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666666, 66.33333333333333, 1.0, 2.0, 0.5537453639461969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773800.466035995, 773800.4660359956, 192447.4811714834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3779400.0000, 
sim time next is 3780000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5600600753774084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 782627.856937847, 782627.8569378477, 193542.8126304921], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.67, 1.0, 1.0, 0.46995189804507037, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21739662692717973, 0.2173966269271799, 0.2888698695977494], 
reward next is 0.7111, 
noisyNet noise sample is [array([0.24909101], dtype=float32), -0.08011077]. 
=============================================
[2019-03-27 04:06:02,947] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.13141 ]
 [75.22071 ]
 [74.41393 ]
 [70.73278 ]
 [62.062405]], R is [[75.453125  ]
 [75.41136169]
 [75.3707962 ]
 [75.33192444]
 [74.57860565]].
[2019-03-27 04:06:04,937] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2222896: loss 0.0317
[2019-03-27 04:06:04,941] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2222896: learning rate 0.0005
[2019-03-27 04:06:09,499] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 04:06:09,500] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:06:09,501] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:06:09,502] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:09,503] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:06:09,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:06:09,504] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:09,505] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:06:09,506] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:09,505] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:09,507] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:09,510] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:09,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5205
[2019-03-27 04:06:09,519] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 92.33333333333334, 1.0, 2.0, 0.5711563119389853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798139.5617838425, 798139.5617838431, 195494.9711987829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3900000.0000, 
sim time next is 3900600.0000, 
raw observation next is [27.5, 91.5, 1.0, 2.0, 0.5719451471865765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 799242.3028045539, 799242.3028045546, 195635.3687388565], 
processed observation next is [0.0, 0.13043478260869565, 0.5023696682464456, 0.915, 1.0, 1.0, 0.4842712616705741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22201175077904275, 0.22201175077904295, 0.29199308766993504], 
reward next is 0.7080, 
noisyNet noise sample is [array([1.145792], dtype=float32), 2.1765492]. 
=============================================
[2019-03-27 04:06:09,538] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-03-27 04:06:09,561] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-03-27 04:06:09,562] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-03-27 04:06:09,582] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-03-27 04:06:09,602] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-03-27 04:06:39,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.037328478]
[2019-03-27 04:06:39,650] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.23333333333333, 86.66666666666667, 1.0, 2.0, 0.4959851917591979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 693060.4151499391, 693060.4151499397, 182969.4248038846]
[2019-03-27 04:06:39,651] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:06:39,654] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22110670725489168
[2019-03-27 04:06:56,271] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.037328478]
[2019-03-27 04:06:56,272] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.66666666666666, 75.66666666666666, 1.0, 2.0, 0.4819381726124791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673425.7185285833, 673425.7185285833, 180814.7536619801]
[2019-03-27 04:06:56,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:06:56,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9368069594752327
[2019-03-27 04:07:15,195] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.037328478]
[2019-03-27 04:07:15,198] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.04788715, 61.71814740666666, 1.0, 2.0, 0.524133054102173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732406.1350623824, 732406.135062383, 187463.0970456809]
[2019-03-27 04:07:15,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:07:15,204] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24298725345780958
[2019-03-27 04:07:24,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.037328478]
[2019-03-27 04:07:24,321] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.53333333333333, 88.66666666666667, 1.0, 2.0, 0.6271497783242929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876417.6759226759, 876417.6759226759, 205913.3451163872]
[2019-03-27 04:07:24,324] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:07:24,326] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8553162042744522
[2019-03-27 04:08:03,742] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18813051], dtype=float32), 0.037328478]
[2019-03-27 04:08:03,743] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.25, 63.0, 1.0, 2.0, 0.3932755594480326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610311.0925578944, 610311.0925578944, 175996.1987962998]
[2019-03-27 04:08:03,745] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:08:03,750] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05749602162842904
[2019-03-27 04:08:04,536] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:08:04,738] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:08:05,010] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:08:05,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:08:05,101] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:08:06,115] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2225000, evaluation results [2225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:08:06,480] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2225166: loss 0.0287
[2019-03-27 04:08:06,481] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2225167: learning rate 0.0005
[2019-03-27 04:08:06,799] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2225313: loss 0.0560
[2019-03-27 04:08:06,801] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2225313: learning rate 0.0005
[2019-03-27 04:08:08,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:08,094] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0306
[2019-03-27 04:08:08,098] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 90.66666666666667, 1.0, 2.0, 0.8263011242058245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1154875.345869785, 1154875.345869785, 250469.8526333685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4083600.0000, 
sim time next is 4084200.0000, 
raw observation next is [27.0, 91.5, 1.0, 2.0, 0.8671886007029677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1212054.125597888, 1212054.125597888, 261072.4786098207], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.915, 1.0, 1.0, 0.8399862659071899, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33668170155496885, 0.33668170155496885, 0.3896604158355533], 
reward next is 0.6103, 
noisyNet noise sample is [array([1.370131], dtype=float32), 0.20380543]. 
=============================================
[2019-03-27 04:08:08,622] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2226127: loss 0.0717
[2019-03-27 04:08:08,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2226127: learning rate 0.0005
[2019-03-27 04:08:09,971] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2226723: loss 0.0196
[2019-03-27 04:08:09,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2226723: learning rate 0.0005
[2019-03-27 04:08:10,442] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2226937: loss 0.0228
[2019-03-27 04:08:10,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2226938: learning rate 0.0005
[2019-03-27 04:08:11,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3902086e-35 1.0000000e+00 0.0000000e+00 7.2606539e-33 6.0924254e-35], sum to 1.0000
[2019-03-27 04:08:11,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3710
[2019-03-27 04:08:11,963] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 85.66666666666667, 1.0, 2.0, 0.9084117563382776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1269705.450704207, 1269705.450704207, 272265.2237850439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [28.5, 86.5, 1.0, 2.0, 0.9235018350140526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1290809.980066686, 1290809.980066686, 276487.3305453388], 
processed observation next is [1.0, 0.13043478260869565, 0.5497630331753555, 0.865, 1.0, 1.0, 0.9078335361615091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35855832779630165, 0.35855832779630165, 0.41266765753035645], 
reward next is 0.5873, 
noisyNet noise sample is [array([-1.7097666], dtype=float32), -0.13615607]. 
=============================================
[2019-03-27 04:08:12,208] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2227722: loss 63.4470
[2019-03-27 04:08:12,210] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2227722: learning rate 0.0005
[2019-03-27 04:08:12,290] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2227763: loss 0.0680
[2019-03-27 04:08:12,293] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2227763: learning rate 0.0005
[2019-03-27 04:08:13,206] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2228166: loss 80.4964
[2019-03-27 04:08:13,210] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2228167: learning rate 0.0005
[2019-03-27 04:08:13,625] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2228356: loss 0.0430
[2019-03-27 04:08:13,632] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2228358: learning rate 0.0005
[2019-03-27 04:08:13,837] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2228451: loss 0.0899
[2019-03-27 04:08:13,840] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2228452: learning rate 0.0005
[2019-03-27 04:08:14,659] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2228800: loss 0.0234
[2019-03-27 04:08:14,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2228801: learning rate 0.0005
[2019-03-27 04:08:15,051] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2228974: loss 0.0186
[2019-03-27 04:08:15,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2228974: learning rate 0.0005
[2019-03-27 04:08:15,101] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:15,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6362
[2019-03-27 04:08:15,115] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 73.66666666666666, 1.0, 2.0, 0.6007832373284243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 839556.8869685862, 839556.8869685855, 200892.2111110543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3919200.0000, 
sim time next is 3919800.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6032727929619313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843037.2654826796, 843037.2654826796, 201357.582725948], 
processed observation next is [0.0, 0.34782608695652173, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5220154132071461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2341770181896332, 0.2341770181896332, 0.3005337055611164], 
reward next is 0.6995, 
noisyNet noise sample is [array([0.4077686], dtype=float32), 0.1978769]. 
=============================================
[2019-03-27 04:08:15,383] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2229122: loss 82.1762
[2019-03-27 04:08:15,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2229123: learning rate 0.0005
[2019-03-27 04:08:16,293] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2229529: loss 72.3638
[2019-03-27 04:08:16,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2229529: learning rate 0.0005
[2019-03-27 04:08:17,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5146705e-15 1.0000000e+00 1.6528127e-17 1.0677374e-12 7.9469858e-10], sum to 1.0000
[2019-03-27 04:08:17,491] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2748
[2019-03-27 04:08:17,498] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2230063: loss 81.6161
[2019-03-27 04:08:17,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2450202.955122611 W.
[2019-03-27 04:08:17,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2230064: learning rate 0.0005
[2019-03-27 04:08:17,506] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.16666666666666, 79.00000000000001, 1.0, 2.0, 0.876036422561389, 1.0, 1.0, 0.876036422561389, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2450202.955122611, 2450202.955122611, 458592.0400201577], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4093800.0000, 
sim time next is 4094400.0000, 
raw observation next is [30.33333333333334, 79.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.018659273629153, 6.9112, 168.9068042996456, 3070043.834090911, 2284402.725527942, 473487.9658967908], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783573, 0.79, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.11074592736291526, 0.0, 0.829409734980852, 0.8527899539141419, 0.6345563126466506, 0.7066984565623743], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5583993], dtype=float32), -1.3707831]. 
=============================================
[2019-03-27 04:08:19,202] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5960552e-11 1.0000000e+00 3.2590125e-13 4.8147980e-10 4.4536865e-08], sum to 1.0000
[2019-03-27 04:08:19,209] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9880
[2019-03-27 04:08:19,219] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3195431.04997385 W.
[2019-03-27 04:08:19,225] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 65.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.310128128709554, 6.9112, 170.5573041426782, 3195431.04997385, 2909662.604020434, 551538.6017672115], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4113600.0000, 
sim time next is 4114200.0000, 
raw observation next is [35.66666666666666, 64.5, 1.0, 2.0, 0.9003310126010206, 1.0, 2.0, 0.7707555458147729, 1.0, 1.0, 1.03, 7.005113533389591, 6.9112, 170.5573041426782, 3234625.58213916, 3167351.498013904, 592076.8036656929], 
processed observation next is [1.0, 0.6086956521739131, 0.889415481832543, 0.645, 1.0, 1.0, 0.879916882651832, 1.0, 1.0, 0.7238018624274373, 1.0, 0.5, 1.0365853658536586, 0.009391353338959085, 0.0, 0.8375144448122397, 0.8985071061497667, 0.8798198605594177, 0.8836967218890939], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8356145], dtype=float32), 0.7481689]. 
=============================================
[2019-03-27 04:08:19,671] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2231027: loss 105.6555
[2019-03-27 04:08:19,674] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2231028: learning rate 0.0005
[2019-03-27 04:08:23,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:23,179] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1704
[2019-03-27 04:08:23,186] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5435752090157918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759583.6708194437, 759583.6708194437, 190705.1947949817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4050600.0000, 
sim time next is 4051200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5446465093164412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761081.2257639385, 761081.2257639385, 190886.9765032092], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45138133652583273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21141145160109404, 0.21141145160109404, 0.28490593507941675], 
reward next is 0.7151, 
noisyNet noise sample is [array([-0.64326704], dtype=float32), 0.21239659]. 
=============================================
[2019-03-27 04:08:24,679] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2233249: loss 115.3757
[2019-03-27 04:08:24,681] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2233249: learning rate 0.0005
[2019-03-27 04:08:25,056] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2233418: loss 105.1937
[2019-03-27 04:08:25,059] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2233418: learning rate 0.0005
[2019-03-27 04:08:26,595] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2234107: loss -34.9871
[2019-03-27 04:08:26,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2234107: learning rate 0.0005
[2019-03-27 04:08:28,024] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2234741: loss 137.4778
[2019-03-27 04:08:28,028] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2234741: learning rate 0.0005
[2019-03-27 04:08:28,591] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2234992: loss 125.4062
[2019-03-27 04:08:28,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2234992: learning rate 0.0005
[2019-03-27 04:08:29,970] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2235590: loss 0.3610
[2019-03-27 04:08:29,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2235590: learning rate 0.0005
[2019-03-27 04:08:30,395] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2235777: loss 117.7140
[2019-03-27 04:08:30,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2235777: learning rate 0.0005
[2019-03-27 04:08:30,962] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2236029: loss 0.5130
[2019-03-27 04:08:30,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2236029: learning rate 0.0005
[2019-03-27 04:08:31,621] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2236322: loss 138.8988
[2019-03-27 04:08:31,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2236323: learning rate 0.0005
[2019-03-27 04:08:31,791] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2236397: loss 126.5460
[2019-03-27 04:08:31,797] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2236400: learning rate 0.0005
[2019-03-27 04:08:32,716] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2236804: loss 172.6899
[2019-03-27 04:08:32,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2236804: learning rate 0.0005
[2019-03-27 04:08:33,062] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2236957: loss 0.5921
[2019-03-27 04:08:33,063] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2236957: learning rate 0.0005
[2019-03-27 04:08:33,085] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2236966: loss 138.2559
[2019-03-27 04:08:33,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2236966: learning rate 0.0005
[2019-03-27 04:08:33,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:33,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4568
[2019-03-27 04:08:33,645] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4483200.0000, 
sim time next is 4483800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5504258815900456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769160.1617394736, 769160.1617394736, 191873.5016085919], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4583444356506573, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2136556004831871, 0.2136556004831871, 0.2863783606098387], 
reward next is 0.7136, 
noisyNet noise sample is [array([-1.1708806], dtype=float32), -0.32051462]. 
=============================================
[2019-03-27 04:08:34,196] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2237468: loss 0.3320
[2019-03-27 04:08:34,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2237468: learning rate 0.0005
[2019-03-27 04:08:35,207] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2237919: loss 0.2964
[2019-03-27 04:08:35,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2237920: learning rate 0.0005
[2019-03-27 04:08:37,384] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2238886: loss 0.1984
[2019-03-27 04:08:37,387] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2238887: learning rate 0.0005
[2019-03-27 04:08:42,510] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2241169: loss 0.3079
[2019-03-27 04:08:42,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2241169: learning rate 0.0005
[2019-03-27 04:08:42,866] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2241325: loss 0.2534
[2019-03-27 04:08:42,868] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2241326: learning rate 0.0005
[2019-03-27 04:08:44,091] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:44,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6254
[2019-03-27 04:08:44,102] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 79.00000000000001, 1.0, 2.0, 0.5922119200917899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 827574.3424542204, 827574.3424542211, 199304.188262228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4435800.0000, 
sim time next is 4436400.0000, 
raw observation next is [30.33333333333334, 79.0, 1.0, 2.0, 0.5951730778916804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831713.9715415365, 831713.9715415365, 199850.5617176967], 
processed observation next is [0.0, 0.34782608695652173, 0.6366508688783573, 0.79, 1.0, 1.0, 0.5122567203514221, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23103165876153792, 0.23103165876153792, 0.29828442047417414], 
reward next is 0.7017, 
noisyNet noise sample is [array([-0.5548321], dtype=float32), 1.4347562]. 
=============================================
[2019-03-27 04:08:44,339] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2241989: loss 3.1977
[2019-03-27 04:08:44,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2241990: learning rate 0.0005
[2019-03-27 04:08:45,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:45,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0996
[2019-03-27 04:08:45,902] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5208554026639163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727824.4846277545, 727824.4846277551, 186928.4273228047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5137200.0000, 
sim time next is 5137800.0000, 
raw observation next is [31.16666666666667, 63.66666666666666, 1.0, 2.0, 0.5272032012877225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736697.7445281718, 736697.7445281718, 187968.524784506], 
processed observation next is [0.0, 0.4782608695652174, 0.6761453396524489, 0.6366666666666666, 1.0, 1.0, 0.4303653027562921, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20463826236893662, 0.20463826236893662, 0.28055003699180003], 
reward next is 0.7194, 
noisyNet noise sample is [array([-0.41943958], dtype=float32), 0.459903]. 
=============================================
[2019-03-27 04:08:45,949] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2242691: loss 0.4158
[2019-03-27 04:08:45,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2242692: learning rate 0.0005
[2019-03-27 04:08:46,539] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2242954: loss 0.1871
[2019-03-27 04:08:46,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2242954: learning rate 0.0005
[2019-03-27 04:08:47,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:08:47,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9354
[2019-03-27 04:08:47,394] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 83.16666666666666, 1.0, 2.0, 0.5125261470826995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716181.5608043185, 716181.5608043191, 185581.8764421499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4747800.0000, 
sim time next is 4748400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5113348595370742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714516.3479738876, 714516.3479738869, 185390.992328483], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4112468187193665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19847676332607989, 0.1984767633260797, 0.2767029736246015], 
reward next is 0.7233, 
noisyNet noise sample is [array([0.6656328], dtype=float32), 1.4092782]. 
=============================================
[2019-03-27 04:08:48,187] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2243689: loss 55.5758
[2019-03-27 04:08:48,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2243691: learning rate 0.0005
[2019-03-27 04:08:48,523] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2243840: loss 0.2618
[2019-03-27 04:08:48,526] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2243840: learning rate 0.0005
[2019-03-27 04:08:49,089] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1380002e-10 9.9999976e-01 1.8916917e-12 5.1042641e-09 2.7007781e-07], sum to 1.0000
[2019-03-27 04:08:49,104] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6133
[2019-03-27 04:08:49,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2690037.825742678 W.
[2019-03-27 04:08:49,120] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6411292518477353, 1.0, 2.0, 0.6411292518477353, 1.0, 1.0, 1.03, 7.004993851002125, 6.9112, 170.5573041426782, 2690037.825742678, 2622849.474979931, 503374.4239629289], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4615200.0000, 
sim time next is 4615800.0000, 
raw observation next is [32.0, 74.33333333333333, 1.0, 2.0, 0.9223430682359932, 1.0, 2.0, 0.9223430682359932, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2579852.600526989, 2579852.600526989, 483839.0993049738], 
processed observation next is [1.0, 0.43478260869565216, 0.7156398104265403, 0.7433333333333333, 1.0, 1.0, 0.9064374316096304, 1.0, 1.0, 0.9064374316096304, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.716625722368608, 0.716625722368608, 0.7221479094104086], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0493524], dtype=float32), -0.4576793]. 
=============================================
[2019-03-27 04:08:49,347] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2244202: loss -97.7626
[2019-03-27 04:08:49,349] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2244203: learning rate 0.0005
[2019-03-27 04:08:49,780] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2244395: loss 0.2977
[2019-03-27 04:08:49,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2244395: learning rate 0.0005
[2019-03-27 04:08:49,887] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2244442: loss 0.2882
[2019-03-27 04:08:49,891] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2244444: learning rate 0.0005
[2019-03-27 04:08:50,763] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2244830: loss 0.4837
[2019-03-27 04:08:50,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2244831: learning rate 0.0005
[2019-03-27 04:08:50,989] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2244934: loss 0.2818
[2019-03-27 04:08:50,992] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2244934: learning rate 0.0005
[2019-03-27 04:08:51,390] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2245111: loss -123.2704
[2019-03-27 04:08:51,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2245111: learning rate 0.0005
[2019-03-27 04:08:52,525] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2245570: loss -37.6957
[2019-03-27 04:08:52,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2245570: learning rate 0.0005
[2019-03-27 04:08:53,689] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2246080: loss -38.9986
[2019-03-27 04:08:53,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2246081: learning rate 0.0005
[2019-03-27 04:08:55,616] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2246978: loss 47.0517
[2019-03-27 04:08:55,618] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2246978: learning rate 0.0005
[2019-03-27 04:08:55,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3759465e-29 1.0000000e+00 5.2052124e-33 1.7426561e-26 1.9859714e-29], sum to 1.0000
[2019-03-27 04:08:55,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3270
[2019-03-27 04:08:55,854] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 94.0, 1.0, 2.0, 0.9336194138699042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1304960.355947803, 1304960.355947803, 279355.7588205851], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4598400.0000, 
sim time next is 4599000.0000, 
raw observation next is [27.5, 94.0, 1.0, 2.0, 0.9477929068566072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1324783.622482717, 1324783.622482717, 283425.849704527], 
processed observation next is [1.0, 0.21739130434782608, 0.5023696682464456, 0.94, 1.0, 1.0, 0.9370998877790448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3679954506896436, 0.3679954506896436, 0.42302365627541344], 
reward next is 0.5770, 
noisyNet noise sample is [array([0.41921714], dtype=float32), 0.86786073]. 
=============================================
[2019-03-27 04:08:55,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[39.429344]
 [38.425056]
 [37.23749 ]
 [37.20466 ]
 [36.895233]], R is [[39.19512939]
 [39.38622665]
 [39.57051849]
 [39.76289368]
 [39.94369507]].
[2019-03-27 04:09:00,635] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2249303: loss 48.4266
[2019-03-27 04:09:00,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2249303: learning rate 0.0005
[2019-03-27 04:09:00,954] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2249455: loss 22.3879
[2019-03-27 04:09:00,956] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2249456: learning rate 0.0005
[2019-03-27 04:09:02,140] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 04:09:02,141] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:09:02,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:02,141] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:09:02,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:09:02,144] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:02,144] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:02,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:09:02,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:02,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:09:02,150] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:02,170] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-03-27 04:09:02,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-03-27 04:09:02,219] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-03-27 04:09:02,219] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-03-27 04:09:02,219] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-03-27 04:09:31,591] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13306578], dtype=float32), 0.039382514]
[2019-03-27 04:09:31,592] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.5, 94.0, 1.0, 2.0, 0.5047153747862756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705263.5069356282, 705263.5069356275, 184338.2908602052]
[2019-03-27 04:09:31,594] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:09:31,595] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7421205753807373
[2019-03-27 04:09:41,587] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13306578], dtype=float32), 0.039382514]
[2019-03-27 04:09:41,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.77997163, 89.53197486, 1.0, 2.0, 0.3070713960026882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492214.6841779737, 492214.6841779737, 166534.4901342889]
[2019-03-27 04:09:41,589] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:09:41,592] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7946622510318433
[2019-03-27 04:10:04,133] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13306578], dtype=float32), 0.039382514]
[2019-03-27 04:10:04,133] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.44148945, 80.75008056333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 1.0, 2.0, 1.03, 12.46488594878303, 6.9112, 184.5923449428631, 8050559.409847667, 3744854.177926891, 650183.6680700668]
[2019-03-27 04:10:04,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:10:04,136] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3175561e-09 9.9999511e-01 3.6050451e-11 3.3855937e-07 4.5266611e-06], sampled 0.6273422095629178
[2019-03-27 04:10:04,138] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 8050559.409847667 W.
[2019-03-27 04:10:43,927] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13306578], dtype=float32), 0.039382514]
[2019-03-27 04:10:43,928] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.7, 81.0, 1.0, 2.0, 0.5005193872639854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699398.3179173326, 699398.3179173332, 183676.4022559553]
[2019-03-27 04:10:43,929] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:10:43,935] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33693902400728715
[2019-03-27 04:10:55,338] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13306578], dtype=float32), 0.039382514]
[2019-03-27 04:10:55,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.70000926, 57.43843476666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.01179348665051, 6.9112, 168.912620018317, 1541303.529892047, 1469939.190467003, 313939.6712204323]
[2019-03-27 04:10:55,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:10:55,343] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7088186327091086
[2019-03-27 04:10:57,182] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:10:57,640] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:10:57,788] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:10:57,890] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:10:57,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:10:58,916] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2250000, evaluation results [2250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:10:59,114] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2250092: loss -93.5293
[2019-03-27 04:10:59,117] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2250093: learning rate 0.0005
[2019-03-27 04:10:59,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0684520e-15 1.0000000e+00 1.3620844e-17 6.4441707e-12 4.7810502e-09], sum to 1.0000
[2019-03-27 04:10:59,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6896
[2019-03-27 04:10:59,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3432458.049288656 W.
[2019-03-27 04:10:59,501] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.26666666666667, 65.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.640628972145147, 6.9112, 170.5573041426782, 3432458.049288656, 2909938.405267564, 549585.2903273834], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5416800.0000, 
sim time next is 5417400.0000, 
raw observation next is [31.58333333333333, 68.0, 1.0, 2.0, 0.8297660662855727, 1.0, 2.0, 0.7354730726570489, 1.0, 1.0, 1.03, 7.005107966321294, 6.9112, 170.5573041426782, 3086372.853187585, 3019102.756979804, 565447.6940268832], 
processed observation next is [1.0, 0.6956521739130435, 0.6958925750394943, 0.68, 1.0, 1.0, 0.7948988750428587, 1.0, 1.0, 0.6812928586229504, 1.0, 0.5, 1.0365853658536586, 0.009390796632129383, 0.0, 0.8375144448122397, 0.8573257925521068, 0.8386396547166123, 0.8439517821296765], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6079108], dtype=float32), 0.4125969]. 
=============================================
[2019-03-27 04:11:00,619] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2250757: loss -20.7930
[2019-03-27 04:11:00,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2250757: learning rate 0.0005
[2019-03-27 04:11:01,167] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2251000: loss 1.0607
[2019-03-27 04:11:01,169] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2251000: learning rate 0.0005
[2019-03-27 04:11:02,440] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2251563: loss 2.5954
[2019-03-27 04:11:02,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2251563: learning rate 0.0005
[2019-03-27 04:11:03,341] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2251966: loss -67.3423
[2019-03-27 04:11:03,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2251967: learning rate 0.0005
[2019-03-27 04:11:03,558] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2252059: loss 3.1209
[2019-03-27 04:11:03,560] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2252060: learning rate 0.0005
[2019-03-27 04:11:04,191] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2252343: loss -5.8338
[2019-03-27 04:11:04,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2252343: learning rate 0.0005
[2019-03-27 04:11:04,405] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2252433: loss -29.7288
[2019-03-27 04:11:04,407] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2252434: learning rate 0.0005
[2019-03-27 04:11:05,360] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2252860: loss 50.5295
[2019-03-27 04:11:05,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2252860: learning rate 0.0005
[2019-03-27 04:11:05,431] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2252891: loss -60.2924
[2019-03-27 04:11:05,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2252891: learning rate 0.0005
[2019-03-27 04:11:05,518] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2252929: loss 2.9186
[2019-03-27 04:11:05,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2252930: learning rate 0.0005
[2019-03-27 04:11:06,523] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2253379: loss 2.6279
[2019-03-27 04:11:06,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2253379: learning rate 0.0005
[2019-03-27 04:11:07,741] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2253922: loss 3.7351
[2019-03-27 04:11:07,745] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2253923: learning rate 0.0005
[2019-03-27 04:11:08,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7513387e-11 9.9999738e-01 6.6169490e-15 4.3089418e-08 2.6191497e-06], sum to 1.0000
[2019-03-27 04:11:08,080] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7442
[2019-03-27 04:11:08,089] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1977256.297802658 W.
[2019-03-27 04:11:08,094] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.9, 63.0, 1.0, 2.0, 0.4713903860051251, 1.0, 1.0, 0.4713903860051251, 1.0, 2.0, 0.8074720830568386, 6.9112, 6.9112, 170.5573041426782, 1977256.297802658, 1977256.297802658, 394024.1467651641], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4980600.0000, 
sim time next is 4981200.0000, 
raw observation next is [30.93333333333333, 63.0, 1.0, 2.0, 0.8805678482045914, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982325862637014, 6.9112, 168.9125334019273, 2127837.217352939, 2077378.208829594, 429571.1441111311], 
processed observation next is [1.0, 0.6521739130434783, 0.6650868878357029, 0.63, 1.0, 1.0, 0.8561058412103512, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00711258626370137, 0.0, 0.8294378674958496, 0.5910658937091496, 0.5770495024526651, 0.6411509613598971], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41689977], dtype=float32), 0.051516008]. 
=============================================
[2019-03-27 04:11:08,999] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:11:09,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6709
[2019-03-27 04:11:09,011] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5156883475053693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720601.7797672314, 720601.7797672321, 186090.2565777657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5096400.0000, 
sim time next is 5097000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.516654614022118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721952.4598670278, 721952.4598670271, 186246.2210037227], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4176561614724313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20054234996306328, 0.20054234996306308, 0.2779794343339145], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.7443035], dtype=float32), -0.11648155]. 
=============================================
[2019-03-27 04:11:09,026] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[82.17117]
 [82.10686]
 [82.07989]
 [82.0795 ]
 [82.10086]], R is [[82.06782532]
 [81.9693985 ]
 [81.8717804 ]
 [81.7749939 ]
 [81.67930603]].
[2019-03-27 04:11:09,807] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2254839: loss 3.2529
[2019-03-27 04:11:09,812] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2254839: learning rate 0.0005
[2019-03-27 04:11:10,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9577899e-33 1.0000000e+00 0.0000000e+00 1.6396694e-24 3.5300314e-30], sum to 1.0000
[2019-03-27 04:11:10,293] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7612
[2019-03-27 04:11:10,305] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5091368395051649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711443.9050444425, 711443.9050444419, 185040.0746899869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4927800.0000, 
sim time next is 4928400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5099145363569004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712530.9865658786, 712530.9865658792, 185164.0716666581], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40953558597216916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1979252740460774, 0.19792527404607754, 0.276364286069639], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.91122144], dtype=float32), 0.562455]. 
=============================================
[2019-03-27 04:11:12,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:11:12,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3435
[2019-03-27 04:11:12,301] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 63.0, 1.0, 2.0, 0.531511780117165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742720.5266115637, 742720.5266115637, 188680.8784446958], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [31.16666666666667, 63.0, 1.0, 2.0, 0.5270728645859347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736515.5528379027, 736515.5528379027, 187946.740540994], 
processed observation next is [0.0, 0.5652173913043478, 0.6761453396524489, 0.63, 1.0, 1.0, 0.4302082705854635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20458765356608408, 0.20458765356608408, 0.28051752319551343], 
reward next is 0.7195, 
noisyNet noise sample is [array([1.1889403], dtype=float32), 0.96844876]. 
=============================================
[2019-03-27 04:11:12,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.6801  ]
 [80.44395 ]
 [80.242294]
 [80.102905]
 [80.0118  ]], R is [[80.7820816 ]
 [80.69264221]
 [80.60301971]
 [80.51320648]
 [80.42385864]].
[2019-03-27 04:11:12,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6821568e-31 1.0000000e+00 1.7010092e-33 9.4066884e-22 3.9026725e-27], sum to 1.0000
[2019-03-27 04:11:12,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9032
[2019-03-27 04:11:12,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 86.83333333333333, 1.0, 2.0, 0.5786719237302794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808645.950957457, 808645.950957457, 196840.0829238772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5277000.0000, 
sim time next is 5277600.0000, 
raw observation next is [28.6, 87.0, 1.0, 2.0, 0.5802270925260518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810819.9996689438, 810819.9996689438, 197120.3107537131], 
processed observation next is [1.0, 0.08695652173913043, 0.5545023696682465, 0.87, 1.0, 1.0, 0.49424950906753223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22522777768581773, 0.22522777768581773, 0.29420941903539266], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.04940433], dtype=float32), 0.88819593]. 
=============================================
[2019-03-27 04:11:14,336] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:11:14,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2490
[2019-03-27 04:11:14,352] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5170087466958854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722447.478955866, 722447.478955866, 186303.4505701619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5164354593231327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721646.1178474353, 721646.1178474353, 186210.8231642146], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4173921196664249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20045725495762093, 0.20045725495762093, 0.2779266017376337], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.5463746], dtype=float32), 0.12888105]. 
=============================================
[2019-03-27 04:11:15,252] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2257261: loss 3.6297
[2019-03-27 04:11:15,254] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2257262: learning rate 0.0005
[2019-03-27 04:11:15,498] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2257370: loss 4.1520
[2019-03-27 04:11:15,501] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2257370: learning rate 0.0005
[2019-03-27 04:11:17,066] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2258073: loss 8.1380
[2019-03-27 04:11:17,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2258074: learning rate 0.0005
[2019-03-27 04:11:17,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9935734e-10 9.9981803e-01 4.5395293e-13 5.5655910e-06 1.7638758e-04], sum to 1.0000
[2019-03-27 04:11:17,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9882
[2019-03-27 04:11:17,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2688213.97753195 W.
[2019-03-27 04:11:17,128] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.4, 60.0, 1.0, 2.0, 0.64069503276469, 1.0, 2.0, 0.64069503276469, 1.0, 1.0, 1.03, 7.004146007787673, 6.9112, 170.5573041426782, 2688213.97753195, 2621632.971351772, 503206.3350520359], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5306400.0000, 
sim time next is 5307000.0000, 
raw observation next is [34.73333333333333, 58.5, 1.0, 2.0, 0.6399692304439223, 1.0, 2.0, 0.6399692304439223, 1.0, 2.0, 1.03, 7.002728830611581, 6.9112, 170.5573041426782, 2685165.399672473, 2619599.575150612, 502926.5086081257], 
processed observation next is [1.0, 0.43478260869565216, 0.8451816745655606, 0.585, 1.0, 1.0, 0.5662279884866533, 1.0, 1.0, 0.5662279884866533, 1.0, 1.0, 1.0365853658536586, 0.009152883061158069, 0.0, 0.8375144448122397, 0.745879277686798, 0.7276665486529478, 0.7506365800121279], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.95824003], dtype=float32), 2.2363067]. 
=============================================
[2019-03-27 04:11:17,149] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[33.468067]
 [33.284134]
 [31.929008]
 [31.551765]
 [30.311176]], R is [[33.40792847]
 [33.07384872]
 [32.97612381]
 [32.6463623 ]
 [32.31990051]].
[2019-03-27 04:11:18,544] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2258716: loss 3.3102
[2019-03-27 04:11:18,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2258716: learning rate 0.0005
[2019-03-27 04:11:18,941] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2258892: loss 3.4768
[2019-03-27 04:11:18,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2258892: learning rate 0.0005
[2019-03-27 04:11:20,963] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2259793: loss -25.2121
[2019-03-27 04:11:20,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2259794: learning rate 0.0005
[2019-03-27 04:11:21,244] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2259921: loss 2.2676
[2019-03-27 04:11:21,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2259921: learning rate 0.0005
[2019-03-27 04:11:21,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2260216: loss -22.8386
[2019-03-27 04:11:21,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2260218: learning rate 0.0005
[2019-03-27 04:11:22,128] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2260313: loss 1.3854
[2019-03-27 04:11:22,133] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2260313: learning rate 0.0005
[2019-03-27 04:11:22,319] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2260396: loss 1.8285
[2019-03-27 04:11:22,322] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2260396: learning rate 0.0005
[2019-03-27 04:11:23,100] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:11:23,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2035
[2019-03-27 04:11:23,119] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.553675118974134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773702.2703814851, 773702.2703814845, 192433.2994364631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5144400.0000, 
sim time next is 5145000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5584796310012539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780418.5328980722, 780418.5328980722, 193265.5700643767], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.46804774819428174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21678292580502004, 0.21678292580502004, 0.28845607472295026], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.8556532], dtype=float32), 1.6601381]. 
=============================================
[2019-03-27 04:11:23,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.838326]
 [70.50094 ]
 [70.39215 ]
 [70.26512 ]
 [70.13798 ]], R is [[70.8430481 ]
 [70.84740448]
 [70.85076141]
 [70.85316467]
 [70.85456085]].
[2019-03-27 04:11:23,268] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2260811: loss 1.3084
[2019-03-27 04:11:23,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2260812: learning rate 0.0005
[2019-03-27 04:11:23,456] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2260896: loss 1.5386
[2019-03-27 04:11:23,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2260897: learning rate 0.0005
[2019-03-27 04:11:23,606] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4204254e-09 9.9994791e-01 5.0308464e-12 3.8996630e-05 1.3141890e-05], sum to 1.0000
[2019-03-27 04:11:23,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9214
[2019-03-27 04:11:23,624] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2804897.051123359 W.
[2019-03-27 04:11:23,630] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 68.5, 1.0, 2.0, 1.002710233896941, 1.0, 2.0, 1.002710233896941, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2804897.051123359, 2804897.051123359, 530623.7820898149], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5243400.0000, 
sim time next is 5244000.0000, 
raw observation next is [31.33333333333333, 69.0, 1.0, 2.0, 0.6308136003395014, 1.0, 2.0, 0.6308136003395014, 1.0, 1.0, 1.03, 6.98485215215727, 6.9112, 170.5573041426782, 2646709.766773737, 2593949.734205485, 499421.3714988313], 
processed observation next is [1.0, 0.6956521739130435, 0.6840442338072668, 0.69, 1.0, 1.0, 0.5551971088427727, 1.0, 1.0, 0.5551971088427727, 1.0, 0.5, 1.0365853658536586, 0.007365215215726995, 0.0, 0.8375144448122397, 0.7351971574371492, 0.7205415928348569, 0.745405032087808], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00697899], dtype=float32), 0.61289895]. 
=============================================
[2019-03-27 04:11:23,648] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[29.011654]
 [28.562376]
 [28.788074]
 [29.041761]
 [28.7475  ]], R is [[29.2183876 ]
 [29.13422966]
 [28.84288788]
 [28.55445862]
 [28.5651226 ]].
[2019-03-27 04:11:24,040] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2261155: loss -51.2575
[2019-03-27 04:11:24,046] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2261157: learning rate 0.0005
[2019-03-27 04:11:24,870] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2261522: loss -162.1534
[2019-03-27 04:11:24,872] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2261522: learning rate 0.0005
[2019-03-27 04:11:26,094] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2262069: loss -18.7067
[2019-03-27 04:11:26,097] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2262069: learning rate 0.0005
[2019-03-27 04:11:28,209] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2263012: loss -34.0060
[2019-03-27 04:11:28,211] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2263012: learning rate 0.0005
[2019-03-27 04:11:28,851] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.715580e-36 1.000000e+00 0.000000e+00 8.098811e-29 0.000000e+00], sum to 1.0000
[2019-03-27 04:11:28,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5657
[2019-03-27 04:11:28,870] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 81.5, 1.0, 2.0, 0.7578257801204309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059123.418745315, 1059123.418745315, 233820.490975015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5207400.0000, 
sim time next is 5208000.0000, 
raw observation next is [27.66666666666666, 80.66666666666667, 1.0, 2.0, 0.7683279187139259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073808.43622509, 1073808.43622509, 236283.6146859817], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.8066666666666668, 1.0, 1.0, 0.7208770104987059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2982801211736361, 0.2982801211736361, 0.35266211147161447], 
reward next is 0.6473, 
noisyNet noise sample is [array([1.002844], dtype=float32), 0.45984074]. 
=============================================
[2019-03-27 04:11:28,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[47.75246 ]
 [47.68335 ]
 [47.456703]
 [46.254654]
 [45.905956]], R is [[47.75201035]
 [47.92550278]
 [48.09819031]
 [48.26483917]
 [48.41313934]].
[2019-03-27 04:11:33,429] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2265337: loss -103.6744
[2019-03-27 04:11:33,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2265338: learning rate 0.0005
[2019-03-27 04:11:33,723] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2265458: loss -44.7556
[2019-03-27 04:11:33,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2265458: learning rate 0.0005
[2019-03-27 04:11:35,142] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2266093: loss -66.0426
[2019-03-27 04:11:35,144] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2266093: learning rate 0.0005
[2019-03-27 04:11:35,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.6532603e-38 1.0000000e+00 0.0000000e+00 1.8506345e-29 0.0000000e+00], sum to 1.0000
[2019-03-27 04:11:35,409] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4028477e-09 9.9999249e-01 1.1069548e-10 7.4043346e-06 1.5904186e-07], sum to 1.0000
[2019-03-27 04:11:35,415] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2278
[2019-03-27 04:11:35,415] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1702
[2019-03-27 04:11:35,420] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.51666666666667, 81.66666666666667, 1.0, 2.0, 0.5852508628324261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817843.0113683267, 817843.0113683267, 198030.4800677657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5440200.0000, 
sim time next is 5440800.0000, 
raw observation next is [29.43333333333334, 82.33333333333334, 1.0, 2.0, 0.5860506028011502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818961.0172362131, 818961.0172362131, 198176.0797642226], 
processed observation next is [1.0, 1.0, 0.5939968404423385, 0.8233333333333335, 1.0, 1.0, 0.5012657865074098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22748917145450365, 0.22748917145450365, 0.2957851936779442], 
reward next is 0.7042, 
noisyNet noise sample is [array([1.4667269], dtype=float32), 1.2127775]. 
=============================================
[2019-03-27 04:11:35,423] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2369538.550798531 W.
[2019-03-27 04:11:35,427] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.1, 67.33333333333334, 1.0, 2.0, 0.5648155270280684, 1.0, 2.0, 0.5648155270280684, 1.0, 1.0, 0.9808977830986886, 6.9112, 6.9112, 170.5573041426782, 2369538.550798531, 2369538.550798531, 462862.2287540162], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5389800.0000, 
sim time next is 5390400.0000, 
raw observation next is [33.3, 66.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.025490228096978, 6.9112, 168.9121516326768, 2364833.428656165, 2283752.393634675, 475830.335306733], 
processed observation next is [1.0, 0.391304347826087, 0.7772511848341231, 0.6666666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.01142902280969782, 0.0, 0.8294359928340549, 0.6568981746267125, 0.6343756648985208, 0.7101945303085567], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21563031], dtype=float32), 1.3570553]. 
=============================================
[2019-03-27 04:11:36,678] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2266775: loss -108.3087
[2019-03-27 04:11:36,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2266775: learning rate 0.0005
[2019-03-27 04:11:36,897] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2266867: loss -64.4275
[2019-03-27 04:11:36,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2266868: learning rate 0.0005
[2019-03-27 04:11:37,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3299229e-12 1.0000000e+00 2.2711314e-15 4.0161794e-08 1.0203674e-11], sum to 1.0000
[2019-03-27 04:11:37,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8256
[2019-03-27 04:11:37,197] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.23333333333333, 81.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.13542675899951, 6.9112, 168.9114202113907, 1612936.604697984, 1453863.871387565, 311355.9506040112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5380800.0000, 
sim time next is 5381400.0000, 
raw observation next is [30.41666666666667, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.231930301401244, 6.9112, 168.9107479819956, 1681444.903759142, 1453910.764296407, 311355.9275346305], 
processed observation next is [1.0, 0.2608695652173913, 0.6406003159557664, 0.8016666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03207303014012437, 0.0, 0.8294291002666162, 0.4670680288219839, 0.4038641011934464, 0.46471033960392616], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38756087], dtype=float32), 0.66249496]. 
=============================================
[2019-03-27 04:11:38,610] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2267631: loss 9.7611
[2019-03-27 04:11:38,613] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2267632: learning rate 0.0005
[2019-03-27 04:11:39,269] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2267907: loss -125.6992
[2019-03-27 04:11:39,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2267910: learning rate 0.0005
[2019-03-27 04:11:39,509] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2268013: loss 9.5020
[2019-03-27 04:11:39,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2268013: learning rate 0.0005
[2019-03-27 04:11:40,097] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2268276: loss 9.5099
[2019-03-27 04:11:40,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2268277: learning rate 0.0005
[2019-03-27 04:11:40,598] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2268498: loss 5.1833
[2019-03-27 04:11:40,599] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2268499: learning rate 0.0005
[2019-03-27 04:11:41,299] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2268809: loss -19.7232
[2019-03-27 04:11:41,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2268809: learning rate 0.0005
[2019-03-27 04:11:41,524] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2268909: loss -1.6403
[2019-03-27 04:11:41,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2268910: learning rate 0.0005
[2019-03-27 04:11:41,630] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2268959: loss 9.3773
[2019-03-27 04:11:41,635] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2268960: learning rate 0.0005
[2019-03-27 04:11:42,441] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2269316: loss 9.6235
[2019-03-27 04:11:42,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2269316: learning rate 0.0005
[2019-03-27 04:11:43,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3468515e-26 1.0000000e+00 1.1153922e-32 7.6198472e-18 1.0761257e-24], sum to 1.0000
[2019-03-27 04:11:43,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7060
[2019-03-27 04:11:43,462] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.0, 1.0, 2.0, 0.7691406544920268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1074944.883617544, 1074944.883617544, 236477.1538081006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6145200.0000, 
sim time next is 6145800.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.7764128767016052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1085113.676703261, 1085113.676703261, 238203.5778158018], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.7306179237368737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3014204657509058, 0.3014204657509058, 0.35552772808328625], 
reward next is 0.6445, 
noisyNet noise sample is [array([0.45921642], dtype=float32), -1.1268207]. 
=============================================
[2019-03-27 04:11:43,848] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2269937: loss 11.7104
[2019-03-27 04:11:43,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2269938: learning rate 0.0005
[2019-03-27 04:11:46,036] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2270867: loss 13.1768
[2019-03-27 04:11:46,038] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2270867: learning rate 0.0005
[2019-03-27 04:11:46,591] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:11:46,597] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6230
[2019-03-27 04:11:46,601] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 90.0, 1.0, 2.0, 0.5099661090461131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712603.0760283974, 712603.0760283968, 185172.0625785768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5721600.0000, 
sim time next is 5722200.0000, 
raw observation next is [26.2, 89.0, 1.0, 2.0, 0.5105956776912598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713483.1016054852, 713483.1016054852, 185272.6464170006], 
processed observation next is [0.0, 0.21739130434782608, 0.44075829383886256, 0.89, 1.0, 1.0, 0.4103562381822407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19818975044596812, 0.19818975044596812, 0.27652633793582176], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.0308915], dtype=float32), -1.9033655]. 
=============================================
[2019-03-27 04:11:51,351] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2273331: loss 11.9527
[2019-03-27 04:11:51,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2273331: learning rate 0.0005
[2019-03-27 04:11:51,672] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2273480: loss 12.7002
[2019-03-27 04:11:51,681] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2273483: learning rate 0.0005
[2019-03-27 04:11:53,275] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2274216: loss 0.2855
[2019-03-27 04:11:53,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2274217: learning rate 0.0005
[2019-03-27 04:11:54,659] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2274830: loss 12.4372
[2019-03-27 04:11:54,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2274831: learning rate 0.0005
[2019-03-27 04:11:54,900] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2274945: loss 12.3023
[2019-03-27 04:11:54,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2274945: learning rate 0.0005
[2019-03-27 04:11:55,026] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 04:11:55,028] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:11:55,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:11:55,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:11:55,032] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:11:55,033] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:11:55,035] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:11:55,035] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:11:55,034] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:11:55,036] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:11:55,040] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:11:55,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-03-27 04:11:55,085] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-03-27 04:11:55,105] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-03-27 04:11:55,126] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-03-27 04:11:55,154] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-03-27 04:11:58,603] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:11:58,605] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.41666666666667, 93.0, 1.0, 2.0, 0.2898629806330963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465430.5006459423, 465430.5006459423, 164640.5959671809]
[2019-03-27 04:11:58,606] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:11:58,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.49457310907661656
[2019-03-27 04:12:04,473] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:04,474] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.75, 85.0, 1.0, 2.0, 0.3154352471887549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502854.734285067, 502854.734285067, 167299.1474475567]
[2019-03-27 04:12:04,475] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:12:04,478] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6693287e-37 0.0000000e+00], sampled 0.6638732098353586
[2019-03-27 04:12:05,340] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:05,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.54807466333333, 66.290300465, 1.0, 2.0, 0.2409087033732143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399641.6690640493, 399641.6690640493, 159817.7390146174]
[2019-03-27 04:12:05,344] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:12:05,347] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0783328050321207
[2019-03-27 04:12:14,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:14,167] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.59821078666667, 77.5360457, 1.0, 2.0, 0.9234601071622511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1290751.620171999, 1290751.620171999, 276469.7726095971]
[2019-03-27 04:12:14,170] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:12:14,173] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.2320475e-33 1.0000000e+00 0.0000000e+00 1.6179456e-19 1.1659335e-31], sampled 0.6274510437956935
[2019-03-27 04:12:38,352] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:38,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3045042132623334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484905.0750497521, 484905.0750497521, 165974.590092472]
[2019-03-27 04:12:38,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:12:38,356] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6511781e-35 0.0000000e+00], sampled 0.19059232522033798
[2019-03-27 04:12:40,267] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:40,269] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.96666666666667, 65.33333333333333, 1.0, 2.0, 0.6442424199935024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 900314.1088522772, 900314.1088522779, 209273.1797708775]
[2019-03-27 04:12:40,270] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:12:40,273] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19398043163171375
[2019-03-27 04:12:46,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:46,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.55, 67.5, 1.0, 2.0, 0.8590242213375119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1200636.465488945, 1200636.465488946, 258917.5453100287]
[2019-03-27 04:12:46,464] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:12:46,468] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.2850317e-34 1.0000000e+00 0.0000000e+00 4.0830564e-21 3.9205492e-34], sampled 0.5995695184993352
[2019-03-27 04:12:48,361] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:12:48,363] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.18873248333334, 61.90572189, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.073117703346343, 6.9112, 168.9120686382888, 1568702.982305348, 1453833.594575504, 311354.3556884178]
[2019-03-27 04:12:48,365] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:12:48,368] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9737897e-28 1.0000000e+00 1.6995213e-36 8.0879085e-15 1.6991752e-24], sampled 0.8212421999142777
[2019-03-27 04:13:24,892] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:13:24,894] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.45484275333333, 88.52415856166667, 1.0, 2.0, 0.6496911318213303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 907931.8142381236, 907931.8142381236, 210370.2251397702]
[2019-03-27 04:13:24,895] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:13:24,898] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.30526342045862565
[2019-03-27 04:13:36,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1143537], dtype=float32), 0.059325892]
[2019-03-27 04:13:36,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.07141827666667, 63.12786233666667, 1.0, 2.0, 0.4954499069103979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692312.1959773203, 692312.1959773203, 182886.1191817478]
[2019-03-27 04:13:36,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:13:36,952] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4444366510565091
[2019-03-27 04:13:50,798] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8267.0170 2926355995.0170 1312.0000
[2019-03-27 04:13:51,090] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7925.4274 3159194333.1172 1659.0000
[2019-03-27 04:13:51,169] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8674.4891 2777592677.7246 891.0000
[2019-03-27 04:13:51,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8011.1593 3005969304.9470 1729.0000
[2019-03-27 04:13:51,252] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8511.5456 2840495626.7266 1076.0000
[2019-03-27 04:13:52,271] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2275000, evaluation results [2275000.0, 7925.4273833965035, 3159194333.1171713, 1659.0, 8267.016992158648, 2926355995.017026, 1312.0, 8674.489077018852, 2777592677.724577, 891.0, 8011.159265307333, 3005969304.947045, 1729.0, 8511.54563288177, 2840495626.7265544, 1076.0]
[2019-03-27 04:13:52,472] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9731863e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 04:13:52,483] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0426
[2019-03-27 04:13:52,488] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 91.0, 1.0, 2.0, 0.5095889781190139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712075.9140668063, 712075.9140668063, 185111.8365138987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5721000.0000, 
sim time next is 5721600.0000, 
raw observation next is [26.03333333333333, 90.0, 1.0, 2.0, 0.5099661090461131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712603.0760283974, 712603.0760283968, 185172.0625785768], 
processed observation next is [0.0, 0.21739130434782608, 0.4328593996840442, 0.9, 1.0, 1.0, 0.40959772174230485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19794529889677706, 0.1979452988967769, 0.27637621280384594], 
reward next is 0.7236, 
noisyNet noise sample is [array([1.0680501], dtype=float32), 0.23032483]. 
=============================================
[2019-03-27 04:13:53,571] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2275580: loss -38.1101
[2019-03-27 04:13:53,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2275580: learning rate 0.0005
[2019-03-27 04:13:54,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.03073865e-33
 0.00000000e+00], sum to 1.0000
[2019-03-27 04:13:54,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9113
[2019-03-27 04:13:54,398] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.8, 53.33333333333334, 1.0, 2.0, 0.5354331742055592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748202.109028389, 748202.1090283897, 189334.8043667382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5748600.0000, 
sim time next is 5749200.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5386016581491159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752631.2448169832, 752631.2448169838, 189866.068643478], 
processed observation next is [0.0, 0.5652173913043478, 0.8104265402843602, 0.53, 1.0, 1.0, 0.44409838331218776, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20906423467138424, 0.20906423467138438, 0.28338219200519105], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.64860106], dtype=float32), -0.02303945]. 
=============================================
[2019-03-27 04:13:54,495] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0585943e-20 9.9999988e-01 2.6759867e-29 6.6333151e-08 1.9819438e-15], sum to 1.0000
[2019-03-27 04:13:54,504] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9171
[2019-03-27 04:13:54,509] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 91.00000000000001, 1.0, 2.0, 0.5334273627281741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745398.2503657328, 745398.2503657328, 188999.2466940823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5965800.0000, 
sim time next is 5966400.0000, 
raw observation next is [26.6, 91.0, 1.0, 2.0, 0.5315310234553857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742747.4261455601, 742747.4261455601, 188683.8667689177], 
processed observation next is [1.0, 0.043478260869565216, 0.4597156398104266, 0.91, 1.0, 1.0, 0.43557954633179, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2063187294848778, 0.2063187294848778, 0.28161771159539956], 
reward next is 0.7184, 
noisyNet noise sample is [array([-0.12529147], dtype=float32), -0.9195656]. 
=============================================
[2019-03-27 04:13:54,572] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2276017: loss 14.7811
[2019-03-27 04:13:54,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2276017: learning rate 0.0005
[2019-03-27 04:13:54,610] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2276041: loss 1.0003
[2019-03-27 04:13:54,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2276042: learning rate 0.0005
[2019-03-27 04:13:55,241] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2276323: loss 12.6021
[2019-03-27 04:13:55,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2276323: learning rate 0.0005
[2019-03-27 04:13:55,796] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2276567: loss 6.3402
[2019-03-27 04:13:55,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2276567: learning rate 0.0005
[2019-03-27 04:13:56,343] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2276810: loss 12.3765
[2019-03-27 04:13:56,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2276811: learning rate 0.0005
[2019-03-27 04:13:56,644] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2276943: loss 11.3129
[2019-03-27 04:13:56,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2276943: learning rate 0.0005
[2019-03-27 04:13:56,741] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2276984: loss -12.0946
[2019-03-27 04:13:56,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2276986: learning rate 0.0005
[2019-03-27 04:13:57,223] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2277201: loss -37.7512
[2019-03-27 04:13:57,233] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2277204: learning rate 0.0005
[2019-03-27 04:13:57,965] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7128910e-20 1.0000000e+00 3.5979306e-27 4.3166013e-09 3.6855892e-18], sum to 1.0000
[2019-03-27 04:13:57,974] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1887
[2019-03-27 04:13:57,981] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 92.0, 1.0, 2.0, 0.7604910440428951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062850.206865031, 1062850.206865031, 234443.2284224976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5808000.0000, 
sim time next is 5808600.0000, 
raw observation next is [26.45, 91.0, 1.0, 2.0, 0.7641237128576236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1067929.720005651, 1067929.720005651, 235294.4849185148], 
processed observation next is [1.0, 0.21739130434782608, 0.45260663507109006, 0.91, 1.0, 1.0, 0.7158117022381008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29664714444601414, 0.29664714444601414, 0.35118579838584296], 
reward next is 0.6488, 
noisyNet noise sample is [array([1.4462568], dtype=float32), 0.06458605]. 
=============================================
[2019-03-27 04:13:58,562] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2277798: loss -76.4793
[2019-03-27 04:13:58,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2277798: learning rate 0.0005
[2019-03-27 04:14:00,666] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2278738: loss -27.4850
[2019-03-27 04:14:00,669] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2278738: learning rate 0.0005
[2019-03-27 04:14:04,197] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.43722550e-35 1.00000000e+00 0.00000000e+00 1.37826116e-17
 2.82196415e-31], sum to 1.0000
[2019-03-27 04:14:04,204] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1831
[2019-03-27 04:14:04,208] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.8, 66.0, 1.0, 2.0, 0.4191698083768157, 1.0, 2.0, 0.4191698083768157, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1171703.931271237, 1171703.931271237, 277071.1041435361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5850600.0000, 
sim time next is 5851200.0000, 
raw observation next is [31.6, 67.0, 1.0, 2.0, 0.5242925147824464, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732629.0369667193, 732629.0369667193, 187493.1678875357], 
processed observation next is [1.0, 0.7391304347826086, 0.6966824644549764, 0.67, 1.0, 1.0, 0.42685845154511615, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2035080658240887, 0.2035080658240887, 0.2798405490858742], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.32830346], dtype=float32), 0.20690693]. 
=============================================
[2019-03-27 04:14:06,504] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2281340: loss -96.1244
[2019-03-27 04:14:06,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2281340: learning rate 0.0005
[2019-03-27 04:14:06,802] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2281477: loss -109.2894
[2019-03-27 04:14:06,804] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2281477: learning rate 0.0005
[2019-03-27 04:14:07,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1685708e-13 3.8910368e-01 4.2559056e-20 6.1089569e-01 6.7824323e-07], sum to 1.0000
[2019-03-27 04:14:07,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0837
[2019-03-27 04:14:07,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2369847.414564029 W.
[2019-03-27 04:14:07,359] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.3, 70.5, 1.0, 2.0, 0.8473336194865763, 1.0, 2.0, 0.8473336194865763, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2369847.414564029, 2369847.414564029, 443570.5006417408], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6187800.0000, 
sim time next is 6188400.0000, 
raw observation next is [30.2, 71.0, 1.0, 2.0, 0.8257608599834113, 1.0, 2.0, 0.8257608599834113, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2309456.339578824, 2309456.339578824, 432605.5134542544], 
processed observation next is [1.0, 0.6521739130434783, 0.6303317535545023, 0.71, 1.0, 1.0, 0.7900733252812184, 1.0, 1.0, 0.7900733252812184, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6415156498830067, 0.6415156498830067, 0.6456798708272454], 
reward next is 0.3543, 
noisyNet noise sample is [array([-0.64956427], dtype=float32), 0.35734183]. 
=============================================
[2019-03-27 04:14:08,577] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2282267: loss 123.7596
[2019-03-27 04:14:08,579] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2282268: learning rate 0.0005
[2019-03-27 04:14:09,927] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2282870: loss -133.6273
[2019-03-27 04:14:09,928] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2282870: learning rate 0.0005
[2019-03-27 04:14:10,021] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2282909: loss -125.9907
[2019-03-27 04:14:10,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2282909: learning rate 0.0005
[2019-03-27 04:14:11,423] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2283532: loss 0.0074
[2019-03-27 04:14:11,424] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2283532: learning rate 0.0005
[2019-03-27 04:14:12,517] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2284016: loss 0.0545
[2019-03-27 04:14:12,519] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2284016: learning rate 0.0005
[2019-03-27 04:14:12,562] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2284035: loss -93.9889
[2019-03-27 04:14:12,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2284035: learning rate 0.0005
[2019-03-27 04:14:13,177] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2284312: loss -59.6732
[2019-03-27 04:14:13,179] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2284312: learning rate 0.0005
[2019-03-27 04:14:13,741] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2284557: loss -36.1728
[2019-03-27 04:14:13,743] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2284558: learning rate 0.0005
[2019-03-27 04:14:14,083] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:14:14,094] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7282
[2019-03-27 04:14:14,097] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.25, 79.5, 1.0, 2.0, 0.5291039197801833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739354.6751770646, 739354.6751770653, 188281.7942024401], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6291000.0000, 
sim time next is 6291600.0000, 
raw observation next is [28.1, 80.66666666666666, 1.0, 2.0, 0.5301681045499398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740842.2550885848, 740842.2550885843, 188457.8951091644], 
processed observation next is [0.0, 0.8260869565217391, 0.5308056872037916, 0.8066666666666665, 1.0, 1.0, 0.4339374753613732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20578951530238468, 0.2057895153023845, 0.2812804404614394], 
reward next is 0.7187, 
noisyNet noise sample is [array([-1.76486], dtype=float32), -1.3072798]. 
=============================================
[2019-03-27 04:14:14,381] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2284842: loss -73.7940
[2019-03-27 04:14:14,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2284842: learning rate 0.0005
[2019-03-27 04:14:14,508] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2284894: loss 0.0985
[2019-03-27 04:14:14,510] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2284894: learning rate 0.0005
[2019-03-27 04:14:14,643] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2284953: loss -39.2921
[2019-03-27 04:14:14,646] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2284953: learning rate 0.0005
[2019-03-27 04:14:15,293] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2285239: loss 0.0480
[2019-03-27 04:14:15,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2285240: learning rate 0.0005
[2019-03-27 04:14:16,290] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2285691: loss 0.0369
[2019-03-27 04:14:16,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2285693: learning rate 0.0005
[2019-03-27 04:14:18,593] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2286703: loss 0.0391
[2019-03-27 04:14:18,596] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2286703: learning rate 0.0005
[2019-03-27 04:14:22,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3009452e-32 1.0000000e+00 0.0000000e+00 8.8964689e-21 3.5756667e-36], sum to 1.0000
[2019-03-27 04:14:22,227] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6546
[2019-03-27 04:14:22,236] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 91.33333333333334, 1.0, 2.0, 0.7212835638125493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008028.447919334, 1008028.447919334, 225505.3897366343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6492000.0000, 
sim time next is 6492600.0000, 
raw observation next is [26.25, 91.5, 1.0, 2.0, 0.7653272860402697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1069612.665953984, 1069612.665953983, 235576.9830237671], 
processed observation next is [1.0, 0.13043478260869565, 0.4431279620853081, 0.915, 1.0, 1.0, 0.7172617904099634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2971146294316622, 0.29711462943166195, 0.3516074373489061], 
reward next is 0.6484, 
noisyNet noise sample is [array([-2.0446856], dtype=float32), -0.7685003]. 
=============================================
[2019-03-27 04:14:24,475] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2289318: loss 1.2617
[2019-03-27 04:14:24,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2289319: learning rate 0.0005
[2019-03-27 04:14:24,729] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2289432: loss 0.8967
[2019-03-27 04:14:24,730] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2289432: learning rate 0.0005
[2019-03-27 04:14:26,393] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2290174: loss 47.6635
[2019-03-27 04:14:26,395] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2290174: learning rate 0.0005
[2019-03-27 04:14:27,924] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2290854: loss 0.0097
[2019-03-27 04:14:27,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2290854: learning rate 0.0005
[2019-03-27 04:14:28,042] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2290911: loss 0.0083
[2019-03-27 04:14:28,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2290911: learning rate 0.0005
[2019-03-27 04:14:29,687] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2291641: loss 325.8047
[2019-03-27 04:14:29,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2291643: learning rate 0.0005
[2019-03-27 04:14:30,642] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2292071: loss 176.4783
[2019-03-27 04:14:30,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2292073: learning rate 0.0005
[2019-03-27 04:14:30,681] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2292090: loss 0.6574
[2019-03-27 04:14:30,685] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2292091: learning rate 0.0005
[2019-03-27 04:14:31,258] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2292345: loss 0.3310
[2019-03-27 04:14:31,262] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2292346: learning rate 0.0005
[2019-03-27 04:14:31,844] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2292607: loss 0.0117
[2019-03-27 04:14:31,846] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2292607: learning rate 0.0005
[2019-03-27 04:14:32,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0318934e-32 1.0000000e+00 0.0000000e+00 3.6737542e-19 3.0655010e-33], sum to 1.0000
[2019-03-27 04:14:32,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4438
[2019-03-27 04:14:32,184] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 84.33333333333333, 1.0, 2.0, 0.7629382917603506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1088594.82446674, 1088594.82446674, 238089.1137964427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7009800.0000, 
sim time next is 7010400.0000, 
raw observation next is [25.43333333333334, 84.66666666666667, 1.0, 2.0, 0.7030153623709926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1002518.760874684, 1002518.760874685, 224181.3914568223], 
processed observation next is [1.0, 0.13043478260869565, 0.40442338072669864, 0.8466666666666667, 1.0, 1.0, 0.6421871835795091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2784774335763011, 0.2784774335763014, 0.33459909172660046], 
reward next is 0.6654, 
noisyNet noise sample is [array([0.697166], dtype=float32), -0.30590886]. 
=============================================
[2019-03-27 04:14:32,302] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2292808: loss 0.0017
[2019-03-27 04:14:32,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2292811: learning rate 0.0005
[2019-03-27 04:14:32,663] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2292968: loss 0.1399
[2019-03-27 04:14:32,665] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2292969: learning rate 0.0005
[2019-03-27 04:14:32,850] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2293053: loss 180.9628
[2019-03-27 04:14:32,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2293053: learning rate 0.0005
[2019-03-27 04:14:33,391] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2293292: loss 78.0165
[2019-03-27 04:14:33,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2293294: learning rate 0.0005
[2019-03-27 04:14:34,510] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2293791: loss 19.4820
[2019-03-27 04:14:34,513] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2293791: learning rate 0.0005
[2019-03-27 04:14:37,027] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2294841: loss 230.3958
[2019-03-27 04:14:37,028] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2294841: learning rate 0.0005
[2019-03-27 04:14:38,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1342714e-30 1.0000000e+00 4.3342691e-37 2.3232597e-16 1.4508047e-33], sum to 1.0000
[2019-03-27 04:14:39,008] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4423
[2019-03-27 04:14:39,012] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.18333333333333, 92.16666666666667, 1.0, 2.0, 0.6882910113865879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 961898.8885065397, 961898.8885065403, 218337.324987471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6495000.0000, 
sim time next is 6495600.0000, 
raw observation next is [26.16666666666667, 92.33333333333334, 1.0, 2.0, 0.6693307651779183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 935389.93940007, 935389.9394000694, 214363.7850902913], 
processed observation next is [1.0, 0.17391304347826086, 0.4391785150078992, 0.9233333333333335, 1.0, 1.0, 0.6016033315396606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25983053872224166, 0.2598305387222415, 0.31994594789595715], 
reward next is 0.6801, 
noisyNet noise sample is [array([-0.17136353], dtype=float32), -0.7218557]. 
=============================================
[2019-03-27 04:14:40,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4788130e-29 1.0000000e+00 3.6566334e-38 2.5935454e-15 3.4832403e-30], sum to 1.0000
[2019-03-27 04:14:40,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2575
[2019-03-27 04:14:40,406] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.06666666666667, 47.0, 1.0, 2.0, 0.8560152899144252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1341146.278708486, 1341146.278708485, 277200.9394247295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6787200.0000, 
sim time next is 6787800.0000, 
raw observation next is [29.15, 46.5, 1.0, 2.0, 0.9281475714722659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1454998.426247872, 1454998.426247872, 299392.5541753127], 
processed observation next is [1.0, 0.5652173913043478, 0.5805687203791469, 0.465, 1.0, 1.0, 0.91343080900273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4041662295132978, 0.4041662295132978, 0.44685455847061595], 
reward next is 0.5531, 
noisyNet noise sample is [array([0.22679974], dtype=float32), 0.3759758]. 
=============================================
[2019-03-27 04:14:40,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8956566e-21 1.0000000e+00 4.2549984e-28 1.0956245e-10 5.2515153e-21], sum to 1.0000
[2019-03-27 04:14:40,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9901
[2019-03-27 04:14:40,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1715725.449328427 W.
[2019-03-27 04:14:40,760] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.8, 56.0, 1.0, 2.0, 0.4090896609829406, 1.0, 2.0, 0.4090896609829406, 1.0, 2.0, 0.6959370032998368, 6.9112, 6.9112, 170.5573041426782, 1715725.449328427, 1715725.449328427, 355865.3997769268], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6526800.0000, 
sim time next is 6527400.0000, 
raw observation next is [31.85, 55.83333333333334, 1.0, 2.0, 0.3700200289400895, 1.0, 2.0, 0.3700200289400895, 1.0, 2.0, 0.6288749440363421, 6.9112, 6.9112, 170.5573041426782, 1551748.456803223, 1551748.456803223, 335138.9833006304], 
processed observation next is [1.0, 0.5652173913043478, 0.7085308056872038, 0.5583333333333335, 1.0, 1.0, 0.24098798667480664, 1.0, 1.0, 0.24098798667480664, 1.0, 1.0, 0.5474084683370026, 0.0, 0.0, 0.8375144448122397, 0.4310412380008952, 0.4310412380008952, 0.5002074377621349], 
reward next is 0.4998, 
noisyNet noise sample is [array([-0.14863883], dtype=float32), 1.3576938]. 
=============================================
[2019-03-27 04:14:42,510] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2297370: loss 103.9966
[2019-03-27 04:14:42,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2297371: learning rate 0.0005
[2019-03-27 04:14:42,694] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2297450: loss 226.4331
[2019-03-27 04:14:42,697] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2297451: learning rate 0.0005
[2019-03-27 04:14:44,232] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2298162: loss 88.7861
[2019-03-27 04:14:44,233] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2298162: learning rate 0.0005
[2019-03-27 04:14:45,731] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2298859: loss 20.4476
[2019-03-27 04:14:45,733] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2298859: learning rate 0.0005
[2019-03-27 04:14:45,835] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2298911: loss -4.9611
[2019-03-27 04:14:45,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2298912: learning rate 0.0005
[2019-03-27 04:14:47,260] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2299567: loss 32.7351
[2019-03-27 04:14:47,263] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2299567: learning rate 0.0005
[2019-03-27 04:14:47,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6447183e-33 0.0000000e+00], sum to 1.0000
[2019-03-27 04:14:47,938] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8664
[2019-03-27 04:14:47,944] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.08333333333334, 82.33333333333334, 1.0, 2.0, 0.3384398181616116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527583.3709242905, 527583.3709242905, 168987.0938982597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6840600.0000, 
sim time next is 6841200.0000, 
raw observation next is [23.06666666666667, 82.66666666666667, 1.0, 2.0, 0.3399067465857903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 529475.2795447053, 529475.279544706, 169128.0409553437], 
processed observation next is [0.0, 0.17391304347826086, 0.29225908372827825, 0.8266666666666667, 1.0, 1.0, 0.2047069235973377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1470764665401959, 0.1470764665401961, 0.25242991187364733], 
reward next is 0.7476, 
noisyNet noise sample is [array([-1.5405165], dtype=float32), 1.4921551]. 
=============================================
[2019-03-27 04:14:48,197] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2299996: loss -17.8762
[2019-03-27 04:14:48,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2299997: learning rate 0.0005
[2019-03-27 04:14:48,209] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 04:14:48,211] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:14:48,212] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:14:48,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:14:48,214] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:14:48,214] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:14:48,214] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:14:48,215] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:14:48,219] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:14:48,220] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:14:48,221] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:14:48,238] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-03-27 04:14:48,259] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-03-27 04:14:48,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-03-27 04:14:48,261] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-03-27 04:14:48,323] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-03-27 04:14:53,030] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:14:53,031] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.03333333333333, 74.0, 1.0, 2.0, 0.2480492543083349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 409379.503844773, 409379.5038447736, 160673.2015517791]
[2019-03-27 04:14:53,033] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:14:53,035] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.073039e-32 0.000000e+00], sampled 0.336731622352863
[2019-03-27 04:15:12,666] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:15:12,667] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.28333333333333, 84.00000000000001, 1.0, 2.0, 0.5332195634577036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768461.706497076, 768461.7064970753, 191931.8676050729]
[2019-03-27 04:15:12,667] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:12,672] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7914263e-33 0.0000000e+00], sampled 0.20058707048952018
[2019-03-27 04:15:25,048] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:15:25,050] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.6, 57.0, 1.0, 2.0, 0.6025393718119594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842011.9474990651, 842011.9474990651, 201212.844757359]
[2019-03-27 04:15:25,050] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:25,055] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8384547e-30 0.0000000e+00], sampled 0.5459815961171484
[2019-03-27 04:15:35,423] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:15:35,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.9, 65.66666666666667, 1.0, 2.0, 0.5475092120592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765082.9662115196, 765082.9662115201, 191373.8172011623]
[2019-03-27 04:15:35,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:15:35,430] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0863956e-25 0.0000000e+00], sampled 0.9277100825336793
[2019-03-27 04:15:43,752] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:15:43,752] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.7, 64.5, 1.0, 2.0, 0.5590044539198971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781152.1892689279, 781152.1892689279, 193356.9223586575]
[2019-03-27 04:15:43,754] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:43,757] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1143812e-30 0.0000000e+00], sampled 0.30186654455376816
[2019-03-27 04:15:44,161] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:15:44,163] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 72.0, 1.0, 2.0, 0.5231519576711582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731034.7117226602, 731034.7117226602, 187302.3168603835]
[2019-03-27 04:15:44,164] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:44,168] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03799102903270635
[2019-03-27 04:15:53,539] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:15:53,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.95, 74.33333333333334, 1.0, 2.0, 0.5731062707510027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800865.481068374, 800865.4810683734, 195841.5799108524]
[2019-03-27 04:15:53,543] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:15:53,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.470946e-31 0.000000e+00], sampled 0.7084123328347648
[2019-03-27 04:16:09,599] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.060511854]
[2019-03-27 04:16:09,603] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.68333333333333, 84.66666666666666, 1.0, 2.0, 0.6822967528636861, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974178241212, 6.9112, 168.9123158150468, 1850355.976882444, 1783120.167992441, 380845.7609762381]
[2019-03-27 04:16:09,605] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:16:09,607] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8912282e-27 1.0000000e+00 2.7863590e-35 2.9779235e-12 1.0815452e-25], sampled 0.23404366789233222
[2019-03-27 04:16:09,608] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1850355.976882444 W.
[2019-03-27 04:16:43,635] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.2887 2927070019.6746 1334.0000
[2019-03-27 04:16:44,028] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.2045 2778961019.0248 929.0000
[2019-03-27 04:16:44,111] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3534 2842064669.2648 1126.0000
[2019-03-27 04:16:44,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7897.8620 3162691303.1012 1742.0000
[2019-03-27 04:16:44,390] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.8367 3007434102.8546 1764.0000
[2019-03-27 04:16:45,409] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2300000, evaluation results [2300000.0, 7897.861993104973, 3162691303.101168, 1742.0, 8257.28873230408, 2927070019.6745987, 1334.0, 8662.20448150199, 2778961019.0247684, 929.0, 8000.8367269003875, 3007434102.854595, 1764.0, 8496.353351561904, 2842064669.264845, 1126.0]
[2019-03-27 04:16:45,480] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2300040: loss 32.3827
[2019-03-27 04:16:45,484] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2300041: learning rate 0.0005
[2019-03-27 04:16:46,032] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2300280: loss -73.4363
[2019-03-27 04:16:46,043] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2300283: learning rate 0.0005
[2019-03-27 04:16:46,815] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2300630: loss -83.8509
[2019-03-27 04:16:46,816] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2300630: learning rate 0.0005
[2019-03-27 04:16:47,233] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2300813: loss -87.2819
[2019-03-27 04:16:47,237] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2300813: learning rate 0.0005
[2019-03-27 04:16:47,431] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2300900: loss 34.8813
[2019-03-27 04:16:47,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2300900: learning rate 0.0005
[2019-03-27 04:16:47,579] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2300968: loss -0.0386
[2019-03-27 04:16:47,580] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2300968: learning rate 0.0005
[2019-03-27 04:16:48,021] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2301160: loss 30.3208
[2019-03-27 04:16:48,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2301161: learning rate 0.0005
[2019-03-27 04:16:49,223] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2301698: loss 28.5402
[2019-03-27 04:16:49,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2301698: learning rate 0.0005
[2019-03-27 04:16:51,777] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2302845: loss 34.5416
[2019-03-27 04:16:51,781] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2302847: learning rate 0.0005
[2019-03-27 04:16:51,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:16:51,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2410
[2019-03-27 04:16:51,805] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 53.66666666666666, 1.0, 2.0, 0.4566937806234246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645723.409455838, 645723.4094558386, 178068.5655310678], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6955800.0000, 
sim time next is 6956400.0000, 
raw observation next is [31.33333333333334, 53.33333333333334, 1.0, 2.0, 0.4607192371704784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649184.6980748904, 649184.6980748904, 178370.7886708213], 
processed observation next is [0.0, 0.5217391304347826, 0.6840442338072673, 0.5333333333333334, 1.0, 1.0, 0.35026414116925114, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18032908279858068, 0.18032908279858068, 0.2662250577176437], 
reward next is 0.7338, 
noisyNet noise sample is [array([0.8028948], dtype=float32), -0.05598292]. 
=============================================
[2019-03-27 04:16:52,642] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5581865e-24 1.0000000e+00 1.4752103e-32 4.5013138e-12 8.3513000e-23], sum to 1.0000
[2019-03-27 04:16:52,650] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1668
[2019-03-27 04:16:52,659] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2054237.392718791 W.
[2019-03-27 04:16:52,669] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 61.5, 1.0, 2.0, 0.8279829766717198, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.963813620645183, 6.9112, 168.9126431309759, 2054237.392718791, 2016911.548661744, 416215.9369396117], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6705000.0000, 
sim time next is 6705600.0000, 
raw observation next is [30.06666666666667, 62.0, 1.0, 2.0, 0.4901431483557598, 1.0, 1.0, 0.4901431483557598, 1.0, 2.0, 0.8289549201255416, 6.911199999999999, 6.9112, 170.5573041426782, 2055990.60976478, 2055990.609764781, 404390.6109799554], 
processed observation next is [1.0, 0.6086956521739131, 0.6240126382306479, 0.62, 1.0, 1.0, 0.38571463657320454, 1.0, 0.5, 0.38571463657320454, 1.0, 1.0, 0.7914084391774897, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5711085027124388, 0.5711085027124392, 0.6035680760894857], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25640225], dtype=float32), 0.29100645]. 
=============================================
[2019-03-27 04:16:57,347] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2305314: loss 26.5701
[2019-03-27 04:16:57,350] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2305315: learning rate 0.0005
[2019-03-27 04:16:57,427] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2305346: loss 25.4788
[2019-03-27 04:16:57,429] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2305347: learning rate 0.0005
[2019-03-27 04:16:58,754] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2305941: loss 0.0169
[2019-03-27 04:16:58,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2305941: learning rate 0.0005
[2019-03-27 04:17:00,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.158195e-36 0.000000e+00], sum to 1.0000
[2019-03-27 04:17:00,558] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3022
[2019-03-27 04:17:00,565] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 80.66666666666666, 1.0, 2.0, 0.4172341074361636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612791.8599090766, 612791.8599090766, 175457.8818381976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6911400.0000, 
sim time next is 6912000.0000, 
raw observation next is [25.2, 81.0, 1.0, 2.0, 0.4173682019448138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613015.4431024941, 613015.4431024941, 175479.8766840029], 
processed observation next is [0.0, 0.0, 0.3933649289099526, 0.81, 1.0, 1.0, 0.2980339782467636, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17028206752847058, 0.17028206752847058, 0.261910263707467], 
reward next is 0.7381, 
noisyNet noise sample is [array([-0.9076567], dtype=float32), 0.5156029]. 
=============================================
[2019-03-27 04:17:00,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.6259  ]
 [80.64389 ]
 [80.64739 ]
 [80.670364]
 [80.6886  ]], R is [[75.88240814]
 [75.86170959]
 [75.84126282]
 [75.82108307]
 [75.80118561]].
[2019-03-27 04:17:00,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2306759: loss 28.9678
[2019-03-27 04:17:00,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2306760: learning rate 0.0005
[2019-03-27 04:17:00,904] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2306901: loss 25.8195
[2019-03-27 04:17:00,907] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2306901: learning rate 0.0005
[2019-03-27 04:17:02,900] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2307771: loss -83.6248
[2019-03-27 04:17:02,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2307771: learning rate 0.0005
[2019-03-27 04:17:03,328] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2307963: loss 28.5901
[2019-03-27 04:17:03,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2307963: learning rate 0.0005
[2019-03-27 04:17:03,781] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2308161: loss -31.0541
[2019-03-27 04:17:03,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2308161: learning rate 0.0005
[2019-03-27 04:17:04,313] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2308399: loss 23.8628
[2019-03-27 04:17:04,314] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2308399: learning rate 0.0005
[2019-03-27 04:17:04,876] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2308648: loss 24.0618
[2019-03-27 04:17:04,877] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2308648: learning rate 0.0005
[2019-03-27 04:17:05,288] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2308832: loss 26.8634
[2019-03-27 04:17:05,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2308832: learning rate 0.0005
[2019-03-27 04:17:05,451] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2308906: loss 23.3143
[2019-03-27 04:17:05,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2308906: learning rate 0.0005
[2019-03-27 04:17:05,933] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2309122: loss -59.9854
[2019-03-27 04:17:05,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2309122: learning rate 0.0005
[2019-03-27 04:17:06,334] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2309300: loss -267.5540
[2019-03-27 04:17:06,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2309300: learning rate 0.0005
[2019-03-27 04:17:07,564] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2309843: loss -14.4598
[2019-03-27 04:17:07,567] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2309844: learning rate 0.0005
[2019-03-27 04:17:10,064] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2310958: loss -124.8048
[2019-03-27 04:17:10,065] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2310958: learning rate 0.0005
[2019-03-27 04:17:10,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0485737e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 04:17:10,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1443
[2019-03-27 04:17:10,709] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 83.0, 1.0, 2.0, 0.4602755579210497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653117.9531069922, 653117.9531069922, 178890.5725801206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7005600.0000, 
sim time next is 7006200.0000, 
raw observation next is [25.75, 83.16666666666667, 1.0, 2.0, 0.9417134402916928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336893.568229379, 1336893.568229379, 284793.4860074467], 
processed observation next is [1.0, 0.08695652173913043, 0.41943127962085314, 0.8316666666666667, 1.0, 1.0, 0.9297752292670998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3713593245081608, 0.3713593245081608, 0.4250649044887264], 
reward next is 0.5749, 
noisyNet noise sample is [array([1.2455978], dtype=float32), -1.3201133]. 
=============================================
[2019-03-27 04:17:15,586] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2313418: loss -200.6269
[2019-03-27 04:17:15,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2313420: learning rate 0.0005
[2019-03-27 04:17:15,722] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2313477: loss -145.7997
[2019-03-27 04:17:15,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2313477: learning rate 0.0005
[2019-03-27 04:17:16,820] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2313967: loss -180.0093
[2019-03-27 04:17:16,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2313968: learning rate 0.0005
[2019-03-27 04:17:17,439] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3478617e-30 1.0000000e+00 2.0723668e-37 5.3035876e-17 8.8718198e-31], sum to 1.0000
[2019-03-27 04:17:17,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3202
[2019-03-27 04:17:17,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1713560.013020746 W.
[2019-03-27 04:17:17,465] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.13333333333333, 85.33333333333333, 1.0, 2.0, 0.6128606352331407, 1.0, 2.0, 0.6128606352331407, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1713560.013020746, 1713560.013020746, 339040.5594474727], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7144800.0000, 
sim time next is 7145400.0000, 
raw observation next is [26.11666666666667, 85.66666666666667, 1.0, 2.0, 0.6040258790788627, 0.0, 1.0, 0.0, 1.0, 1.0, 1.013166931704125, 6.9112, 6.9112, 168.912956510431, 1688851.500832615, 1688851.500832615, 362182.2088334862], 
processed observation next is [1.0, 0.6956521739130435, 0.43680884676145365, 0.8566666666666667, 1.0, 1.0, 0.5229227458781478, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0160572337855183, 0.0, 0.0, 0.8294399451523027, 0.4691254168979486, 0.4691254168979486, 0.5405704609455018], 
reward next is 0.4594, 
noisyNet noise sample is [array([-0.39567536], dtype=float32), -1.4532475]. 
=============================================
[2019-03-27 04:17:18,677] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2314784: loss -123.8615
[2019-03-27 04:17:18,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2314785: learning rate 0.0005
[2019-03-27 04:17:19,059] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2314953: loss -64.8977
[2019-03-27 04:17:19,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2314955: learning rate 0.0005
[2019-03-27 04:17:20,367] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2315539: loss 0.0354
[2019-03-27 04:17:20,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2315540: learning rate 0.0005
[2019-03-27 04:17:21,190] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2315906: loss 0.0096
[2019-03-27 04:17:21,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2315908: learning rate 0.0005
[2019-03-27 04:17:21,567] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2316073: loss -127.8581
[2019-03-27 04:17:21,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2316073: learning rate 0.0005
[2019-03-27 04:17:22,744] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2316583: loss -26.7545
[2019-03-27 04:17:22,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2316583: learning rate 0.0005
[2019-03-27 04:17:23,258] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2316813: loss -199.8245
[2019-03-27 04:17:23,262] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2316816: learning rate 0.0005
[2019-03-27 04:17:23,441] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2316893: loss 0.0037
[2019-03-27 04:17:23,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2316894: learning rate 0.0005
[2019-03-27 04:17:23,477] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2316908: loss -5.1356
[2019-03-27 04:17:23,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2316908: learning rate 0.0005
[2019-03-27 04:17:23,632] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2316978: loss -92.4395
[2019-03-27 04:17:23,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2316979: learning rate 0.0005
[2019-03-27 04:17:23,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2317055: loss 0.0196
[2019-03-27 04:17:23,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2317055: learning rate 0.0005
[2019-03-27 04:17:25,099] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2317629: loss 0.0426
[2019-03-27 04:17:25,101] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2317629: learning rate 0.0005
[2019-03-27 04:17:26,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:17:26,542] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:17:26,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-03-27 04:17:27,284] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2318656: loss 0.0104
[2019-03-27 04:17:27,285] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2318656: learning rate 0.0005
[2019-03-27 04:17:32,972] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2321256: loss 0.0313
[2019-03-27 04:17:32,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2321256: learning rate 0.0005
[2019-03-27 04:17:33,074] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2321307: loss 0.0510
[2019-03-27 04:17:33,076] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2321307: learning rate 0.0005
[2019-03-27 04:17:35,962] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2322647: loss 0.0066
[2019-03-27 04:17:35,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2322649: learning rate 0.0005
[2019-03-27 04:17:36,169] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1005341e-28 0.0000000e+00], sum to 1.0000
[2019-03-27 04:17:36,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9509
[2019-03-27 04:17:36,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 85.0, 1.0, 2.0, 0.5165411808302021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721793.8990162755, 721793.899016275, 186228.7630499495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7761600.0000, 
sim time next is 7762200.0000, 
raw observation next is [27.15, 85.33333333333334, 1.0, 2.0, 0.5179917238877224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723821.5202064664, 723821.5202064664, 186463.3624524275], 
processed observation next is [1.0, 0.8695652173913043, 0.485781990521327, 0.8533333333333334, 1.0, 1.0, 0.41926713721412334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2010615333906851, 0.2010615333906851, 0.27830352604839925], 
reward next is 0.7217, 
noisyNet noise sample is [array([1.2031057], dtype=float32), -0.66430354]. 
=============================================
[2019-03-27 04:17:36,182] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2322744: loss 0.0036
[2019-03-27 04:17:36,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2322745: learning rate 0.0005
[2019-03-27 04:17:36,909] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7447327e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 04:17:36,919] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4821
[2019-03-27 04:17:36,924] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333334, 87.33333333333333, 1.0, 2.0, 0.7174171810198852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1002622.441370717, 1002622.441370718, 224646.8171354372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7787400.0000, 
sim time next is 7788000.0000, 
raw observation next is [25.96666666666667, 87.66666666666667, 1.0, 2.0, 0.655604936112846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 916199.8153989692, 916199.8153989692, 211551.6433033448], 
processed observation next is [1.0, 0.13043478260869565, 0.42969984202211703, 0.8766666666666667, 1.0, 1.0, 0.5850661880877662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2544999487219359, 0.2544999487219359, 0.31574872134827586], 
reward next is 0.6843, 
noisyNet noise sample is [array([0.41271472], dtype=float32), -1.8053552]. 
=============================================
[2019-03-27 04:17:36,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.100567]
 [62.11116 ]
 [62.331924]
 [62.45274 ]
 [62.493954]], R is [[62.4022789 ]
 [62.44296265]
 [62.4871788 ]
 [62.53224945]
 [62.56721878]].
[2019-03-27 04:17:38,322] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2323726: loss -160.5970
[2019-03-27 04:17:38,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2323727: learning rate 0.0005
[2019-03-27 04:17:38,691] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2323897: loss 0.0036
[2019-03-27 04:17:38,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2323898: learning rate 0.0005
[2019-03-27 04:17:38,998] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2324039: loss -127.8668
[2019-03-27 04:17:39,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2324039: learning rate 0.0005
[2019-03-27 04:17:39,706] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2324367: loss 0.0028
[2019-03-27 04:17:39,710] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2324367: learning rate 0.0005
[2019-03-27 04:17:40,247] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2324620: loss 0.0045
[2019-03-27 04:17:40,256] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2324622: learning rate 0.0005
[2019-03-27 04:17:40,433] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2324701: loss 0.0105
[2019-03-27 04:17:40,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2324701: learning rate 0.0005
[2019-03-27 04:17:40,766] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2324854: loss 0.0036
[2019-03-27 04:17:40,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2324854: learning rate 0.0005
[2019-03-27 04:17:41,094] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 04:17:41,096] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:17:41,098] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:17:41,099] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:17:41,100] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:17:41,101] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:17:41,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:17:41,101] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:17:41,103] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:17:41,104] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:17:41,108] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:17:41,136] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-03-27 04:17:41,157] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-03-27 04:17:41,176] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-03-27 04:17:41,203] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-03-27 04:17:41,227] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-03-27 04:18:24,373] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.05108721]
[2019-03-27 04:18:24,373] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.80583443833333, 87.140800275, 1.0, 2.0, 0.3679160774722379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569428.6895192347, 569428.6895192347, 172361.5104578754]
[2019-03-27 04:18:24,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:18:24,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9752777033159035
[2019-03-27 04:18:52,938] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.05108721]
[2019-03-27 04:18:52,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.6599435366153074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 922265.5886796128, 922265.5886796123, 212434.3622953894]
[2019-03-27 04:18:52,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:18:52,945] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6973215332659048
[2019-03-27 04:19:16,331] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.05108721]
[2019-03-27 04:19:16,332] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.85, 55.83333333333334, 1.0, 2.0, 0.5555398754251132, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9361786310241379, 6.911200000000001, 6.9112, 168.9129489364075, 1553185.82978231, 1553185.829782309, 333944.1428585511]
[2019-03-27 04:19:16,333] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:19:16,338] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.0461986e-33 0.0000000e+00], sampled 0.7338080313960483
[2019-03-27 04:19:36,901] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:19:37,160] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:19:37,221] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:19:37,257] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:19:37,287] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:19:38,301] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2325000, evaluation results [2325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:19:38,437] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2325071: loss -143.2219
[2019-03-27 04:19:38,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2325071: learning rate 0.0005
[2019-03-27 04:19:38,664] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2325167: loss -135.4344
[2019-03-27 04:19:38,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2325168: learning rate 0.0005
[2019-03-27 04:19:39,426] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.8225315e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 04:19:39,439] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5096
[2019-03-27 04:19:39,446] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 86.66666666666666, 1.0, 2.0, 0.5209922654622194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728015.7972841791, 728015.7972841791, 186950.6140732788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7764600.0000, 
sim time next is 7765200.0000, 
raw observation next is [26.9, 87.0, 1.0, 2.0, 0.521490308296969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728711.9830900668, 728711.9830900675, 187031.7457191566], 
processed observation next is [1.0, 0.9130434782608695, 0.4739336492890995, 0.87, 1.0, 1.0, 0.42348229915297464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20241999530279634, 0.20241999530279653, 0.2791518592823233], 
reward next is 0.7208, 
noisyNet noise sample is [array([-0.02114597], dtype=float32), 1.1826957]. 
=============================================
[2019-03-27 04:19:39,955] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2325742: loss -26.4884
[2019-03-27 04:19:39,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2325742: learning rate 0.0005
[2019-03-27 04:19:41,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2986817e-31 0.0000000e+00], sum to 1.0000
[2019-03-27 04:19:41,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2202
[2019-03-27 04:19:41,792] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.0, 1.0, 2.0, 0.5075820176449558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709270.5446718894, 709270.5446718894, 184792.8573793201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7864200.0000, 
sim time next is 7864800.0000, 
raw observation next is [26.16666666666666, 90.33333333333334, 1.0, 2.0, 0.5079851915844985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709834.1086543124, 709834.1086543124, 184856.9613210805], 
processed observation next is [1.0, 0.0, 0.4391785150078987, 0.9033333333333334, 1.0, 1.0, 0.40721107419819097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19717614129286457, 0.19717614129286457, 0.27590591241952317], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.0163045], dtype=float32), -0.14625731]. 
=============================================
[2019-03-27 04:19:42,151] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2326725: loss -14.9237
[2019-03-27 04:19:42,155] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2326726: learning rate 0.0005
[2019-03-27 04:19:45,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:45,036] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:45,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-03-27 04:19:45,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:45,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:45,655] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-03-27 04:19:46,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2329044: loss -52.4784
[2019-03-27 04:19:46,983] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2329044: learning rate 0.0005
[2019-03-27 04:19:47,111] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2329098: loss -95.7165
[2019-03-27 04:19:47,118] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2329099: learning rate 0.0005
[2019-03-27 04:19:47,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4254693e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 04:19:47,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9505
[2019-03-27 04:19:47,248] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 80.5, 1.0, 2.0, 0.5874190559732858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940185.5759398754, 940185.5759398754, 211811.7673065752], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 405000.0000, 
sim time next is 405600.0000, 
raw observation next is [22.1, 81.0, 1.0, 2.0, 0.5941626311677893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 950651.4846372332, 950651.4846372338, 213200.4507746677], 
processed observation next is [1.0, 0.6956521739130435, 0.24644549763033188, 0.81, 1.0, 1.0, 0.5110393146599871, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2640698568436759, 0.26406985684367607, 0.31820962802189207], 
reward next is 0.6818, 
noisyNet noise sample is [array([-0.58968323], dtype=float32), 0.6439001]. 
=============================================
[2019-03-27 04:19:47,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:47,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:47,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-03-27 04:19:47,559] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:47,559] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:47,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-03-27 04:19:48,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:48,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:48,728] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-03-27 04:19:49,207] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2330231: loss -170.6703
[2019-03-27 04:19:49,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2330231: learning rate 0.0005
[2019-03-27 04:19:49,284] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2330276: loss -197.5743
[2019-03-27 04:19:49,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2330276: learning rate 0.0005
[2019-03-27 04:19:49,988] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:49,988] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:50,052] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-03-27 04:19:51,231] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2331338: loss -167.1777
[2019-03-27 04:19:51,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2331338: learning rate 0.0005
[2019-03-27 04:19:52,254] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2331805: loss -213.9953
[2019-03-27 04:19:52,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2331805: learning rate 0.0005
[2019-03-27 04:19:52,615] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2331965: loss -106.4233
[2019-03-27 04:19:52,619] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2331965: learning rate 0.0005
[2019-03-27 04:19:53,150] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2332206: loss -78.9788
[2019-03-27 04:19:53,153] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2332206: learning rate 0.0005
[2019-03-27 04:19:53,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2491748e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 04:19:53,274] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2332257: loss -140.0872
[2019-03-27 04:19:53,278] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2332257: learning rate 0.0005
[2019-03-27 04:19:53,278] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8568
[2019-03-27 04:19:53,284] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.86666666666667, 77.0, 1.0, 2.0, 0.5229475161067949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730748.9337854412, 730748.9337854405, 187270.6914511451], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7841400.0000, 
sim time next is 7842000.0000, 
raw observation next is [28.73333333333333, 78.0, 1.0, 2.0, 0.5261354190154246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735205.1416437647, 735205.1416437647, 187793.2281707274], 
processed observation next is [1.0, 0.782608695652174, 0.560821484992101, 0.78, 1.0, 1.0, 0.429078818090873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2042236504566013, 0.2042236504566013, 0.28028840025481705], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.84108204], dtype=float32), -0.40289584]. 
=============================================
[2019-03-27 04:19:53,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.85124]
 [73.35895]
 [73.52295]
 [73.95999]
 [73.75382]], R is [[73.12254333]
 [73.11180878]
 [73.10276031]
 [73.09651184]
 [73.0931778 ]].
[2019-03-27 04:19:54,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:54,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:54,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-03-27 04:19:54,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:54,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:55,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-03-27 04:19:57,388] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:57,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:57,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:57,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:57,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-03-27 04:19:57,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-03-27 04:19:59,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:19:59,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:19:59,651] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-03-27 04:20:00,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:20:00,399] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:00,460] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-03-27 04:20:00,831] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:20:00,831] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:00,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-03-27 04:20:01,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:20:01,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:01,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-03-27 04:20:01,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:20:01,258] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:01,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-03-27 04:20:01,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:01,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3565
[2019-03-27 04:20:01,490] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 86.0, 1.0, 2.0, 0.2677749890158124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434603.4281754196, 434603.4281754189, 162575.8184142708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 343200.0000, 
sim time next is 343800.0000, 
raw observation next is [20.65, 86.0, 1.0, 2.0, 0.2672657958880256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433896.570959082, 433896.570959082, 162528.8583907534], 
processed observation next is [0.0, 1.0, 0.1777251184834123, 0.86, 1.0, 1.0, 0.11718770588918749, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12052682526641166, 0.12052682526641166, 0.24258038565784087], 
reward next is 0.7574, 
noisyNet noise sample is [array([1.4107201], dtype=float32), -1.3021836]. 
=============================================
[2019-03-27 04:20:06,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.270074e-30 0.000000e+00], sum to 1.0000
[2019-03-27 04:20:06,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9059
[2019-03-27 04:20:06,521] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.36666666666667, 89.0, 1.0, 2.0, 0.3454056423985683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536040.1334539771, 536040.1334539765, 169605.132465738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 92400.0000, 
sim time next is 93000.0000, 
raw observation next is [22.38333333333333, 89.0, 1.0, 2.0, 0.3465905827298968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537635.9804984198, 537635.9804984193, 169728.4461324258], 
processed observation next is [1.0, 0.043478260869565216, 0.25987361769352274, 0.89, 1.0, 1.0, 0.2127597382287913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14934332791622773, 0.14934332791622756, 0.2533260390036206], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.2462234], dtype=float32), 1.3134357]. 
=============================================
[2019-03-27 04:20:06,541] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.935135]
 [72.168495]
 [72.401054]
 [72.58717 ]
 [72.829765]], R is [[71.78102112]
 [71.81006622]
 [71.83895874]
 [71.86764526]
 [71.89603424]].
[2019-03-27 04:20:07,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:07,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5189
[2019-03-27 04:20:07,561] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
processed observation next is [1.0, 0.30434782608695654, 0.15797788309636643, 0.8083333333333332, 1.0, 1.0, 0.11282927181585911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12070006551133781, 0.12070006551133781, 0.24218520355860731], 
reward next is 0.7578, 
noisyNet noise sample is [array([-0.5481161], dtype=float32), 1.468535]. 
=============================================
[2019-03-27 04:20:08,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:08,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0520
[2019-03-27 04:20:08,162] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333333, 82.0, 1.0, 2.0, 0.2286532391269225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377902.9934132327, 377902.9934132327, 158809.3642383329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 452400.0000, 
sim time next is 453000.0000, 
raw observation next is [19.86666666666666, 82.0, 1.0, 2.0, 0.229338830578283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378904.9235685278, 378904.9235685278, 158880.38700392], 
processed observation next is [1.0, 0.21739130434782608, 0.14060031595576594, 0.82, 1.0, 1.0, 0.07149256696178673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1052513676579244, 0.1052513676579244, 0.237134905976], 
reward next is 0.7629, 
noisyNet noise sample is [array([-1.545022], dtype=float32), -0.42239606]. 
=============================================
[2019-03-27 04:20:08,179] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[67.975685]
 [68.01886 ]
 [68.04587 ]
 [68.02691 ]
 [68.19378 ]], R is [[67.98664093]
 [68.06974792]
 [68.15207672]
 [68.23355865]
 [68.31324005]].
[2019-03-27 04:20:09,310] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:09,319] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2784
[2019-03-27 04:20:09,325] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 87.5, 1.0, 2.0, 0.2567408921909044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418251.4206480645, 418251.4206480645, 161518.4713780084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 366600.0000, 
sim time next is 367200.0000, 
raw observation next is [20.3, 87.0, 1.0, 2.0, 0.2569084511794936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418598.7393006994, 418598.7393006994, 161538.0374686603], 
processed observation next is [1.0, 0.2608695652173913, 0.16113744075829392, 0.87, 1.0, 1.0, 0.1047089773246911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1162774275835276, 0.1162774275835276, 0.24110154846068702], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.5566135], dtype=float32), -0.15221934]. 
=============================================
[2019-03-27 04:20:11,154] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:11,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7441
[2019-03-27 04:20:11,168] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457800.0000, 
sim time next is 458400.0000, 
raw observation next is [20.26666666666667, 80.66666666666667, 1.0, 2.0, 0.24365008911439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401497.8336004322, 401497.8336004322, 160269.8607273031], 
processed observation next is [1.0, 0.30434782608695654, 0.15955766192733034, 0.8066666666666668, 1.0, 1.0, 0.08873504712577109, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11152717600012006, 0.11152717600012006, 0.23920874735418374], 
reward next is 0.7608, 
noisyNet noise sample is [array([0.3246616], dtype=float32), -0.77653897]. 
=============================================
[2019-03-27 04:20:12,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:12,084] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1187
[2019-03-27 04:20:12,089] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 73.66666666666667, 1.0, 2.0, 0.4362221574589196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716878.5457219598, 716878.5457219598, 185004.9911250182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 465600.0000, 
sim time next is 466200.0000, 
raw observation next is [21.55, 73.0, 1.0, 2.0, 0.4506235576429891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740363.6387297891, 740363.6387297891, 187328.642472771], 
processed observation next is [1.0, 0.391304347826087, 0.22037914691943136, 0.73, 1.0, 1.0, 0.338100671859023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2056565663138303, 0.2056565663138303, 0.27959498876532984], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.88419944], dtype=float32), -0.20844638]. 
=============================================
[2019-03-27 04:20:15,014] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:15,025] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9936
[2019-03-27 04:20:15,031] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.3967005], dtype=float32), -0.75823027]. 
=============================================
[2019-03-27 04:20:15,265] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:15,276] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1107
[2019-03-27 04:20:15,283] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.43333333333333, 86.0, 1.0, 2.0, 0.258034591050252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420407.3492760002, 420407.3492760009, 161650.391173464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 368400.0000, 
sim time next is 369000.0000, 
raw observation next is [20.5, 85.5, 1.0, 2.0, 0.2575769303980422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419634.9768516916, 419634.9768516916, 161603.3024957541], 
processed observation next is [1.0, 0.2608695652173913, 0.1706161137440759, 0.855, 1.0, 1.0, 0.1055143739735448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11656527134769211, 0.11656527134769211, 0.2411989589488867], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.9957868], dtype=float32), 1.3289981]. 
=============================================
[2019-03-27 04:20:15,297] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.81225 ]
 [70.8397  ]
 [70.93366 ]
 [70.81481 ]
 [70.882256]], R is [[70.88996887]
 [70.93979645]
 [70.98868561]
 [71.03769684]
 [71.08625031]].
[2019-03-27 04:20:15,557] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:15,567] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1630
[2019-03-27 04:20:15,571] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 86.83333333333333, 1.0, 2.0, 0.3124297950515363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 494635.7626226757, 494635.7626226764, 166637.5361426853], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 229800.0000, 
sim time next is 230400.0000, 
raw observation next is [21.8, 87.0, 1.0, 2.0, 0.3115171818356229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493323.9484018233, 493323.9484018227, 166543.1612208481], 
processed observation next is [0.0, 0.6956521739130435, 0.23222748815165886, 0.87, 1.0, 1.0, 0.17050262871761793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1370344301116176, 0.13703443011161742, 0.24857188241917627], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.83923095], dtype=float32), -0.23408365]. 
=============================================
[2019-03-27 04:20:16,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:16,579] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9539
[2019-03-27 04:20:16,585] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 86.0, 1.0, 2.0, 0.286087918019572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459034.5913646868, 459034.5913646868, 164199.5907626482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 331200.0000, 
sim time next is 331800.0000, 
raw observation next is [21.26666666666667, 86.0, 1.0, 2.0, 0.2851089079714543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457733.6440263194, 457733.64402632, 164113.0896229131], 
processed observation next is [0.0, 0.8695652173913043, 0.2069510268562403, 0.86, 1.0, 1.0, 0.13868543129090877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12714823445175538, 0.12714823445175555, 0.24494490988494494], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.27365756], dtype=float32), -0.8896434]. 
=============================================
[2019-03-27 04:20:19,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:19,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1485
[2019-03-27 04:20:19,432] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 90.0, 1.0, 2.0, 0.2790752241858487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 450205.4066099429, 450205.4066099423, 163613.9061713835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 282600.0000, 
sim time next is 283200.0000, 
raw observation next is [20.7, 89.33333333333334, 1.0, 2.0, 0.280079816169869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 451249.1622703146, 451249.1622703146, 163683.3435934653], 
processed observation next is [0.0, 0.2608695652173913, 0.18009478672985785, 0.8933333333333334, 1.0, 1.0, 0.1326262845420108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12534698951953183, 0.12534698951953183, 0.2443034979006945], 
reward next is 0.7557, 
noisyNet noise sample is [array([0.4755555], dtype=float32), -0.95804405]. 
=============================================
[2019-03-27 04:20:20,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:20,920] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3393
[2019-03-27 04:20:20,925] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 80.5, 1.0, 2.0, 0.2435427851892651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401270.2285228275, 401270.2285228275, 160261.07886859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [20.33333333333333, 80.33333333333333, 1.0, 2.0, 0.2443283175102852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402511.2493185347, 402511.249318534, 160338.3618880211], 
processed observation next is [1.0, 0.30434782608695654, 0.16271721958925733, 0.8033333333333332, 1.0, 1.0, 0.08955218977142793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11180868036625963, 0.11180868036625945, 0.23931098789256883], 
reward next is 0.7607, 
noisyNet noise sample is [array([0.09275622], dtype=float32), 0.4135526]. 
=============================================
[2019-03-27 04:20:24,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:24,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2248
[2019-03-27 04:20:24,491] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 66.66666666666667, 1.0, 2.0, 0.2479955038477095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408306.4379170497, 408306.4379170503, 160700.9683011164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 672600.0000, 
sim time next is 673200.0000, 
raw observation next is [22.1, 68.0, 1.0, 2.0, 0.2483964544502939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 409016.1123429117, 409016.1123429111, 160739.05740801], 
processed observation next is [1.0, 0.8260869565217391, 0.24644549763033188, 0.68, 1.0, 1.0, 0.09445355957866734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11361558676191992, 0.11361558676191974, 0.2399090409074776], 
reward next is 0.7601, 
noisyNet noise sample is [array([-0.6377921], dtype=float32), -0.47687754]. 
=============================================
[2019-03-27 04:20:25,612] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:25,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3160
[2019-03-27 04:20:25,627] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 54.0, 1.0, 2.0, 0.6266273055150398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1028773.728638557, 1028773.728638557, 221450.6096715211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 666000.0000, 
sim time next is 666600.0000, 
raw observation next is [24.48333333333333, 55.0, 1.0, 2.0, 0.3202940534713662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525968.4570514825, 525968.4570514825, 168698.9697377339], 
processed observation next is [1.0, 0.7391304347826086, 0.3593996840442337, 0.55, 1.0, 1.0, 0.1810771728570677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14610234918096737, 0.14610234918096737, 0.2517895070712446], 
reward next is 0.7482, 
noisyNet noise sample is [array([-1.1198694], dtype=float32), 1.1285689]. 
=============================================
[2019-03-27 04:20:31,333] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 04:20:31,335] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:20:31,337] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:20:31,339] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:31,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:31,340] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:20:31,341] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:20:31,343] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:20:31,343] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:31,344] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:31,344] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:20:31,368] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-03-27 04:20:31,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-03-27 04:20:31,410] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-03-27 04:20:31,412] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-03-27 04:20:31,429] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-03-27 04:20:55,063] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:20:55,065] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.740348735, 95.28793125499999, 1.0, 2.0, 0.3941095613094467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598202.7635317744, 598202.7635317744, 174643.7008001354]
[2019-03-27 04:20:55,066] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:20:55,070] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3731319813238355
[2019-03-27 04:21:03,435] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:21:03,437] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.934165815, 80.894425445, 1.0, 2.0, 0.6295298754018902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 879745.1475059509, 879745.1475059515, 206377.0409403563]
[2019-03-27 04:21:03,437] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:21:03,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5165433037669663
[2019-03-27 04:21:16,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:21:16,873] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 75.0, 1.0, 2.0, 0.597699948860203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835246.4905922757, 835246.4905922757, 200318.4407519266]
[2019-03-27 04:21:16,874] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:21:16,880] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.031714555557292856
[2019-03-27 04:21:29,556] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:21:29,558] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.16845605333334, 95.47293563, 1.0, 2.0, 0.8771003306212329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1225915.572432669, 1225915.572432669, 263713.825220522]
[2019-03-27 04:21:29,559] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:21:29,560] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8874620866765497
[2019-03-27 04:21:30,041] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:21:30,044] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.55, 65.5, 1.0, 2.0, 0.6602679983766496, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005972802743775, 6.9112, 168.9123160684608, 1819531.119724685, 1752296.286555334, 376271.1009852895]
[2019-03-27 04:21:30,045] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:21:30,048] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7829954e-38 0.0000000e+00], sampled 0.7213586015056056
[2019-03-27 04:21:30,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1819531.119724685 W.
[2019-03-27 04:22:06,674] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:22:06,675] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.8, 86.0, 1.0, 2.0, 0.5111532683365179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714262.5152825058, 714262.5152825058, 185362.1584143999]
[2019-03-27 04:22:06,676] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:22:06,681] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8542594631559506
[2019-03-27 04:22:14,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:22:14,198] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.35924277, 84.7090439, 1.0, 2.0, 0.8393541375423929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173128.898247431, 1173128.898247431, 253799.4278658295]
[2019-03-27 04:22:14,200] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:22:14,204] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3959978253556222
[2019-03-27 04:22:19,758] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.058357816]
[2019-03-27 04:22:19,760] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.08333333333334, 85.00000000000001, 1.0, 2.0, 0.5251823077431538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733872.8337572224, 733872.8337572224, 187635.2184346701]
[2019-03-27 04:22:19,762] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:22:19,764] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7162543111254951
[2019-03-27 04:22:25,532] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:22:25,906] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:22:26,190] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:22:26,367] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:22:26,431] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:22:27,452] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2350000, evaluation results [2350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:22:28,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6205621e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 04:22:28,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1550
[2019-03-27 04:22:28,347] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 89.0, 1.0, 2.0, 0.2535701614886153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 416470.0307441608, 416470.0307441608, 161264.4806142714], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 777600.0000, 
sim time next is 778200.0000, 
raw observation next is [19.5, 89.33333333333334, 1.0, 2.0, 0.2540526982891675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417111.0264477726, 417111.0264477726, 161313.052710072], 
processed observation next is [0.0, 0.0, 0.12322274881516594, 0.8933333333333334, 1.0, 1.0, 0.10126831119176805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11586417401327016, 0.11586417401327016, 0.24076575031354028], 
reward next is 0.7592, 
noisyNet noise sample is [array([-0.31257984], dtype=float32), 0.62705356]. 
=============================================
[2019-03-27 04:22:30,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:22:30,778] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7761
[2019-03-27 04:22:30,786] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 80.33333333333334, 1.0, 2.0, 0.297552861713451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474741.94150616, 474741.94150616, 165261.1024417164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 848400.0000, 
sim time next is 849000.0000, 
raw observation next is [22.26666666666667, 80.66666666666666, 1.0, 2.0, 0.2966932051623078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473573.2531905661, 473573.2531905668, 165181.3424400975], 
processed observation next is [0.0, 0.8260869565217391, 0.2543443917851502, 0.8066666666666665, 1.0, 1.0, 0.15264241585820215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13154812588626835, 0.13154812588626855, 0.2465393170747724], 
reward next is 0.7535, 
noisyNet noise sample is [array([0.9466425], dtype=float32), -1.7918206]. 
=============================================
[2019-03-27 04:22:30,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.4725 ]
 [78.47362]
 [78.48841]
 [78.4996 ]
 [78.51127]], R is [[78.43682861]
 [78.40579987]
 [78.37496185]
 [78.34433746]
 [78.31396484]].
[2019-03-27 04:22:31,871] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:22:31,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8458
[2019-03-27 04:22:31,887] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.0, 1.0, 2.0, 0.5758176870902904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 947255.4411877558, 947255.4411877552, 210515.0070299353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 655200.0000, 
sim time next is 655800.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.5379671656988837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 885088.8569347081, 885088.8569347075, 202939.2965670972], 
processed observation next is [1.0, 0.6086956521739131, 0.3696682464454976, 0.53, 1.0, 1.0, 0.4433339345769683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2458580158151967, 0.24585801581519653, 0.3028944724882048], 
reward next is 0.6971, 
noisyNet noise sample is [array([0.29938075], dtype=float32), -1.2085364]. 
=============================================
[2019-03-27 04:22:38,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:22:38,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1900
[2019-03-27 04:22:38,830] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3695972679602647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561246.9547247476, 561246.9547247476, 171367.356081686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1033200.0000, 
sim time next is 1033800.0000, 
raw observation next is [22.03333333333333, 97.83333333333334, 1.0, 2.0, 0.3713452241191626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563523.7318817053, 563523.731881706, 171552.6863757075], 
processed observation next is [1.0, 1.0, 0.2432859399684044, 0.9783333333333334, 1.0, 1.0, 0.24258460737248505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15653436996714037, 0.15653436996714057, 0.25604878563538436], 
reward next is 0.7440, 
noisyNet noise sample is [array([-2.287061], dtype=float32), 0.33960584]. 
=============================================
[2019-03-27 04:22:53,032] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:22:53,042] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1935
[2019-03-27 04:22:53,046] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 77.33333333333333, 1.0, 2.0, 0.3232193547300174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 505436.0950720437, 505436.095072043, 167302.1162418337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 928200.0000, 
sim time next is 928800.0000, 
raw observation next is [23.6, 78.0, 1.0, 2.0, 0.3240325173460962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506330.6217864238, 506330.6217864231, 167360.0045710331], 
processed observation next is [0.0, 0.782608695652174, 0.3175355450236968, 0.78, 1.0, 1.0, 0.18558134620011593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14064739494067327, 0.14064739494067308, 0.24979105159855686], 
reward next is 0.7502, 
noisyNet noise sample is [array([0.27634844], dtype=float32), 0.40468162]. 
=============================================
[2019-03-27 04:22:54,589] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.403369e-28 0.000000e+00], sum to 1.0000
[2019-03-27 04:22:54,596] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0815
[2019-03-27 04:22:54,608] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 67.0, 1.0, 2.0, 0.7717465397206623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1190770.815690985, 1190770.815690985, 251603.0383437812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [25.66666666666666, 67.5, 1.0, 2.0, 0.4232138857200688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653470.7032023642, 653470.7032023648, 179974.1591324478], 
processed observation next is [1.0, 0.7391304347826086, 0.4154818325434437, 0.675, 1.0, 1.0, 0.3050769707470708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1815196397784345, 0.1815196397784347, 0.2686181479588773], 
reward next is 0.7314, 
noisyNet noise sample is [array([-2.0522661], dtype=float32), -0.47041103]. 
=============================================
[2019-03-27 04:22:54,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:22:54,850] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7794
[2019-03-27 04:22:54,856] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333333, 95.16666666666667, 1.0, 2.0, 0.3863456947753357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598072.0404958345, 598072.0404958345, 174882.9244018733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 997800.0000, 
sim time next is 998400.0000, 
raw observation next is [21.66666666666667, 95.33333333333334, 1.0, 2.0, 0.4818392120468271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745854.2547514256, 745854.2547514256, 189439.2363740163], 
processed observation next is [1.0, 0.5652173913043478, 0.22590837282780438, 0.9533333333333335, 1.0, 1.0, 0.3757098940323218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20718173743095156, 0.20718173743095156, 0.2827451289164422], 
reward next is 0.7173, 
noisyNet noise sample is [array([-1.3870983], dtype=float32), -0.02578559]. 
=============================================
[2019-03-27 04:23:03,100] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:03,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2794
[2019-03-27 04:23:03,112] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 72.0, 1.0, 2.0, 0.446095451224536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695558.2386561892, 695558.2386561886, 184143.6675486088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1083600.0000, 
sim time next is 1084200.0000, 
raw observation next is [24.76666666666667, 71.5, 1.0, 2.0, 0.4523465566367458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703886.8113209782, 703886.8113209788, 184994.2462644377], 
processed observation next is [1.0, 0.5652173913043478, 0.3728278041074251, 0.715, 1.0, 1.0, 0.3401765742611395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19552411425582727, 0.19552411425582744, 0.27611081532005627], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.82452965], dtype=float32), -1.174825]. 
=============================================
[2019-03-27 04:23:09,080] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:09,088] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5722
[2019-03-27 04:23:09,093] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 89.0, 1.0, 2.0, 0.3856454651342247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586264.3113747791, 586264.3113747791, 173589.0443797303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1233600.0000, 
sim time next is 1234200.0000, 
raw observation next is [23.25, 88.5, 1.0, 2.0, 0.3846810340594198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583261.3342351102, 583261.3342351095, 173278.2983729321], 
processed observation next is [1.0, 0.2608695652173913, 0.30094786729857825, 0.885, 1.0, 1.0, 0.25865184826436116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1620170372875306, 0.16201703728753042, 0.2586243259297494], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.4049196], dtype=float32), 0.0035981804]. 
=============================================
[2019-03-27 04:23:11,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:11,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2406
[2019-03-27 04:23:11,210] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 94.83333333333333, 1.0, 2.0, 0.3246202013418588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511649.2636062397, 511649.2636062391, 167874.5646775391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1371000.0000, 
sim time next is 1371600.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3227843354490936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508922.2850250268, 508922.2850250274, 167669.4591381972], 
processed observation next is [1.0, 0.9130434782608695, 0.19431279620853087, 0.95, 1.0, 1.0, 0.18407751258926935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14136730139584078, 0.14136730139584094, 0.2502529240868615], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.90369123], dtype=float32), -0.23048298]. 
=============================================
[2019-03-27 04:23:14,848] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:14,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3746
[2019-03-27 04:23:14,860] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 85.0, 1.0, 2.0, 0.6278567526424211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958441.4162134654, 958441.4162134654, 216058.5645501286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1586400.0000, 
sim time next is 1587000.0000, 
raw observation next is [23.48333333333333, 85.0, 1.0, 2.0, 0.6512030949133567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 993527.5107505558, 993527.5107505564, 221069.339980343], 
processed observation next is [1.0, 0.34782608695652173, 0.3120063191153238, 0.85, 1.0, 1.0, 0.5797627649558514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2759798640973766, 0.2759798640973768, 0.3299542387766314], 
reward next is 0.6700, 
noisyNet noise sample is [array([-2.0006654], dtype=float32), 0.3354919]. 
=============================================
[2019-03-27 04:23:14,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.097046]
 [76.437515]
 [76.92433 ]
 [77.64972 ]
 [77.462524]], R is [[76.01678467]
 [75.93413544]
 [75.87319183]
 [75.83344269]
 [75.81425476]].
[2019-03-27 04:23:20,291] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:20,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6691
[2019-03-27 04:23:20,302] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 96.33333333333334, 1.0, 2.0, 0.3211144020052021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504130.2420193978, 504130.2420193972, 167255.4211590555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [21.28333333333333, 95.66666666666667, 1.0, 2.0, 0.3255316276889377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509261.1217800908, 509261.1217800908, 167600.9652240079], 
processed observation next is [0.0, 0.21739130434782608, 0.20774091627172192, 0.9566666666666667, 1.0, 1.0, 0.18738750323968395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14146142271669188, 0.14146142271669188, 0.2501506943641909], 
reward next is 0.7498, 
noisyNet noise sample is [array([2.0383852], dtype=float32), 0.86229885]. 
=============================================
[2019-03-27 04:23:22,254] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 04:23:22,255] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:23:22,256] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:23:22,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,257] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,258] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:23:22,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:23:22,259] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:23:22,261] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,262] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,262] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,290] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-03-27 04:23:22,291] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-03-27 04:23:22,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-03-27 04:23:22,341] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-03-27 04:23:22,342] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-03-27 04:23:52,212] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.086836696]
[2019-03-27 04:23:52,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.09779958333333, 80.29434389, 1.0, 2.0, 0.7116761124272496, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005976409447884, 6.9112, 168.9116092681413, 1891467.943328253, 1824230.832798159, 387200.7563238172]
[2019-03-27 04:23:52,410] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:23:52,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.683404e-37 0.000000e+00], sampled 0.4784291530196101
[2019-03-27 04:23:52,413] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1891467.943328253 W.
[2019-03-27 04:23:59,390] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.086836696]
[2019-03-27 04:23:59,391] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.4, 86.0, 1.0, 2.0, 0.5078105219803178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709589.9522173873, 709589.9522173873, 184828.3220821704]
[2019-03-27 04:23:59,392] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:23:59,395] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1625578575441683
[2019-03-27 04:24:20,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.086836696]
[2019-03-27 04:24:20,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 75.0, 1.0, 2.0, 0.6000366209014909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838513.12550312, 838513.12550312, 200752.8255633044]
[2019-03-27 04:24:20,908] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:24:20,911] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11254862341297878
[2019-03-27 04:24:34,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.086836696]
[2019-03-27 04:24:34,557] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.17033841, 67.49979331, 1.0, 2.0, 0.6031233107446364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 842828.2902376532, 842828.2902376525, 201328.5722983457]
[2019-03-27 04:24:34,558] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:24:34,560] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4402858411530516
[2019-03-27 04:25:05,337] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.086836696]
[2019-03-27 04:25:05,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 83.0, 1.0, 2.0, 0.7979735061402288, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981401670416, 6.9112, 168.9123160051069, 2012237.267249692, 1944996.333754179, 407330.2221671297]
[2019-03-27 04:25:05,342] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:25:05,344] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.368669e-38 0.000000e+00], sampled 0.8354109214410277
[2019-03-27 04:25:05,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2012237.267249692 W.
[2019-03-27 04:25:15,702] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:25:16,131] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:25:16,170] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:25:16,186] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:25:16,240] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:25:17,260] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2375000, evaluation results [2375000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:25:18,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:18,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3830
[2019-03-27 04:25:18,703] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 80.0, 1.0, 2.0, 0.4123818573780608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606327.0677499198, 606327.0677499193, 174866.3882194718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [25.03333333333333, 81.33333333333334, 1.0, 2.0, 0.4090937364745168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602988.650792751, 602988.6507927517, 174598.3464678014], 
processed observation next is [0.0, 0.782608695652174, 0.38546603475513425, 0.8133333333333335, 1.0, 1.0, 0.2880647427403817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16749684744243082, 0.167496847442431, 0.26059454696686774], 
reward next is 0.7394, 
noisyNet noise sample is [array([2.037481], dtype=float32), -0.059588876]. 
=============================================
[2019-03-27 04:25:20,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:20,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2362
[2019-03-27 04:25:20,423] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 87.33333333333334, 1.0, 2.0, 0.3288193058236363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512919.0675145621, 512919.0675145627, 167843.0005517647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1578000.0000, 
sim time next is 1578600.0000, 
raw observation next is [22.5, 87.0, 1.0, 2.0, 0.3286454606919901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511899.3823875451, 511899.3823875444, 167742.5529905303], 
processed observation next is [1.0, 0.2608695652173913, 0.2654028436018958, 0.87, 1.0, 1.0, 0.19113910926745792, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14219427288542918, 0.142194272885429, 0.25036201938885116], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.47046095], dtype=float32), -0.98965746]. 
=============================================
[2019-03-27 04:25:22,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.619455e-31 0.000000e+00], sum to 1.0000
[2019-03-27 04:25:22,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0004
[2019-03-27 04:25:22,233] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 87.66666666666666, 1.0, 2.0, 0.7740902642945382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1154497.710712508, 1154497.710712507, 247411.900702819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1608000.0000, 
sim time next is 1608600.0000, 
raw observation next is [23.76666666666667, 88.33333333333334, 1.0, 2.0, 0.7809257562176644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163778.242535404, 1163778.242535404, 249046.0578140412], 
processed observation next is [1.0, 0.6086956521739131, 0.32543443917851517, 0.8833333333333334, 1.0, 1.0, 0.7360551279730896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3232717340376122, 0.3232717340376122, 0.3717105340508078], 
reward next is 0.6283, 
noisyNet noise sample is [array([0.06004097], dtype=float32), -1.0847756]. 
=============================================
[2019-03-27 04:25:22,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:22,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9940
[2019-03-27 04:25:22,367] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 51.0, 1.0, 2.0, 0.3480171406727051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534257.5264887547, 534257.5264887554, 169288.3780549595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1512000.0000, 
sim time next is 1512600.0000, 
raw observation next is [29.16666666666667, 51.0, 1.0, 2.0, 0.3501266927854602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536672.4447150303, 536672.4447150296, 169459.796027057], 
processed observation next is [0.0, 0.5217391304347826, 0.581358609794629, 0.51, 1.0, 1.0, 0.2170201117897111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1490756790875084, 0.1490756790875082, 0.25292506869709996], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.69721264], dtype=float32), 0.055967752]. 
=============================================
[2019-03-27 04:25:22,687] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6724508e-37 0.0000000e+00], sum to 1.0000
[2019-03-27 04:25:22,696] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2472
[2019-03-27 04:25:22,701] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 85.5, 1.0, 2.0, 0.3115334276579558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491038.7222584822, 491038.7222584822, 166324.0711696194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [22.13333333333333, 86.0, 1.0, 2.0, 0.3135263268917957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494203.9631851632, 494203.9631851626, 166558.9988818591], 
processed observation next is [1.0, 0.7391304347826086, 0.24802527646129527, 0.86, 1.0, 1.0, 0.17292328541180205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13727887866254534, 0.13727887866254515, 0.24859552071919266], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.48359358], dtype=float32), -1.0974044]. 
=============================================
[2019-03-27 04:25:22,926] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:22,933] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0007
[2019-03-27 04:25:22,940] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.96666666666667, 85.16666666666667, 1.0, 2.0, 0.5477983218924735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846861.7074088061, 846861.7074088061, 201136.9342255705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1774200.0000, 
sim time next is 1774800.0000, 
raw observation next is [22.9, 85.0, 1.0, 2.0, 0.5627371642723477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872093.8119475187, 872093.8119475187, 204226.336034002], 
processed observation next is [1.0, 0.5652173913043478, 0.2843601895734597, 0.85, 1.0, 1.0, 0.47317730635222605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24224828109653299, 0.24224828109653299, 0.3048154269164209], 
reward next is 0.6952, 
noisyNet noise sample is [array([-0.6383273], dtype=float32), -0.17592742]. 
=============================================
[2019-03-27 04:25:33,711] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 9.00093e-35 0.00000e+00], sum to 1.0000
[2019-03-27 04:25:33,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3339
[2019-03-27 04:25:33,728] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 89.0, 1.0, 2.0, 0.3197827404127536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504608.5482846917, 504608.5482846923, 167350.5550932643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1796400.0000, 
sim time next is 1797000.0000, 
raw observation next is [21.63333333333333, 89.33333333333334, 1.0, 2.0, 0.3198937329153505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505013.0015271572, 505013.0015271578, 167385.8123236103], 
processed observation next is [1.0, 0.8260869565217391, 0.2243285939968403, 0.8933333333333334, 1.0, 1.0, 0.18059485893415722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14028138931309922, 0.14028138931309939, 0.24982957063225417], 
reward next is 0.7502, 
noisyNet noise sample is [array([0.7552744], dtype=float32), 0.2402846]. 
=============================================
[2019-03-27 04:25:33,759] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.91074]
 [70.7515 ]
 [70.83004]
 [70.87643]
 [70.82563]], R is [[70.95651245]
 [70.99716949]
 [71.03749847]
 [71.07772827]
 [71.11724854]].
[2019-03-27 04:25:38,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.629247e-36 0.000000e+00], sum to 1.0000
[2019-03-27 04:25:38,724] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1523
[2019-03-27 04:25:38,732] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 89.0, 1.0, 2.0, 0.6847610812767095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080473.910355778, 1080473.910355779, 232388.5543588489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1787400.0000, 
sim time next is 1788000.0000, 
raw observation next is [21.93333333333333, 87.33333333333334, 1.0, 2.0, 0.686824532390248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083132.043582678, 1083132.043582678, 232826.8951584674], 
processed observation next is [1.0, 0.6956521739130435, 0.23854660347551332, 0.8733333333333334, 1.0, 1.0, 0.6226801595063228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3008700121062995, 0.3008700121062995, 0.3475028285947275], 
reward next is 0.6525, 
noisyNet noise sample is [array([1.9253298], dtype=float32), 0.110787064]. 
=============================================
[2019-03-27 04:25:38,747] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.94483 ]
 [75.09161 ]
 [74.799576]
 [75.26125 ]
 [75.41603 ]], R is [[74.99263   ]
 [74.89585876]
 [74.81491852]
 [74.72663116]
 [74.66593933]].
[2019-03-27 04:25:40,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:40,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3493
[2019-03-27 04:25:40,239] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 76.66666666666666, 1.0, 2.0, 0.5675127653699574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793046.1325086608, 793046.1325086608, 194850.2080356091], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2122800.0000, 
sim time next is 2123400.0000, 
raw observation next is [30.0, 76.83333333333334, 1.0, 2.0, 0.5684791045238861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 794397.0062718627, 794397.006271862, 195021.0877695475], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.7683333333333334, 1.0, 1.0, 0.4800953066552844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2206658350755174, 0.2206658350755172, 0.2910762504023097], 
reward next is 0.7089, 
noisyNet noise sample is [array([1.1510301], dtype=float32), -2.1798606]. 
=============================================
[2019-03-27 04:25:42,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.418594e-36 0.000000e+00], sum to 1.0000
[2019-03-27 04:25:42,886] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6744
[2019-03-27 04:25:42,894] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 74.83333333333333, 1.0, 2.0, 0.9468065251502062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331721.333333103, 1331721.333333103, 284404.5735693309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1948200.0000, 
sim time next is 1948800.0000, 
raw observation next is [27.33333333333334, 74.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.740145373015009, 6.9112, 168.9087942460629, 2048836.053820078, 1460768.167322442, 312411.6116267493], 
processed observation next is [1.0, 0.5652173913043478, 0.4944707740916275, 0.7466666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08289453730150091, 0.0, 0.8294195065287625, 0.5691211260611327, 0.405768935367345, 0.46628598750261085], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7267715], dtype=float32), 1.158222]. 
=============================================
[2019-03-27 04:25:45,119] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:45,125] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7434
[2019-03-27 04:25:45,130] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 85.0, 1.0, 2.0, 0.5087270310955853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710871.0664116039, 710871.0664116046, 184974.9145333862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2044800.0000, 
sim time next is 2045400.0000, 
raw observation next is [26.83333333333334, 85.16666666666667, 1.0, 2.0, 0.5077811021742075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709548.8286681351, 709548.8286681351, 184824.3547853149], 
processed observation next is [0.0, 0.6956521739130435, 0.4707740916271725, 0.8516666666666667, 1.0, 1.0, 0.4069651833424186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19709689685225976, 0.19709689685225976, 0.2758572459482312], 
reward next is 0.7241, 
noisyNet noise sample is [array([-1.0527085], dtype=float32), 0.7172332]. 
=============================================
[2019-03-27 04:25:46,752] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:46,760] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2496
[2019-03-27 04:25:46,764] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 96.16666666666666, 1.0, 2.0, 0.6231774175942596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 870864.1788365295, 870864.1788365302, 205134.0757047103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [24.7, 96.33333333333333, 1.0, 2.0, 0.5916131441029758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826737.2696399585, 826737.2696399585, 199186.5375727023], 
processed observation next is [1.0, 0.17391304347826086, 0.3696682464454976, 0.9633333333333333, 1.0, 1.0, 0.5079676434975612, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22964924156665514, 0.22964924156665514, 0.2972933396607497], 
reward next is 0.7027, 
noisyNet noise sample is [array([0.53510106], dtype=float32), 1.2884525]. 
=============================================
[2019-03-27 04:26:01,075] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9444865e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 04:26:01,085] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3331
[2019-03-27 04:26:01,090] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 94.66666666666666, 1.0, 2.0, 0.8384846329673163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1171912.958737343, 1171912.958737343, 253573.8587463729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2169600.0000, 
sim time next is 2170200.0000, 
raw observation next is [25.05, 94.83333333333333, 1.0, 2.0, 0.7996795206832504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117648.222401079, 1117648.222401079, 243829.3396713943], 
processed observation next is [1.0, 0.08695652173913043, 0.3862559241706162, 0.9483333333333333, 1.0, 1.0, 0.7586500249195788, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31045783955585526, 0.31045783955585526, 0.3639243875692452], 
reward next is 0.6361, 
noisyNet noise sample is [array([-0.45046374], dtype=float32), 1.2688316]. 
=============================================
[2019-03-27 04:26:03,006] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7100558e-22 1.0000000e+00 9.4882835e-29 5.8467085e-11 8.3254485e-21], sum to 1.0000
[2019-03-27 04:26:03,015] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6757
[2019-03-27 04:26:03,023] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2857258.697638716 W.
[2019-03-27 04:26:03,027] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.9, 65.66666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.718980842651863, 6.9112, 168.9084500448098, 2857258.697638716, 2284206.455184205, 474257.1484152034], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2209200.0000, 
sim time next is 2209800.0000, 
raw observation next is [32.0, 65.33333333333334, 1.0, 2.0, 0.6153097529280923, 1.0, 1.0, 0.6153097529280923, 1.0, 2.0, 1.03, 6.954581617480246, 6.9112, 170.5573041426782, 2581592.942965215, 2550516.92571274, 493596.3107791411], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.6533333333333334, 1.0, 1.0, 0.5365177746121594, 1.0, 0.5, 0.5365177746121594, 1.0, 1.0, 1.0365853658536586, 0.004338161748024571, 0.0, 0.8375144448122397, 0.7171091508236709, 0.7084769238090944, 0.7367109116106584], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11132592], dtype=float32), 0.6385096]. 
=============================================
[2019-03-27 04:26:07,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:26:07,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5354
[2019-03-27 04:26:07,647] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.0, 1.0, 2.0, 0.3118763279392696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495249.3745411828, 495249.3745411834, 166709.224319104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.3094566324914813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491993.0004156154, 491993.0004156154, 166478.719867586], 
processed observation next is [1.0, 0.7391304347826086, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16802003914636301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13666472233767094, 0.13666472233767094, 0.24847570129490448], 
reward next is 0.7515, 
noisyNet noise sample is [array([-0.6620063], dtype=float32), -0.30936816]. 
=============================================
[2019-03-27 04:26:11,369] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:26:11,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9359
[2019-03-27 04:26:11,382] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.540977909640847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807130.6990653153, 807130.6990653153, 196616.662729004], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3070200.0000, 
sim time next is 3070800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.4581022426458296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683473.5253452257, 683473.5253452263, 182696.7856311444], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.94, 1.0, 1.0, 0.347111135717867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18985375704034047, 0.18985375704034063, 0.272681769598723], 
reward next is 0.7273, 
noisyNet noise sample is [array([-2.8997917], dtype=float32), 1.9064687]. 
=============================================
[2019-03-27 04:26:13,145] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 04:26:13,147] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:26:13,148] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:26:13,149] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:26:13,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:13,151] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:26:13,150] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:13,151] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:26:13,152] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:13,156] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:13,153] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:13,179] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-03-27 04:26:13,200] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-03-27 04:26:13,200] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-03-27 04:26:13,244] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-03-27 04:26:13,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-03-27 04:26:38,891] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:26:38,892] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.11866719333334, 95.54050666, 1.0, 2.0, 0.7526222278093179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1063775.406624269, 1063775.406624269, 234253.0079003109]
[2019-03-27 04:26:38,896] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:26:38,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8156923228877855
[2019-03-27 04:26:50,081] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:26:50,083] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.7352653, 93.46349933, 1.0, 2.0, 0.6147990295916503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859150.9991165745, 859150.9991165745, 203530.7743941281]
[2019-03-27 04:26:50,084] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:26:50,085] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7610119000394558
[2019-03-27 04:26:59,813] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:26:59,814] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.30726423333333, 64.06814789, 1.0, 2.0, 0.6013176165856781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 840303.943859862, 840303.9438598613, 200992.2219971804]
[2019-03-27 04:26:59,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:26:59,818] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5727481920940415
[2019-03-27 04:27:09,669] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:27:09,671] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.18333333333334, 62.5, 1.0, 2.0, 0.5675849289443125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 793147.012020267, 793147.0120202675, 194861.8260446945]
[2019-03-27 04:27:09,672] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:27:09,678] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0491009796096995
[2019-03-27 04:27:18,575] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:27:18,577] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.96666666666667, 46.50000000000001, 1.0, 2.0, 0.6917594254860845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 966748.2648950148, 966748.2648950142, 219080.7233865182]
[2019-03-27 04:27:18,577] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:27:18,579] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9691182820446501
[2019-03-27 04:27:22,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:27:22,126] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 84.0, 1.0, 2.0, 0.9793588156964964, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992255305599, 6.9112, 168.9123159483817, 2266111.277928011, 2198862.644541927, 457011.0687618355]
[2019-03-27 04:27:22,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:27:22,130] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3595341071525877
[2019-03-27 04:27:22,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2266111.277928011 W.
[2019-03-27 04:27:38,994] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:27:38,997] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.53333333333333, 59.66666666666667, 1.0, 2.0, 0.8539520055723991, 1.0, 2.0, 0.8539520055723991, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2388396.082106141, 2388396.082106141, 446721.2465738312]
[2019-03-27 04:27:38,999] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:27:39,004] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0101769e-35 1.0000000e+00 0.0000000e+00 4.2562572e-24 0.0000000e+00], sampled 0.5576358282338056
[2019-03-27 04:27:39,006] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2388396.082106141 W.
[2019-03-27 04:28:03,193] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.08471588]
[2019-03-27 04:28:03,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.08116889333333, 93.95465866833334, 1.0, 2.0, 0.5307321556678909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741630.7196813175, 741630.7196813168, 188551.0661726234]
[2019-03-27 04:28:03,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:28:03,206] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12980888672692892
[2019-03-27 04:28:08,925] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:28:09,083] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:28:09,235] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:28:09,329] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:28:09,427] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:28:10,446] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2400000, evaluation results [2400000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:28:20,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3101745e-38 0.0000000e+00], sum to 1.0000
[2019-03-27 04:28:20,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2537
[2019-03-27 04:28:20,989] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3499797968893803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539133.7468120259, 539133.7468120259, 169745.3673435445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2854200.0000, 
sim time next is 2854800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3494694749731769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538347.7601012872, 538347.7601012879, 169680.8413601649], 
processed observation next is [1.0, 0.043478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2162282831002131, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14954104447257976, 0.14954104447257996, 0.2532549871047237], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.6166503], dtype=float32), 0.3001005]. 
=============================================
[2019-03-27 04:28:21,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:21,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5864
[2019-03-27 04:28:21,300] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.6692399577254918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027449.271806899, 1027449.271806899, 225822.4052598489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2902800.0000, 
sim time next is 2903400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.6907842199617018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1058590.6258932, 1058590.625893201, 230597.951246472], 
processed observation next is [1.0, 0.6086956521739131, 0.2654028436018958, 0.915, 1.0, 1.0, 0.6274508674237371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.294052951637, 0.2940529516370003, 0.34417604663652535], 
reward next is 0.6558, 
noisyNet noise sample is [array([1.198049], dtype=float32), 1.0053056]. 
=============================================
[2019-03-27 04:28:27,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:27,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7458
[2019-03-27 04:28:27,444] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3931210197645402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586595.8043169726, 586595.804316972, 173306.165648713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2742000.0000, 
sim time next is 2742600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3927254813660197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586005.8747645393, 586005.87476454, 173252.3685203411], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2683439534530358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16277940965681648, 0.16277940965681667, 0.2585856246572255], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.53985286], dtype=float32), -1.0777855]. 
=============================================
[2019-03-27 04:28:30,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:30,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7100
[2019-03-27 04:28:30,740] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4755042199235282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664432.5644943232, 664432.5644943238, 179847.056403567], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2700000.0000, 
sim time next is 2700600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4758230458241832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664878.2063941116, 664878.206394111, 179894.6935287791], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.3684615009929918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.184688390665031, 0.18468839066503084, 0.2684995425802673], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.6799291], dtype=float32), -1.452206]. 
=============================================
[2019-03-27 04:28:40,579] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:40,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5204
[2019-03-27 04:28:40,590] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3028372869783388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482251.184449583, 482251.1844495836, 165783.3446633421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.302087770492226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481058.0224425817, 481058.0224425824, 165697.6778390874], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15914189215930843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1336272284562727, 0.1336272284562729, 0.24730996692401105], 
reward next is 0.7527, 
noisyNet noise sample is [array([1.8077952], dtype=float32), 0.918262]. 
=============================================
[2019-03-27 04:28:40,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.07165 ]
 [78.253174]
 [78.41144 ]
 [78.808754]
 [80.964554]], R is [[77.86756897]
 [77.84146118]
 [77.81546783]
 [77.78968048]
 [77.76424408]].
[2019-03-27 04:28:41,810] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:41,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3881
[2019-03-27 04:28:41,825] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.00000000000001, 1.0, 2.0, 0.5573041287044418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778775.2857123173, 778775.285712318, 193062.1271549221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3244800.0000, 
sim time next is 3245400.0000, 
raw observation next is [32.5, 63.0, 1.0, 2.0, 0.5598152607579185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782285.6270513296, 782285.6270513296, 193499.3783222623], 
processed observation next is [0.0, 0.5652173913043478, 0.7393364928909952, 0.63, 1.0, 1.0, 0.469656940672191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21730156306981377, 0.21730156306981377, 0.28880504227203324], 
reward next is 0.7112, 
noisyNet noise sample is [array([1.1779971], dtype=float32), -0.7820379]. 
=============================================
[2019-03-27 04:28:42,923] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:42,929] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0718
[2019-03-27 04:28:42,933] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6798689586545278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1046995.521519257, 1046995.521519256, 228620.2719340856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2910000.0000, 
sim time next is 2910600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6990864653169488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076596.452656342, 1076596.452656341, 233145.4535431424], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.6374535726710227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2990545701823172, 0.29905457018231696, 0.34797828887036175], 
reward next is 0.6520, 
noisyNet noise sample is [array([-0.7224985], dtype=float32), -0.4548587]. 
=============================================
[2019-03-27 04:28:43,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:43,575] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4383
[2019-03-27 04:28:43,580] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.4581022426458296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683473.5253452257, 683473.5253452263, 182696.7856311444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3070800.0000, 
sim time next is 3071400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.5296480638879271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787887.8744350799, 787887.8744350792, 194310.1199839892], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.95, 1.0, 1.0, 0.43331092034690005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2188577428986333, 0.2188577428986331, 0.2900151044537152], 
reward next is 0.7100, 
noisyNet noise sample is [array([-1.1919785], dtype=float32), 0.26549867]. 
=============================================
[2019-03-27 04:28:43,600] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:43,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3355
[2019-03-27 04:28:43,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 92.0, 1.0, 2.0, 0.5709794074358326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884525.0061302553, 884525.0061302553, 205810.0352175441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2971200.0000, 
sim time next is 2971800.0000, 
raw observation next is [22.0, 91.0, 1.0, 2.0, 0.590560166453826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 917938.325498177, 917938.3254981777, 210056.5198575559], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.91, 1.0, 1.0, 0.5066989957275012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25498286819393806, 0.25498286819393823, 0.3135171938172476], 
reward next is 0.6865, 
noisyNet noise sample is [array([-0.6383531], dtype=float32), -0.58842033]. 
=============================================
[2019-03-27 04:28:43,760] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:43,769] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6332
[2019-03-27 04:28:43,774] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 97.0, 1.0, 2.0, 0.8129721434911567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1174031.285721376, 1174031.285721376, 252419.0025796037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [23.66666666666667, 96.0, 1.0, 2.0, 0.8303671613011229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1197061.096569232, 1197061.096569233, 256670.0873613879], 
processed observation next is [1.0, 0.6521739130434783, 0.3206951026856243, 0.96, 1.0, 1.0, 0.7956230859049673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3325169712692311, 0.3325169712692314, 0.38308968262893717], 
reward next is 0.6169, 
noisyNet noise sample is [array([-0.2764998], dtype=float32), 0.41578403]. 
=============================================
[2019-03-27 04:28:44,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:44,205] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9188
[2019-03-27 04:28:44,209] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 90.0, 1.0, 2.0, 0.6884180546139048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1083074.385866981, 1083074.385866981, 232968.1364439665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2981400.0000, 
sim time next is 2982000.0000, 
raw observation next is [21.33333333333334, 92.0, 1.0, 2.0, 0.527824670655018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 832003.0330721037, 832003.0330721043, 198957.6325639436], 
processed observation next is [1.0, 0.5217391304347826, 0.2101105845181678, 0.92, 1.0, 1.0, 0.43111406103014216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23111195363113993, 0.2311119536311401, 0.29695169039394564], 
reward next is 0.7030, 
noisyNet noise sample is [array([0.05798961], dtype=float32), -1.8071293]. 
=============================================
[2019-03-27 04:28:44,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.457565]
 [76.62373 ]
 [76.758224]
 [76.845276]
 [77.08874 ]], R is [[76.96915436]
 [76.85174561]
 [76.7712326 ]
 [76.69971466]
 [76.63175964]].
[2019-03-27 04:28:45,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:45,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6184
[2019-03-27 04:28:45,858] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 80.66666666666667, 1.0, 2.0, 0.530733519724695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741632.6264427055, 741632.6264427049, 188551.2806606337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [28.0, 79.83333333333334, 1.0, 2.0, 0.5266959904319644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735988.7379402305, 735988.7379402305, 187884.2469297457], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.7983333333333335, 1.0, 1.0, 0.4297542053397161, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20444131609450847, 0.20444131609450847, 0.2804242491488742], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.8664797], dtype=float32), 2.0682318]. 
=============================================
[2019-03-27 04:28:47,366] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:47,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0072
[2019-03-27 04:28:47,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5975090512203506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834979.618799215, 834979.618799215, 200283.1510616608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3336600.0000, 
sim time next is 3337200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.595696175992508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832445.2520829113, 832445.2520829113, 199947.2292352715], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5128869590271181, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23123479224525315, 0.23123479224525315, 0.2984287003511515], 
reward next is 0.7016, 
noisyNet noise sample is [array([-0.06803038], dtype=float32), -0.33914936]. 
=============================================
[2019-03-27 04:28:52,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:52,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4792
[2019-03-27 04:28:52,620] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 77.33333333333334, 1.0, 2.0, 0.4801970682299169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670992.0562059153, 670992.0562059153, 180551.6786721637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3226800.0000, 
sim time next is 3227400.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4813187504993471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672559.9086517622, 672559.9086517622, 180721.0391912017], 
processed observation next is [0.0, 0.34782608695652173, 0.5023696682464456, 0.765, 1.0, 1.0, 0.3750828319269242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1868221968477117, 0.1868221968477117, 0.26973289431522646], 
reward next is 0.7303, 
noisyNet noise sample is [array([0.8381727], dtype=float32), -2.3428059]. 
=============================================
[2019-03-27 04:28:58,246] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:58,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0688
[2019-03-27 04:28:58,263] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 76.0, 1.0, 2.0, 0.5450201728930366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761603.5650822597, 761603.5650822604, 190950.6089418575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3879600.0000, 
sim time next is 3880200.0000, 
raw observation next is [29.16666666666667, 77.5, 1.0, 2.0, 0.5483971457951963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766324.2020067868, 766324.2020067868, 191526.43717965], 
processed observation next is [0.0, 0.9130434782608695, 0.581358609794629, 0.775, 1.0, 1.0, 0.45590017565686297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21286783389077413, 0.21286783389077413, 0.2858603539994776], 
reward next is 0.7141, 
noisyNet noise sample is [array([-0.75498044], dtype=float32), 0.529718]. 
=============================================
[2019-03-27 04:28:59,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:59,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7722
[2019-03-27 04:28:59,763] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4861444358929827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679305.1273804711, 679305.1273804705, 181453.6716423996], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3196200.0000, 
sim time next is 3196800.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4865236335153158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679835.1618634638, 679835.1618634638, 181511.5270902766], 
processed observation next is [0.0, 0.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3813537753196576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18884310051762884, 0.18884310051762884, 0.2709127270004128], 
reward next is 0.7291, 
noisyNet noise sample is [array([0.25777048], dtype=float32), 0.83901656]. 
=============================================
[2019-03-27 04:29:02,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7238422e-27 1.0000000e+00 1.4112578e-32 1.8648386e-19 2.9077069e-28], sum to 1.0000
[2019-03-27 04:29:02,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4525
[2019-03-27 04:29:02,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2275033.186854331 W.
[2019-03-27 04:29:02,879] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 70.16666666666667, 1.0, 2.0, 0.9857331830987184, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005992553307724, 6.9112, 168.9123931175183, 2275033.186854331, 2207784.311332835, 458931.7919425416], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3406200.0000, 
sim time next is 3406800.0000, 
raw observation next is [31.33333333333334, 70.33333333333334, 1.0, 2.0, 0.5857772676872246, 1.0, 1.0, 0.5857772676872246, 1.0, 2.0, 1.017301394470078, 6.911199999999999, 6.9112, 170.5573041426782, 2457564.627069551, 2457564.627069551, 479529.456177634], 
processed observation next is [1.0, 0.43478260869565216, 0.6840442338072673, 0.7033333333333335, 1.0, 1.0, 0.5009364670930416, 1.0, 0.5, 0.5009364670930416, 1.0, 1.0, 1.0210992615488756, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6826568408526531, 0.6826568408526531, 0.7157156062352746], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9015955], dtype=float32), -0.28440005]. 
=============================================
[2019-03-27 04:29:06,160] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 04:29:06,162] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:29:06,164] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:29:06,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:06,165] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:29:06,166] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:29:06,166] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:06,168] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:29:06,165] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:06,172] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:06,169] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:06,191] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-03-27 04:29:06,216] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-03-27 04:29:06,247] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-03-27 04:29:06,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-03-27 04:29:06,248] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-03-27 04:29:11,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:29:11,836] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.5, 85.0, 1.0, 2.0, 0.2886586707403454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462513.7222599693, 462513.72225997, 164432.0790236724]
[2019-03-27 04:29:11,839] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:29:11,842] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4596715684155058
[2019-03-27 04:29:15,225] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:29:15,226] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.6, 63.0, 1.0, 2.0, 0.2683829603352175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 443319.9333533203, 443319.933353321, 162719.0182749075]
[2019-03-27 04:29:15,229] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:29:15,234] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5333259595000598
[2019-03-27 04:29:19,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:29:19,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.74097527666667, 74.27585471333333, 1.0, 2.0, 0.3753933057269085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564840.4386233725, 564840.4386233725, 171515.1273704888]
[2019-03-27 04:29:19,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:29:19,018] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8784739229466262
[2019-03-27 04:29:59,837] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:29:59,839] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.80960768666667, 70.417615415, 1.0, 2.0, 0.5468510522420048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764162.9305603411, 764162.9305603411, 191261.0154242766]
[2019-03-27 04:29:59,842] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:29:59,844] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32342252178109676
[2019-03-27 04:30:03,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:30:03,597] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.16666666666667, 69.33333333333334, 1.0, 2.0, 0.8177711580701801, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987195318508176, 6.9112, 168.9124414085915, 2039945.130205557, 1986031.600109231, 412969.0134932424]
[2019-03-27 04:30:03,600] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:30:03,602] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7305856e-34 1.0000000e+00 0.0000000e+00 4.0629586e-25 0.0000000e+00], sampled 0.5351763322532831
[2019-03-27 04:30:03,605] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2039945.130205557 W.
[2019-03-27 04:30:06,920] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:30:06,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.73333333333333, 63.0, 1.0, 2.0, 0.592517336961457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828001.3073217467, 828001.3073217467, 199359.390208793]
[2019-03-27 04:30:06,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:30:06,924] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7045483241918935
[2019-03-27 04:30:12,840] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:30:12,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.71364716, 83.82615526, 1.0, 2.0, 0.8532267689721371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1192528.96227043, 1192528.962270429, 257398.0228057353]
[2019-03-27 04:30:12,844] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:30:12,849] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08085535932200105
[2019-03-27 04:30:42,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:30:42,514] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 70.66666666666667, 1.0, 2.0, 0.9015718479068409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1260139.499035446, 1260139.499035446, 270371.6866639307]
[2019-03-27 04:30:42,517] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:30:42,521] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3968737952373639
[2019-03-27 04:30:58,151] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:30:58,153] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.73333333333333, 73.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.720829050891753, 6.9112, 168.9085061297949, 2028511.949891668, 1454148.369168947, 311346.7399073906]
[2019-03-27 04:30:58,154] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:30:58,157] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.631273e-36 0.000000e+00], sampled 0.39498731348666205
[2019-03-27 04:30:58,158] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2028511.949891668 W.
[2019-03-27 04:30:59,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07798323], dtype=float32), 0.07314012]
[2019-03-27 04:30:59,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.3, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.044748335347718, 6.9112, 168.9119790668083, 2390468.727222177, 2295725.489998601, 476374.5193744649]
[2019-03-27 04:30:59,520] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:30:59,524] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.512612e-36 1.000000e+00 0.000000e+00 1.378571e-25 0.000000e+00], sampled 0.3612101612788309
[2019-03-27 04:30:59,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2390468.727222177 W.
[2019-03-27 04:31:02,078] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:31:02,205] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:31:02,456] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:31:02,486] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:31:02,544] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:31:03,565] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2425000, evaluation results [2425000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:31:05,664] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:05,679] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8446
[2019-03-27 04:31:05,685] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.553952466700862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774089.9756430094, 774089.9756430094, 192480.7351894168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3352800.0000, 
sim time next is 3353400.0000, 
raw observation next is [28.5, 81.5, 1.0, 2.0, 0.5522574443033643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771720.5007004391, 771720.5007004397, 192188.5110260873], 
processed observation next is [0.0, 0.8260869565217391, 0.5497630331753555, 0.815, 1.0, 1.0, 0.46055113771489675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21436680575012196, 0.21436680575012212, 0.2868485239195333], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.89243186], dtype=float32), 1.3948286]. 
=============================================
[2019-03-27 04:31:05,821] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:05,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8223
[2019-03-27 04:31:05,840] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163119925786053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 186191.4310685439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3444000.0000, 
sim time next is 3444600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5171221407009972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722605.9851274666, 722605.9851274659, 186322.3274356583], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4182194466277074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20072388475762962, 0.20072388475762942, 0.2780930260233706], 
reward next is 0.7219, 
noisyNet noise sample is [array([-1.6564538], dtype=float32), -0.9845412]. 
=============================================
[2019-03-27 04:31:08,934] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6008521e-21 1.0000000e+00 6.7313616e-28 2.7957796e-14 1.6471107e-24], sum to 1.0000
[2019-03-27 04:31:08,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5309
[2019-03-27 04:31:08,955] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2098485.801054772 W.
[2019-03-27 04:31:08,960] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.5, 71.0, 1.0, 2.0, 0.5002639853098012, 1.0, 2.0, 0.5002639853098012, 1.0, 2.0, 0.8687931026551139, 6.911199999999999, 6.9112, 170.5573041426782, 2098485.801054772, 2098485.801054773, 415331.3602785895], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4105800.0000, 
sim time next is 4106400.0000, 
raw observation next is [33.66666666666666, 71.0, 1.0, 2.0, 0.7907904067587558, 1.0, 2.0, 0.7907904067587558, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2211565.621330468, 2211565.621330468, 415434.2388624112], 
processed observation next is [1.0, 0.5217391304347826, 0.7946287519747232, 0.71, 1.0, 1.0, 0.7479402491069347, 1.0, 1.0, 0.7479402491069347, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6143237837029077, 0.6143237837029077, 0.6200511027797182], 
reward next is 0.3799, 
noisyNet noise sample is [array([-0.32750806], dtype=float32), -0.029468507]. 
=============================================
[2019-03-27 04:31:16,763] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:16,770] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3006
[2019-03-27 04:31:16,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5347241616499112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747211.0025785528, 747211.0025785528, 189215.616815302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3813000.0000, 
sim time next is 3813600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5348235321367516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747349.9094147426, 747349.9094147426, 189232.2036326701], 
processed observation next is [0.0, 0.13043478260869565, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4395464242611465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2075971970596507, 0.2075971970596507, 0.28243612482488073], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.96284986], dtype=float32), 1.5238087]. 
=============================================
[2019-03-27 04:31:17,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6283946e-30 0.0000000e+00], sum to 1.0000
[2019-03-27 04:31:17,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8731
[2019-03-27 04:31:17,475] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.83333333333334, 1.0, 2.0, 0.4995048238479894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697980.1569303558, 697980.1569303565, 183518.2135709811], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3545400.0000, 
sim time next is 3546000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.494663753188603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691213.3141815951, 691213.3141815951, 182763.8815167615], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.74, 1.0, 1.0, 0.3911611484200036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19200369838377643, 0.19200369838377643, 0.2727819127115843], 
reward next is 0.7272, 
noisyNet noise sample is [array([-0.02932584], dtype=float32), 1.0964837]. 
=============================================
[2019-03-27 04:31:17,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.23127 ]
 [65.24635 ]
 [65.34684 ]
 [65.459526]
 [65.87543 ]], R is [[65.55149078]
 [65.62206268]
 [65.69087219]
 [65.75808716]
 [65.82382202]].
[2019-03-27 04:31:29,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6539640e-08 5.7921690e-01 2.6937324e-12 4.2078060e-01 2.5573286e-06], sum to 1.0000
[2019-03-27 04:31:29,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6050
[2019-03-27 04:31:29,628] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.7444888595652626, 1.0, 2.0, 0.7444888595652626, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2081950.464492365, 2081950.464492366, 393745.4943431252], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4008600.0000, 
sim time next is 4009200.0000, 
raw observation next is [28.33333333333334, 77.33333333333333, 1.0, 2.0, 0.7605055775104496, 1.0, 2.0, 0.7605055775104496, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2126785.408077593, 2126785.408077593, 401096.2788347309], 
processed observation next is [1.0, 0.391304347826087, 0.5418641390205374, 0.7733333333333333, 1.0, 1.0, 0.711452503024638, 1.0, 1.0, 0.711452503024638, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5907737244659981, 0.5907737244659981, 0.5986511624398969], 
reward next is 0.4013, 
noisyNet noise sample is [array([0.03090452], dtype=float32), -1.2581908]. 
=============================================
[2019-03-27 04:31:34,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2621198e-36 0.0000000e+00], sum to 1.0000
[2019-03-27 04:31:34,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5071
[2019-03-27 04:31:34,914] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 82.5, 1.0, 2.0, 0.5867467770759431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 819934.2431858237, 819934.2431858242, 198302.7192295657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5822518676951293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813650.5414273574, 813650.5414273574, 197485.9854785991], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49668899722304727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22601403928537706, 0.22601403928537706, 0.2947552022068643], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.23143226], dtype=float32), -0.25605175]. 
=============================================
[2019-03-27 04:31:42,015] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.304512e-37 0.000000e+00], sum to 1.0000
[2019-03-27 04:31:42,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5259
[2019-03-27 04:31:42,033] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.8447759565364706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1180710.952941077, 1180710.952941077, 255199.7918672434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4252800.0000, 
sim time next is 4253400.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.8495957953195709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1187451.223647226, 1187451.223647226, 256450.2272039607], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.79, 1.0, 1.0, 0.8187901148428565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3298475621242295, 0.3298475621242295, 0.38276153314023986], 
reward next is 0.6172, 
noisyNet noise sample is [array([-0.58051914], dtype=float32), -1.8401104]. 
=============================================
[2019-03-27 04:31:46,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4696568e-08 2.5521570e-01 1.4137534e-12 7.4478006e-01 4.2458610e-06], sum to 1.0000
[2019-03-27 04:31:46,254] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5110
[2019-03-27 04:31:46,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3389897.77071959 W.
[2019-03-27 04:31:46,270] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.581284649640608, 6.9112, 170.5573041426782, 3389897.77071959, 2909888.878881199, 549949.3347921111], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4026600.0000, 
sim time next is 4027200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.8527937160796475, 1.0, 2.0, 0.7469868975540863, 1.0, 1.0, 1.03, 7.005109782892559, 6.9112, 170.5573041426782, 3134750.632755838, 3067479.235264167, 573933.0027535256], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.6, 1.0, 1.0, 0.822643031421262, 1.0, 1.0, 0.6951649368121522, 1.0, 0.5, 1.0365853658536586, 0.009390978289255881, 0.0, 0.8375144448122397, 0.8707640646543994, 0.8520775653511575, 0.8566164220201875], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.214557], dtype=float32), -0.3953803]. 
=============================================
[2019-03-27 04:31:56,834] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:56,845] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7751
[2019-03-27 04:31:56,852] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.33333333333333, 58.66666666666667, 1.0, 2.0, 0.5831943954864909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 814968.154129363, 814968.1541293636, 197658.2819980008], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4214400.0000, 
sim time next is 4215000.0000, 
raw observation next is [34.16666666666667, 59.33333333333333, 1.0, 2.0, 0.5857031586283181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818475.3034357281, 818475.3034357287, 198113.9034681573], 
processed observation next is [1.0, 0.782608695652174, 0.8183254344391787, 0.5933333333333333, 1.0, 1.0, 0.5008471790702628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2273542509543689, 0.22735425095436906, 0.2956923932360557], 
reward next is 0.7043, 
noisyNet noise sample is [array([-1.462361], dtype=float32), 0.74896353]. 
=============================================
[2019-03-27 04:31:56,879] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[72.35125 ]
 [72.615074]
 [73.042114]
 [72.96024 ]
 [73.89659 ]], R is [[71.29568481]
 [71.2877121 ]
 [71.279953  ]
 [71.27198029]
 [71.26161957]].
[2019-03-27 04:31:58,711] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3691218e-28 0.0000000e+00], sum to 1.0000
[2019-03-27 04:31:58,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8591
[2019-03-27 04:31:58,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6230108680228896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870631.3374349982, 870631.3374349982, 205111.4622171202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4321200.0000, 
sim time next is 4321800.0000, 
raw observation next is [30.5, 81.5, 1.0, 2.0, 0.6211749877591769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868064.7239830779, 868064.7239830779, 204757.4684565242], 
processed observation next is [1.0, 0.0, 0.6445497630331753, 0.815, 1.0, 1.0, 0.543584322601418, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24112908999529942, 0.24112908999529942, 0.30560816187540923], 
reward next is 0.6944, 
noisyNet noise sample is [array([0.7980453], dtype=float32), -0.045648694]. 
=============================================
[2019-03-27 04:31:59,371] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 04:31:59,373] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:31:59,373] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:31:59,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:31:59,375] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:31:59,376] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:31:59,375] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:31:59,378] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:31:59,380] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:31:59,379] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:31:59,380] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:31:59,403] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-03-27 04:31:59,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-03-27 04:31:59,448] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-03-27 04:31:59,449] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-03-27 04:31:59,494] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-03-27 04:32:14,171] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:32:14,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.3, 90.0, 1.0, 2.0, 0.3094045338574712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491526.6299693136, 491526.629969313, 166438.5873973051]
[2019-03-27 04:32:14,173] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:32:14,177] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5257087002274612
[2019-03-27 04:32:21,759] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:32:21,760] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.86666666666667, 86.0, 1.0, 2.0, 0.3176535856692739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503945.6590247849, 503945.6590247849, 167349.7154552081]
[2019-03-27 04:32:21,762] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:32:21,765] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8669336991844542
[2019-03-27 04:32:49,798] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:32:49,800] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.55468809166667, 89.77651039166666, 1.0, 2.0, 0.7323670276944146, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.999197786731822, 6.9112, 168.9123629481475, 1920422.690659843, 1857994.251477311, 392082.6999543367]
[2019-03-27 04:32:49,801] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:32:49,806] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8100556e-29 0.0000000e+00], sampled 0.6439312679600493
[2019-03-27 04:32:49,808] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1920422.690659843 W.
[2019-03-27 04:33:15,055] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:33:15,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.43333333333334, 55.66666666666667, 1.0, 2.0, 0.8333501840031375, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983611246062, 6.9112, 168.9123159906965, 2061749.31391644, 1994506.812883596, 416251.2210871734]
[2019-03-27 04:33:15,062] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:33:15,066] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2194778e-37 1.0000000e+00 0.0000000e+00 2.3254658e-25 0.0000000e+00], sampled 0.6170210284524816
[2019-03-27 04:33:15,068] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2061749.31391644 W.
[2019-03-27 04:33:17,920] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:33:17,921] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.58333333333333, 67.16666666666667, 1.0, 2.0, 0.5756468350030285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804417.039957859, 804417.039957859, 196296.2475110228]
[2019-03-27 04:33:17,922] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:33:17,926] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04816183974649069
[2019-03-27 04:33:27,796] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:33:27,798] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.93409546, 98.51854564333334, 1.0, 2.0, 0.6974749525187568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 974739.4991036709, 974739.4991036709, 220299.2639251611]
[2019-03-27 04:33:27,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:33:27,801] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12632444611460736
[2019-03-27 04:33:46,613] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.06889508]
[2019-03-27 04:33:46,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.58333333333334, 61.33333333333334, 1.0, 2.0, 0.950936085372047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1329179.77071945, 1329179.77071945, 284337.7248646286]
[2019-03-27 04:33:46,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:33:46,616] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34188783747475604
[2019-03-27 04:33:54,919] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:33:55,707] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:33:55,748] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:33:55,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:33:55,804] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:33:56,821] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2450000, evaluation results [2450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:34:03,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:34:03,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0556
[2019-03-27 04:34:03,190] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 73.0, 1.0, 2.0, 0.6370113381570961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890204.6016731247, 890204.6016731247, 207844.546868255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4447800.0000, 
sim time next is 4448400.0000, 
raw observation next is [32.66666666666667, 72.33333333333333, 1.0, 2.0, 0.6385049556519021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 892292.7649808782, 892292.7649808788, 208139.5210176102], 
processed observation next is [0.0, 0.4782608695652174, 0.7472353870458138, 0.7233333333333333, 1.0, 1.0, 0.5644638019902435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24785910138357728, 0.24785910138357745, 0.3106560015188212], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.42704195], dtype=float32), -0.26306307]. 
=============================================
[2019-03-27 04:34:10,270] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:34:10,280] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4063
[2019-03-27 04:34:10,287] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.551201893325089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770244.9467074851, 770244.9467074844, 192006.7182166297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4479600.0000, 
sim time next is 4480200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5509894483882853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769947.97032294, 769947.97032294, 191970.2304227675], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45902343179311483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21387443620081667, 0.21387443620081667, 0.28652273197427985], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.53943014], dtype=float32), -0.43692893]. 
=============================================
[2019-03-27 04:34:12,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3726333e-29 0.0000000e+00], sum to 1.0000
[2019-03-27 04:34:12,593] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0352
[2019-03-27 04:34:12,599] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.5883744232739123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822209.6359954597, 822209.6359954597, 198592.6564126737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4773000.0000, 
sim time next is 4773600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6414176210425633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896364.8508104136, 896364.8508104136, 208705.0979931641], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5679730374006786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.248990236336226, 0.248990236336226, 0.31150014625845385], 
reward next is 0.6885, 
noisyNet noise sample is [array([-0.07077183], dtype=float32), -0.2372095]. 
=============================================
[2019-03-27 04:34:17,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7417497e-29 1.0000000e+00 1.4457301e-37 1.0111631e-14 7.3972762e-34], sum to 1.0000
[2019-03-27 04:34:17,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5142
[2019-03-27 04:34:17,017] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.723536433732406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1011178.44088567, 1011178.440885671, 226005.2025475325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4935000.0000, 
sim time next is 4935600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7242456710397478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1012170.107943948, 1012170.107943947, 226163.3074419636], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.6677658687225877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28115836331776334, 0.28115836331776306, 0.33755717528651286], 
reward next is 0.6624, 
noisyNet noise sample is [array([-0.3578908], dtype=float32), 0.39030367]. 
=============================================
[2019-03-27 04:34:19,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0212346e-18 4.1980080e-08 2.4690972e-26 1.0000000e+00 2.0618672e-11], sum to 1.0000
[2019-03-27 04:34:19,230] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7964
[2019-03-27 04:34:19,234] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.7386637111700899, 1.0, 2.0, 0.7386637111700899, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2065644.83012435, 2065644.83012435, 391128.7655913382], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4624200.0000, 
sim time next is 4624800.0000, 
raw observation next is [33.33333333333333, 65.66666666666667, 1.0, 2.0, 0.8465632435963074, 1.0, 2.0, 0.8465632435963074, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2367690.763618644, 2367690.763618644, 443187.7647310605], 
processed observation next is [1.0, 0.5217391304347826, 0.7788309636650866, 0.6566666666666667, 1.0, 1.0, 0.8151364380678402, 1.0, 1.0, 0.8151364380678402, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6576918787829567, 0.6576918787829567, 0.6614742757180008], 
reward next is 0.3385, 
noisyNet noise sample is [array([-1.7387816], dtype=float32), -0.80584985]. 
=============================================
[2019-03-27 04:34:20,852] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3486924e-33 1.0000000e+00 0.0000000e+00 1.2799745e-10 9.3328585e-34], sum to 1.0000
[2019-03-27 04:34:20,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1540
[2019-03-27 04:34:20,866] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4895930152108962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684125.4889527609, 684125.4889527604, 181981.4572746074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4901598713567438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684917.8322771245, 684917.8322771239, 182068.4723652152], 
processed observation next is [1.0, 0.8695652173913043, 0.5102685624012641, 0.7566666666666667, 1.0, 1.0, 0.3857347847671613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19025495341031237, 0.1902549534103122, 0.2717439886047988], 
reward next is 0.7283, 
noisyNet noise sample is [array([1.0942783], dtype=float32), -0.596525]. 
=============================================
[2019-03-27 04:34:22,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2071785e-30 1.0000000e+00 2.8310123e-37 1.9796405e-14 2.3846270e-35], sum to 1.0000
[2019-03-27 04:34:22,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2981
[2019-03-27 04:34:22,128] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.7939864652292251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1109687.335726188, 1109687.335726188, 242440.7364978558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4690200.0000, 
sim time next is 4690800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.8087786723191052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1130372.149146386, 1130372.149146385, 246077.820539602], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.84, 1.0, 1.0, 0.7696128582157894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31399226365177385, 0.31399226365177363, 0.36728032916358505], 
reward next is 0.6327, 
noisyNet noise sample is [array([2.0865276], dtype=float32), 0.22940922]. 
=============================================
[2019-03-27 04:34:24,025] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1456389e-17 1.9202575e-09 3.0775953e-24 1.0000000e+00 5.0658024e-08], sum to 1.0000
[2019-03-27 04:34:24,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2518
[2019-03-27 04:34:24,041] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 68.66666666666667, 1.0, 2.0, 0.7346597551858157, 1.0, 2.0, 0.7346597551858157, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2054437.183643052, 2054437.183643052, 389317.5649759572], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4706400.0000, 
sim time next is 4707000.0000, 
raw observation next is [31.0, 68.0, 1.0, 2.0, 0.7543377077185316, 1.0, 2.0, 0.7543377077185316, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2109519.720207228, 2109519.720207228, 398254.787333658], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.68, 1.0, 1.0, 0.7040213346006404, 1.0, 1.0, 0.7040213346006404, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5859777000575633, 0.5859777000575633, 0.5944101303487433], 
reward next is 0.4056, 
noisyNet noise sample is [array([2.1463904], dtype=float32), -0.10259595]. 
=============================================
[2019-03-27 04:34:24,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.909676]
 [72.214165]
 [70.57574 ]
 [69.32864 ]
 [68.31797 ]], R is [[72.18202972]
 [71.87914276]
 [71.59311676]
 [71.20350647]
 [70.79424286]].
[2019-03-27 04:34:28,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5545868e-11 3.1199548e-03 5.1431322e-16 9.9653637e-01 3.4366897e-04], sum to 1.0000
[2019-03-27 04:34:28,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6626
[2019-03-27 04:34:28,691] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.7736827498800005, 1.0, 2.0, 0.7736827498800005, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2163673.134930977, 2163673.134930976, 407259.2174945103], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4784400.0000, 
sim time next is 4785000.0000, 
raw observation next is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.7397950528312027, 1.0, 2.0, 0.7397950528312027, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2068811.638777533, 2068811.638777533, 391623.547766538], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235385, 0.6933333333333335, 1.0, 1.0, 0.6865000636520515, 1.0, 1.0, 0.6865000636520515, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5746698996604258, 0.5746698996604258, 0.5845127578605045], 
reward next is 0.4155, 
noisyNet noise sample is [array([0.38000914], dtype=float32), 0.97765476]. 
=============================================
[2019-03-27 04:34:28,705] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[57.894318]
 [57.173496]
 [56.83525 ]
 [56.540375]
 [56.52967 ]], R is [[57.6450386 ]
 [57.46073914]
 [57.2726326 ]
 [56.6999054 ]
 [56.13290787]].
[2019-03-27 04:34:31,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8037437e-27 0.0000000e+00], sum to 1.0000
[2019-03-27 04:34:31,258] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5918
[2019-03-27 04:34:31,266] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5406710671499779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755524.0242093195, 755524.0242093201, 190213.936129436], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5161800.0000, 
sim time next is 5162400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5409040698056533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755849.733772564, 755849.7337725633, 190253.2434629186], 
processed observation next is [0.0, 0.782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44687237325982326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20995825938126778, 0.20995825938126758, 0.28396006487002773], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.4028814], dtype=float32), -0.17268829]. 
=============================================
[2019-03-27 04:34:31,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5297814e-26 9.9999595e-01 2.2467154e-33 4.0759674e-06 1.3837128e-25], sum to 1.0000
[2019-03-27 04:34:31,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9709
[2019-03-27 04:34:31,954] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 66.5, 1.0, 2.0, 0.4919242275272838, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687384.0276035912, 687384.0276035905, 182342.9129856797], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4987800.0000, 
sim time next is 4988400.0000, 
raw observation next is [30.33333333333334, 67.66666666666667, 1.0, 2.0, 0.5005537596610574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699446.3638309042, 699446.3638309042, 183684.996130613], 
processed observation next is [1.0, 0.7391304347826086, 0.6366508688783573, 0.6766666666666667, 1.0, 1.0, 0.39825754176031014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19429065661969563, 0.19429065661969563, 0.27415671064270597], 
reward next is 0.7258, 
noisyNet noise sample is [array([-0.07636324], dtype=float32), -2.3151107]. 
=============================================
[2019-03-27 04:34:33,442] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.9928919e-35 1.0000000e+00 0.0000000e+00 6.6292986e-14 0.0000000e+00], sum to 1.0000
[2019-03-27 04:34:33,455] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6734
[2019-03-27 04:34:33,461] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4943562013292679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690783.4200185259, 690783.4200185259, 182716.3507877169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5001600.0000, 
sim time next is 5002200.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4976110439504541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695333.028532831, 695333.028532831, 183222.5383173265], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.815, 1.0, 1.0, 0.3947121011451254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19314806348134195, 0.19314806348134195, 0.2734664751004873], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.29048687], dtype=float32), 1.5524501]. 
=============================================
[2019-03-27 04:34:35,808] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0107291e-19 2.3895083e-12 2.9939854e-26 1.0000000e+00 2.4225269e-08], sum to 1.0000
[2019-03-27 04:34:35,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0875
[2019-03-27 04:34:35,824] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8923176418600224, 1.0, 2.0, 0.8923176418600224, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2495785.68457573, 2495785.684575729, 467320.6590540752], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5236800.0000, 
sim time next is 5237400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9285665939154538, 1.0, 2.0, 0.9285665939154538, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2597278.293453612, 2597278.293453612, 487318.3675302928], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9139356553198239, 1.0, 1.0, 0.9139356553198239, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7214661926260033, 0.7214661926260033, 0.7273408470601385], 
reward next is 0.2727, 
noisyNet noise sample is [array([0.7635851], dtype=float32), -1.3762639]. 
=============================================
[2019-03-27 04:34:39,301] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7479199e-16 1.9434246e-04 3.2813884e-23 9.9980563e-01 1.9110876e-12], sum to 1.0000
[2019-03-27 04:34:39,308] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8508
[2019-03-27 04:34:39,313] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 69.33333333333334, 1.0, 2.0, 0.6072177038399492, 1.0, 2.0, 0.6072177038399492, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1697769.862994947, 1697769.862994947, 336931.4344628109], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4961400.0000, 
sim time next is 4962000.0000, 
raw observation next is [30.0, 68.66666666666667, 1.0, 2.0, 0.7003105862744154, 1.0, 2.0, 0.7003105862744154, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1958293.755653212, 1958293.755653211, 374253.9947867014], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6866666666666668, 1.0, 1.0, 0.6389284171980909, 1.0, 1.0, 0.6389284171980909, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5439704876814478, 0.5439704876814475, 0.5585880519204498], 
reward next is 0.4414, 
noisyNet noise sample is [array([-2.3331063], dtype=float32), 0.5143816]. 
=============================================
[2019-03-27 04:34:39,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[60.3901  ]
 [60.92676 ]
 [60.461338]
 [59.79073 ]
 [59.52239 ]], R is [[60.70488739]
 [60.59495544]
 [60.46223068]
 [60.27700806]
 [60.10695267]].
[2019-03-27 04:34:43,380] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:34:43,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8312
[2019-03-27 04:34:43,398] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4996577848754543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698193.9663732055, 698193.966373205, 183542.4784896659], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5032200.0000, 
sim time next is 5032800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4997780815167864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698362.1174565919, 698362.1174565919, 183561.3093507194], 
processed observation next is [0.0, 0.2608695652173913, 0.4312796208530806, 0.89, 1.0, 1.0, 0.39732298977926067, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19398947707127553, 0.19398947707127553, 0.27397210350853646], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.7031015], dtype=float32), -0.4702216]. 
=============================================
[2019-03-27 04:34:47,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.280725e-15 0.000000e+00], sum to 1.0000
[2019-03-27 04:34:47,554] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1405
[2019-03-27 04:34:47,563] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.81666666666667, 78.83333333333334, 1.0, 2.0, 0.5452327300927857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761900.6960647892, 761900.6960647892, 190986.6117764753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5255400.0000, 
sim time next is 5256000.0000, 
raw observation next is [28.8, 79.0, 1.0, 2.0, 0.5473916124312921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764918.5746976368, 764918.5746976368, 191354.3720096959], 
processed observation next is [1.0, 0.8695652173913043, 0.5639810426540285, 0.79, 1.0, 1.0, 0.4546886896762555, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21247738186045464, 0.21247738186045464, 0.2856035403129789], 
reward next is 0.7144, 
noisyNet noise sample is [array([0.21533321], dtype=float32), -0.6223467]. 
=============================================
[2019-03-27 04:34:47,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.74571 ]
 [70.70686 ]
 [70.800995]
 [70.80882 ]
 [70.7466  ]], R is [[71.09669495]
 [71.10066986]
 [71.10535431]
 [71.10974884]
 [71.11374664]].
[2019-03-27 04:34:48,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5081195e-32 0.0000000e+00], sum to 1.0000
[2019-03-27 04:34:48,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1740
[2019-03-27 04:34:48,193] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.5363698225970476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749511.4219028318, 749511.4219028311, 189490.8007199865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5163600.0000, 
sim time next is 5164200.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5340302231083116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746240.9685164052, 746240.9685164046, 189099.758521216], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.43859063025097783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20728915792122365, 0.20728915792122352, 0.2822384455540537], 
reward next is 0.7178, 
noisyNet noise sample is [array([-2.1652844], dtype=float32), 0.9237233]. 
=============================================
[2019-03-27 04:34:52,406] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 04:34:52,407] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:34:52,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:34:52,408] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:34:52,409] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:34:52,410] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:34:52,410] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:34:52,411] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:34:52,413] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:34:52,417] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:34:52,416] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:34:52,431] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-03-27 04:34:52,454] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-03-27 04:34:52,475] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-03-27 04:34:52,476] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-03-27 04:34:52,526] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-03-27 04:34:54,098] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:34:54,101] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.57922270333333, 68.35977686666668, 1.0, 2.0, 0.7737010041801496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1125894.558560539, 1125894.558560539, 243657.8040389064]
[2019-03-27 04:34:54,102] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:34:54,104] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4598014e-37 0.0000000e+00], sampled 0.9708840619534582
[2019-03-27 04:34:56,224] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:34:56,227] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.15491865333333, 94.59186249666668, 1.0, 2.0, 0.3167092342229063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501304.627874682, 501304.627874682, 167132.7791760181]
[2019-03-27 04:34:56,227] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:34:56,230] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9013535915609264
[2019-03-27 04:35:09,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:35:09,836] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.2, 78.83333333333333, 1.0, 2.0, 0.6374081044945384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1002854.363744387, 1002854.363744386, 221190.4372191227]
[2019-03-27 04:35:09,836] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:35:09,839] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.62398978371484
[2019-03-27 04:35:25,629] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:35:25,630] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.7262191595118556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014929.476273502, 1014929.476273502, 226603.468039346]
[2019-03-27 04:35:25,633] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:35:25,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4871836588341899
[2019-03-27 04:35:51,423] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:35:51,427] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 79.0, 1.0, 2.0, 0.5654753550618963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790197.9826860642, 790197.9826860642, 194492.0020715584]
[2019-03-27 04:35:51,429] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:35:51,431] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1860189422662517
[2019-03-27 04:36:05,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:36:05,285] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.73333333333333, 57.83333333333334, 1.0, 2.0, 0.9655157272808053, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991528832851, 6.9112, 168.9122895805565, 2246735.963385384, 2179487.855879792, 452878.5066707667]
[2019-03-27 04:36:05,289] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:36:05,292] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0198235e-15 9.5258051e-01 1.4857223e-21 4.7419451e-02 1.1358539e-13], sampled 0.2040716368951092
[2019-03-27 04:36:05,293] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2246735.963385384 W.
[2019-03-27 04:36:37,273] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.08911932]
[2019-03-27 04:36:37,274] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.4, 81.0, 1.0, 2.0, 0.5857716199720548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818571.0097849716, 818571.0097849716, 198124.6373703475]
[2019-03-27 04:36:37,275] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:36:37,281] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6441189e-36 0.0000000e+00], sampled 0.2562960474196607
[2019-03-27 04:36:48,417] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.1489 2779040633.6933 931.0000
[2019-03-27 04:36:48,774] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.8523 2842148396.6909 1126.0000
[2019-03-27 04:36:48,857] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.4986 2927261860.7761 1338.0000
[2019-03-27 04:36:48,902] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7902.6067 3162292431.8151 1733.0000
[2019-03-27 04:36:48,907] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8006.6567 3006491613.6674 1745.0000
[2019-03-27 04:36:49,924] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2475000, evaluation results [2475000.0, 7902.606713158539, 3162292431.815095, 1733.0, 8255.49861199577, 2927261860.776133, 1338.0, 8662.148859204797, 2779040633.693279, 931.0, 8006.656742513362, 3006491613.6673894, 1745.0, 8500.852299532873, 2842148396.6908603, 1126.0]
[2019-03-27 04:36:55,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.717225e-29 0.000000e+00], sum to 1.0000
[2019-03-27 04:36:55,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0499
[2019-03-27 04:36:55,162] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 86.0, 1.0, 2.0, 0.6006835132853975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 839417.4738056152, 839417.4738056146, 200873.6446078068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5364000.0000, 
sim time next is 5364600.0000, 
raw observation next is [29.25, 86.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 11.41401588712133, 6.9112, 170.3810597889865, 4677914.128340551, 1455697.048234397, 301122.304110757], 
processed observation next is [1.0, 0.08695652173913043, 0.5853080568720379, 0.865, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.45028158871213303, 0.0, 0.8366490043505993, 1.2994205912057086, 0.4043602911762214, 0.44943627479217463], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6192454], dtype=float32), 0.16931683]. 
=============================================
[2019-03-27 04:36:56,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:36:56,400] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2275
[2019-03-27 04:36:56,404] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.98333333333333, 73.33333333333333, 1.0, 2.0, 0.5481823870337493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766023.9922396305, 766023.9922396305, 191490.0313709882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5597400.0000, 
sim time next is 5598000.0000, 
raw observation next is [29.7, 75.0, 1.0, 2.0, 0.546834199335838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764139.3720458178, 764139.3720458171, 191259.9212479961], 
processed observation next is [1.0, 0.8260869565217391, 0.6066350710900474, 0.75, 1.0, 1.0, 0.4540171076335397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21226093667939383, 0.21226093667939364, 0.28546256902685985], 
reward next is 0.7145, 
noisyNet noise sample is [array([-0.3306781], dtype=float32), -1.0549945]. 
=============================================
[2019-03-27 04:36:56,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[66.30286 ]
 [66.14424 ]
 [65.66233 ]
 [64.98962 ]
 [64.540695]], R is [[66.36949921]
 [66.41999817]
 [66.47012329]
 [66.52095032]
 [66.5729599 ]].
[2019-03-27 04:37:00,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0014241e-07 6.1517362e-02 2.4008658e-11 9.3817896e-01 3.0345612e-04], sum to 1.0000
[2019-03-27 04:37:00,490] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2952
[2019-03-27 04:37:00,497] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.63333333333333, 54.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.378510426188856, 6.9112, 170.5573041426782, 3244473.131651845, 2909719.664399988, 551151.0039015351], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5409600.0000, 
sim time next is 5410200.0000, 
raw observation next is [37.81666666666666, 54.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 8.283223930384679, 6.9112, 170.5573041426782, 3893311.349000805, 2910474.797301064, 545354.1206825691], 
processed observation next is [1.0, 0.6086956521739131, 0.9913112164296997, 0.54, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.1372023930384679, 0.0, 0.8375144448122397, 1.081475374722446, 0.8084652214725178, 0.8139613741530882], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6991129], dtype=float32), 0.46589476]. 
=============================================
[2019-03-27 04:37:04,618] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:37:04,626] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5354
[2019-03-27 04:37:04,633] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.2, 60.0, 1.0, 2.0, 0.5372583710751685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750753.4988334499, 750753.4988334499, 189640.0579212312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5671800.0000, 
sim time next is 5672400.0000, 
raw observation next is [32.16666666666666, 60.0, 1.0, 2.0, 0.5366002143213054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749833.4798758344, 749833.479875835, 189529.6988751857], 
processed observation next is [0.0, 0.6521739130434783, 0.7235387045813582, 0.6, 1.0, 1.0, 0.4416870052063921, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20828707774328734, 0.2082870777432875, 0.28288014757490404], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.8737185], dtype=float32), 0.346574]. 
=============================================
[2019-03-27 04:37:05,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5125878e-29 0.0000000e+00], sum to 1.0000
[2019-03-27 04:37:05,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6474
[2019-03-27 04:37:05,827] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 92.0, 1.0, 2.0, 1.013365492973042, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129416416607, 1416499.273721443, 1416499.273721444, 303028.315983406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5454000.0000, 
sim time next is 5454600.0000, 
raw observation next is [27.86666666666667, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.936059197135925, 6.9112, 168.9126397886343, 1471402.942443774, 1453767.005476502, 311353.9102700952], 
processed observation next is [1.0, 0.13043478260869565, 0.519747235387046, 0.92, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.002485919713592466, 0.0, 0.829438389903285, 0.408723039567715, 0.4038241681879172, 0.46470732876133614], 
reward next is 0.4110, 
noisyNet noise sample is [array([-1.0233701], dtype=float32), -0.25425333]. 
=============================================
[2019-03-27 04:37:06,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8083813e-37 1.0000000e+00 0.0000000e+00 2.8876818e-25 0.0000000e+00], sum to 1.0000
[2019-03-27 04:37:06,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1639
[2019-03-27 04:37:06,877] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 86.33333333333334, 1.0, 2.0, 0.8800687012622707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1230066.84180666, 1230066.841806659, 264513.9030617427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5811600.0000, 
sim time next is 5812200.0000, 
raw observation next is [27.55, 85.5, 1.0, 2.0, 0.956055345539492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1336339.767816107, 1336339.767816107, 285822.5644311356], 
processed observation next is [1.0, 0.2608695652173913, 0.504739336492891, 0.855, 1.0, 1.0, 0.9470546331801108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3712054910600297, 0.3712054910600297, 0.4266008424345307], 
reward next is 0.5734, 
noisyNet noise sample is [array([-1.1927254], dtype=float32), 0.06516122]. 
=============================================
[2019-03-27 04:37:08,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:37:08,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9879
[2019-03-27 04:37:08,542] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.06666666666666, 63.83333333333334, 1.0, 2.0, 0.5572636009613726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778718.6315937896, 778718.6315937903, 193054.878127043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5763000.0000, 
sim time next is 5763600.0000, 
raw observation next is [31.9, 65.0, 1.0, 2.0, 0.5598090003584952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782276.8755501541, 782276.8755501541, 193497.828176362], 
processed observation next is [0.0, 0.7391304347826086, 0.7109004739336492, 0.65, 1.0, 1.0, 0.4696493980222834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21729913209726504, 0.21729913209726504, 0.2888027286214358], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.79802], dtype=float32), -0.44747898]. 
=============================================
[2019-03-27 04:37:19,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1994149e-23 9.9309641e-01 4.3667433e-31 6.9035566e-03 1.7934498e-22], sum to 1.0000
[2019-03-27 04:37:19,537] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7068
[2019-03-27 04:37:19,541] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 67.0, 1.0, 2.0, 0.3549035067469336, 1.0, 2.0, 0.3549035067469336, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 991977.4595684069, 991977.4595684069, 261367.2069359531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6023400.0000, 
sim time next is 6024000.0000, 
raw observation next is [30.66666666666667, 68.0, 1.0, 2.0, 0.5001938378069627, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698943.2633181883, 698943.2633181883, 183629.588614927], 
processed observation next is [1.0, 0.7391304347826086, 0.6524486571879939, 0.68, 1.0, 1.0, 0.3978239009722442, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1941509064772745, 0.1941509064772745, 0.2740740128581], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.4239712], dtype=float32), -1.0540957]. 
=============================================
[2019-03-27 04:37:19,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.01896 ]
 [65.029076]
 [66.155075]
 [66.66217 ]
 [66.768425]], R is [[68.4511261 ]
 [68.37651825]
 [68.09007263]
 [67.76344299]
 [67.42848969]].
[2019-03-27 04:37:26,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2920603e-13 4.4940185e-07 8.5636135e-21 9.9797982e-01 2.0197781e-03], sum to 1.0000
[2019-03-27 04:37:26,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7241
[2019-03-27 04:37:26,486] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.55, 67.33333333333334, 1.0, 2.0, 0.8405236319407589, 1.0, 2.0, 0.8405236319407589, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2350783.128013104, 2350783.128013104, 440077.4112448896], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6106200.0000, 
sim time next is 6106800.0000, 
raw observation next is [30.5, 67.66666666666667, 1.0, 2.0, 0.8436808580976434, 1.0, 2.0, 0.8436808580976434, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2359621.619639097, 2359621.619639097, 441692.5752810849], 
processed observation next is [1.0, 0.6956521739130435, 0.6445497630331753, 0.6766666666666667, 1.0, 1.0, 0.811663684454992, 1.0, 1.0, 0.811663684454992, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6554504498997491, 0.6554504498997491, 0.6592426496732611], 
reward next is 0.3408, 
noisyNet noise sample is [array([0.8906774], dtype=float32), 0.14791347]. 
=============================================
[2019-03-27 04:37:29,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6676844e-11 3.3376093e-08 1.5315021e-17 9.7975290e-01 2.0247092e-02], sum to 1.0000
[2019-03-27 04:37:29,924] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1301
[2019-03-27 04:37:29,929] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.13333333333333, 85.0, 1.0, 2.0, 0.7616316549085643, 1.0, 2.0, 0.7616316549085643, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2129937.661096368, 2129937.661096369, 401625.6697651451], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6165600.0000, 
sim time next is 6166200.0000, 
raw observation next is [28.21666666666667, 84.5, 1.0, 2.0, 0.7323074517542776, 1.0, 2.0, 0.7323074517542776, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2047852.803981987, 2047852.803981987, 388263.5741339164], 
processed observation next is [1.0, 0.34782608695652173, 0.5363349131121644, 0.845, 1.0, 1.0, 0.6774788575352741, 1.0, 1.0, 0.6774788575352741, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5688480011061076, 0.5688480011061076, 0.5794978718416662], 
reward next is 0.4205, 
noisyNet noise sample is [array([-0.14957225], dtype=float32), -0.92589146]. 
=============================================
[2019-03-27 04:37:32,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:37:32,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3731
[2019-03-27 04:37:32,419] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 89.5, 1.0, 2.0, 0.5272986350211235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736831.1469815219, 736831.1469815212, 187984.0942567497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6240600.0000, 
sim time next is 6241200.0000, 
raw observation next is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.52786251708802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737619.3722879834, 737619.3722879834, 188077.1312553247], 
processed observation next is [0.0, 0.21739130434782608, 0.4723538704581361, 0.8933333333333333, 1.0, 1.0, 0.43115965914219273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2048942700799954, 0.2048942700799954, 0.28071213620197716], 
reward next is 0.7193, 
noisyNet noise sample is [array([-0.5840704], dtype=float32), 0.5587501]. 
=============================================
[2019-03-27 04:37:33,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6553568e-35 0.0000000e+00], sum to 1.0000
[2019-03-27 04:37:33,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7194
[2019-03-27 04:37:33,477] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.33333333333334, 1.0, 2.0, 0.5419008612283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757243.1288852462, 757243.1288852456, 190422.0233671023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6258000.0000, 
sim time next is 6258600.0000, 
raw observation next is [30.15, 71.5, 1.0, 2.0, 0.5420248353274145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757416.4299662584, 757416.4299662578, 190442.9864727354], 
processed observation next is [0.0, 0.43478260869565216, 0.6279620853080569, 0.715, 1.0, 1.0, 0.4482226931655596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2103934527684051, 0.21039345276840493, 0.2842432633921424], 
reward next is 0.7158, 
noisyNet noise sample is [array([1.1816996], dtype=float32), -2.3916218]. 
=============================================
[2019-03-27 04:37:34,791] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4273459e-22 9.9807978e-01 3.4109334e-31 1.9202828e-03 6.9773840e-21], sum to 1.0000
[2019-03-27 04:37:34,801] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9752
[2019-03-27 04:37:34,806] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.2, 79.33333333333334, 1.0, 2.0, 0.279698739356506, 1.0, 2.0, 0.279698739356506, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 781698.8306203239, 781698.8306203239, 246087.0425430723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5937600.0000, 
sim time next is 5938200.0000, 
raw observation next is [30.1, 79.5, 1.0, 2.0, 0.5533050294444863, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773184.9212816999, 773184.9212817005, 192372.3205533544], 
processed observation next is [1.0, 0.7391304347826086, 0.6255924170616115, 0.795, 1.0, 1.0, 0.46181328848733294, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21477358924491663, 0.2147735892449168, 0.28712286649754387], 
reward next is 0.7129, 
noisyNet noise sample is [array([-1.043707], dtype=float32), 0.5577571]. 
=============================================
[2019-03-27 04:37:45,506] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 04:37:45,507] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:37:45,510] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:37:45,512] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:37:45,513] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:37:45,514] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:37:45,515] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:37:45,516] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:37:45,517] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:37:45,518] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:37:45,521] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:37:46,288] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-03-27 04:37:46,366] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-03-27 04:37:46,410] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-03-27 04:37:46,456] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-03-27 04:37:46,467] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-03-27 04:37:52,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:37:52,543] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.1, 58.16666666666667, 1.0, 2.0, 0.2151551000829839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 358799.3469391321, 358799.3469391321, 157054.330433517]
[2019-03-27 04:37:52,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:37:52,547] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6253893891598442
[2019-03-27 04:38:31,761] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:38:31,764] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.6, 73.0, 1.0, 2.0, 0.5763327248813804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805375.8755648817, 805375.8755648811, 196419.3674930695]
[2019-03-27 04:38:31,765] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:38:31,767] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.345468e-26 0.000000e+00], sampled 0.4516248784120982
[2019-03-27 04:38:32,852] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:38:32,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.16666666666666, 56.33333333333334, 1.0, 2.0, 0.5942076185799596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830364.2803165006, 830364.2803165013, 199670.806556981]
[2019-03-27 04:38:32,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:38:32,858] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4173063e-24 0.0000000e+00], sampled 0.11538723461298173
[2019-03-27 04:38:33,081] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:38:33,081] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.245055535, 85.69289754333334, 1.0, 2.0, 0.4181219367724976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617693.3729050629, 617693.3729050636, 176022.7778683704]
[2019-03-27 04:38:33,081] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:38:33,083] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4877242e-37 0.0000000e+00], sampled 0.9457772080278661
[2019-03-27 04:38:42,113] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:38:42,115] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 79.0, 1.0, 2.0, 0.5502098921638179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768858.2308287701, 768858.2308287694, 191837.0223467204]
[2019-03-27 04:38:42,115] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:38:42,118] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 5.55726e-26 0.00000e+00], sampled 0.7197271939760079
[2019-03-27 04:39:08,596] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:39:08,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.80959319166667, 65.27655268333334, 1.0, 2.0, 0.485972739568981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679065.1339470189, 679065.1339470189, 181428.1885402952]
[2019-03-27 04:39:08,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:39:08,600] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7087036e-25 0.0000000e+00], sampled 0.7158165571129874
[2019-03-27 04:39:24,398] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:39:24,399] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.63333333333334, 95.0, 1.0, 2.0, 0.549366217819777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767678.8620519555, 767678.8620519561, 191687.3618304007]
[2019-03-27 04:39:24,400] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:39:24,405] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.369355e-37 0.000000e+00], sampled 0.04975141739207012
[2019-03-27 04:39:30,455] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:39:30,455] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.33333333333333, 88.33333333333334, 1.0, 2.0, 0.4744718242976024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664514.5440947058, 664514.5440947058, 179889.8016697373]
[2019-03-27 04:39:30,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:39:30,462] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0881691e-28 0.0000000e+00], sampled 0.5170486703082667
[2019-03-27 04:39:36,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-03-27 04:39:36,128] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.98097988166667, 72.13735340833333, 1.0, 2.0, 0.2638002731581928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 435413.9239511364, 435413.9239511364, 162256.4829378719]
[2019-03-27 04:39:36,129] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:39:36,132] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1036302e-36 0.0000000e+00], sampled 0.6515878821169745
[2019-03-27 04:39:41,970] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8350.7081 2972053298.7636 857.0000
[2019-03-27 04:39:42,931] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8194.7651 3131016883.7917 919.0000
[2019-03-27 04:39:43,175] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8675.5550 2824082714.3912 654.0000
[2019-03-27 04:39:43,220] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8394.5942 2914727085.1635 1009.0000
[2019-03-27 04:39:43,252] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8748.1876 2770577976.5921 712.0000
[2019-03-27 04:39:44,269] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2500000, evaluation results [2500000.0, 8194.765080083645, 3131016883.79174, 919.0, 8394.594230008866, 2914727085.163511, 1009.0, 8748.187558025566, 2770577976.592085, 712.0, 8350.708101556995, 2972053298.7635956, 857.0, 8675.555040771205, 2824082714.391172, 654.0]
