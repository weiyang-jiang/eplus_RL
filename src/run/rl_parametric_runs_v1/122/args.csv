reward_func,output,save_scope,action_repeat_n,check_args_only,rmsprop_epsil,rwd_p_para,env,weight_initer,job_mode,is_greedy_policy,num_threads,rwd_e_para,is_warm_start,eval_epi_num,dropout_prob,window_len,sharedNet_type,h_decay_bounds,rmsprop_decay,init_e,is_learning_rate_decay_staircase,v_loss_frac,agent_num,save_freq,test_env,action_space,metric_func,rmsprop_momet,isNoisyNetEval_rmNoise,eval_freq,debug_log_prob,learning_rate_decay_rate,model_dir,state_dim,model_param,clip_norm,test_mode,learning_rate_decay_steps,gamma,max_interactions,activation,learning_rate,forecast_dim,train_act_func,isNoisyNet,train_freq,decay_steps,model_type,violation_penalty_scl,save_max_to_keep,p_loss_frac,end_e,eval_act_func,raw_state_prcs_func,h_regu_frac
cslDxCool_1,a3c-res-v0.1,all,1,True,1e-10,1.0,Model1-Cool-v1,glorot_uniform,Train,False,16,1.0,False,1,0.0,6,Dense,[],0.99,0.0,False,0.5,5,500000,Model1-Test-Cool-v1,cslDxCool_1,cslDxCool_1,0.0,False,250000,0.0005,0.9,None,49,"[256, 4, 256, 4]",5.0,Multiple,100000,0.99,10000000,tanh/linear,0.001,0,cslDxActCool_1,False,5,1000000,lstm,10.0,5,1.0,0.0,cslDxActCool_1,cslDx_1,[0.1]
